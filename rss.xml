<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
  <channel>
    <title>JavaChen Blog</title>
    <description>Ramblings of a coder</description>
    <link>http://blog.javachen.com/rss.xml</link>
    <lastBuildDate>2015-10-26T21:45:09+08:00</lastBuildDate>
    <pubDate>2015-10-26T21:45:09+08:00</pubDate>
    <ttl>1800</ttl>
    
    <item>
      <title>Bash内部变量</title>
      <description>&lt;p&gt;Bash中存在一些内部变量。&lt;/p&gt;

&lt;h2 id=&quot;bash&quot;&gt;&lt;code&gt;$BASH&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Bash的二进制程序文件的路径。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo $BASH
/bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;bashenv&quot;&gt;&lt;code&gt;$BASH_ENV&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这个环境变量会指向一个Bash的启动文件，当一个脚本被调用的时候，这个启动文件将会被读取。&lt;/p&gt;

&lt;h2 id=&quot;bashsubshell&quot;&gt;&lt;code&gt;$BASH_SUBSHELL&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这个变量用来提示子shell的层次。这是一个Bash的新特性，直到版本3的Bash才被引入近来。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# subshell.sh

echo &quot;Subshell level OUTSIDE subshell = $BASH_SUBSHELL&quot;
outer_variable=Outer

(
echo &quot;Subshell level INSIDE subshell = $BASH_SUBSHELL&quot;
inner_variable=Inner
 
echo &quot;From subshell, \&quot;inner_variable\&quot; = $inner_variable&quot;
echo &quot;From subshell, \&quot;outer\&quot; = $outer_variable&quot;
)

echo &quot;Subshell level OUTSIDE subshell = $BASH_SUBSHELL&quot;

if [ -z &quot;$inner_variable&quot; ]
then
    echo &quot;inner_variable undefined in main body of shell&quot;
else
    echo &quot;inner_variable defined in main body of shell&quot;
fi

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;bashversinfon&quot;&gt;&lt;code&gt;$BASH_VERSINFO[n]&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这是一个含有6个元素的数组，它包含了所安装的Bash的版本信息。这与下边的$BASH_VERSION很相像，但是这个更加详细一些。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for n in 0 1 2 3 4 5
do
    echo &quot;BASH_VERSINFO[$n] = ${BASH_VERSINFO[$n]}&quot;
done 

# BASH_VERSINFO[0] = 3                      # 主版本号.
# BASH_VERSINFO[1] = 2                     # 次版本号.
# BASH_VERSINFO[2] = 25                    # 补丁次数.
# BASH_VERSINFO[3] = 1                      # 编译版本.
# BASH_VERSINFO[4] = release                # 发行状态.
# BASH_VERSINFO[5] = x86_64-redhat-linux-gnu  # 结构体系
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;bashversion&quot;&gt;&lt;code&gt;$BASH_VERSION&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;安装在系统上的Bash版本号。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ echo $BASH_VERSION
3.2.25(1)-release

tcsh% echo $BASH_VERSION
BASH_VERSION: Undefined variable.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查$BASH_VERSION对于判断系统上到底运行的是哪个shell来说是一种非常好的方法。变量$SHELL有时候不能够给出正确的答案。&lt;/p&gt;

&lt;h2 id=&quot;dirstack&quot;&gt;&lt;code&gt;$DIRSTACK&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;在目录栈中最顶端的值(将会受到pushd和popd的影响)。这个内建变量与dirs命令相符，但是dirs命令会显示目录栈的整个内容。&lt;/p&gt;

&lt;h2 id=&quot;editor&quot;&gt;&lt;code&gt;$EDITOR&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;脚本所调用的默认编辑器，通常情况下是vi或者是emacs。&lt;/p&gt;

&lt;h2 id=&quot;euid&quot;&gt;&lt;code&gt;$EUID&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;有效用户ID。不管当前用户被假定成什么用户，这个数都用来表示当前用户的标识号，也可能使用su命令来达到假定的目的。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$EUID并不一定与$UID相同。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;funcname&quot;&gt;&lt;code&gt;$FUNCNAME&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;当前函数的名字。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;xyz23 (){
    echo &quot;$FUNCNAME now executing.&quot;  # 打印: xyz23 now executing.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;globignore&quot;&gt;&lt;code&gt;$GLOBIGNORE&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;一个文件名的模式匹配列表，如果在通配中匹配到的文件包含有这个列表中的某个文件，那么这个文件将被从匹配到的结果中去掉。&lt;/p&gt;

&lt;h2 id=&quot;groups&quot;&gt;&lt;code&gt;$GROUPS&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;目前用户所属的组。这是一个当前用户的组id列表(数组)，与记录在/etc/passwd文件中的内容一样。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;root# echo $GROUPS
0

root# echo ${GROUPS[1]}
1

root# echo ${GROUPS[5]}
6
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;home&quot;&gt;&lt;code&gt;$HOME&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;用户的home目录。&lt;/p&gt;

&lt;h2 id=&quot;hostname&quot;&gt;&lt;code&gt;$HOSTNAME&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;hostname放在一个初始化脚本中，在系统启动的时候分配一个系统名字。然而，gethostname()函数可以用来设置这个Bash内部变量$HOSTNAME。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hostname
localhost

$ cat /etc/sysconfig/network
NETWORKING=yes
NETWORKING_IPV6=no
HOSTNAME=localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hosttype&quot;&gt;&lt;code&gt;$HOSTTYPE&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;主机类型，就像&lt;code&gt;$MACHTYPE&lt;/code&gt;，用来识别系统硬件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ echo $HOSTTYPE
x86_64
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ifs&quot;&gt;&lt;code&gt;$IFS&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;内部域分隔符，这个变量用来决定Bash在解释字符串时如何识别域或者单词边界。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$IFS&lt;/code&gt;默认为空白(空格、制表符和换行符)，但这是可以修改的，比如，在分析逗号分隔的数据文件时，就可以设置为逗号。注意，&lt;code&gt;$*&lt;/code&gt;使用的是保存在&lt;code&gt;$IFS&lt;/code&gt;中的第一个字符。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ echo $IFS | cat -vte
$

bash$ bash -c &#39;set w x y z;echo &quot;$*&quot;&#39;
w x y z

bash$ bash -c &#39;set w x y z; IFS=&quot;:-;&quot;; echo &quot;$*&quot;&#39;
w:x:y:z
#从字符串中读取命令，并分配参数给位置参数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$IFS处理其他字符与处理空白字符不同。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;output_args_one_per_line(){
    for arg
        do echo &quot;[$arg]&quot;
    done
}

IFS=&quot; &quot;

#空白做分隔符
var=&quot; a  b c   &quot;
output_args_one_per_line $var 
#[a]
#[b]
#[c]


IFS=:

#:做分隔符
var=&quot;:a::b:c:::&quot; 
output_args_one_per_line $var
#[]
#[a]
#[]
#[b]
#[c]
#[]
#[]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面，同样的事情也会发生在awk的”FS”域中。&lt;/p&gt;

&lt;h2 id=&quot;ignoreeof&quot;&gt;&lt;code&gt;$IGNOREEOF&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;忽略&lt;code&gt;EOF&lt;/code&gt;：告诉shell在log out之前要忽略多少文件结束符(control-D)。&lt;/p&gt;

&lt;h2 id=&quot;lccollate&quot;&gt;&lt;code&gt;$LC_COLLATE&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;常在&lt;code&gt;.bashrc&lt;/code&gt;或&lt;code&gt;/etc/profile&lt;/code&gt;中设置，这个变量用来控制文件名扩展和模式匹配的展开顺序。如果$LC_COLLATE设置得不正确的话，LC_COLLATE会在文件名匹配中产生不可预料的结果。&lt;/p&gt;

&lt;h2 id=&quot;lcctype&quot;&gt;&lt;code&gt;$LC_CTYPE&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这个内部变量用来控制通配和模式匹配中的字符串解释。&lt;/p&gt;

&lt;h2 id=&quot;lineno&quot;&gt;&lt;code&gt;$LINENO&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这个变量用来记录自身在脚本中所在的行号。这个变量只有在脚本使用这个变量的时候才有意义，并且这个变量一般用于调试目的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# *** 调试代码块开始 ***
last_cmd_arg=$_  # Save it.

echo &quot;At line number $LINENO, variable \&quot;v1\&quot; = $v1&quot;
echo &quot;Last command argument processed = $last_cmd_arg&quot;
# *** 调试代码块结束 ***
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;machtype&quot;&gt;&lt;code&gt;$MACHTYPE&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;机器类型，标识系统的硬件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ echo $MACHTYPE
x86_64-redhat-linux-gnu
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;oldpwd&quot;&gt;&lt;code&gt;$OLDPWD&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;之前的工作目录。&lt;/p&gt;

&lt;h2 id=&quot;ostype&quot;&gt;&lt;code&gt;$OSTYPE&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;操作系统类型。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ echo $OSTYPE
linux
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;path&quot;&gt;&lt;code&gt;$PATH&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;可执行文件的搜索路径，一般为/usr/bin/, /usr/X11R6/bin/, /usr/local/bin等等。&lt;/p&gt;

&lt;p&gt;当给出一个命令时，shell会自动生成一张哈希表，并且在这张哈希表中按照path变量中所列出的路径来搜索这个可执行命令。路径会存储在环境变量中，$PATH变量本身就一个以冒号分隔的目录列表。通常情况下，系统都是在&lt;code&gt;/etc/profile&lt;/code&gt;和&lt;code&gt;~/.bashrc&lt;/code&gt;中存储$PATH的定义。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo $PATH
/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin://usr/java/latest/bin:/root/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;当前的工作目录，通常是不会出现在$PATH中的，这样做的目的是出于安全的考虑。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;pipestatus&quot;&gt;&lt;code&gt;$PIPESTATUS&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这个数组变量将保存最后一个运行的前台管道的退出状态码。相当有趣的是，这个退出状态码和最后一个命令运行的退出状态码并不一定相同。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ echo $PIPESTATUS
0

bash$ ls -al | bogus_command
-bash: bogus_command: command not found

bash$ echo $PIPESTATUS
141

bash$ ls -al | bogus_command
-bash: bogus_command: command not found

bash$ echo $?
127
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;$PIPESTATUS&lt;/code&gt;数组的每个成员都保存了运行在管道中的相应命令的退出状态码。&lt;code&gt;$PIPESTATUS[0]&lt;/code&gt;保存管道中第一个命令的退出状态码，&lt;code&gt;$PIPESTATUS[1]&lt;/code&gt;保存第二个命令的退出状态码，依此类推。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ who | grep nobody | sort
bash$ echo ${PIPESTATUS[*]}
0 1 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在某些上下文中，变量&lt;code&gt;$PIPESTATUS&lt;/code&gt;可能不会给出期望的结果。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ $ ls | bogus_command | wc
bash: bogus_command: command not found
 0       0       0

bash$ echo ${PIPESTATUS[@]}
141 127 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上边输出不正确的原因归咎于ls的行为。因为如果把ls的结果放到管道上，并且这个输出并没有被读取，那么SIGPIPE将会杀掉它，同时退出状态码变为141，而不是我们所期望的0。这种情况也会发生在tr命令中。&lt;/p&gt;

&lt;h2 id=&quot;ppid&quot;&gt;&lt;code&gt;$PPID&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;进程的&lt;code&gt;$PPID&lt;/code&gt;就是这个进程的父进程的进程ID。&lt;/p&gt;

&lt;h2 id=&quot;promptcommand&quot;&gt;&lt;code&gt;$PROMPT_COMMAND&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这个变量保存了在主提示符&lt;code&gt;$PS1&lt;/code&gt;显示之前需要执行的命令。&lt;/p&gt;

&lt;h2 id=&quot;ps1&quot;&gt;&lt;code&gt;$PS1&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这是主提示符，可以在命令行中见到它。&lt;/p&gt;

&lt;h2 id=&quot;ps2&quot;&gt;&lt;code&gt;$PS2&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;第二提示符，当你需要额外输入的时候，你就会看到它，默认显示&lt;code&gt;&amp;gt;&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;ps3&quot;&gt;&lt;code&gt;$PS3&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;第三提示符，它在一个&lt;code&gt;select&lt;/code&gt;循环中显示。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

PS3=&#39;Choose your favorite vegetable: &#39; # 设置提示符字串.

select vegetable in &quot;beans&quot; &quot;carrots&quot; &quot;potatoes&quot; &quot;onions&quot; &quot;rutabagas&quot;

do
    echo &quot;Your favorite veggie is $vegetable.&quot;
    echo &quot;Yuck!&quot;
    break  # 如果这里没有 &#39;break&#39; 会发生什么?
done

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ps4&quot;&gt;&lt;code&gt;$PS4&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;第四提示符，当你使用&lt;code&gt;-x&lt;/code&gt;选项来调用脚本时，这个提示符会出现在每行输出的开头。默认显示&lt;code&gt;+&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;pwd&quot;&gt;&lt;code&gt;$PWD&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;工作目录，你当前所在的目录。&lt;/p&gt;

&lt;h2 id=&quot;reply&quot;&gt;&lt;code&gt;$REPLY&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;当没有参数变量提供给&lt;code&gt;read&lt;/code&gt;命令的时候，这个变量会作为默认变量提供给&lt;code&gt;read&lt;/code&gt;命令，也可以用于&lt;code&gt;select&lt;/code&gt;菜单，但是只提供所选择变量的编号，而不是变量本身的值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# reply.sh

# REPLY是提供给&#39;read&#39;命令的默认变量.

echo -n &quot;What is your favorite vegetable? &quot;
read

echo &quot;Your favorite vegetable is $REPLY.&quot;
#  当且仅当没有变量提供给&quot;read&quot;命令时，REPLY才保存最后一个&quot;read&quot;命令读入的值

echo -n &quot;What is your favorite fruit? &quot;
read fruit
echo &quot;Your favorite fruit is $fruit.&quot;
echo &quot;but...&quot;
echo &quot;Value of \$REPLY is still $REPLY.&quot;
#  $REPLY还是保存着上一个read命令的值，因为变量$fruit被传入到了这个新的&quot;read&quot;命令中

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;seconds&quot;&gt;&lt;code&gt;$SECONDS&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这个脚本已经运行的时间(以秒为单位)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

TIME_LIMIT=10
INTERVAL=1

echo &quot;Hit Control-C to exit before $TIME_LIMIT seconds.&quot;

while [ &quot;$SECONDS&quot; -le &quot;$TIME_LIMIT&quot; ]
do
    if [ &quot;$SECONDS&quot; -eq 1 ]
    then
        units=second
    else  
        units=seconds
    fi

    echo &quot;This script has been running $SECONDS $units.&quot;
    #  在一台比较慢或者是附载过大的机器上, 
    #+ 在单次循环中, 脚本可能会忽略计数. 
    sleep $INTERVAL
done

echo -e &quot;\a&quot;  # Beep!(哔哔声!)

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;shellopts&quot;&gt;&lt;code&gt;$SHELLOPTS&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;shell中已经激活的选项的列表，这是一个只读变量。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ echo $SHELLOPTS
braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;shlvl&quot;&gt;&lt;code&gt;$SHLVL&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Shell级别，就是Bash被嵌套的深度。如果是在命令行中，那么$SHLVL为1，如果在脚本中那么$SHLVL为2。&lt;/p&gt;

&lt;h2 id=&quot;tmout&quot;&gt;&lt;code&gt;$TMOUT&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;如果$TMOUT环境变量被设置为非零值time的话，那么经过time秒后，shell提示符将会超时，这将会导致登出(logout)。&lt;/p&gt;

&lt;p&gt;在2.05b版本的Bash中, $TMOUT变量与命令read可以在脚本中结合使用.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 只能够在Bash脚本中使用, 必须使用2.05b或之后版本的Bash.

TMOUT=3    # 提示输入时间为3秒.

echo &quot;What is your favorite song?&quot;
echo &quot;Quickly now, you only have $TMOUT seconds to answer!&quot;

read song

if [ -z &quot;$song&quot; ]
then
    song=&quot;(no answer)&quot;
    # 默认响应.
fi
 
echo &quot;Your favorite song is $song.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有更加复杂的办法可以在脚本中实现定时输入。一种办法就是建立一个定式循环，当超时的时候给脚本发个信号。不过这也需要有一个信号处理例程能够捕捉由定时循环所产生的中断。&lt;/p&gt;

&lt;p&gt;定时输入的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# timed-input.sh

TIMELIMIT=3  # 这个例子中设置的是3秒. 也可以设置为其他的时间值.

PrintAnswer(){
    if [ &quot;$answer&quot; = TIMEOUT ]
    then
        echo $answer
    else       # 别和上边的例子弄混了.
        echo &quot;Your favorite veggie is $answer&quot;
        kill $!  # 不再需要后台运行的TimerOn函数了, kill了吧.
              # $! 变量是上一个在后台运行的作业的PID.
    fi
}  

TimerOn()
{
    sleep $TIMELIMIT &amp;amp;&amp;amp; kill -s 14 $$ &amp;amp;
     # 等待3秒, 然后给脚本发送一个信号.
}  

Int14Vector()
{
    answer=&quot;TIMEOUT&quot;
    PrintAnswer
    exit 14
}  

trap Int14Vector 14   # 定时中断(14)会暗中给定时间限制. 

echo &quot;What is your favorite vegetable &quot;
TimerOn
read answer
PrintAnswer

#  无可否认, 这是一个定时输入的复杂实现,
 #+ 然而&quot;read&quot;命令的&quot;-t&quot;选项可以简化这个任务. 
#  参考后边的&quot;t-out.sh&quot;.

#  如果你需要一个真正优雅的写法...
#+ 建议你使用C或C++来重写这个应用,
 #+ 你可以使用合适的函数库, 比如&#39;alarm&#39;和&#39;setitimer&#39;来完成这个任务.

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另一种选择是使用&lt;code&gt;stty&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;可能最简单的办法就是使用&lt;code&gt;-t&lt;/code&gt;选项来&lt;code&gt;read&lt;/code&gt;了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# t-out.sh

TIMELIMIT=4         # 4秒

read -t $TIMELIMIT variable &amp;lt;&amp;amp;1
#                           ^^^
#  在这个例子中，对于Bash 1.x和2.x就需要&quot;&amp;lt;&amp;amp;1&quot;了，但是Bash 3.x就不需要.

if [ -z &quot;$variable&quot; ]  # 值为null?
then
    echo &quot;Timed out, variable still unset.&quot;
else  
    echo &quot;variable = $variable&quot;
fi  

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;uid&quot;&gt;&lt;code&gt;$UID&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;用户ID号，当前用户的用户标识号，记录在/etc/passwd文件中。&lt;/p&gt;

&lt;p&gt;这是当前用户的真实id，即使只是通过使用su命令来临时改变为另一个用户标识，这个id也不会被改变。$UID是一个只读变量，不能在命令行或者脚本中修改它，并且和id内建命令很相像。&lt;/p&gt;

&lt;p&gt;变量&lt;code&gt;$ENV&lt;/code&gt;、&lt;code&gt;$LOGNAME&lt;/code&gt;、&lt;code&gt;$MAIL&lt;/code&gt;、&lt;code&gt;$TERM&lt;/code&gt;、&lt;code&gt;$USER&lt;/code&gt;和&lt;code&gt;$USERNAME&lt;/code&gt;都不是Bash的内建变量。然而这些变量经常在Bash的启动文件中被当作环境变量来设置。$SHELL是用户登陆shell的名字，它可以在/etc/passwd中设置，或者也可以在”init”脚本中设置，并且它也不是Bash内建的。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;位置参数&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;&lt;code&gt;$0&lt;/code&gt;, &lt;code&gt;$1&lt;/code&gt;, &lt;code&gt;$2&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;位置参数，从命令行传递到脚本，或者传递给函数，或者&lt;code&gt;set&lt;/code&gt;给变量。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;&lt;code&gt;$#&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;命令行参数或者位置参数的个数。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;&lt;code&gt;$*&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;所有的位置参数都被看作为一个单词。&lt;code&gt;$*&lt;/code&gt;必须被引用起来。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;&lt;code&gt;$@&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;与&lt;code&gt;$*&lt;/code&gt;相同，但是每个参数都是一个独立的引用字符串，这就意味着，参数是被完整传递的，并没有被解释或扩展。这也意味着，参数列表中每个参数都被看作为单独的单词。当然，&lt;code&gt;$@&lt;/code&gt;应该被引用起来。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# arglist.sh
# 多使用几个参数来调用这个脚本，比如&quot;one two three&quot;

E_BADARGS=65

if [ ! -n &quot;$1&quot; ]
then
    echo &quot;Usage: `basename $0` argument1 argument2 etc.&quot;
    exit $E_BADARGS
fi

echo

index=1 
echo &quot;Listing args with \&quot;\$*\&quot;:&quot;
for arg in &quot;$*&quot;  # 如果&quot;$*&quot;不被&quot;&quot;引用，那么将不能正常地工作
do
    echo &quot;Arg #$index = $arg&quot;
    let &quot;index+=1&quot;
done
# $* 将所有的参数看成一个单词

echo

index=1    

echo &quot;Listing args with \&quot;\$@\&quot;:&quot;
for arg in &quot;$@&quot;  # 如果&quot;$*&quot;不被&quot;&quot;引用，那么将不能正常地工作
do
    echo &quot;Arg #$index = $arg&quot;
    let &quot;index+=1&quot;
done 
# $@ 把每个参数都看成是单独的单词

echo

index=1 

echo &quot;Listing args with \$*:&quot;
for arg in $*
do
    echo &quot;Arg #$index = $arg&quot;
    let &quot;index+=1&quot;
done
# 未引用的$*将会把参数看成单独的单词

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;shift&lt;/code&gt;命令执行以后, &lt;code&gt;$@&lt;/code&gt;将会保存命令行中剩余的参数, 但是没有之前的&lt;code&gt;$1&lt;/code&gt;, 因为被丢弃了.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# 使用 ./scriptname 1 2 3 4 5 来调用这个脚本

echo &quot;$@&quot;    # 1 2 3 4 5
shift
echo &quot;$@&quot;    # 2 3 4 5
shift
echo &quot;$@&quot;    # 3 4 5

# 每次&quot;shift&quot;都会丢弃$1.
# &quot;$@&quot; 将包含剩下的参数.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;$@&lt;/code&gt;也可以作为工具使用，用来过滤传递给脚本的输入。&lt;code&gt;cat &quot;$@&quot;&lt;/code&gt;结构既可以接受从stdin传递给脚本的输入，也可以接受从参数中指定的文件中传递给脚本的输入。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# rot13.sh: 典型的rot13算法

# 用法: ./rot13.sh filename
# 或     ./rot13.sh &amp;lt;filename
# 或     ./rot13.sh and supply keyboard input (stdin)

cat &quot;$@&quot; | tr &#39;a-zA-Z&#39; &#39;n-za-mN-ZA-M&#39;   # &quot;a&quot;变为&quot;n&quot;，&quot;b&quot;变为&quot;o&quot;，等等
#  &#39;cat &quot;$@&quot;&#39;结构允许从stdin或者从文件中获得输入. 

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;$*&lt;/code&gt;和&lt;code&gt;$@&lt;/code&gt;中的参数有时候会表现出不一致而且令人迷惑的行为，这都依赖于&lt;code&gt;$IFS&lt;/code&gt;的设置。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

#  内部Bash变量&quot;$*&quot;和&quot;$@&quot;的古怪行为,
#+ 都依赖于它们是否被双引号引用起来.
#  单词拆分与换行的不一致的处理.

set -- &quot;First one&quot; &quot;second&quot; &quot;third:one&quot; &quot;&quot; &quot;Fifth: :one&quot;
# 设置这个脚本的参数, $1, $2, 等等

echo &#39;IFS unchanged, using &quot;$*&quot;&#39;
c=0
for i in &quot;$*&quot;               # 引用起来
do 
    echo &quot;$((c+=1)): [$i]&quot;   # 这行在下边每个例子中都一样
done
echo ---

echo &#39;IFS unchanged, using $*&#39;
c=0
for i in $*               # 未引用
do 
    echo &quot;$((c+=1)): [$i]&quot;   
done
echo ---

echo &#39;IFS unchanged, using &quot;$@&quot;&#39;
c=0
for i in &quot;$@&quot;               
do 
    echo &quot;$((c+=1)): [$i]&quot;   
done
echo ---

echo &#39;IFS unchanged, using $@&#39;
c=0
for i in $@             
do 
    echo &quot;$((c+=1)): [$i]&quot;   
done
echo ---

IFS=:
echo &#39;IFS=&quot;:&quot;, using &quot;$*&quot;&#39;
c=0
for i in &quot;$*&quot;               
do 
    echo &quot;$((c+=1)): [$i]&quot;   
done
echo ---

echo &#39;IFS=&quot;:&quot;, using $*&#39;
c=0
for i in $*              
do 
    echo &quot;$((c+=1)): [$i]&quot;   
done
echo ---

var=$*
echo &#39;IFS=&quot;:&quot;, using &quot;$var&quot; (var=$*)&#39;
c=0
for i in &quot;$var&quot;
do 
    echo &quot;$((c+=1)): [$i]&quot;
done
echo ---

echo &#39;IFS=&quot;:&quot;, using $var (var=$*)&#39;
c=0
for i in $var
do echo &quot;$((c+=1)): [$i]&quot;
done
echo ---

var=&quot;$*&quot;
echo &#39;IFS=&quot;:&quot;, using $var (var=&quot;$*&quot;)&#39;
c=0
for i in $var
do echo &quot;$((c+=1)): [$i]&quot;
done
echo ---

echo &#39;IFS=&quot;:&quot;, using &quot;$var&quot; (var=&quot;$*&quot;)&#39;
c=0
for i in &quot;$var&quot;
do echo &quot;$((c+=1)): [$i]&quot;
done
echo ---

echo &#39;IFS=&quot;:&quot;, using &quot;$@&quot;&#39;
c=0
for i in &quot;$@&quot;
do echo &quot;$((c+=1)): [$i]&quot;
done
echo ---

echo &#39;IFS=&quot;:&quot;, using $@&#39;
c=0
for i in $@
do echo &quot;$((c+=1)): [$i]&quot;
done
echo ---
 
var=$@
echo &#39;IFS=&quot;:&quot;, using $var (var=$@)&#39;
c=0
for i in $var
do echo &quot;$((c+=1)): [$i]&quot;
done
echo ---
 
echo &#39;IFS=&quot;:&quot;, using &quot;$var&quot; (var=$@)&#39;
c=0
for i in &quot;$var&quot;
do echo &quot;$((c+=1)): [$i]&quot;
done
echo ---

var=&quot;$@&quot;
echo &#39;IFS=&quot;:&quot;, using &quot;$var&quot; (var=&quot;$@&quot;)&#39;
c=0
for i in &quot;$var&quot;
do echo &quot;$((c+=1)): [$i]&quot;
done
echo ---

echo &#39;IFS=&quot;:&quot;, using $var (var=&quot;$@&quot;)&#39;
c=0
for i in $var
do echo &quot;$((c+=1)): [$i]&quot;
done

echo

# 使用ksh或者zsh -y来试试这个脚本.

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;$@&lt;/code&gt;与&lt;code&gt;$*&lt;/code&gt;中的参数只有在被双引号引用起来的时候才会不同。&lt;/p&gt;

&lt;p&gt;当&lt;code&gt;$IFS&lt;/code&gt;值为空时, &lt;code&gt;$*&lt;/code&gt;和&lt;code&gt;$@&lt;/code&gt;的行为&lt;strong&gt;依赖于正在运行的Bash或者sh的版本&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
#  如果$IFS被设置，但其值为空，那么&quot;$*&quot;和&quot;$@&quot;将不会像期望的那样显示位置参数.

mecho ()       # 打印位置参数
{
    echo &quot;$1,$2,$3&quot;;
}

IFS=&quot;&quot;         # 设置了，但值为空
set a b c      # 位置参数

mecho &quot;$*&quot;     # abc,,
mecho $*       # a,b,c

mecho $@       # a,b,c
mecho &quot;$@&quot;     # a,b,c
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;其他的特殊参数&lt;/h2&gt;

&lt;h3 id=&quot;section-6&quot;&gt;&lt;code&gt;$- &lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;传递给脚本的标记(使用&lt;code&gt;set&lt;/code&gt;命令)。&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;&lt;code&gt;$! &lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;运行在后台的最后一个作业的PID。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sleep ${TIMEOUT}; eval &#39;kill -9 $!&#39; &amp;amp;&amp;gt; /dev/null;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-8&quot;&gt;&lt;code&gt;$_&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;这个变量保存之前执行的命令的最后一个参数的值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

echo $_      # /bin/bash
                    # 只是调用/bin/bash来运行这个脚本

du &amp;gt;/dev/null        # 这么做命令行上将没有输出
echo $_              # du

ls -al &amp;gt;/dev/null    # 这么做命令行上将没有输出.
echo $_              # -al  (这是最后的参数)

:
echo $_              # :
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-9&quot;&gt;&lt;code&gt;$?&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;命令、函数或者是脚本本身的退出状态码。&lt;/p&gt;

&lt;h3 id=&quot;section-10&quot;&gt;&lt;code&gt;$$&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;脚本自身的进程ID。&lt;code&gt;$$&lt;/code&gt;变量在脚本中经常用来构造”唯一的”临时文件名。这么做通常比调用&lt;code&gt;mktemp&lt;/code&gt;命令来的简单。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# temp.sh

mktemp

TMPFILE=/tmp/ftp.$$
echo $TMPFILE

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行脚本，会输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/tmp/tmp.lrRiA13050
/tmp/ftp.13049
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-11&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.javachen.com/static/doc/abs-guide/html/index.html&quot;&gt;高级Bash脚本编程指南-中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/07/09/bash-internal-variables.html</link>
      <guid>http://blog.javachen.com/2015/07/09/bash-internal-variables.html</guid>
      <pubDate>2015-07-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Bash条件判断</title>
      <description>&lt;p&gt;每个完整并且合理的程序语言都具有条件判断的功能，并且可以根据条件测试的结果做下一步的处理。Bash有test命令、各种中括号和圆括号操作，和if/then结构。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;条件测试&lt;/h1&gt;

&lt;p&gt;if/then结构用来判断命令列表的退出状态码是否为0。&lt;/p&gt;

&lt;p&gt;有一个专有命令&lt;code&gt;[ &lt;/code&gt;(左中括号，特殊字符)。这个命令与&lt;code&gt;test&lt;/code&gt;命令等价，并且出于效率上的考虑，这是一个&lt;code&gt;内建命令&lt;/code&gt;。这个命令把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码(0 表示真，1表示假)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;if  [ 0 ]      
then
    echo &quot;0 is true.&quot;
else
    echo &quot;0 is false.&quot;
fi            
# 0 is true.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在版本2.02的Bash中，引入了&lt;code&gt;[[ ... ]]&lt;/code&gt;扩展测试命令，因为这种表现形式可能对某些语言的程序员来说更容易熟悉一些。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;code&gt;[[&lt;/code&gt;是一个&lt;code&gt;关键字&lt;/code&gt;，并不是一个命令。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[[ 1 &amp;lt; 3 ]]
echo $?  
# 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bash把&lt;code&gt;[[ $a -lt $b ]]&lt;/code&gt;看作一个单独的元素，并且返回一个退出状态码。&lt;code&gt;(( ... ))&lt;/code&gt;和&lt;code&gt;let ...&lt;/code&gt;结构也能够返回退出状态码，&lt;strong&gt;当它们所测试的算术表达式的结果为非零的时候，将会返回退出状态码0&lt;/strong&gt;。这些算术扩展结构被用来做算术比较。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ let &quot;1&amp;lt;2&quot;
$ echo $?
0

$ (( 0 &amp;amp;&amp;amp; 1 ))
$ echo $?
1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;if命令能够测试任何命令，并不仅仅是&lt;code&gt;中括号&lt;/code&gt;中的条件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cmp a b &amp;amp;&amp;gt; /dev/null 

echo $?    # 2

if cmp a b &amp;amp;&amp;gt; /dev/null  # 禁止输出.
then 
    echo &quot;Files a and b are identical.&quot;
else 
    echo &quot;Files a and b differ.&quot;
fi

# 非常有用的&quot;if-grep&quot;结构:
if grep -q bash /etc/profile
then 
    echo &quot;File contains at least one occurrence of Bash.&quot;
fi

word=Linux
letter_sequence=inu
if echo &quot;$word&quot; | grep -q &quot;$letter_sequence&quot;
# &quot;-q&quot; 选项是用来禁止输出的.
then
    echo &quot;$letter_sequence found in $word&quot;
else
    echo &quot;$letter_sequence not found in $word&quot;
fi

if COMMAND_WHOSE_EXIT_STATUS_IS_0_UNLESS_ERROR_OCCURRED
then 
    echo &quot;Command succeeded.&quot;
else 
    echo &quot;Command failed.&quot;
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;条件判断主要判断的是条件是否为真或者假。那么，在什么情况下才为真呢？&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 0 为真
if [ 0 ]      
then
    echo &quot;0 is true.&quot;
else
    echo &quot;0 is false.&quot;
fi

# 1 为真
if [ 1 ]      
then
    echo &quot;1 is true.&quot;
else
    echo &quot;1 is false.&quot;
fi

# -1 为真
if [ -1 ]      
then
    echo &quot;-1 is true.&quot;
else
    echo &quot;-1 is false.&quot;
fi

# NULL 为假
if [  ]      
then
    echo &quot;NULL is true.&quot;
else
    echo &quot;NULL is false.&quot;
fi

# 随便的一串字符为真
if [ xyz ]      
then
    echo &quot;Random string is true.&quot;
else
    echo &quot;Random string is false.&quot;
fi

# 未初始化的变量为假
if [ $xyz ]      
then
    echo &quot;Uninitialized variable is true.&quot;
else
    echo &quot;Uninitialized variable is false.&quot;
fi

# 更加正规的条件检查
if [ -n &quot;$xyz&quot; ]      
then
    echo &quot;Uninitialized variable is true.&quot;
else
    echo &quot;Uninitialized variable is false.&quot;
fi

xyz=     # 初始化了, 但是赋null值

# null变量为假
if [ -n &quot;$xyz&quot; ]      
then
    echo &quot;Null variable is true.&quot;
else
    echo &quot;Null variable is false.&quot;
fi

# &quot;false&quot; 为真
if [  &quot;false&quot; ]      
then
    echo &quot;\&quot;false\&quot; is true.&quot; 
else
    echo &quot;\&quot;false\&quot; is false.&quot; 
fi

# 再来一个, 未初始化的变量
# &quot;$false&quot; 为假
if [  &quot;$false&quot; ]      
then
    echo &quot;\&quot;\$false\&quot; is true.&quot; 
else
    echo &quot;\&quot;\$false\&quot; is false.&quot; 
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;[ &lt;/code&gt;这个命令与&lt;code&gt;test&lt;/code&gt;命令等价，把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码(0 表示真，1表示假)，&lt;strong&gt;如果参数为确定的一个值或者有初始化，则退出状态码为0；否者为空值或者未初始化，则为1&lt;/strong&gt;。上面例子也可以通过状态码来验证：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ [ 0 ]  ;  echo $?
0
$ [ 1 ]  ;  echo $?
0
$ [ -1 ]  ;  echo $?
0
$ [  ]  ;  echo $?
1
$ [ xyz ]  ;  echo $?
0
$ [ $xyz ] ;  echo $?
1
$ [ -n &quot;$xyz&quot; ] ;  echo $?
1
$ xyz= ;  [ -n &quot;$xyz&quot; ] ;  echo $?
1
$ [ &quot;false&quot; ] ;  echo $?
0
$ [  &quot;$false&quot; ] ;  echo $?
1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果if和then在条件判断的同一行上的话，必须使用分号来结束if表达式。if和then都是关键字，关键字(或者命令)如果作为表达式的开头，并且如果想在同一行上再写一个新的表达式的话，那么必须使用分号来结束上一句表达式。&lt;/p&gt;

&lt;p&gt;if语句里还可以加上elif分支，elif是else if的缩写形式，作用是在外部的判断结构中再嵌入一个内部的if/then结构。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;if [ condition1 ]
then
    command1
    command2
    command3
elif [ condition2 ]
# 与else if一样
then
    command4
    command5
else
    default-command
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;if test condition-true&lt;/code&gt;结构与&lt;code&gt;if [ condition-true ]&lt;/code&gt;完全相同. 就像我们前面所看到的，左中括号是调用&lt;code&gt;test&lt;/code&gt;命令的标识，而关闭条件判断用的的右中括号在if/test结构中并不是严格必需的，但是在Bash的新版本中必须要求使用。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;br /&gt;
&lt;code&gt;test&lt;/code&gt;命令在Bash中是内建命令，用来测试文件类型，或者用来比较字符串。因此，在Bash脚本中，test命令并不会调用外部的/usr/bin/test中的test命令，这是sh-utils工具包中的一部分。同样的，&lt;code&gt;[&lt;/code&gt;也并不会调用&lt;code&gt;/usr/bin/[&lt;/code&gt;，这是/usr/bin/test的符号链接。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面测试type命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash$ type test
test is a shell builtin
bash$ type &#39;[&#39;
[ is a shell builtin
bash$ type &#39;[[&#39;
[[ is a shell keyword
bash$ type &#39;]]&#39;
]] is a shell keyword
bash$ type &#39;]&#39;
bash: type: ]: not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;test&lt;/code&gt;、&lt;code&gt;/usr/bin/test&lt;/code&gt;、&lt;code&gt;[ ]&lt;/code&gt;和&lt;code&gt;/usr/bin/[&lt;/code&gt;都是等价命令。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;if test -z &quot;$1&quot;
then
    echo &quot;No command-line arguments.&quot;
else
    echo &quot;First command-line argument is $1.&quot;
fi

if /usr/bin/test -z &quot;$1&quot;
then
    echo &quot;No command-line arguments.&quot;
else
    echo &quot;First command-line argument is $1.&quot;
fi

if [ -z &quot;$1&quot; ] 
#   if [ -z &quot;$1&quot;                应该能够运行，但是Bash报错, 提示缺少关闭条件测试的右中括号
then
    echo &quot;No command-line arguments.&quot;
else
    echo &quot;First command-line argument is $1.&quot;
fi

if /usr/bin/[ -z &quot;$1&quot; ]  
# if /usr/bin/[ -z &quot;$1&quot;       # 能够工作，但是还是给出一个错误消息。注意：在版本3.x的Bash中, 这个bug已经被修正了
then
    echo &quot;No command-line arguments.&quot;
else
    echo &quot;First command-line argument is $1.&quot;
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;[[ ]]&lt;/code&gt;结构比&lt;code&gt;[   ]&lt;/code&gt;结构更加通用。这是一个扩展的test命令，是从ksh88中引进的。在&lt;code&gt;[[&lt;/code&gt;和&lt;code&gt;]]&lt;/code&gt;之间&lt;strong&gt;所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;file=/etc/passwd

if [[ -e $file ]]
then
    echo &quot;Password file exists.&quot;
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用&lt;code&gt;[[ ... ]]&lt;/code&gt;条件判断结构而不是&lt;code&gt;[ ... ]&lt;/code&gt;，能够防止脚本中的许多逻辑错误。比如&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;、&lt;code&gt;||&lt;/code&gt;、&lt;code&gt;&amp;lt;&lt;/code&gt;和&lt;code&gt;&amp;gt;&lt;/code&gt;操作符能够正常存在于&lt;code&gt;[[ ]]&lt;/code&gt;条件判断结构中,，但是如果出现在&lt;code&gt;[ ]&lt;/code&gt;结构中的话，会报错。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在if后面也不一定非得是test命令或者是用于条件判断的中括号结构&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;dir=/home/bozo

if cd &quot;$dir&quot; 2&amp;gt;/dev/null; then   # &quot;2&amp;gt;/dev/null&quot; 会隐藏错误信息.
    echo &quot;Now in $dir.&quot;
else
    echo &quot;Can&#39;t change to $dir.&quot;
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;“if COMMAND”结构将会返回COMMAND的退出状态码。与此相似，在中括号中的条件判断也不一定非得要if不可，也可以使用&lt;code&gt;列表结构&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;var1=20
var2=22
[ &quot;$var1&quot; -ne &quot;$var2&quot; ] &amp;amp;&amp;amp; echo &quot;$var1 is not equal to $var2&quot;

home=/home/bozo
[ -d &quot;$home&quot; ] || echo &quot;$home directory does not exist.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;(( ))&lt;/code&gt;结构扩展并计算一个算术表达式的值。如果表达式的结果为0，那么返回的退出状态码为1，或者是”假”。而&lt;strong&gt;一个非零值的表达式所返回的退出状态码将为0&lt;/strong&gt;，或者是”true”。&lt;strong&gt;这种情况和先前所讨论的&lt;code&gt;test&lt;/code&gt;命令和&lt;code&gt;[ ]&lt;/code&gt;结构的行为正好相反&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# 算术测试.

# (( ... ))结构可以用来计算并测试算术表达式的结果. 
# 退出状态将会与[ ... ]结构完全相反!

(( 0 ))
echo &quot;Exit status of \&quot;(( 0 ))\&quot; is $?.&quot;         # 1

(( 1 ))
echo &quot;Exit status of \&quot;(( 1 ))\&quot; is $?.&quot;         # 0

(( 5 &amp;gt; 4 ))                                      # 真
echo &quot;Exit status of \&quot;(( 5 &amp;gt; 4 ))\&quot; is $?.&quot;     # 0
  
(( 5 &amp;gt; 9 ))                                      # 假
echo &quot;Exit status of \&quot;(( 5 &amp;gt; 9 ))\&quot; is $?.&quot;     # 1

(( 5 - 5 ))                                      # 0
echo &quot;Exit status of \&quot;(( 5 - 5 ))\&quot; is $?.&quot;     # 1

(( 5 / 4 ))                                      # 除法也可以.
echo &quot;Exit status of \&quot;(( 5 / 4 ))\&quot; is $?.&quot;     # 0

(( 1 / 2 ))                                      # 除法的计算结果 &amp;lt; 1.
echo &quot;Exit status of \&quot;(( 1 / 2 ))\&quot; is $?.&quot;     # 截取之后的结果为 0.
                                                    # 1

(( 1 / 0 )) 2&amp;gt;/dev/null                          # 除数为0, 非法计算. 
#            ^^^^^^^^^^^
echo &quot;Exit status of \&quot;(( 1 / 0 ))\&quot; is $?.&quot;     # 1

# &quot;2&amp;gt;/dev/null&quot;起了什么作用?
# 如果这句被删除会怎样?
(( 1 / 0 )) 
echo &quot;Exit status of \&quot;(( 1 / 0 ))\&quot; is $?.&quot;     # 2

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就如文章开头所言，&lt;code&gt;(( ... ))&lt;/code&gt;和&lt;code&gt;let ...&lt;/code&gt;结构也能够返回退出状态码，当它们所测试的算术表达式的结果为非零的时候，将会返回退出状态码0。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;测试操作符&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;文件测试操作符&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;-e&lt;/code&gt; 文件存在&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-a&lt;/code&gt; 文件存在，这个选项的效果与-e相同. 但是它已经被”弃用”了, 并且不鼓励使用.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-f&lt;/code&gt;  表示这个文件是一个一般文件(并不是目录或者设备文件)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-s&lt;/code&gt; 文件大小不为零&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-d&lt;/code&gt; 表示这是一个目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-b&lt;/code&gt; 表示这是一个块设备(软盘、光驱等。)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-c&lt;/code&gt; 表示这是一个字符设备(键盘, modem, 声卡, 等等.)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-p&lt;/code&gt; 这个文件是一个管道&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-h&lt;/code&gt; 这是一个符号链接&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-L&lt;/code&gt; 这是一个符号链接&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-S&lt;/code&gt; 表示这是一个socket&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-t &lt;/code&gt; 文件(描述符)被关联到一个终端设备上。这个测试选项一般被用来检测脚本中的stdin([ -t 0 ]) 或者stdout([ -t 1 ])是否来自于一个终端.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-r&lt;/code&gt; 文件是否具有可读权限(指的是正在运行这个测试命令的用户是否具有读权限)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-w&lt;/code&gt; 文件是否具有可写权限(指的是正在运行这个测试命令的用户是否具有写权限)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-x&lt;/code&gt; 文件是否具有可执行权限(指的是正在运行这个测试命令的用户是否具有可执行权限)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-g&lt;/code&gt; set-group-id(sgid)标记被设置到文件或目录上。如果目录具有sgid标记的话, 那么在这个目录下所创建的文件将属于拥有这个目录的用户组，而不必是创建这个文件的用户组. 这个特性对于在一个工作组中共享目录非常有用。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-u&lt;/code&gt; set-user-id (suid)标记被设置到文件上。如果一个root用户所拥有的二进制可执行文件设置了set-user-id标记位的话，那么普通用户也会以root权限来运行这个文件。这对于需要访问系统硬件的执行程序非常有用。如果没有suid标志的话，这些二进制执行程序是不能够被非root用户调用的。对于设置了suid标志的文件，在它的权限列中将会以s表示。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-k&lt;/code&gt; 设置粘贴位。粘贴位设置在目录中，它将限制写权限，在它们的权限标记列中将会显示t。如果用户并不拥有这个设置了粘贴位的目录，但是他在这个目录下具有写权限，那么这个用户只能在这个目录下删除自己所拥有的文件。这将有效的防止用户在一个公共目录中不慎覆盖或者删除别人的文件，比如说/tmp目录。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-O&lt;/code&gt; 判断你是否是文件的拥有者&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-G&lt;/code&gt; 文件的group-id是否与你的相同&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-N&lt;/code&gt; 从文件上一次被读取到现在为止，文件是否被修改过&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;f1 -nt f2&lt;/code&gt; 文件f1比文件f2新&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;f1 -ot f2&lt;/code&gt; 文件f1比文件f2旧&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;f1 -ef f2&lt;/code&gt; 文件f1和文件f2是相同文件的硬链接&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;!&lt;/code&gt; 反转上边所有测试的结果(如果没给出条件，那么返回真)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;二元比较操作符&lt;/strong&gt;，用来比较两个变量或数字：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;-eq&lt;/code&gt; 等于 &lt;code&gt;if [ &quot;$a&quot; -eq &quot;$b&quot; ]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-ne&lt;/code&gt; 不等于 &lt;code&gt;if [ &quot;$a&quot; -ne &quot;$b&quot; ]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-gt&lt;/code&gt; 大于 &lt;code&gt;if [ &quot;$a&quot; -gt &quot;$b&quot; ]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-ge&lt;/code&gt; 大于等于 &lt;code&gt;if [ &quot;$a&quot; -ge &quot;$b&quot; ]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-lt&lt;/code&gt; 小于 &lt;code&gt;if [ &quot;$a&quot; -lt &quot;$b&quot; ]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-le&lt;/code&gt; 小于等于 &lt;code&gt;if [ &quot;$a&quot; -le &quot;$b&quot; ]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;&amp;lt;&lt;/code&gt; 小于(在&lt;code&gt;双括号&lt;/code&gt;中使用) &lt;code&gt;((&quot;$a&quot; &amp;lt; &quot;$b&quot;))&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;&amp;lt;=&lt;/code&gt; 小于等于(在&lt;code&gt;双括号&lt;/code&gt;中使用) &lt;code&gt;((&quot;$a&quot; &amp;lt;= &quot;$b&quot;))&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;&amp;gt;&lt;/code&gt; 大于(在&lt;code&gt;双括号&lt;/code&gt;中使用) &lt;code&gt;((&quot;$a&quot; &amp;gt; &quot;$b&quot;))&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;&amp;gt;=&lt;/code&gt; 大于等于(在&lt;code&gt;双括号&lt;/code&gt;中使用) &lt;code&gt;((&quot;$a&quot; &amp;gt;= &quot;$b&quot;))&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-a&lt;/code&gt; 逻辑与，一般都是和&lt;code&gt;test&lt;/code&gt;命令或者是&lt;code&gt;单中括号&lt;/code&gt;结构一起使用的 &lt;code&gt;if [ &quot;$exp1&quot; -a &quot;$exp2&quot; ]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-o&lt;/code&gt; 逻辑或，一般都是和&lt;code&gt;test&lt;/code&gt;命令或者是&lt;code&gt;单中括号&lt;/code&gt;结构一起使用的 &lt;code&gt;if [ &quot;$exp1&quot; -o &quot;$exp2&quot; ]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;字符串比较&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;=&lt;/code&gt; 等于 &lt;code&gt;if [ &quot;$a&quot; = &quot;$b&quot; ]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;==&lt;/code&gt;等于 &lt;code&gt;if [ &quot;$a&quot; == &quot;$b&quot; ]&lt;/code&gt; 与&lt;code&gt;=&lt;/code&gt;等价&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;!=&lt;/code&gt; 不等号 &lt;code&gt;if [ &quot;$a&quot; != &quot;$b&quot; ]&lt;/code&gt; 这个操作符将在&lt;code&gt;[[ ... ]]&lt;/code&gt;结构中使用模式匹配&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;&amp;lt;&lt;/code&gt; 小于，按照ASCII字符进行排序 &lt;code&gt;if [[ &quot;$a&quot; &amp;lt; &quot;$b&quot; ]]&lt;/code&gt; &lt;code&gt;if [ &quot;$a&quot; \&amp;lt; &quot;$b&quot; ]&lt;/code&gt; 注意使用在&lt;code&gt;[ ]&lt;/code&gt;结构中的时候需要被转义。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;&amp;gt;&lt;/code&gt; 大于，按照ASCII字符进行排序 &lt;code&gt;if [[ &quot;$a&quot; &amp;gt; &quot;$b&quot; ]]&lt;/code&gt; &lt;code&gt;if [ &quot;$a&quot; \&amp;gt; &quot;$b&quot; ]&lt;/code&gt; 注意使用在&lt;code&gt;[ ]&lt;/code&gt;结构中的时候需要被转义。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-z&lt;/code&gt; 字符串为”null”，意思就是字符串长度为零&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-n&lt;/code&gt; 字符串不为”null”，当&lt;code&gt;-n&lt;/code&gt;使用在中括号中进行条件测试的时候，必须要把字符串用双引号引用起来。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：&lt;strong&gt;==比较操作符在&lt;code&gt;双中括号对&lt;/code&gt;和&lt;code&gt;单中括号对&lt;/code&gt;中的行为是不同的&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;if [[ $a == z* ]]    # 如果$a以&quot;z&quot;开头(模式匹配)，那么结果将为真
then echo true
else echo false
fi

if [[ $a == &quot;z*&quot; ]]  # 如果$a与z*相等，就是字面意思完全一样，那么结果为真。
then echo true
else echo false
fi

if [ $a == z* ]      # 如果$a与z*相等，就是字面意思完全一样，那么结果为真。不存在模式匹配
then echo true
else echo false
fi

if [ &quot;$a&quot; == &quot;z*&quot; ]  # 如果$a与z*相等，就是字面意思完全一样，那么结果为真。
then echo true
else echo false
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;检查字符串是否为null&lt;/strong&gt;，有以下几种方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
#  str-test.sh: 检查null字符串和未引用的字符串

if [ -n $string1 ]    # $string1 没有被声明和初始化
then
    echo &quot;String \&quot;string1\&quot; is not null.&quot;
else  
    echo &quot;String \&quot;string1\&quot; is null.&quot;
fi

if [ -n &quot;$string1&quot; ]  # 这次$string1被引号扩起来了.
then
    echo &quot;String \&quot;string1\&quot; is not null.&quot;
else  
    echo &quot;String \&quot;string1\&quot; is null.&quot;
fi

if [ $string1 ]       # 这次，就一个$string1，什么都不加
then
    echo &quot;String \&quot;string1\&quot; is not null.&quot;
else  
    echo &quot;String \&quot;string1\&quot; is null.&quot;
fi
# [ ] 测试操作符能够独立检查string是否为null，然而，使用(&quot;$string1&quot;)是一种非常好的习惯
# if [ $string1 ]    只有一个参数 &quot;]&quot;
# if [ &quot;$string1&quot; ]  有两个参数，一个是空的&quot;$string1&quot;，另一个是&quot;]&quot; 

string1=initialized

if [ $string1 ]        # 再来，还是只有$string1，什么都不加
then
    echo &quot;String \&quot;string1\&quot; is not null.&quot;
else  
    echo &quot;String \&quot;string1\&quot; is null.&quot;
fi
# 这个例子运行还是给出了正确的结果，但是使用引用的(&quot;$string1&quot;)还是更好一些

string1=&quot;a = b&quot;

if [ $string1 ]        # 再来，还是只有$string1，什么都不加
then
    echo &quot;String \&quot;string1\&quot; is not null.&quot;
else  
    echo &quot;String \&quot;string1\&quot; is null.&quot;
fi
# 未引用的&quot;$string1&quot;，这回给出了错误的结果

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;本篇文章介绍了如何使用Bash的条件判断，涉及到test命令、各种中括号和圆括号操作，以及if/then结构。需要知道，test、/usr/bin/test、[ ]和/usr/bin/[都是等价命令，使用type命令可以分区内建命令和关键字。另外，需要区分单中括号和双中括号以及双圆括号的含义是不同的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;单中括号&lt;code&gt;[&lt;/code&gt;，与test命令等价，是个内建命令&lt;/li&gt;
  &lt;li&gt;双中括号&lt;code&gt;[[&lt;/code&gt;，是扩展测试命令，是个关键字&lt;/li&gt;
  &lt;li&gt;双圆括号&lt;code&gt;(())&lt;/code&gt;，是根据算术表达式运行结果来返回不同状态码&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用&lt;code&gt;[[ ... ]]&lt;/code&gt;条件判断结构而不是&lt;code&gt;[ ... ]&lt;/code&gt;，能够防止脚本中的许多逻辑错误。比如&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;、&lt;code&gt;||&lt;/code&gt;、&lt;code&gt;&amp;lt;&lt;/code&gt;和&lt;code&gt;&amp;gt;&lt;/code&gt;操作符能够正常存在于&lt;code&gt;[[ ]]&lt;/code&gt;条件判断结构中,，但是如果出现在&lt;code&gt;[ ]&lt;/code&gt;结构中的话，会报错。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.javachen.com/static/doc/abs-guide/html/index.html&quot;&gt;高级Bash脚本编程指南-中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/07/08/bash-if-else.html</link>
      <guid>http://blog.javachen.com/2015/07/08/bash-if-else.html</guid>
      <pubDate>2015-07-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Bash中的变量</title>
      <description>&lt;p&gt;变量是脚本编程中进行数据表现的一种方法。说白了，变量不过是计算机为了保留数据项，而在内存中分配的一个位置或一组位置的标识或名字。变量既可以出现在算术操作中，也可以出现在字符串分析过程中。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;变量赋值&lt;/h1&gt;

&lt;p&gt;变量使用&lt;code&gt;=&lt;/code&gt;来实现赋值操作，前后都不能有空白。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;a=314
echo &quot;The value of \&quot;a\&quot; is $a.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用let来赋值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;let a=16+5
echo &quot;The value of \&quot;a\&quot; is now $a.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用&lt;code&gt;read&lt;/code&gt;命令进行赋值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo -n &quot;Enter \&quot;a\&quot; &quot;
read a
echo &quot;The value of \&quot;a\&quot; is now $a.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是复杂一点的赋值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;a=23            
echo $a
b=$a
echo $b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们也可以使用&lt;code&gt;命令替换&lt;/code&gt;来赋值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;a=`echo Hello!`   # 把&#39;echo&#39;命令的结果传给变量&#39;a&#39;
echo $a

a=`ls -l`         # 把&#39;ls -l&#39;的结果赋值给&#39;a&#39;
echo $a        # 然而, 如果没有引号的话将会删除ls结果中多余的tab和换行符.
echo &quot;$a&quot;      # 如果加上引号的话, 那么就会保留ls结果中的空白符.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;br /&gt;
上面的代码有无引号，在linux系统中，运行结果是不一样的；在mac系统上，结果是一致的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;也可以使用&lt;code&gt;$(...)&lt;/code&gt;机制来进行变量赋值，这其实还是&lt;code&gt;命令替换&lt;/code&gt;的一种形式。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# From /etc/rc.d/rc.local
R=$(cat /etc/redhat-release)
arch=$(uname -m)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;变量的类型&lt;/h1&gt;

&lt;p&gt;不像其他程序语言一样，Bash并不对变量区分”类型”。本质上，Bash变量都是字符串。但是依赖于具体的上下文，Bash也允许比较操作和整数操作。其中的关键因素就是，变量中的值是否只有数字。&lt;/p&gt;

&lt;p&gt;整型的变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;a=2334                   # 整型.
let &quot;a += 1&quot;
echo &quot;a = $a &quot;        # a = 2335
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把变量编程字符串：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;b=${a/23/BB}     # 将&quot;23&quot;替换成&quot;BB&quot;
echo &quot;b = $b&quot;     # b = BB35
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对字符串类型变量计算加法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;let &quot;b += 1&quot;             # BB35 + 1 =
echo &quot;b = $b&quot;        # b = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面过程反过来操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;c=BB34
echo &quot;c = $c&quot;               # c = BB34
d=${c/BB/23}               # 将&quot;BB&quot;替换成&quot;23&quot;.

# 这使得变量$d变为一个整形.
echo &quot;d = $d&quot;               # d = 2334
let &quot;d += 1&quot;                # 2334 + 1 =
echo &quot;d = $d&quot;               # d = 2335
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;null变量可以用在算术操作中，下面例子将null变量转换成一个整型变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;e=&quot;&quot;
echo &quot;e = $e&quot;            # e =
let &quot;e += 1&quot;            # 算术操作允许一个null变量?
echo &quot;e = $e&quot;            # e = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于没有声明的变量，也可以转换成一个整型变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;f = $f&quot;               # f =
let &quot;f += 1&quot;                # 算术操作能通过么?
echo &quot;f = $f&quot;               # f = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不区分变量的类型既是幸运的事情也是悲惨的事情。它允许你在编写脚本的时候更加的灵活(但是也足够把你搞晕!)，并且可以让你能够更容易的编写代码。然而，这也很容易产生错误，并且让你养成糟糕的编程习惯。&lt;/p&gt;

&lt;p&gt;另外，变量还分局部变量和环境变量。&lt;code&gt;局部变量&lt;/code&gt;只在代码块或者函数中可见；&lt;code&gt;环境变量&lt;/code&gt;将影响用户接口和shell行为。&lt;/p&gt;

&lt;p&gt;在通常情况下，每个进程都有自己的”环境”，这个环境是由一组变量组成的，这些变量中存有进程可能需要引用的信息。在这种情况下，shell与一个一般的进程没什么区别。&lt;/p&gt;

&lt;p&gt;但是，分配给环境变量的空间是有限的。创建太多环境变量，或者给一个环境变量分配太多的空间都会引起错误。&lt;/p&gt;

&lt;p&gt;如果一个脚本要设置一个环境变量，那么需要将这些变量&lt;code&gt;export&lt;/code&gt;出来，也就是需要通知到脚本本地的环境。但是，子进程是不能够&lt;code&gt;export&lt;/code&gt;变量来影响产生自己的父进程的环境的。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;位置参数&lt;/h1&gt;

&lt;p&gt;变量是有位置参数的，从命令行传递到脚本的参数: $0, $1, $2, $3 . . .&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$0&lt;/code&gt;就是脚本文件自身的名字，&lt;code&gt;$1&lt;/code&gt;是第一个参数，&lt;code&gt;$2&lt;/code&gt;是第二个参数，&lt;code&gt;$3&lt;/code&gt;是第三个参数，然后是第四个。&lt;code&gt;$9&lt;/code&gt;之后的位置参数就必须用大括号括起来了，比如：&lt;code&gt;${10}, ${11}, ${12}&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;两个比较特殊的变量&lt;code&gt;$*&lt;/code&gt;和&lt;code&gt;$@&lt;/code&gt;表示所有的位置参数。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;{}&lt;/code&gt;标记法提供了一种提取从命令行传递到脚本的最后一个位置参数的简单办法，但是这种方法同时还需要使用&lt;code&gt;间接引用&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;args=$#           # 位置参数的个数.
lastarg=${!args}
# 或:  lastarg=${!#}
# 注意，不能直接使用 lastarg=${!$#} ，这会产生错误
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果脚本需要一个命令行参数，而在调用的时候，这个参数没被提供，那么这就可能造成给这个参数赋一个null变量，通常情况下，这都会产生问题。一种解决这个问题的办法就是使用添加额外字符的方法，在使用这个位置参数的变量和位置参数本身的后边全部添加同样的额外字符。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;variable1_=$1_  # 而不是 variable1=$1
# 这将阻止报错, 即使在调用时没提供这个位置参数

critical_argument01=$variable1_

#使用正则表达式替换_为空，得到输入的变量
variable1=${variable1_/_/}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，一种方法是判断是否存在：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;if [ -z $1 ]
then
    exit $E_MISSING_POS_PARAM
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更好的方法是使用参数替换：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;DefaultVal=xxxx
${1:-$DefaultVal}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;shift&lt;/code&gt;命令会重新分配位置参数，其实就是把所有的位置参数都向左移动一个位置。这样的话，原来的&lt;code&gt;$1&lt;/code&gt;就消失了，但是&lt;code&gt;$0&lt;/code&gt;(脚本名)是不会改变的。如果传递了大量的位置参数到脚本中，那么&lt;code&gt;shift&lt;/code&gt;命令允许你访问的位置参数的数量超过10个，当然&lt;code&gt;{}&lt;/code&gt;标记法也提供了这样的功能。&lt;/p&gt;

&lt;p&gt;下面是使用shift命令的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# 使用&#39;shift&#39;来逐步存取所有的位置参数. 

#  给脚本命个名，比如shft，然后给脚本传递一些位置参数, 比如: 
#          ./shft a b c def 23 skidoo

until [ -z &quot;$1&quot; ]  # 直到所有的位置参数都被存取完...
do
    echo -n &quot;$1 &quot;
    shift
done
 
echo                # 额外的换行.
 
exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，shift命令也可以用在函数当中。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;multiply ()                     # 将乘数作为参数传递进来. 
{                                     # 可以接受多个参数. 
 
local product=1

until [ -z &quot;$1&quot; ]               # 直到处理完所有的参数...
do
    let &quot;product *= $1&quot;
    shift
done

echo $product               #  不会echo到stdout
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;变量替换&lt;/h1&gt;

&lt;p&gt;变量的名字就是变量保存值的地方，引用变量的值就叫做变量替换。&lt;/p&gt;

&lt;p&gt;如果variable1是一个变量的名字，那么$variable1就是引用这变量的值，即这边变量所包含的数据。&lt;/p&gt;

&lt;p&gt;没有&lt;code&gt;$&lt;/code&gt;前缀的时候，变量可能存在如下几种情况：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;变量被声明或被赋值&lt;/li&gt;
  &lt;li&gt;变量被unset&lt;/li&gt;
  &lt;li&gt;变量被export&lt;/li&gt;
  &lt;li&gt;变量代表一种信号&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;变量赋值可以使用&lt;code&gt;=&lt;/code&gt;，也可以在&lt;code&gt;read&lt;/code&gt;命令中或者循环头进行赋值，例如&lt;code&gt;for var2 in 1 2 3&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;注意，&lt;code&gt;$variable&lt;/code&gt;事实上只是&lt;code&gt;${variable}&lt;/code&gt;的简写形式。在某些上下文中&lt;code&gt;$variable&lt;/code&gt;可能会引起错误，这时候你就需要用&lt;code&gt;${variable}&lt;/code&gt;了。&lt;/p&gt;

&lt;p&gt;被一对双引号括起来的变量替换是不会被禁止的，所以双引号被称为部分引用，有时候又被称为”弱引用”。但是，如果使用单引号的话，那么变量替换就会被禁止了，变量名只会被解释成字面的意思，不会发生变量替换，所以单引号被称为全引用，有时候也被称为”强引用”。&lt;/p&gt;

&lt;p&gt;变量赋值和替换，举例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;a=375
hello=$a

echo hello    # 没有变量引用，只是个hello字符串

echo $hello         # 375
echo ${hello}      # 375

echo &quot;$hello&quot;       # 375
echo &quot;${hello}&quot;     # 375

# 全引用的作用将会导致&quot;$&quot;被解释为单独的字符，而不是变量前缀
echo &#39;$hello&#39;          # $hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;引用一个变量将保留其中的空白，但如果是变量替换，就不会保留了&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hello=&quot;A B  C   D&quot;
echo $hello                  # A B C D
echo &quot;$hello&quot;               # A B  C   D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个未初始化的变量是没有值的，但是在做算术操作的时候，这个未初始化的变量看起来值为0。这是一个未文档化(并且可能不具可移植性)的行为。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;$uninitialized&quot;                # (blank line)
let &quot;uninitialized += 5&quot;                # Add 5 to it.
echo &quot;$uninitialized&quot;                # 5
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;变量引用&lt;/h1&gt;

&lt;p&gt;在一个双引号中通过直接使用变量名的方法来引用变量，一般情况下都是没问题的。这么做将阻止所有在引号中的特殊字符被重新解释，包括变量名，但是$、`(后置引用)、和\除外。保留$作为特殊字符的意义是为了能够在双引号中也能够正常的引用变量，也就是说，这个变量将被它的值所取代。&lt;/p&gt;

&lt;p&gt;使用双引号还能够阻止单词分割，如果一个参数被双引号扩起来的话，那么这个参数将认为是一个单元，即使这个参数包含有空白，那里面的单词也不会被分隔开。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hello=&quot;A B  C   D&quot;
echo $hello                  # A B C D
echo &quot;$hello&quot;               # A B  C   D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在echo语句中，只有在单词分割或者需要保留空白的时候，才需要把参数用双引号括起来。&lt;/p&gt;

&lt;p&gt;谈到空白，我们可以通过IFS变量修改默认的空白符，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;var=&quot;&#39;(]\\{}\$\&quot;&quot;

# 下面输出一样
echo $var        # &#39;(]\{}$&quot;
echo &quot;$var&quot;      # &#39;(]\{}$&quot;

IFS=&#39;\&#39;
# 下面输出不一样了，因为这里空白字符定义为\了，变量替换的时候，会将其替换为一个空格
echo $var        # &#39;(] {}$&quot;     
echo &quot;$var&quot;      # &#39;(]\{}$&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;单引号操作与双引号基本一样，但是不允许引用变量，因为$的特殊意义被关闭了。在单引号中，任何特殊字符都按照字面的意思进行解释，除了’,所以说单引号是一种比双引号更严格的引用方法。&lt;/p&gt;

&lt;p&gt;因为即使是转义符在单引号中也是按照字面意思解释的，所以如果想在一对单引号中显示一个单引号是不行的。对于下面的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;Why can&#39;t I write &#39;s between single quotes&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用单引号来引用，可以这样做:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &#39;Why can&#39;\&#39;&#39;t I write &#39;&quot;&#39;&quot;&#39;s between single quotes&#39;   # Why can&#39;t I write &#39;s between single quotes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;三个被单引号引用的字符串，在这三个字符串之间有一个用转义符转义的单引号，和一个用双引号括起来的单引号。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.javachen.com/static/doc/abs-guide/html/index.html&quot;&gt;高级Bash脚本编程指南-中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/07/07/bash-variable.html</link>
      <guid>http://blog.javachen.com/2015/07/07/bash-variable.html</guid>
      <pubDate>2015-07-07T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Bash中的特殊字符</title>
      <description>&lt;p&gt;Bash中，用在脚本和其他地方的字符叫做特殊字符。下面依次举例介绍每个字符的用途。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;&lt;code&gt;#&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;行首以&lt;code&gt;#&lt;/code&gt;(&lt;code&gt;#!&lt;/code&gt;是个例外)开头是注释。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# This line is a comment.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注释也可以放在于本行命令的后边。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;A comment will follow.&quot;   # 注释在这里。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;命令是不能放在同一行上注释的后边的。因为没有办法把注释结束掉，好让同一行上后边的”代码生效”，只能够另起一行来使用下一个命令。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在echo中转义的&lt;code&gt;#&lt;/code&gt;是不能作为注释的，同样也可以出现在特定的参数替换结构中，或者是出现在数字常量表达式中。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;The # here does not begin a comment.&quot; 
echo &#39;The # here does not begin a comment.&#39; 
echo The \# here does not begin a comment.    #转义字符\
echo The # 这里开始一个注释.
echo ${PATH#*:}   # 参数替换, 不是一个注释
echo $(( 2#101011 ))   # 数制转换, 不是一个注释
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;标准的引用和转义字符&lt;code&gt;&quot; &#39; \&lt;/code&gt;可以用来转义&lt;code&gt;#&lt;/code&gt;，某些特定的模式匹配操作也可以使用&lt;code&gt;#&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;&lt;code&gt;;&lt;/code&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;分号作为命令行分隔符，可以在同一行上写两个或两个以上的命令。

echo hello; echo there

if [ -x &quot;$filename&quot; ]; then
    echo &quot;File $filename exists.&quot;;
else
    echo &quot;File $filename not found.&quot;; touch $filename 
fi; echo &quot;File test complete.&quot;    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在某些情况下，&lt;code&gt;; &lt;/code&gt;也可以被转义。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;&lt;code&gt;;;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;终止case选项。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;case &quot;$variable&quot; in
    abc) echo &quot;\$variable = abc&quot; ;; 
    xyz) echo &quot;\$variable = xyz&quot; ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;&lt;code&gt;.&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;点命令等价于&lt;code&gt;source&lt;/code&gt;命令，这是一个bash的内建命令。&lt;/p&gt;

&lt;p&gt;如果点放在文件名的开头的话，那么这个文件将会成为”隐藏”文件，并且&lt;code&gt;ls&lt;/code&gt;命令将不会正常的显示出这个文件。&lt;/p&gt;

&lt;p&gt;如果作为目录名的话，一个单独的点代表当前的工作目录，而两个点表示上一级目录。&lt;/p&gt;

&lt;p&gt;点也可以表示当前目录。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp /home/javachen/current_work/* .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当用作匹配字符的作用时，通常都是作为正则表达式的一部分来使用，点用来匹配任何的单个字符。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;&lt;code&gt;&quot;&lt;/code&gt;,&lt;code&gt;&#39;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;双引号为部分引用，单引号为全引用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;The # here does not begin a comment.&quot; 
echo &#39;The # here does not begin a comment.&#39; 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;&lt;code&gt;,&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;逗号操作费，链接了一系列的算术操作。 虽然里边所有的内容都被运行了，但只有最后一项被返回。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;let &quot;t2 = ((a = 9, 15 / 3))&quot; # a = 9   t2 = 15 / 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以这样使用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir -p {a,b,c}  #创建三个目录a、b、c
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;&lt;code&gt;\&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;转义符，一种对单字符的引用机制，通常是用于对单字符进行转义。&lt;code&gt;\&lt;/code&gt;通常用来转义&lt;code&gt;&quot;&lt;/code&gt;和&lt;code&gt;&#39;&lt;/code&gt;，这样双引号和但引号就不会被解释成特殊含义了。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;&lt;code&gt;/&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;文件名路径分隔符，分隔文件名不同的部分，也可以用来作为除法算术操作符。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/home/bozo/projects/Makefile
let &quot;t2 = ((a = 9, 15 / 3))&quot; # a = 9   t2 = 15 / 3
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-8&quot;&gt;`&lt;/h2&gt;

&lt;p&gt;命令替换，&lt;code&gt;command&lt;/code&gt;结构可以将命令的输出赋值到一个变量中去。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;date=`date`
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-9&quot;&gt;&lt;code&gt;:&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;空命令，等价于”NOP”，什么都不做，也可以被认为与shell的内建命令true作用相同。”:”命令是一个bash的内建命令，它的退出码是”true”，即为0。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;:
echo $? # 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;死循环：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;while :   # while true
do
    date
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 if/then 中的占位符，什么都不做，引出分支。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;if condition
then :          # 什么都不做,引出分支. 
else
    take-some-action
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在一个二元命令中提供一个占位符。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;n=1
: $((n = $n + 1))  # 如果没有&quot;:&quot;的话，Bash 将会尝试把 $((n = $n + 1)) 解释为一个命令，运行时会报错
echo -n &quot;$n &quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在here document中提供一个命令所需的占位符或者用于注释代码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;: &amp;lt;&amp;lt;TESTVARIABLES
${HOSTNAME?}${USER?}${MAIL?} # 如果其中某个变量没被设置, 那么就打印错误信息. 
TESTVARIABLES


: &amp;lt;&amp;lt;COMMENTBLOCK
echo &quot;This line will not echo.&quot;
This is a comment line missing the &quot;#&quot; prefix.
This is another comment line missing the &quot;#&quot; prefix.
&amp;amp;*@!!++=
The above line will cause no error message,
because the Bash interpreter will ignore it.
COMMENTBLOCK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用参数替换来评估字符串变量。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;: ${HOSTNAME ?} ${USER?} ${MAIL?}  #如果一个或多个必要的环境变量没被设置的话，就打印错误信息.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在与&lt;code&gt;&amp;gt;&lt;/code&gt;重定向操作符结合使用时，将会把一个文件清空，但是并不会修改这个文件的权限。如果之前这个文件并不存在，那么就创建这个文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;: &amp;gt; data.xxx  # 文件&quot;data.xxx&quot; 现在被清空了
# 与 cat /dev/null &amp;gt;data.xxx 的作用相同
# 然而，这并不会产生一个新的进程，因为&quot;:&quot;是一个内建命令
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在与&lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt;重定向操作符结合使用时，将不会对预先存在的目标文件产生任何影响。如果这个文件之前并不存在，那么就创建它。&lt;code&gt;这只适用于正规文件,，而不适用于管道、符号连接和某些特殊文件&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;: &amp;gt;&amp;gt; target_file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可能用来作为注释行，虽然我们不推荐这么做。使用#来注释的话，将关闭剩余行的错误检查，所以可以在注释行中写任何东西。然而，使用:的话将不会这样。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;: This is a comment that generates an error, ( if [ $x -eq 3] fi ).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;”:”还用来在/etc/passwd和&lt;code&gt;$PATH&lt;/code&gt;变量中做分隔符。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo $PATH
/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-10&quot;&gt;&lt;code&gt;!&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;取反操作符。&lt;code&gt;! &lt;/code&gt;操作符将会反转命令的退出码的结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;true        # &quot;true&quot; 是内建命令
echo &quot;exit status of \&quot;true\&quot; = $?&quot;         # 0

! true
echo &quot;exit status of \&quot;! true\&quot; = $?&quot;       # 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果一个命令以&lt;code&gt;!&lt;/code&gt;开头，那么会启用Bash的历史机制。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;true
!true
# 这次就没有错误了, 也没有反转结果.它只是重复了之前的命令(true).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在另一种上下文中，如命令行模式下，&lt;code&gt;!&lt;/code&gt;还能反转bash的历史机制。需要注意 的是，在一个脚本中，历史机制是􏰌􏱆被􏱆禁用的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ history |head -n 10
   18  date
   19  ls
   20  cd
   21  pwd
   22  jps
   23  java
   24  ll
   25  ps
   26  history

#执行bash历史中的一条命令
$ !25

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;! &lt;/code&gt;操作符还是Bash的关键字。&lt;/p&gt;

&lt;p&gt;在一个不同的上下文中，&lt;code&gt;! &lt;/code&gt;也会出现在变量的&lt;code&gt;间接引用&lt;/code&gt;中。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;a=letter_of_alphabet
letter_of_alphabet=z

# 直接引用.
echo &quot;a = $a&quot;   # a = letter_of_alphabet

# 间接引用.
eval a=\$$a 
echo &quot;Now a=$a&quot;    #Now a = z

# 间接引用.
echo ${!a}  # a = z
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-11&quot;&gt;&lt;code&gt;*&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;用来做文件名匹配：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo *
abs-book.sgml add-drive.sh agram.sh alias.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以用在正则表达式中，用来匹配任意个数(包含0个)的字符。&lt;/p&gt;

&lt;p&gt;在算术操作符的上下文中， &lt;code&gt;*&lt;/code&gt;号表示􏱉乘法运算。如果要做􏱊􏱋幂运算，使用&lt;code&gt;**&lt;/code&gt;，这是􏱊􏱋求幂操作符。&lt;/p&gt;

&lt;h2 id=&quot;section-12&quot;&gt;&lt;code&gt;？&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;测试操作符。在一个特定的表达式中，&lt;code&gt;?&lt;/code&gt;用来测试一个条件的结果。&lt;/p&gt;

&lt;p&gt;在一个双括号结构中，&lt;code&gt;?&lt;/code&gt;就是C语言的三元操作符。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;(( t = a&amp;lt;45?7:11 )) # C语言风格的三元操作
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在参数替换表达式中，&lt;code&gt;?&lt;/code&gt;用来测试一个变量是否􏰌set􏰝。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$􏱯parameter?err􏱸msg􏱰   #如果parameter􏱶􏰫􏰌􏱴􏱵􏱶􏰫􏰌􏱴􏱵已经被声明，那么就使用声明的值，否则打印err􏱸msg 错误消息.
$􏱯parameter:?err􏱸msg􏱰  #如果parameter􏱶􏰫􏰌已经被设置，那么就使用设置的值，否则打印 err􏱸msg错误消息.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在通配中，用来做匹配单个字符的”通配符”，在正则表达式中，也是用来表示一个字符。&lt;/p&gt;

&lt;h2 id=&quot;section-13&quot;&gt;&lt;code&gt;$&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;在&lt;code&gt;变量替换&lt;/code&gt;中，用于引用变量的内容。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;var1=5
echo $var1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在一个变量前面加上&lt;code&gt;$&lt;/code&gt;用来引用这个变量的值。&lt;/p&gt;

&lt;p&gt;在正则表达式中，表示行结束符。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;${}&lt;/code&gt; 是参数替换，&lt;code&gt;$*&lt;/code&gt;, &lt;code&gt;$@&lt;/code&gt;是位置参数，&lt;code&gt;$?&lt;/code&gt; 是退出状态码变量，&lt;code&gt;$$&lt;/code&gt;是进程id变量，保存􏰝􏰈所在脚本的进程 ID。&lt;/p&gt;

&lt;h2 id=&quot;section-14&quot;&gt;&lt;code&gt;()&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;命令组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;(a=hello; echo $a)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;在括号中的命令列表，将会作为一个子shell来运行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;例外: 在pipe中的一个大括号中的代码段可能运行在一个 子shell中。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ls | { read firstline; read secondline; }

#错误. 在大括号中的代码段, 将运行到子shell中, 所以&quot;ls&quot;的输出将不能传递到代码块中
echo &quot;First line is $firstline; second line is $secondline&quot;  # 不能工作
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化数组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;Array=(element1 element2 element3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;xxxyyyzzz&quot;&gt;&lt;code&gt;􏱯{xxx,yyy,zzz,...􏱰}&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;大括号扩展：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cat {file1,file2,file3} &amp;gt; combined_file

cp file22.{txt,backup} # 拷贝&quot;file22.txt&quot;到&quot;file22.backup&quot;中
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个命令可能会对大括号中的以􏰮号分􏱱的文件列表起作用。在通配符中，将对大括号中的文件名做扩展。&lt;/p&gt;

&lt;p&gt;在大括号中，不允许有空白，除非这个空白􏰌引用或转义。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo {file1,file2}\ :{\ A,&quot; B&quot;,&#39; C&#39;}
file1 : A file1 : B file1 : C file2 : A file2 : B file2 : C
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-15&quot;&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;代码块，又被称为内部组，这个结构事实上创建􏰝一个匿名函数。与”标准”函数不同的是，在其中􏱴􏱵的变量，对于脚本其他部分的代码来􏱟还是可见的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;{ local a; a=1; }
-bash: local: can only be used in a function
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;a=123
{ a=321; }
echo &quot;a = $a&quot;  # a = 321 (说明在代码块中对变量a所作的修改影响了外边的变量)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下边的代码展示􏰝在大括号结构中代码的I/O 重定向。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# 从/etc/fstab中读行. 3
File=/etc/fstab

{
 read line1
 read line2
} &amp;lt; $File

echo &quot;First line in $File is:&quot; &quot;$line1&quot;
echo &quot;Second line in $File is:&quot; &quot;$line2&quot;
exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与上面所讲到的&lt;code&gt;()&lt;/code&gt;中的命令组不同的是，大括号􏱰中的代码块将不会开􏲇一个新的子shell。&lt;/p&gt;

&lt;h2 id=&quot;section-16&quot;&gt;􏱯&lt;code&gt;&#39;{}&#39; \;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;􏰯􏰰路径名。一般都在&lt;code&gt;find&lt;/code&gt;命令中使用，这不是一个shell内建命令。&lt;code&gt;;&lt;/code&gt;用来结束find命令序列的&lt;code&gt;-exec&lt;/code&gt;选项，它需要􏰌􏰌被保护以􏲈􏰧􏰌防止被shell所解释。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;find . -mtime -1 -type f -exec tar rvf archive.tar &#39;{}&#39; \;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-17&quot;&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;条件测试。&lt;/p&gt;

&lt;p&gt;在一个array结构的上下文中，中括号用来引用数组中每个元素的编号。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;Array[1]=a 
echo ${Array[1]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用作正则表达式的一部分，方括号描􏲁一个匹配的字符范围。例如，正则表达式中，”[xyz]” 将会匹配字符x, y, 或z。&lt;/p&gt;

&lt;h2 id=&quot;section-18&quot;&gt;&lt;code&gt;[[ ]] &lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;测试表达式􏰞在&lt;code&gt;[[ ]]&lt;/code&gt;中。&lt;/p&gt;

&lt;h2 id=&quot;section-19&quot;&gt;&lt;code&gt;(( ))&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;双圆括号结构，扩展并计算在&lt;code&gt;(( ))&lt;/code&gt;中的整数表达式。&lt;/p&gt;

&lt;p&gt;与let命令很相􏱠，(&lt;code&gt;(...))&lt;/code&gt;结构允许算术扩展和赋值。举个简单的例子，&lt;code&gt;a=$(( 5 + 3 ))&lt;/code&gt;，将把变量”a”设为”5 + 3”或者8。&lt;/p&gt;

&lt;h2 id=&quot;section-20&quot;&gt;&lt;code&gt;&amp;gt;&lt;/code&gt; &lt;code&gt;&amp;amp;&amp;gt;&lt;/code&gt; &lt;code&gt;&amp;gt;&amp;amp;&lt;/code&gt; &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; &lt;code&gt;&amp;lt;&lt;/code&gt; &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;重定向。&lt;/p&gt;

&lt;p&gt;重定向scriptname的输出到文件filename中。如果filename存在的􏰩，那么将会􏰌􏲋􏲌被覆盖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;scriptname &amp;gt;filename 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以清空文件内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;&amp;gt; a.log 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重定向command的stdout和stderr到filename中:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;command &amp;amp;&amp;gt;filename
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重定向command的stdout到stderr中:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;command &amp;gt;&amp;amp;2 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把scriptname的输出􏱜加到文件filename中。如果filename不存在的􏰩话，将被创建。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;scriptname &amp;gt;&amp;gt; filename 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开文件filename用来读写，并􏰄分配文件描􏲁符i给这个文件。如果filename不存在，这个文件将会􏰌创建：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[i]&amp;lt;&amp;gt;filename 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;”&amp;lt;”或”&amp;gt;”还可以用于&lt;code&gt;进程替换&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在一种不同的上下文中，”&amp;lt;”和”&amp;gt;”可用来做字符串比较操作或者整数比较。&lt;/p&gt;

&lt;h2 id=&quot;section-21&quot;&gt;&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;用在here document中的重定向。&lt;/p&gt;

&lt;h2 id=&quot;section-22&quot;&gt;&lt;code&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;用在here string中的重定向。&lt;/p&gt;

&lt;h2 id=&quot;section-23&quot;&gt;􏰢􏲄&lt;code&gt;\&amp;lt;&lt;/code&gt;,&lt;code&gt;\&amp;gt;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;正则表达式中的单词边􏲃：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;grep &#39;\&amp;lt;the\&amp;gt;&#39; textfile
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-24&quot;&gt;&lt;code&gt;|&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;管道。分析前边命令的输出，并将输出作为后边命令的输入。这是一种产生命令链的好方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 与一个简单的&quot;ls -l&quot;结果相同
echo ls -l | sh

# 合并和排序所有的&quot;.lst&quot;文件, 然后删除所有重复的行.
cat *.lst | sort | uniq
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出的命令也可以传递到脚本中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# uppercase.sh : 修改输入, 全部转换为大写

tr &#39;a-z&#39; &#39;A-Z&#39;

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在􏰠我􏰋输送&lt;code&gt;ls -l&lt;/code&gt;的输出到该脚本中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ls -l | ./uppercase.sh
DRWXR-X--- 2 ROOT ROOT      4096 05-28 10:06 ML-1M
-RW-R--R-- 1 ROOT ROOT         0 2014-11-21 MONITOR_DISK.TXT
DRWXR-XR-X 2 ROOT ROOT      4096 2014-11-21 SCRIPT
-RW-R--R-- 1 ROOT ROOT        16 07-06 16:09 UPPERCASE.SH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;管道中的每个进程的stdout比􏰗􏰌下一个进程作为stdin来读入，否则，数据流会􏰬􏲠阻塞，并且管道将产生一些非预期的行为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 从&quot;cat file1 file2&quot;中的输出并没出现
cat file1 file2 | ls -l | sort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;作为子进程的运行的管道，不能够改变脚本的变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;variable=&quot;initial_value&quot;
echo &quot;new_value&quot; | read variable
echo &quot;variable = $variable&quot; # variable =initial_value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果管道中的􏰣个命令产生􏰝一个异常，并中途失败，那么这个管道将过早的终止，这种行为􏰌􏱈叫做broken pipe，并􏰄这种状态下将发送一个&lt;code&gt;SIGPIPE&lt;/code&gt;信号。&lt;/p&gt;

&lt;h2 id=&quot;section-25&quot;&gt;&lt;code&gt;&amp;gt;|&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;强制重定向(􏰥使设置􏰝&lt;code&gt;noclobber&lt;/code&gt;选项，就是&lt;code&gt;-C&lt;/code&gt;选项)，这将强制的􏲋􏲌覆盖一个现存文件。&lt;/p&gt;

&lt;h2 id=&quot;section-26&quot;&gt;&lt;code&gt;||&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;或操作，在一个条件测试结构中，如果条件测试结构两边中的任意一边结果为true的􏰩，&lt;code&gt;||&lt;/code&gt;操作就会返回0(代表􏰔行成功)。&lt;/p&gt;

&lt;h2 id=&quot;section-27&quot;&gt;&lt;code&gt;&amp;amp;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;后台运行命令，一个命令后边跟一个&lt;code&gt;&amp;amp;&lt;/code&gt;表示在后台运行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sleep 10 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在一个脚本中，命令和循环都可能运行在后台：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for i in1 2 3 4 5 6 7 8 9 10
do
    echo -n &quot;$i &quot;
done &amp;amp; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在一个脚本中，使用后台运行命令可能会使这个脚本挂起，直到敲&lt;code&gt;ENTER&lt;/code&gt;键，挂起的脚本才会恢复。看起来只有在这个命令的结果需要输出到stdout的时􏰀，这种现象􏱒才会出现。&lt;/p&gt;

&lt;p&gt;只要在后台运行命令的后边加上一个&lt;code&gt;wait&lt;/code&gt;命令就会解􏰕这个问题。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash 
# test.sh

ls -l &amp;amp;
echo &quot;Done.&quot;
wait
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果将后台运行命令的输出重定向到文件中或&lt;code&gt;/dev/null&lt;/code&gt;中，也能解􏰕这个 问题。&lt;/p&gt;

&lt;h2 id=&quot;section-28&quot;&gt;&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;与逻辑操作。在一个条件测试结构中，只有在条件测试结构的两边结果都为true的时􏰀，&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;操作才会返回0(代表sucess)。&lt;/p&gt;

&lt;h2 id=&quot;section-29&quot;&gt;&lt;code&gt;-&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;选项，前缀。在所有的命令内如果􏱩使用选项参数的􏰩话，前边都要加上&lt;code&gt;-&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ls -al

sort -dfu $filename

set -- $variable

if [ $file1 -ot $file2 ]
then
    echo &quot;File $file1 is older than $file2.&quot; 
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用于重定向stdin或stdout：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 从一个目录移动整个目录树到另一个目录
(cd /source/directory &amp;amp;&amp;amp; tar cf - . ) | (cd /dest/directory &amp;amp;&amp;amp; tar xpvf -)

# 当然也可以这样写：
cp -a /source/directory/* /dest/directory
cp -a /source/directory/* /source/directory/.[^.]* /dest/directory  # 如果在/source/directory中有隐藏文件的话

bunzip2 -c linux-2.6.16.tar.bz2 | tar xvf -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，在这个上下文中，&lt;code&gt;-&lt;/code&gt;本身并不是一个Bash操作，而是一个可以􏰌被特定的UNIX工具识别的选项，这些特定的UNIX工具特指那些可以写输出到stdout的工具，比如tar、cat等等。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo &quot;whatever&quot; | cat -
whatever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用diff命令来和另一个文件的􏰣一段进行比较：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;grep Linux file1 | diff file2 -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个更真实的例子是备份最后一天所有修改的文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

BACKUPFILE=backup-$(date +%m-%d-%Y)

# 如果在命令行中没有指定备份文件的文件名，那么将默认使用&quot;backup-MM-DD-YYYY.tar.gz&quot;
archive=${1:-$BACKUPFILE}

tar cvf - `find . -mtime -1 -type f -print` &amp;gt; $archive.tar

# 还有两种简单写法：
# find . -mtime -1 -type f -print0 | xargs -0 tar rvf  &quot;$archive.tar&quot;
# find . -mtime -1 -type f -exec tar rvf &quot;$archive.tar&quot; &#39;{}&#39; \;

gzip $archive.tar

echo &quot;Directory $PWD backed up in archive file \&quot;$archive.tar.gz\&quot;.&quot;

exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;-&lt;/code&gt;还可以用来指􏰼先前的工作目录，&lt;code&gt;cd -&lt;/code&gt;将会回到􏰼前的工作目录，它使用􏰝了 &lt;code&gt;$OLDPWD&lt;/code&gt; 环境变量。&lt;/p&gt;

&lt;p&gt;另外，还可以当做减号来使用。&lt;/p&gt;

&lt;h2 id=&quot;section-30&quot;&gt;&lt;code&gt;=&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;等号，赋值操作。&lt;/p&gt;

&lt;h2 id=&quot;section-31&quot;&gt;&lt;code&gt;+&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;加号，也可以用在正则表达式中。􏰣某些内建命令使用&lt;code&gt;+&lt;/code&gt;来打开特定的选项，用&lt;code&gt;-&lt;/code&gt;来􏱆用这些特定的选项。&lt;/p&gt;

&lt;h2 id=&quot;section-32&quot;&gt;&lt;code&gt;%&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;取模操作，也可以用于正则表达式。&lt;/p&gt;

&lt;h2 id=&quot;section-33&quot;&gt;&lt;code&gt;~&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;home目录。&lt;/p&gt;

&lt;h2 id=&quot;section-34&quot;&gt;&lt;code&gt;~+&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;当前工作目录，相当于&lt;code&gt;$PWD&lt;/code&gt;内部变量。&lt;/p&gt;

&lt;h2 id=&quot;section-35&quot;&gt;􏲱&lt;code&gt;~-&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;􏰼前的工作目录，相当于&lt;code&gt;$OLDPWD&lt;/code&gt;内部变量。&lt;/p&gt;

&lt;h2 id=&quot;section-36&quot;&gt;&lt;code&gt;^&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;行首，在正则表达式中，&lt;code&gt;^&lt;/code&gt;表示定位到文本行的行首。&lt;/p&gt;

&lt;h2 id=&quot;section-37&quot;&gt;控制字符&lt;/h2&gt;

&lt;p&gt;修改终端或文本显示的行为。控制字符以&lt;code&gt;CONTROL + key&lt;/code&gt;这种方式进行组合(同时按下)。控制字符也可以使用8进制或16进制表示法来进行表示，但是前边必须要加上转义符。&lt;/p&gt;

&lt;p&gt;控制字符比较多，这里不一一列出了。&lt;/p&gt;

&lt;h2 id=&quot;section-38&quot;&gt;空白&lt;/h2&gt;

&lt;p&gt;用来分隔函数，命令或变量。空白包含空格、tab、空行，或者是它们之间任意的组合体。在某些上下文中，比如变量赋值，空白是不被允许的，会产生语法错误。&lt;/p&gt;

&lt;p&gt;空行不会影响脚本的行为，因此使用空行可以很好的划分独立的函数段以增加可读性。&lt;/p&gt;

&lt;p&gt;特殊变量&lt;code&gt;$IFS&lt;/code&gt;用来做一些输入命令的分隔符，默认情况下是空白。&lt;/p&gt;

&lt;p&gt;如果想在字符串或变量中使用空白，那么应该使用引用。例如下面例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hello=&quot;A B  C   D&quot;
echo $hello                  # A B C D
echo &quot;$hello&quot;               # A B  C   D
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-39&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.javachen.com/static/doc/abs-guide/html/index.html&quot;&gt;高级Bash脚本编程指南-中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/07/06/bash-special-characters.html</link>
      <guid>http://blog.javachen.com/2015/07/06/bash-special-characters.html</guid>
      <pubDate>2015-07-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Jekyll kramdown配置</title>
      <description>&lt;p&gt;之前博客是使用的redcarpet的markdown语法，其在_config.yml中的配置方式为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;markdown: redcarpet
redcarpet:
    extensions: [ &quot;fenced_code_blocks&quot;, &quot;hard_wrap&quot;,&quot;autolink&quot;, &quot;tables&quot;, &quot;strikethrough&quot;, &quot;superscript&quot;, &quot;with_toc_data&quot;, &quot;highlight&quot;, &quot;prettify&quot;,&quot;no_intra_emphasis&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种配置支持使用 ``` 高亮代码块、自动链接、表格等特性。&lt;/p&gt;

&lt;p&gt;现在，想尝试使用karkdown的语法。kramdown是一个Markdown解析器，它能够正确解释公式内部的符号，不会与Markdown语法冲突，比如不会将^符号变成&lt;sup&gt;&lt;/sup&gt;标签。&lt;/p&gt;

&lt;p&gt;kramdown支持MathJax，见&lt;a href=&quot;http://www.pkuwwt.tk/linux/2013-12-03-jekyll-using-mathjax/&quot;&gt;Jekyll中使用MathJax&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;kramdown默认是支持TOC，你可以进一步设置TOC相关的参数，见 &lt;a href=&quot;http://loudou.info/blog/2014/08/01/wei-octopress-tian-jia-toc/&quot;&gt;为 Octopress 添加 TOC&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;安装kramdown：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gem install kramdown
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在_config.yml中的配置方式为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;markdown: kramdown
kramdown:
  input:  GFM
  use_coderay: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在编写文章时，插入下面代码，渲染之后就可以生成TOC了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;* TOC
{:toc}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;krmadown支持和github一样的语法高亮，用三个 ```，但是需要安装coderay，而github pages上不支持coderay，所以该方式无法搞定，可行的解决方法是上传本地编译好的html。如果是本地或者自己的空间，可以安装coderay。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gem install coderay
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 ``` 引用代码块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class AdView (object):
    def __init__ (self, name = None):
        self.name = name

    def test (self):
        if self.name == &#39;admin&#39;:
            return False
        else
            return True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多语法，见&lt;a href=&quot;http://blog.will6run.com/tool/2014/11/22/kramdown/&quot;&gt;kramdown语法小记&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;最后的配置为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;kramdown:
  input: GFM
  extensions:
    - autolink
    - footnotes
    - smart
  use_coderay: true
  syntax_highlighter: rouge
  coderay:
    coderay_line_numbers:  nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;coderay支持的语言有限，并且rouge兼容Pygments，故这里使用rouge：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gem install rouge
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://noyobo.com/2014/10/19/jekyll-kramdown-highlight.html&quot;&gt;jekyll kramdown 语法高亮配置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.quts.me/2015/03/05/kramdown-highlight.html&quot;&gt;jekyll kramdown 语法高亮&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://shengmingzhiqing.com/blog/octopress-lean-modification-4.html/&quot;&gt;Octopress 精益修改 (4)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pikipity.github.io/blog/kramdown-syntax-chinese-1.html&quot;&gt;Kramdown 语法文档翻译（一）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.pchou.info/open-source/2014/07/07/something-about-markdown.html&quot;&gt;Markdown的各种扩展&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/06/30/jekyll-kramdown-config.html</link>
      <guid>http://blog.javachen.com/2015/06/30/jekyll-kramdown-config.html</guid>
      <pubDate>2015-06-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>高级Bash脚本编程入门</title>
      <description>&lt;p&gt;最近在看《Advanced Bash Scripting Guide》这本书，第二章举了一个清除日志的例子，来讲述如何使用Bash进行编程并聊到了一些编程规范。本文主要是基于这部分内容记录我的读书笔记并整理一些相关知识点。&lt;/p&gt;

&lt;p&gt;说到清除日志，你可以使用下面命令来完成清除/var/log下的log文件这件事情：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd /var/log
cat /dev/null &amp;gt; messages 
cat /dev/null &amp;gt; wtmp
echo &quot;Logs cleaned up.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更简单的清除日志方法是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;echo &quot;&quot; &amp;gt;messages 
#或者
&amp;gt;messages 
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;br /&gt;
/var/log/messages 记录系统报错信息&lt;br /&gt;
/var/log/wtmp 记录系统登录信息&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在Bash编程时，脚本通常都是放到一个文件里面，该文件可以有后缀名也可以没有，例如，你可以将该文件命名为cleanlog，然后在文件头声明一个命令解释器，这里是&lt;code&gt;#!/bin/bash&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
LOG_DIR=/var/log
cd $LOG_DIR
cat /dev/null &amp;gt; messages
cat /dev/null &amp;gt; wtmp
echo &quot;Logs cleaned up.&quot;
exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，还可以使用其他的命令行解释器，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/sh
#!/bin/bash
#!/usr/bin/perl 
#!/usr/bin/tcl 
#!/bin/sed -f
#!/usr/awk -f

#自删除脚本
#!/bin/rm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;#!&lt;/code&gt; 后面的路径必须真实存在，否则运行时会提示&lt;code&gt;Command not found的错误&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;在UNIX系统中，在&lt;code&gt;!&lt;/code&gt;后边需要一个空格。&lt;/li&gt;
  &lt;li&gt;如果脚本中还包含有其他的&lt;code&gt;#!&lt;/code&gt;行，那么bash将会把它看成是一个一般的注释行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面代码将/var/log定义为变量，这样会比把代码写死好很多，因为如果你可能想修改为其他目录，只需要修改变量的值就可以。&lt;/p&gt;

&lt;p&gt;对于/var/log目录，一般用户没有访问权限，故需要使用root用户来运行上面脚本，另外，用户不一定有修改目录的权限，所以需要增强代码，做一些判断。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

LOG_DIR=/var/log
ROOT_UID=0
LINES=50
E_XCD=66
E_NOTROOT=67

# 当然要使用root 用户来运行.
if [ &quot;$UID&quot; -ne &quot;$ROOT_UID&quot; ]
then
    echo &quot;Must be root to run this script.&quot;
    exit $E_NOTROOT
fi

cd $LOG_DIR

if[ &quot;$PWD&quot; != &quot;$LOG_DIR&quot; ]
then
    echo &quot;Can&#39;t change to $LOG_DIR.&quot;
    exit $E_XCD
fi

cat /dev/null &amp;gt; messages
cat /dev/null &amp;gt; wtmp
echo &quot;Logs cleaned up.&quot;

#返回0表示成功
exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码一样定义了一些变量，然后加了两个判断，去检查脚本运行中可能出现的错误并打印错误说明。如果脚本运行错误，则程序会退出并返回一个错误码，不同类型的错误对应的错误码不一样，这样便于识别错误原因；如果脚本运行正常，则正常退出，默认返回码为0。&lt;/p&gt;

&lt;p&gt;对于&lt;code&gt;cd $LOG_DIR&lt;/code&gt;操作判断是否执行成功，更有效的做法是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#使用或操作替代if else判断
cd /var/log || {
    echo &quot;Cannot change to necessary directory.&quot; &amp;gt;&amp;amp;2
    exit $E_XCD
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常，我们可能不想全部清除日志，而是保留最后几行日志，这样就需要给脚本传入参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

LOG_DIR=/var/log
ROOT_UID=0
LINES=50
E_XCD=66
E_NOTROOT=67

# 当然要使用root 用户来运行.
if [ &quot;$UID&quot; -ne &quot;$ROOT_UID&quot; ]
then
    echo &quot;Must be root to run this script.&quot;
    exit $E_NOTROOT
fi

cd $LOG_DIR

if[ &quot;$PWD&quot; != &quot;$LOG_DIR&quot; ]
then
    echo &quot;Can&#39;t change to $LOG_DIR.&quot;
    exit $E_XCD
fi

# 测试是否有命令行参数，非空判断
if [ -n &quot;$1&quot; ]
then
    lines=$1
else
    lines=$LINES # 默认，如果不在命令行中指定
fi

# 保存log file消息的最后部分
tail -$lines messages &amp;gt; mesg.temp
mv mesg.temp messages

cat /dev/null &amp;gt; wtmp
echo &quot;Logs cleaned up.&quot;

#返回0表示成功
exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面使用if else来判断是否有输入参数，一个更好的检测命令行参数的方式是使用正则表达式做判断，以检查输入参数的合法性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;E_WRONGARGS=65 # 非数值参数(错误的参数格式)

case &quot;$1&quot; in
    &quot;&quot; ) lines=50;;
    *[!0-9]*) echo &quot;Usage: `basename $0` file-to-cleanup&quot;; exit $E_WRONGARGS;; 
    * ) lines=$1;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编写完脚本之后，你可以使用&lt;code&gt;sh scriptname&lt;/code&gt;或者&lt;code&gt;bash scriptname&lt;/code&gt;来调用这个脚本。不推荐使用&lt;code&gt;sh &amp;lt;scriptname&lt;/code&gt;，因为这禁用了脚本从stdin中读数据的功能。更方便的方法是让脚本本身就具有 可执行权限，通过&lt;code&gt;chmod&lt;/code&gt;命令可以修改。比如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;chmod 555 scriptname  #允许任何人都具有可读和执行权限
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;chmod +rx scriptname #允许任何人都具有可读和执行权限 
chmod u+rx scriptname #只给脚本的所有者可读和执行权限
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;既然脚本已经具有了可执行权限，现在你可以使用&lt;code&gt;./scriptname&lt;/code&gt;来测试这个脚本了。如果这个脚本以一个&lt;code&gt;#!&lt;/code&gt;行开头，那么脚本将会调用合适的命令解释器来运行。&lt;/p&gt;

&lt;p&gt;这样一个简单的脚本就编写完成并能运行了，从这个例子中，我们可以学到bash编程的一些代码规范：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用变量&lt;/li&gt;
  &lt;li&gt;脚本运行中，需要做一些异常判断&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，google公司还定义了一份&lt;a href=&quot;https://google-styleguide.googlecode.com/svn/trunk/shell.xml&quot;&gt;Shell Style Guide&lt;/a&gt;，可以仔细阅读并约束自己去遵循这些规范。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.javachen.com/static/doc/abs-guide/html/index.html&quot;&gt;高级Bash脚本编程指南-中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/06/29/advanced-bash-script-programming.html</link>
      <guid>http://blog.javachen.com/2015/06/29/advanced-bash-script-programming.html</guid>
      <pubDate>2015-06-29T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>spark-shell脚本分析</title>
      <description>&lt;p&gt;本文主要分析spark-shell脚本的运行逻辑，涉及到spark-submit、spark-class等脚本的分析，希望通过分析脚本以了解spark中各个进程的参数、JVM参数和内存大小如何设置。&lt;/p&gt;

&lt;h1 id=&quot;spark-shell&quot;&gt;spark-shell&lt;/h1&gt;

&lt;p&gt;使用yum安装spark之后，你可以直接在终端运行spark-shell命令，或者在spark的home目录/usr/lib/spark下运行bin/spark-shell命令，这样就可以进入到spark命令行交互模式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;spark-shell 脚本是如何运行的呢&lt;/strong&gt;？该脚本代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#
# Shell script for starting the Spark Shell REPL

cygwin=false
case &quot;`uname`&quot; in
  CYGWIN*) cygwin=true;;
esac

# Enter posix mode for bash
set -o posix

## Global script variables
FWDIR=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;

function usage() {
  echo &quot;Usage: ./bin/spark-shell [options]&quot;
  &quot;$FWDIR&quot;/bin/spark-submit --help 2&amp;gt;&amp;amp;1 | grep -v Usage 1&amp;gt;&amp;amp;2
  exit 0
}

if [[ &quot;$@&quot; = *--help ]] || [[ &quot;$@&quot; = *-h ]]; then
  usage
fi

source &quot;$FWDIR&quot;/bin/utils.sh
SUBMIT_USAGE_FUNCTION=usage
gatherSparkSubmitOpts &quot;$@&quot;

# SPARK-4161: scala does not assume use of the java classpath,
# so we need to add the &quot;-Dscala.usejavacp=true&quot; flag mnually. We
# do this specifically for the Spark shell because the scala REPL
# has its own class loader, and any additional classpath specified
# through spark.driver.extraClassPath is not automatically propagated.
SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true&quot;

function main() {
  if $cygwin; then
    # Workaround for issue involving JLine and Cygwin
    # (see http://sourceforge.net/p/jline/bugs/40/).
    # If you&#39;re using the Mintty terminal emulator in Cygwin, may need to set the
    # &quot;Backspace sends ^H&quot; setting in &quot;Keys&quot; section of the Mintty options
    # (see https://github.com/sbt/sbt/issues/562).
    stty -icanon min 1 -echo &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
    export SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Djline.terminal=unix&quot;
    &quot;$FWDIR&quot;/bin/spark-submit --class org.apache.spark.repl.Main &quot;${SUBMISSION_OPTS[@]}&quot; spark-shell &quot;${APPLICATION_OPTS[@]}&quot;
    stty icanon echo &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
  else
    export SPARK_SUBMIT_OPTS
    &quot;$FWDIR&quot;/bin/spark-submit --class org.apache.spark.repl.Main &quot;${SUBMISSION_OPTS[@]}&quot; spark-shell &quot;${APPLICATION_OPTS[@]}&quot;
  fi
}

# Copy restore-TTY-on-exit functions from Scala script so spark-shell exits properly even in
# binary distribution of Spark where Scala is not installed
exit_status=127
saved_stty=&quot;&quot;

# restore stty settings (echo in particular)
function restoreSttySettings() {
  stty $saved_stty
  saved_stty=&quot;&quot;
}

function onExit() {
  if [[ &quot;$saved_stty&quot; != &quot;&quot; ]]; then
    restoreSttySettings
  fi
  exit $exit_status
}

# to reenable echo if we are interrupted before completing.
trap onExit INT

# save terminal settings
saved_stty=$(stty -g 2&amp;gt;/dev/null)
# clear on error so we don&#39;t later try to restore them
if [[ ! $? ]]; then
  saved_stty=&quot;&quot;
fi

main &quot;$@&quot;

# record the exit status lest it be overwritten:
# then reenable echo and propagate the code.
exit_status=$?
onExit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上往下一步步分析，首先是判断是否为cygwin，这里用到了bash中的&lt;code&gt;case&lt;/code&gt;语法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cygwin=false
case &quot;`uname`&quot; in
  CYGWIN*) cygwin=true;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;在linux系统中，&lt;code&gt;uname&lt;/code&gt;命令的运行结果为linux，其值不等于&lt;code&gt;CYGWIN*&lt;/code&gt;，故cygwin=false。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;开启bash的posix模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;set -o posix
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取上级目录绝对路径，这里使用到了&lt;code&gt;dirname&lt;/code&gt;命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;FWDIR=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;提示：bash 中，$0 是获取脚本名称&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;判断输入参数中是否有&lt;code&gt;--help&lt;/code&gt;或者&lt;code&gt;-h&lt;/code&gt;，如果有，则打印使用说明，实际上运行的是&lt;code&gt;/bin/spark-submit --help&lt;/code&gt;命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;function usage() {
  echo &quot;Usage: ./bin/spark-shell [options]&quot;
  &quot;$FWDIR&quot;/bin/spark-submit --help 2&amp;gt;&amp;amp;1 | grep -v Usage 1&amp;gt;&amp;amp;2
  exit 0
}

if [[ &quot;$@&quot; = *--help ]] || [[ &quot;$@&quot; = *-h ]]; then
  usage
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;提示：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;2&amp;gt;&amp;amp;1 的意思是将标准错误也输出到标准输出当中；1&amp;gt;&amp;amp;2是将标准输出输出到标准错误当中&lt;/li&gt;
    &lt;li&gt;bash 中，$@ 是获取脚本所有的输入参数&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;再往后面是定义了一个main方法，并将spark-shell的输入参数传给该方法运行，main方法中判断是否是cygwin模式，如果不是，则运行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true&quot;


export SPARK_SUBMIT_OPTS
&quot;$FWDIR&quot;/bin/spark-submit --class org.apache.spark.repl.Main &quot;${SUBMISSION_OPTS[@]}&quot; spark-shell &quot;${APPLICATION_OPTS[@]}&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;提示：”${SUBMISSION_OPTS[@]}” 这是什么意思？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从上面可以看到，其实最后调用的是spark-submit命令，并指定&lt;code&gt;--class&lt;/code&gt;参数为&lt;code&gt;org.apache.spark.repl.Main&lt;/code&gt;类，后面接的是spark-submit的提交参数，再后面是spark-shell，最后是传递应用的参数。&lt;/p&gt;

&lt;p&gt;最后，是获取main方法运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;exit_status=$?
onExit
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;提示： bash 中，&lt;code&gt;$?&lt;/code&gt;是获取上个命令运行结束返回的状态码&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果以调试模式运行spark-shell，在不加参数的情况下，输出内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+ cygwin=false
+ case &quot;`uname`&quot; in
++ uname
+ set -o posix
+++ dirname /usr/lib/spark/bin/spark-shell
++ cd /usr/lib/spark/bin/..
++ pwd
+ FWDIR=/usr/lib/spark
+ [[ &#39;&#39; = *--help ]]
+ [[ &#39;&#39; = *-h ]]
+ source /usr/lib/spark/bin/utils.sh
+ SUBMIT_USAGE_FUNCTION=usage
+ gatherSparkSubmitOpts
+ &#39;[&#39; -z usage &#39;]&#39;
+ SUBMISSION_OPTS=()
+ APPLICATION_OPTS=()
+ (( 0 ))
+ export SUBMISSION_OPTS
+ export APPLICATION_OPTS
+ SPARK_SUBMIT_OPTS=&#39; -Dscala.usejavacp=true&#39;
+ exit_status=127
+ saved_stty=
+ trap onExit INT
++ stty -g
+ saved_stty=500:5:bf:8a3b:3:1c:7f:15:4:0:1:0:11:13:1a:0:12:f:17:16:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0
+ [[ ! -n 0 ]]
+ main
+ false
+ export SPARK_SUBMIT_OPTS
+ /usr/lib/spark/bin/spark-submit --class org.apache.spark.repl.Main spark-shell
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;提示：通过运行&lt;code&gt;set -x&lt;/code&gt;可以开启bash调试代码的特性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;接下来就涉及到spark-submit命令的逻辑了。&lt;/p&gt;

&lt;h1 id=&quot;spark-submit&quot;&gt;spark-submit&lt;/h1&gt;

&lt;p&gt;完整的spark-submit脚本内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# NOTE: Any changes in this file must be reflected in SparkSubmitDriverBootstrapper.scala!

export SPARK_HOME=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;
ORIG_ARGS=(&quot;$@&quot;)

# Set COLUMNS for progress bar
export COLUMNS=`tput cols`

while (($#)); do
  if [ &quot;$1&quot; = &quot;--deploy-mode&quot; ]; then
    SPARK_SUBMIT_DEPLOY_MODE=$2
  elif [ &quot;$1&quot; = &quot;--properties-file&quot; ]; then
    SPARK_SUBMIT_PROPERTIES_FILE=$2
  elif [ &quot;$1&quot; = &quot;--driver-memory&quot; ]; then
    export SPARK_SUBMIT_DRIVER_MEMORY=$2
  elif [ &quot;$1&quot; = &quot;--driver-library-path&quot; ]; then
    export SPARK_SUBMIT_LIBRARY_PATH=$2
  elif [ &quot;$1&quot; = &quot;--driver-class-path&quot; ]; then
    export SPARK_SUBMIT_CLASSPATH=$2
  elif [ &quot;$1&quot; = &quot;--driver-java-options&quot; ]; then
    export SPARK_SUBMIT_OPTS=$2
  elif [ &quot;$1&quot; = &quot;--master&quot; ]; then
    export MASTER=$2
  fi
  shift
done

if [ -z &quot;$SPARK_CONF_DIR&quot; ]; then
  export SPARK_CONF_DIR=&quot;$SPARK_HOME/conf&quot;
fi
DEFAULT_PROPERTIES_FILE=&quot;$SPARK_CONF_DIR/spark-defaults.conf&quot;
if [ &quot;$MASTER&quot; == &quot;yarn-cluster&quot; ]; then
  SPARK_SUBMIT_DEPLOY_MODE=cluster
fi
export SPARK_SUBMIT_DEPLOY_MODE=${SPARK_SUBMIT_DEPLOY_MODE:-&quot;client&quot;}
export SPARK_SUBMIT_PROPERTIES_FILE=${SPARK_SUBMIT_PROPERTIES_FILE:-&quot;$DEFAULT_PROPERTIES_FILE&quot;}

# For client mode, the driver will be launched in the same JVM that launches
# SparkSubmit, so we may need to read the properties file for any extra class
# paths, library paths, java options and memory early on. Otherwise, it will
# be too late by the time the driver JVM has started.

if [[ &quot;$SPARK_SUBMIT_DEPLOY_MODE&quot; == &quot;client&quot; &amp;amp;&amp;amp; -f &quot;$SPARK_SUBMIT_PROPERTIES_FILE&quot; ]]; then
  # Parse the properties file only if the special configs exist
  contains_special_configs=$(
    grep -e &quot;spark.driver.extra*\|spark.driver.memory&quot; &quot;$SPARK_SUBMIT_PROPERTIES_FILE&quot; | \
    grep -v &quot;^[[:space:]]*#&quot;
  )
  if [ -n &quot;$contains_special_configs&quot; ]; then
    export SPARK_SUBMIT_BOOTSTRAP_DRIVER=1
  fi
fi

exec &quot;$SPARK_HOME&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;${ORIG_ARGS[@]}&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先是设置&lt;code&gt;SPARK_HOME&lt;/code&gt;，并保留原始输入参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export SPARK_HOME=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;
ORIG_ARGS=(&quot;$@&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，使用while语句配合&lt;code&gt;shift&lt;/code&gt;命令，依次判断输入参数。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;说明：shift是将输入参数位置向左移位&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;设置&lt;code&gt;SPARK_CONF_DIR&lt;/code&gt;变量，并判断spark-submit部署模式。&lt;/p&gt;

&lt;p&gt;如果&lt;code&gt;$SPARK_CONF_DIR/spark-defaults.conf&lt;/code&gt;文件存在，则检查是否设置&lt;code&gt;spark.driver.extra&lt;/code&gt;开头的和&lt;code&gt;spark.driver.memory&lt;/code&gt;变量，如果设置了，则&lt;code&gt;SPARK_SUBMIT_BOOTSTRAP_DRIVER&lt;/code&gt;设为1。&lt;/p&gt;

&lt;p&gt;最后，执行的是spark-class命令，输入参数为&lt;code&gt;org.apache.spark.deploy.SparkSubmit&lt;/code&gt;类名和原始参数。&lt;/p&gt;

&lt;h1 id=&quot;spark-class&quot;&gt;spark-class&lt;/h1&gt;

&lt;p&gt;该脚本首先还是判断是否是cygwin，并设置SPARK_HOME和SPARK_CONF_DIR变量。&lt;/p&gt;

&lt;p&gt;运行bin/load-spark-env.sh，加载spark环境变量。&lt;/p&gt;

&lt;p&gt;spark-class至少需要传递一个参数，如果没有，则会打印脚本使用说明&lt;code&gt;Usage: spark-class &amp;lt;class&amp;gt; [&amp;lt;args&amp;gt;]&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;如果设置了&lt;code&gt;SPARK_MEM&lt;/code&gt;变量，则提示&lt;code&gt;SPARK_MEM&lt;/code&gt;变量过时，应该使用&lt;code&gt;spark.executor.memory&lt;/code&gt;或者&lt;code&gt;spark.driver.memory&lt;/code&gt;变量。&lt;/p&gt;

&lt;p&gt;设置默认内存&lt;code&gt;DEFAULT_MEM&lt;/code&gt;为512M，如果&lt;code&gt;SPARK_MEM&lt;/code&gt;变量存在，则使用&lt;code&gt;SPARK_MEM&lt;/code&gt;的值。&lt;/p&gt;

&lt;p&gt;使用case语句判断spark-class传入的第一个参数的值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;SPARK_DAEMON_JAVA_OPTS=&quot;$SPARK_DAEMON_JAVA_OPTS -Dspark.akka.logLifecycleEvents=true&quot;

# Add java opts and memory settings for master, worker, history server, executors, and repl.
case &quot;$1&quot; in
  # Master, Worker, and HistoryServer use SPARK_DAEMON_JAVA_OPTS (and specific opts) + SPARK_DAEMON_MEMORY.
  &#39;org.apache.spark.deploy.master.Master&#39;)
    OUR_JAVA_OPTS=&quot;$SPARK_DAEMON_JAVA_OPTS $SPARK_MASTER_OPTS&quot;
    OUR_JAVA_MEM=${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}
    ;;
  &#39;org.apache.spark.deploy.worker.Worker&#39;)
    OUR_JAVA_OPTS=&quot;$SPARK_DAEMON_JAVA_OPTS $SPARK_WORKER_OPTS&quot;
    OUR_JAVA_MEM=${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}
    ;;
  &#39;org.apache.spark.deploy.history.HistoryServer&#39;)
    OUR_JAVA_OPTS=&quot;$SPARK_DAEMON_JAVA_OPTS $SPARK_HISTORY_OPTS&quot;
    OUR_JAVA_MEM=${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}
    ;;

  # Executors use SPARK_JAVA_OPTS + SPARK_EXECUTOR_MEMORY.
  &#39;org.apache.spark.executor.CoarseGrainedExecutorBackend&#39;)
    OUR_JAVA_OPTS=&quot;$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS&quot;
    OUR_JAVA_MEM=${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}
    ;;
  &#39;org.apache.spark.executor.MesosExecutorBackend&#39;)
    OUR_JAVA_OPTS=&quot;$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS&quot;
    OUR_JAVA_MEM=${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}
    export PYTHONPATH=&quot;$FWDIR/python:$PYTHONPATH&quot;
    export PYTHONPATH=&quot;$FWDIR/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH&quot;
    ;;

  # Spark submit uses SPARK_JAVA_OPTS + SPARK_SUBMIT_OPTS +
  # SPARK_DRIVER_MEMORY + SPARK_SUBMIT_DRIVER_MEMORY.
  &#39;org.apache.spark.deploy.SparkSubmit&#39;)
    OUR_JAVA_OPTS=&quot;$SPARK_JAVA_OPTS $SPARK_SUBMIT_OPTS&quot;
    OUR_JAVA_MEM=${SPARK_DRIVER_MEMORY:-$DEFAULT_MEM}
    if [ -n &quot;$SPARK_SUBMIT_LIBRARY_PATH&quot; ]; then
      if [[ $OSTYPE == darwin* ]]; then
       export DYLD_LIBRARY_PATH=&quot;$SPARK_SUBMIT_LIBRARY_PATH:$DYLD_LIBRARY_PATH&quot;
      else
       export LD_LIBRARY_PATH=&quot;$SPARK_SUBMIT_LIBRARY_PATH:$LD_LIBRARY_PATH&quot;
      fi
    fi
    if [ -n &quot;$SPARK_SUBMIT_DRIVER_MEMORY&quot; ]; then
      OUR_JAVA_MEM=&quot;$SPARK_SUBMIT_DRIVER_MEMORY&quot;
    fi
    ;;

  *)
    OUR_JAVA_OPTS=&quot;$SPARK_JAVA_OPTS&quot;
    OUR_JAVA_MEM=${SPARK_DRIVER_MEMORY:-$DEFAULT_MEM}
    ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可能存在以下几种情况：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;org.apache.spark.deploy.master.Master&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;org.apache.spark.deploy.worker.Worker&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;org.apache.spark.deploy.history.HistoryServer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;org.apache.spark.executor.CoarseGrainedExecutorBackend&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;org.apache.spark.executor.MesosExecutorBackend&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;org.apache.spark.deploy.SparkSubmit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并分别设置每种情况下的Java运行参数和使用内存大小，以表格形式表示如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;OUR_JAVA_OPTS&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;OUR_JAVA_MEM&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Master&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$SPARK_DAEMON_JAVA_OPTS $SPARK_MASTER_OPTS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Worker&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$SPARK_DAEMON_JAVA_OPTS $SPARK_WORKER_OPTS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HistoryServer&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$SPARK_DAEMON_JAVA_OPTS $SPARK_HISTORY_OPTS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CoarseGrainedExecutorBackend&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;MesosExecutorBackend&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SparkSubmit&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$SPARK_JAVA_OPTS $SPARK_SUBMIT_OPTS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;${SPARK_DRIVER_MEMORY:-$DEFAULT_MEM}&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通过上表就可以知道每一个spark中每个进程如何设置JVM参数和内存大小。&lt;/p&gt;

&lt;p&gt;接下来是查找JAVA_HOME并检查Java版本。&lt;/p&gt;

&lt;p&gt;设置SPARK_TOOLS_JAR变量。&lt;/p&gt;

&lt;p&gt;运行bin/compute-classpath.sh计算classpath。&lt;/p&gt;

&lt;p&gt;判断&lt;code&gt;SPARK_SUBMIT_BOOTSTRAP_DRIVER&lt;/code&gt;变量值，如果该值为1，则运行&lt;code&gt;org.apache.spark.deploy.SparkSubmitDriverBootstrapper&lt;/code&gt;类，以替换原来的&lt;code&gt;org.apache.spark.deploy.SparkSubmit&lt;/code&gt;的类，执行的脚本为&lt;code&gt;exec &quot;$RUNNER&quot; org.apache.spark.deploy.SparkSubmitDriverBootstrapper &quot;$@&quot;&lt;/code&gt;；否则，运行java命令&lt;code&gt;exec &quot;$RUNNER&quot; -cp &quot;$CLASSPATH&quot; $JAVA_OPTS &quot;$@&quot;&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;从最后运行的脚本可以看到，spark-class脚本的作用主要是查找java命令、计算环境变量、设置&lt;code&gt;JAVA_OPTS&lt;/code&gt;等，至于运行的是哪个java类的main方法，取决于&lt;code&gt;SPARK_SUBMIT_BOOTSTRAP_DRIVER&lt;/code&gt;变量的值。&lt;/p&gt;

&lt;p&gt;接下来，就是要分析&lt;code&gt;org.apache.spark.deploy.SparkSubmitDriverBootstrapper&lt;/code&gt;和&lt;code&gt;org.apache.spark.deploy.SparkSubmit&lt;/code&gt;类的运行逻辑以及两者之间的区别，这部分内容见下篇文章。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2015/06/26/spark-shell-command.html</link>
      <guid>http://blog.javachen.com/2015/06/26/spark-shell-command.html</guid>
      <pubDate>2015-06-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Scala中的对象</title>
      <description>&lt;p&gt;Scala中没有静态方法或静态字段，但可以使用&lt;code&gt;object&lt;/code&gt;这个语法结构来实现相同的功能。对象与类在语法层面上很相似，除了不能提供构造器参数外，对象可以拥有类的所有特性。&lt;/p&gt;

&lt;p&gt;Scala的object定义了&lt;code&gt;单个实例&lt;/code&gt;，其可以用来存放工具函数或常量等：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object Timer {
  var count = 0

  def currentCount(): Long = {
    count += 1
    count
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用object中的常量或方法，通过object名称直接调用，对象构造器在对象第一次被使用时调用（如果某对象一直未被使用，那么其构造器也不会被调用）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; Timer.currentCount()
res52: Long = 1

scala&amp;gt; Timer.currentCount
res53: Long = 2

scala&amp;gt; Timer.count
res54: Int = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;object的构造器不接受参数传递。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;伴生对象&lt;/h1&gt;

&lt;p&gt;对象如果与某个类同名，那么它就是一个&lt;code&gt;伴生对象&lt;/code&gt;。&lt;strong&gt;类和它的伴生对象必须在同一个源文件中&lt;/strong&gt;，可以将在Java类中定义的静态常量、方法等放置到Scala的类的伴生对象中。&lt;/p&gt;

&lt;p&gt;类可以访问伴生对象私有属性，但是必须通过&lt;code&gt;伴生对象.属性名&lt;/code&gt; 或 &lt;code&gt;伴生对象.方法&lt;/code&gt; 调用，伴生对象也可以访问类的私有属性。&lt;/p&gt;

&lt;p&gt;伴生对象是类的一个特殊实例。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Counter{
    def getTotalCounter()= Counter.getCount
}

object Counter{
    private var cnt = 0
    private def getCount()= cnt
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对象可以继承类，以及一个或多个特质，其结果是一个继承了指定类以及特质的类的对象，同时拥有在对象定义中给出的所有特性。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;abstract class Person(var name:String, var age:Int){
    def info():Unit
}

object XiaoMing extends Person(&quot;XiaoMing&quot;, 5){
    def info(){
        println(&quot; name is &quot;+name+&quot;, age is &quot;+age)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java程序通常从一个public类的main方法开始。而&lt;strong&gt;在Scala中，程序从对象的main方法开始&lt;/strong&gt;，方法的类型是 &lt;code&gt;Array[String] =&amp;gt; Unit&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object Hello {
  def main(args: Array[String]) {
    println(&quot;Hello, world!&quot;)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除此之外，&lt;code&gt;还可以扩展App特质&lt;/code&gt;，然后将程序代码放在构造器内即可，命令行参数从args属性获取。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object Hello extends App {
  if (args.length &amp;gt; 0)
    println(&quot;Hello, &quot; + args(0))
  else
    println(&quot;Hello, world!&quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;apply-&quot;&gt;apply 方法&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;apply方法是对象的一类特有方法，一般可用于创建伴生类&lt;/strong&gt;。apply方法可以用简洁的方式调用，形如&lt;code&gt;Object(args..)&lt;/code&gt;， 当然，你也可以跟其他方法一样调用，&lt;code&gt;Object.apply(args...)&lt;/code&gt;，这两种写法的结果是一样的。&lt;/p&gt;

&lt;p&gt;现在，当你看到&lt;code&gt;List(1,2,3)&lt;/code&gt;这样的语句就不会感到奇怪了，这只是&lt;code&gt;List.apply(1,2,3)&lt;/code&gt;的简写而已。&lt;/p&gt;

&lt;p&gt;使用apply方法的好处是，&lt;em&gt;在创建对象时，可以省去使用new关键字&lt;/em&gt;。&lt;/p&gt;

&lt;p&gt;当类或对象有一个主要用途的时候，apply方法为你提供了一个很好的语法糖。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; class Foo {}
defined class Foo

scala&amp;gt; object FooMaker {
     |   def apply() = new Foo
     | }
defined module FooMaker

scala&amp;gt; val newFoo = FooMaker() //没有new
newFoo: Foo = Foo@5b83f762
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; class Bar {
     |   def apply() = 0
     | }
defined class Bar

scala&amp;gt; val bar = new Bar
bar: Bar = Bar@47711479

scala&amp;gt; bar()
res8: Int = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;枚举&lt;/h1&gt;

&lt;p&gt;在Scala中并没有&lt;code&gt;枚举&lt;/code&gt;类型，但在标准类库中提供了&lt;code&gt;Enumeration&lt;/code&gt;类来获得枚举。扩展Enumeration类后，调用&lt;code&gt;Value&lt;/code&gt;方法来初始化枚举中的可能值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object TrafficLightColor extends Enumeration {
  val Red, Yellow, Green = Value
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述实例中代码可以改为下面这样，区别是：Value方法每次返回内部类Value的新实例。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val Red = Value
val Yellow = Value
val Green = Value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用Value方法初始化枚举类变量时，Value方法会返回内部类的新实例，且该内部类也叫Value。另外，在调用Value方法时，也可传入ID、名称两参数。如果未指定ID，默认从零开始，后面参数的ID是前一参数ID值加1。如果未指定名称，默认与属性字段同名。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object TrafficLight extends Enumeration{
     val Red = Value(1, &quot;Stop&quot;)
     val Yellow = Value(&quot;Wait&quot;)    //可以单独传名称
     val Green = Value(4) //可以单独传ID
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上例中，Yellow属性就仅定义了名称，Green仅定义ID。&lt;/p&gt;

&lt;p&gt;参数在不指定名称时，默认参数的Value为字段名。枚举类型的值是 &lt;code&gt;对象名.Value&lt;/code&gt; ，如上例中的枚举类型是 TrafficLight.Value。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;TrafficLight.Green
//TrafficLight.Value = Green
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过id方法获取枚举类型值的ID:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;TrafficLight.Yellow.id
//Int = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过values方法获取所有枚举值的集合:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;TrafficLight.values
//TrafficLight.ValueSet = TrafficLight.ValueSet(Stop, Wait, Green)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过ID来获取对应的枚举对象:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;TrafficLight(1)
//TrafficLight.Value = Stop
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://nerd-is.in/2013-08/scala-learning-objects/&quot;&gt;Scala学习——对象&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/06/19/scala-object.html</link>
      <guid>http://blog.javachen.com/2015/06/19/scala-object.html</guid>
      <pubDate>2015-06-19T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Scala中的类</title>
      <description>&lt;p&gt;阅读《Programming in Scala》，整理Scala类、继承、重载相关的一些知识点。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;类&lt;/h1&gt;

&lt;p&gt;Scala使用class来定义类。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Counter {
  private var value = 0 // 必须初始化字段
  def increment() { value += 1 } // 方法默认公有
  def current() = value  //空括号方法
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Scala中的类不能声明为public，一个Scala源文件中可以有多个类。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;类的初始化和调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val myCounter = new Counter // 或new Counter()
myCounter.increment()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Scala在遇到混合了无参数和空括号方法的情况时很大度。特别是，你可以用空括号方法重载无参数方法，并且反之亦可&lt;/strong&gt;。你还可以在调用任何不带参数的方法时省略空的括号：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;myCounter.increment

myCounter.current()
myCounter.current
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;原则上 Scala 的函数调用中可以省略所有的空括号，但在可能产生副作用的情况下，推荐仍然写一对空的括号。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果将current方法的声明改为下面这种&lt;code&gt;无参方法&lt;/code&gt;的形式，则调用时不能带&lt;code&gt;( )&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Counter {
  private var value = 0 // 必须初始化字段
  def increment() { value += 1 } // 方法默认公有
  def current = value //无参方法
}

val myCounter = new Counter 

myCounter.current()  // 调用必须是myCounter.current这种风格
&amp;lt;console&amp;gt;:10: error: Int does not take parameters
              myCounter.current()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们还可以选择把current作为字段而不是方法来实现，只要简单地在每个实现里把 def 修改成 val 即可：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Counter {
  private var value = 0 // 必须初始化字段
  def increment() { value += 1 } // 方法默认公有
  val current = value 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;唯一的差别是字段的访问或许稍微比方法调用要快，因为字段值在类被初始化的时候被预计算，而方法调用在每次调用的时候都要计算。换句话说，字 段在每个 Element 对象上需要更多的内存空间。&lt;/p&gt;

&lt;h1 id=&quot;gettersetter&quot;&gt;getter和setter&lt;/h1&gt;

&lt;p&gt;Scala对每个字段都提供了getter和setter方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Person {
  var age = 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面例子中，getter和setter分别叫做&lt;code&gt;age&lt;/code&gt;和&lt;code&gt;age_=&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;println(fred.age) // 调用方法fred.age()
fred.age = 21 // 调用方法fred.age_=(21)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将这个简单的Person编译后，使用&lt;code&gt;javap&lt;/code&gt;查看生成的字节码，可以验证这一点。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// -private选项说明显示所有的类和成员
javap -private Person.class
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Person字节码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;public class Person implements scala.ScalaObject {
  private int age;
  public int age();
  public void age_$eq(int); // =号被翻译成了$eq
  public Person();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scala中，字段和getter/setter间的关系，还有其他几种情况。&lt;/p&gt;

&lt;p&gt;使用val声明的字段，是只有getter，因为val声明的是不可变的。Scala中不能实现只有setter的字段。&lt;/p&gt;

&lt;p&gt;还有种对象私有字段。Scala中，方法可以访问该类的所有对象的私有字段，这一点与Java一样。如果通过private[this]来字段来修饰，那么这个字段是对象私有的，这种情况下，不会生成getter和setter。对象私有字段，只能由当前对象的方法访问，而该类的其他对象的方法是无法访问的。&lt;/p&gt;

&lt;p&gt;接下来是一种与private[this]相似的访问控制。Scala中可以使用private[class-name]来指定可以访问该字段的类，class-name必须是当前定义的类，或者是当前定义的类的外部类。这种情况会生成getter和setter方法。&lt;/p&gt;

&lt;p&gt;没有初始值的字段即是&lt;code&gt;抽象字段&lt;/code&gt;，关于&lt;code&gt;抽象类&lt;/code&gt;的说明，后面再讨论。根据是val声明还是var声明，会生成相应的抽象的setter/getter，但是不生成字段。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;abstract class Person {
  val id: Int
  var name: String
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看编译后的字节码，可以得知，JVM类只生成了setter/getter，但没有生成字段。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public abstract class Person implements scala.ScalaObject {
  public abstract int id();
  public abstract java.lang.String name();
  public abstract void name_$eq(java.lang.String);
  public Person();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;bean&quot;&gt;Bean属性&lt;/h1&gt;

&lt;p&gt;使用 &lt;code&gt;@BeanProperty&lt;/code&gt; 注解来为字段生成符合JavaBeans规范的getter/setter方法。使用该注解后，将会生成4个方法：Scala的getter/setter和JavaBeans规范的getter/setter（如果是val声明，就没有setter部分了）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import scala.reflect.BeanProperty
// 在Scala 2.10.0之后已被废弃
// 使用scala.beans.BeanProperty代替
class Person {
  @BeanProperty var name: String = _
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;构造器&lt;/h1&gt;

&lt;p&gt;在Scala中，有两种构造器，主构造器（primary constructor）和辅助构造器（auxiliary constructor）。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;辅助构造器&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;辅助构造器&lt;/code&gt;与Java构造器很相似，但有两点不同：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;名字是this（Java中构造器名称与类名相同）&lt;/li&gt;
  &lt;li&gt;辅助构造器必须以对已经定义的辅助构造器或主构造器的调用开始&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Person {
  private var name = &quot;&quot;
  private var age = 0
 
  def this(name: String) {
    this() // 调用主构造器
    this.name = name
  }
 
  def this(name: String, age: Int) {
    this(name)  // 调用辅助构造器
    this.age = age
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val p1 = new Person // 主构造器
val p2 = new Person(&quot;Fred&quot;)  // 第一个辅助构造器
val p3 = new Person(&quot;Fred&quot;, 42) // 第二个辅助构造器
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;主构造器&lt;/h2&gt;

&lt;p&gt;在scala中每个类都有主构造器，主构造器并不是以&lt;code&gt;this&lt;/code&gt;方法定义，而是与类定义交织在一起。&lt;/p&gt;

&lt;p&gt;1、主构造器参数直接放在类名之后，指的是&lt;code&gt;()&lt;/code&gt;中的参数，主构造器参数会被编译成字段，其值被初始化成构造时传入的参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Person(a: String) {
  val name:String =a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、主构造器会执行类定义中的所有语句，这里是是&lt;code&gt;{}&lt;/code&gt;中的语句。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Person {
  println(0)
  def printNum(num: Int) { println(num) }
  println(1)
  printNum(2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、如果主构造器参数不带val或var，那么会根据是否被方法使用来决定。如果不带val或var的参数被方法使用了，它会变为对象私有字段；如果没有被方法使用，则被当成一个普通的参数，不升级成字段。这部分说明请看&lt;code&gt;参数化字段&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;4、可以将主构造器变为私有的，将private关键字放在圆括号前：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Person private(var name: String,val age: Int){
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译之后：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//javap
public class Person {
  private java.lang.String name;
  private final int age;
  public java.lang.String name();
  public void name_$eq(java.lang.String);
  public int age();
  
  //注意私有构造方法
  private Person(java.lang.String, int);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;参数化字段&lt;/h2&gt;

&lt;p&gt;1、构造参数不带var或val，被类中函数使用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Person(a: String) {
  def name:String =a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则构造参数被升格为私有字段，效果类似&lt;code&gt;private[this] val&lt;/code&gt;，反编译该类为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class Person extends java.lang.Object{
    //私有final
    private final java.lang.String a;

    public static java.lang.String $lessinit$greater$default$1();

    //函数
    public java.lang.String name();

    public Person(java.lang.String);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、构造参数不带var或val，未在类中使用，则该参数为普通参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Person(a: String) {
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;反编译为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class Person extends java.lang.Object{
    public Person(java.lang.String);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、构造参数带var或val。Person类的定义中有一个构造参数a，并在name方法中被使用，如果你想避免这种参数和方法混合在一起的定义方式，你可以使用&lt;code&gt;参数化字段&lt;/code&gt;来定义类，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// 请注意小括号 
class Person( val name: String) {

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是在同一时间使用相同的名称定义参数和属性的一个&lt;code&gt;简写&lt;/code&gt;方式。尤其特别的是，类 Person 现在拥有一个&lt;code&gt;可以从类外部访问的不能重新赋值&lt;/code&gt;的属性name。&lt;/p&gt;

&lt;p&gt;同样也可以使用var前缀类参数，这种情况下相应的字段将能重新被赋值。最终，还有可能添加 如private、protected、或override这类的修饰符到这些参数化字段上，就好象你可以在其他类成员上做的事情。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;嵌套类&lt;/h1&gt;

&lt;p&gt;在Scala中，几乎可以在任何的语法结构中内嵌任何语法结构。可以类中定义类，也可以在方法中定义方法。&lt;/p&gt;

&lt;p&gt;Scala中每个实例都有自己的内部类。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import scala.collection.mutable.ArrayBuffer
 
class NetWork {
  class Member(val name: String) {
  }
 
  private val members = new ArrayBuffer[Member]
 
  def join(m: Member) = {
    members += m
    m
  }
}
 
val chatter = new NetWork
val myFace = new NetWork

val m1=new chatter.Member(&quot;m1&quot;)
val m2=new chatter.Member(&quot;m2&quot;)

chatter.join(m1)
chatter.join(m2)

val m3=new myFace.Member(&quot;m3&quot;)
chatter.join(m3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;往chatter中加入m3时，会出现编译错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;console&amp;gt;:14: error: type mismatch;
 found   : myFace.Member
 required: chatter.Member
              chatter.join(m3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是因为，chatter.Member类和myFace.Member类是不同的两个类。这一点与Java中内部类是不同的。&lt;/p&gt;

&lt;p&gt;如果想产生类似Java中的内部类特性，可以将Member声明到Network的外部，或者使用&lt;code&gt;类型投影&lt;/code&gt;，这里是将内部类中的Member换成NetWork#Member，代码如下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import scala.collection.mutable.ArrayBuffer
 
class NetWork {
  class Member(val name: String) {
  }
 
  private val members = new ArrayBuffer[NetWork#Member]
 
  def join(m: NetWork#Member) = {
    members += m
    m
  }
}
 
val chatter = new NetWork
val myFace = new NetWork

val m1=new chatter.Member(&quot;m1&quot;)
val m2=new chatter.Member(&quot;m2&quot;)

chatter.join(m1)
chatter.join(m2)

val m3=new myFace.Member(&quot;m3&quot;)
chatter.join(m3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与Java中一样，如果需要在内部类中使用外部类的引用，使用 &lt;code&gt;外部类名.class&lt;/code&gt; 的语法即可。不过Scala中有一个为这种情况服务的语法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class NetWork(val name: String) { outer =&amp;gt;
  class Member(val name: String) {
    def description = name + &quot; inside &quot; + outer.name
  }
}

val work = new NetWork(&quot;work&quot;)
work.name
//work

val m1=new work.Member(&quot;m1&quot;)
m1.description
//m1 inside work
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-6&quot;&gt;抽象类&lt;/h1&gt;

&lt;p&gt;和Java一样，Scala用&lt;code&gt;abstract&lt;/code&gt;修饰抽象类，抽象类没有具体实例方法。具有抽象成员的类本身必须被声明为抽象的。抽象类定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;abstract class Element {
  def contents: Array[String]
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请注意类 Element 的 contents 方法并没带有 abstract 修饰符，如果方法没有实现，也就是说没有等号或方法体，它就是抽象的。类 Element &lt;code&gt;声明&lt;/code&gt;了抽象方法 contents，但当前没有&lt;code&gt;定义&lt;/code&gt;具体方法。&lt;/p&gt;

&lt;p&gt;抽象类不能实例化，否则会得到编译器错误：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; new Element
&amp;lt;console&amp;gt;:9: error: class Element is abstract; cannot be instantiated
              new Element
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以向 Element 添加显示宽度和高度的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;abstract class Element {
  def contents: Array[String]
  def height: Int = contents.length
  def width: Int = if (height == 0) 0 else contents(0).length
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请注意 Element 的三个方法没一个有参数列表，甚至连个空列表都没有。这种&lt;code&gt;无参数方法&lt;/code&gt;在Scala里是非常普通的，带有空括号的方法，被称为&lt;code&gt;空括号方法&lt;/code&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-7&quot;&gt;继承&lt;/h1&gt;

&lt;p&gt;继承一个抽象类使用&lt;code&gt;extends&lt;/code&gt;关键字，如果你省略 extends，Scala 编译器隐式地假设你的类扩展自 &lt;code&gt;scala.AnyRef&lt;/code&gt;，在 Java 平台上与 java.lang.Object 一致。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class ArrayElement(conts: Array[String]) extends Element {
    def contents: Array[String] = conts
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果将类声明为final的，则这个类不能被继承。如果将类的方法和字段声明为final，则它们不能被重写。&lt;/p&gt;

&lt;p&gt;子类继承超类中所有&lt;code&gt;非私有&lt;/code&gt;的成员，如果子类中的成员与超类中成员具有相同名称和参数，则成为&lt;code&gt;重载&lt;/code&gt;；如果子类中的成员是具体的而超类中的是抽象的，我们还可以说子类实现了超类中的成员。&lt;/p&gt;

&lt;p&gt;上面例子中，ArrayElement类的contents方法重载或者说实现了超类的contents方法，并继承了width和height方法。我们可以实例一个ArrayElement对象，然后调用方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val ae = new ArrayElement(Array(&quot;hello&quot;, &quot;world&quot;))
ae: ArrayElement = ArrayElement@d94e60
scala&amp;gt; ae.width
res1: Int = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面ae变量的类型是ArrayElement，其实我们也可以将其声明为超类类型，这叫做&lt;code&gt;子类型化&lt;/code&gt;：是指子类的值可以被用在需要其超类的值的任何地方。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val e: Element = new ArrayElement(Array(&quot;hello&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样的话，e变量是声明为Element类型，但是是初始化为ArrayElement类型。这个涉及到&lt;code&gt;多态&lt;/code&gt;的概念。&lt;/p&gt;

&lt;p&gt;如果子类要调用超类的构造器，则需要这样定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class LineElement(s: String) extends ArrayElement(Array(s)) {
  override def width = s.length
  override def height = 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;LineElement类继承自ArrayElement，并且LineElement类的构造器中传入了一个参数s，LineElement类想要调用超类的构造器，需要把要传递的参数或参数列表放在超类名之后的括号里即可：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;... extends ArrayElement(Array(s)) ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，就完成了子类调用父类的构造器进行初始化父类。&lt;/p&gt;

&lt;h1 id=&quot;section-8&quot;&gt;重载&lt;/h1&gt;

&lt;p&gt;统一访问原则只是 Scala 在对待字段和方法方面比 Java 更统一的一个方面；另一个差异是 Scala 里，字段和方法属于相同的命名空间。这使得字段重载无参数方法成为可能。比如说，你可以改变类 ArrayElement 中 contents 的实现，从一个方法变为一个字段，而无需修改类 Element 中 contents 的抽象方法定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class ArrayElement(conts: Array[String]) extends Element {
  val contents: Array[String] = conts
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Scala里禁止在同一个类里用同样的名称定义字段和方法&lt;/code&gt;，例如，下面的代码在Scala中无法通过编译：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class WontCompile {
  private var f = 0  // 编译不过，因为字段和方法重名 
  def f = 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常情况下，Scala 仅为定义准备了两个命名空间：值(字段、方法、包还有单例对象)、类型(类和特质名)，而 Java 有四个：字 段、方法、类型和包。&lt;/p&gt;

&lt;p&gt;Scala把字段和方法放进同一个命名空间，这样你就可以使用val重载无参数的方法。&lt;/p&gt;

&lt;h1 id=&quot;section-9&quot;&gt;重写&lt;/h1&gt;

&lt;p&gt;在Scala中&lt;code&gt;重写一个非抽象方法必须使用override修饰符&lt;/code&gt;，&lt;strong&gt;重写超类的抽象方法时，不需要使用override关键字&lt;/strong&gt;。调用超类的方法就如Java一样，使用&lt;code&gt;super&lt;/code&gt;关键字。&lt;/p&gt;

&lt;p&gt;请注意 LineElement 里 width 和 height 的定义带着 &lt;code&gt;override&lt;/code&gt; 修饰符。&lt;code&gt;Scala里所有重载了父类具体成员的成员都需要这样的修饰符&lt;/code&gt;。如果成员实现的 是同名的抽象成员则这个修饰符是可选的。而如果成员并未重载或实现什么其它基类里的成员则禁用这个修饰符。由于类 LineElement 的 height 和 width 重载了类 Element 的具体成员定义，override 是需要的。&lt;/p&gt;

&lt;p&gt;这条规则给编译器提供了有用的信息来帮助避免某些难以捕捉的错误并使得系统的改进更加安全。&lt;/p&gt;

&lt;h1 id=&quot;section-10&quot;&gt;构造顺序和提前定义&lt;/h1&gt;

&lt;p&gt;现有如下的类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Creature {
  val range: Int = 10
  val env: Array[Int] = new Array[Int](range)
}
 
class Ant extends Creature {
  override val range = 2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在构造时，发生的过程如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ant构造器在构造自己之前，调用超类构造器；&lt;/li&gt;
  &lt;li&gt;Creature的构造器将range字段设为10；&lt;/li&gt;
  &lt;li&gt;Creature的构造器初始化env数组，调用range字段的getter；&lt;/li&gt;
  &lt;li&gt;range的getter被Ant类重写了，返回的Ant类中的range，但是Ant类还未初始化，所以返回了0；&lt;/li&gt;
  &lt;li&gt;env被设置成长度为0的数组&lt;/li&gt;
  &lt;li&gt;Ant构造器继续执行，将range字段设为2。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在Java中也会出现碰见相似的问题，被调用的方法被子类所重写，有可能结果不是预期的。在构造器中，不应该依赖val的值。（只能重写超类抽象的var声明字段，所以没有这个问题；如果是def，也一样会出现这种问题。）&lt;/p&gt;

&lt;p&gt;这个问题的根本原因来自于Java语言的设计决定——允许在超类的构造方法中调用子类的方法。而在C++中，构造前后会更改指向虚函数的指针，所以不会出现这类问题。&lt;/p&gt;

&lt;p&gt;这个问题有几种解决方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将val声明为&lt;code&gt;final&lt;/code&gt;，安全但不灵活；&lt;/li&gt;
  &lt;li&gt;在超类中将val声明为&lt;code&gt;lazy&lt;/code&gt;，安全但不高效；&lt;/li&gt;
  &lt;li&gt;使用提前定义语法。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;提前定义语法&lt;/code&gt;是将需要提前定义的成员放在extends关键字后的一个语法块中，还需要使用&lt;code&gt;with&lt;/code&gt;关键字：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class Ant extends {
  override val range = 2
} with Creature
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;提前定义的等号右侧只能引用之前已经有的提前定义，不能出现类中其他的成员（因为都还没初始化呢）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;使用&lt;code&gt;-Xcheckinit&lt;/code&gt;编译器标志来调试构造顺序问题。这个标志会在有未初始化的字段被访问时抛出异常。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section-11&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://nerd-is.in/2013-08/scala-learning-classes/&quot;&gt;Scala学习——类&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/06/19/scala-class.html</link>
      <guid>http://blog.javachen.com/2015/06/19/scala-class.html</guid>
      <pubDate>2015-06-19T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Scala高价函数简化代码</title>
      <description>&lt;p&gt;在Scala里，带有其他函数做参数的函数叫做&lt;code&gt;高阶函数&lt;/code&gt;，使用高阶函数可以简化代码。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;减少重复代码&lt;/h1&gt;

&lt;p&gt;有这样一段代码，查找当前目录样以某一个字符串结尾的文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object FileMatcher {
  private def filesHere = (new java.io.File(&quot;.&quot;)).listFiles
  def filesEnding(query: String) =
    for (file &amp;lt;- filesHere; if file.getName.endsWith(query))
      yield file
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果，我们想查找包含某一个字符串的文件，则代码需要修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def filesContaining(query: String) =
  for (file &amp;lt;- filesHere; if file.getName.contains(query))
    yield file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的改动只是使用了 contains 替代 endsWith，但是随着需求越来越复杂，我们要不停地去修改这段代码。例如，我想实现正则匹配的查找，则代码会是下面这个样子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def filesRegex(query: String) =
  for (file &amp;lt;- filesHere; if file.getName.matches(query))
    yield file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了应变复杂的需求，我们可以进行重构代码，抽象出变化的代码部分，将其声明为一个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def filesMatching(query: String,matcher: (String, String) =&amp;gt; Boolean) = {
  for (file &amp;lt;- filesHere; if matcher(file.getName, query))
    yield file
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，针对不同的需求，我们可以编写不同的matcher方法实现，该方法返回一个布尔值。&lt;/p&gt;

&lt;p&gt;有了这个新的 filesMatching 帮助方法，你可以通过让三个搜索方法调用它，并传入合适的函数 来简化它们：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def filesEnding(query: String) = filesMatching(query, _.endsWith(_))

def filesContaining(query: String) = filesMatching(query, _.contains(_))

def filesRegex(query: String) = filesMatching(query, _.matches(_))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的例子使用了占位符，例如， filesEnding 方法里的函数文本 &lt;code&gt;_.endsWith(_)&lt;/code&gt; 其实就是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;(fileName: String, query: String) =&amp;gt; fileName.endsWith(query)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为，已经确定了参数类型为字符串，故上面可以省略参数类型。由于第一个参数 fileName 在方法体中被第一个使用，第二个参数 query 第二个使用，你也可以使用占位符语法：&lt;code&gt;_.endsWith(_)&lt;/code&gt;。第一个下划线是第一个参数文件名的占位符，第二个下划线是第二个参数查询字串的占位符。&lt;/p&gt;

&lt;p&gt;因为query参数是从外部传过来的，其可以直接传递给matcher函数，故filesMatching可以只需要一个参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object FileMatcher {
  private def filesHere = (new java.io.File(&quot;.&quot;)).listFiles

  private def filesMatching(matcher: String =&amp;gt; Boolean) =
    for (file &amp;lt;- filesHere; if matcher(file.getName))
      yield file

  def filesEnding(query: String) = filesMatching(_.endsWith(query))

  def filesContaining(query: String) = filesMatching(_.contains(query))

  def filesRegex(query: String) = filesMatching(_.matches(query))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的例子使用了函数作为第一类值帮助你减少代码重复的方式，另外还演示了闭包是如何能帮助你减少代码重复的。前面一个例子里用到的函数文本，如 &lt;code&gt;_.endsWith(_)&lt;/code&gt;和&lt;code&gt;_.contains(_)&lt;/code&gt;都是在运行期实例化成函数值而不是闭包，因为它们没有捕 获任何自由变量。&lt;/p&gt;

&lt;p&gt;举例来说，表达式&lt;code&gt;_.endsWith(_)&lt;/code&gt;里用的两个变量都是用下划线代表的，也就是说它们都是从传递给函数的参数获得的。因此，&lt;code&gt;_.endsWith(_)&lt;/code&gt;使用了两个绑定变量，而不是自由变量。&lt;/p&gt;

&lt;p&gt;相对的，最近的例子里面用到的函数文本&lt;code&gt;_.endsWith(query)&lt;/code&gt;包含一个绑定变量，下划线代表的参数和一个名为 query 的自由变量。仅仅因为 Scala 支持闭包才使得你可以在最近的这个例子里从 &lt;code&gt;filesMatching&lt;/code&gt; 中去掉 query 参数，从而更进一步简化了代码。&lt;/p&gt;

&lt;p&gt;另外一个例子，是循环集合时可以使用&lt;code&gt;exists&lt;/code&gt;方法来简化代码。以下是使用了这种方式的方法去判断是否传入的 List 包含了负数的例子:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def  containsNeg(nums: List[Int]): Boolean = {
    var exists = false
    for (num &amp;lt;- nums)
        if (num &amp;lt; 0)
            exists = true
    exists
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;采用和上面例子同样的方法，我们可以抽象代码，将重要的逻辑抽离到一个独立的方法中去实现。对于上面的查找判断是否存在的逻辑，Scala中提供了高阶函数 exists 来实现，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def containsNeg(nums: List[Int]) = nums.exists(_ &amp;lt; 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样，如果你要查找集合中是否存在偶数，则可以使用下面的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def containsOdd(nums: List[Int]) = nums.exists(_ % 2 == 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;柯里化&lt;/h1&gt;

&lt;p&gt;当函数有多个参数列表时，可以使用&lt;code&gt;柯里化函数&lt;/code&gt;来简化代码调用。例如，对下面的函数，它实现两个 Int 型参数，x 和 y 的加法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def plainOldSum(x: Int, y: Int) = x + y
plainOldSum: (Int,Int)Int

scala&amp;gt; plainOldSum(1, 2)
res4: Int = 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以将其柯里化，代之以一个列表的两个Int参数，实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def curriedSum(x: Int)(y: Int) = x + y
curriedSum: (Int)(Int)Int

scala&amp;gt; curriedSum(1)(2)
res5: Int = 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当你调用 curriedSum，你实际上背靠背地调用了两个传统函数。第一个函数调 用带单个的名为 x 的 Int 参数，并返回第二个函数的函数值，第二个函数带 Int 参数 y。&lt;/p&gt;

&lt;p&gt;你可以使用&lt;code&gt;偏函数&lt;/code&gt;，填上第一个参数并且部分应用第二个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val onePlus = curriedSum(1)_
onePlus: (Int) =&amp;gt; Int = &amp;lt;function&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;curriedSum(1)_&lt;/code&gt;里的下划线是第二个参数列表的占位符。结果就是指向一个函数的参考，这个函数在被调用的时候，对它唯一的Int参数加1并返回结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; onePlus(2)
res7: Int = 3
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;可变长度参数&lt;/h1&gt;

&lt;p&gt;类似柯里化函数，对于&lt;code&gt;同类型的多参数列表&lt;/code&gt;，我们还可以使用&lt;code&gt;可变长度参数&lt;/code&gt;，这部分内容，请参考《Scala基本语法和概念》中的&lt;a href=&quot;/2015/04/20/basic-of-scala.html#可变长度参数&quot;&gt;可变长度参数&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;贷出模式&lt;/h1&gt;

&lt;p&gt;前面的例子提到了使用函数作为参数，我们可以将这个函数的执行结果再次作为参数传入函数，即&lt;code&gt;双倍&lt;/code&gt;控制结构：能够重复一个操作两次并返回结果。&lt;/p&gt;

&lt;p&gt;下面是一个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def twice(op: Double =&amp;gt; Double, x: Double) = op(op(x))
twice: ((Double) =&amp;gt; Double,Double)Double

scala&amp;gt; twice(_ + 1, 5)
res9: Double = 7.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面例子中 op 的类型是 &lt;code&gt;Double =&amp;gt; Double&lt;/code&gt;，就是说它是带一个 Double 做参数并返回另一个 Double 的函数。这里，op函数等同于：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def add(x:Int)=x+1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;op函数会执行两次，第一次是执行&lt;code&gt;add(5)=6&lt;/code&gt;，第二次是执行&lt;code&gt;add(add(5))=add(6)=6+1=7&lt;/code&gt;。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;任何时候，你发现你的代码中多个地方有重复的代码块，你就应该考虑把它实现为这种双重控制结构。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;考虑这样一种需求：打开一个资源，对它进行操作，然后关闭资源，你可以这样实现：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def withPrintWriter(file: File, op: PrintWriter =&amp;gt; Unit) {
  val writer = new PrintWriter(file)
  try {
    op(writer)
  } finally {
    writer.close()
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有了这个方法，你就可以这样使用:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;withPrintWriter(new File(&quot;date.txt&quot;), writer =&amp;gt; writer.println(new java.util.Date) )
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;br /&gt;
这里和上面的例子一样，使用了&lt;code&gt;=&amp;gt;&lt;/code&gt; 来映射式定义函数，其可以看成是没有参数的函数，返回一个匿名函数；调用的时候是调用这个返回的匿名函数。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用这个方法的好处是，调用这个方法只需要关注如何操作资源，而不用去关心资源的打开和关闭。这个技巧被称为&lt;code&gt;贷出模式&lt;/code&gt;：loan pattern，因为该函数要个模板方法一样，实现了资源的打开和关闭，而将使用 PrintWriter 操作资源&lt;code&gt;贷出&lt;/code&gt;给函数，交由调用者来实现。&lt;/p&gt;

&lt;p&gt;例子里的 withPrintWriter 把 PrintWriter 借给函数 op。当函数完成的时候，它发出信号说明它不再需要“借”的资源。于是资源被关闭在 finally 块中，以确信其确实被关闭，而忽略函数是正常结束返回还是抛出了异常。&lt;/p&gt;

&lt;p&gt;因为，这个函数有两个参数，所以你可以将该函数柯里化：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def withPrintWriter(file: File)(op: PrintWriter =&amp;gt; Unit) {
  val writer = new PrintWriter(file)
  try {
    op(writer)
  } finally {
    writer.close()
  } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样的话，你可以如下方式调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val file = new File(&quot;date.txt&quot;)
withPrintWriter(file) {
    writer =&amp;gt; writer.println(new java.util.Date)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个例子里，第一个参数列表，包含了一个 File 参数，被写成包围在小括号中。第二个参数列表，包含了一个函数参数，被包围在大括号中。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;当一个函数只有一个参数时，可以使用大括号代替小括号。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;by-name-parameter&quot;&gt;传名参数 by-name parameter&lt;/h1&gt;

&lt;p&gt;《Programming in Scala》的第九章提到了&lt;code&gt;传名参数&lt;/code&gt;这个概念。其中举的例子是：实现一个称为myAssert的断言函数，该函数将带一个函数值做输入并参考一个标志位来决定该做什么。&lt;/p&gt;

&lt;p&gt;如果没有传名参数，你可以这样写myAssert：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;var assertionsEnabled = true 
def myAssert(predicate: () =&amp;gt; Boolean) =  
    if (assertionsEnabled &amp;amp;&amp;amp; !predicate())  
        throw new AssertionError
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个定义是正确的，但使用它会有点儿难看：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;myAssert(() =&amp;gt; 5 &amp;gt; 3) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你或许很想省略函数文本里的空参数列表和&lt;code&gt;=&amp;gt;&lt;/code&gt;符号，写成如下形式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;myAssert(5 &amp;gt; 3) // 不会有效，因为缺少() =&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;传名函数恰好为了实现你的愿望而出现。要实现一个传名函数，要定义参数的类型开始于&lt;code&gt;=&amp;gt;&lt;/code&gt;而不是&lt;code&gt;() =&amp;gt;&lt;/code&gt;。例如，你可以通过改变其类型&lt;code&gt;() =&amp;gt; Boolean&lt;/code&gt;为&lt;code&gt;=&amp;gt; Boolean&lt;/code&gt;，把myAssert的predicate参数改为传名参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def byNameAssert(predicate: =&amp;gt; Boolean) =  
    if (assertionsEnabled &amp;amp;&amp;amp; !predicate)  
        throw new AssertionError  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在你可以在需要断言的属性里省略空的参数了。使用byNameAssert的结果看上去就好象使用了内建控制结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;byNameAssert(5 &amp;gt; 3)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;传名类型中，空的参数列表&lt;code&gt;()&lt;/code&gt;被省略，它仅在参数中被允许。没有什么传名变量或传名字段这样的东西。&lt;/p&gt;

&lt;p&gt;现在，你或许想知道为什么你不能简化myAssert的编写，使用陈旧的Boolean作为它参数的类型，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def boolAssert(predicate: Boolean) =  
    if (assertionsEnabled &amp;amp;&amp;amp; !predicate)  
        throw new AssertionError         
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然这种格式同样合法，并且使用这个版本boolAssert的代码看上去仍然与前面的一样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;boolAssert(5 &amp;gt; 3)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;虽然如此，这两种方式之间存在一个非常重要的差别须指出&lt;/strong&gt;。因为boolAssert的参数类型是Boolean，在boolAssert(5 &amp;gt; 3)里括号中的表达式先于boolAssert的调用被评估。表达式&lt;code&gt;5 &amp;gt; 3&lt;/code&gt;产生true，被传给boolAssert。相对的，因为byNameAssert的predicate参数的类型是&lt;code&gt;=&amp;gt; Boolean&lt;/code&gt;，&lt;code&gt;byNameAssert(5 &amp;gt; 3)&lt;/code&gt;里括号中的表达式不是先于byNameAssert的调用被评估的。而是代之以先创建一个函数值，其apply方法将评估&lt;code&gt;5 &amp;gt; 3&lt;/code&gt;，而这个函数值将被传递给byNameAssert。&lt;/p&gt;

&lt;p&gt;因此这两种方式之间的差别，在于如果断言被禁用，你会看到boolAssert括号里的表达式的某些副作用，而byNameAssert却没有。例如，如果断言被禁用，boolAssert的例子里尝试对&lt;code&gt;x / 0 == 0&lt;/code&gt;的断言将产生一个异常：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; var assertionsEnabled = false 
assertionsEnabled: Boolean = false 
scala&amp;gt; boolAssert(x / 0 == 0)  
java.lang.ArithmeticException: / by zero  
 at .&amp;lt; init&amp;gt;(&amp;lt; console&amp;gt;:8)  
 at .&amp;lt; clinit&amp;gt;(&amp;lt; console&amp;gt;)  
 at RequestResult$.&amp;lt; init&amp;gt;(&amp;lt; console&amp;gt;:3)  
 at RequestResult$.&amp;lt; clinit&amp;gt;(&amp;lt; console&amp;gt;)...  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但在byNameAssert的例子里尝试同样代码的断言将不产生异常：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; byNameAssert(x / 0 == 0) 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;本文主要总结了几种使用Scala高阶函数简化代码的方法，涉及到的知识点有：柯里化、偏函数、函数映射式定义、可变长度参数、贷出模式以及传名参数。需要意识到的是，灵活使用高阶函数可以简化代码，但也可能会增加代码阅读的复杂度。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2015/06/18/simplify-code-using-scala-higher-order-function.html</link>
      <guid>http://blog.javachen.com/2015/06/18/simplify-code-using-scala-higher-order-function.html</guid>
      <pubDate>2015-06-18T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>解决固定导航时锚点偏移问题</title>
      <description>&lt;p&gt;最近Bootstrap修改了博客主题，使其支持响应式布局，并且将导航菜单固定住，不随滚到条滚动，这样做带来的影响是&lt;a href=&quot;/categories.html&quot;&gt;Categories&lt;/a&gt;和&lt;a href=&quot;/tags.html&quot;&gt;Tags&lt;/a&gt;页面点击某一个分类或者标签链接时，&lt;code&gt;锚点定位必然定位于页面顶部，这样一来就会被固定住的导航遮挡&lt;/code&gt;，例如，我在Categories页面，点击&lt;a href=&quot;/categories.html#hbase&quot;&gt;hbase&lt;/a&gt;分类，锚点定位最后如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/2015/fix-anchor-offset-when-using-bootstrap-navbar-fixed-top.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;网上查找了一些资料，找到一篇文章&lt;a href=&quot;http://www.ldsun.com/1815.html&quot;&gt;点击锚点让定位偏移顶部&lt;/a&gt;，这篇文章提到几种解决办法：&lt;/p&gt;

&lt;p&gt;第一种，使用css将锚点偏移：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;a class=&quot;target-fix&quot; &amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;artivle&amp;gt;主体内容...&amp;lt;/article&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;css如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;.target-fix {
    position: relative;
    top: -44px; /*偏移值*/
    display: block;
    height: 0;
    overflow: hidden;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于现代浏览器如果支持css的&lt;code&gt;:target&lt;/code&gt;声明，可以这么设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;article.a-post:target{
    padding-top:44px;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二种，使用JavaScript去调整scroll值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;$(function(){
  if(location.hash){
     var target = $(location.hash);
     if(target.length==1){
         var top = target.offset().top-44;
         if(top &amp;gt; 0){
             $(&#39;html,body&#39;).animate({scrollTop:top}, 1000);
         }
     }
  }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：上面代码中的44为固定的导航所占的像素高度，根据你的实际情况做修改。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当然，你也可以使用jquery-hashchange插件去实现上面的功能，但是需要注意jquery-hashchange是否支持你使用的JQuery版本。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;$(function(){
        /* 绑定事件*/
        $(window).hashchange(function(){
            var target = $(location.hash);
            if(target.length==1){
                 var top = target.offset().top-44;
                 if(top &amp;gt; 0){
                     $(&#39;html,body&#39;).animate({scrollTop:top}, 1000);
                 }
             } 
        });
        /* 触发事件 */
        $(window).hashchange();
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分析上面两种方法，我最后使用的是第二种方法，在core.js文件中添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;$(&#39;a[href^=#][href!=#]&#39;).click(function() {
  var target = document.getElementById(this.hash.slice(1));
  if (!target) return;
  var targetOffset = $(target).offset().top-70;
  $(&#39;html,body&#39;).animate({scrollTop: targetOffset}, 400);
  return false;
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里，我是在链接上监听单击事件，获取目标对象的偏移，上面减去70是因为下面的css代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;#wrap {
  min-height: 100%;
  height: auto;
  margin: 0 auto -60px;
  padding: 70px 0 60px;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;刷新页面，再次点击目录或者标签，就可以正常的跳到锚点位置了。你可以点击分类&lt;a href=&quot;/categories.html#hbase&quot;&gt;hbase&lt;/a&gt; 试试效果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;但是，还没有结束&lt;/strong&gt;。如果是从其他页面，例如，在文章页面点击分类或标签时，页面却不会跳转到正确的锚点位置。这是因为上面的javascript代码只是考虑了当前页面，是在当前页面获取目标的偏离，而没有考虑在另外一个页面单击链接跳到目标页面的锚点的情况。&lt;/p&gt;

&lt;p&gt;所以，我们需要修改代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var handler=function(hash){
    var target = document.getElementById(hash.slice(1));
    if (!target) return;
    var targetOffset = $(target).offset().top-70;
    $(&#39;html,body&#39;).animate({scrollTop: targetOffset}, 400);
}

$(&#39;a[href^=#][href!=#]&#39;).click(function(){
    handler(this.hash)
});

if(location.hash){ handler(location.hash) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，就大功告成了，希望这篇文章对你有所帮助。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ldsun.com/1815.html&quot;&gt;点击锚点让定位偏移顶部&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2015/06/18/fix-anchor-offset-when-using-bootstrap-navbar-fixed-top.html</link>
      <guid>http://blog.javachen.com/2015/06/18/fix-anchor-offset-when-using-bootstrap-navbar-fixed-top.html</guid>
      <pubDate>2015-06-18T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>推荐系统笔记</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1、产生原因&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;信息过载&lt;/li&gt;
  &lt;li&gt;无明确需求&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2、什么是推荐？&lt;/h1&gt;

&lt;p&gt;在信息过载又没有明确需求的情况下，找到用户感兴趣的东西。&lt;/p&gt;

&lt;p&gt;《Mahout实战》上的定义是：&lt;code&gt;推荐就是通过对喜好的这些模式进行预测，借以发现你尚未知晓，却合乎心意的新事物&lt;/code&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3、推荐和搜索区别：&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;相同点：快速发现有用信息的工具&lt;/li&gt;
  &lt;li&gt;不同点：搜索引擎是用户找信息；推荐系统是信息找用户&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决信息过载的问题，代表性的解决方案是分类目录和搜索引擎。和搜索引擎一样，推荐系统也是一种帮助用户快速发现有用信息的工具。和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。&lt;/p&gt;

&lt;p&gt;因此，从某种意义上说，推荐系统和搜索引擎对于用户来说是两个互补的工具。搜索引擎满足了用户有明确目的时的主动查找需求，而推荐系统能够在用户没有明确目的的时候帮助他们发现感兴趣的新内容。&lt;/p&gt;

&lt;p&gt;从物品的角度出发，推荐系统可以&lt;code&gt;更好地发掘物品的长尾&lt;/code&gt;。长尾商品往往代表了一小部分用户的个性化需求，发掘这类信息正是推荐系统的长项。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4、推荐系统定义&lt;/h1&gt;

&lt;p&gt;推荐系统广泛存在于各类网站中，作为一个应用为用户提供个性化推荐。它需要依赖用户的行为数据，因此一般都由后台日志系统、推荐算法系统和前台展示页面3部分构成。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5、推荐系统任务：&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;发现有价值的信息&lt;/li&gt;
  &lt;li&gt;增加曝光度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;推荐系统的任务就是联系用户和信息，一方面帮助用户发现对自己&lt;br /&gt;
有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;6、推荐系统的模块：&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;获取用户偏好，计算用户模型，需求信息&lt;/li&gt;
  &lt;li&gt;推荐对象模型，特征信息&lt;/li&gt;
  &lt;li&gt;推荐算法&lt;/li&gt;
  &lt;li&gt;推荐评估&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-6&quot;&gt;7、推荐系统原理：&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;基于社交网络推荐&lt;/li&gt;
  &lt;li&gt;基于内容推荐：基于物品的属性进行推荐。&lt;/li&gt;
  &lt;li&gt;热门推荐：推荐热门的商品&lt;/li&gt;
  &lt;li&gt;协同过滤：仅仅通过了解用户与物品之间的关系进行推荐。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-7&quot;&gt;8、常见形式：&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;猜你喜欢&lt;/li&gt;
  &lt;li&gt;买了又买&lt;/li&gt;
  &lt;li&gt;精品推荐&lt;/li&gt;
  &lt;li&gt;关联互补&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-8&quot;&gt;9、十大挑战&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;数据稀疏：基于物品协同&lt;/li&gt;
  &lt;li&gt;冷启动&lt;/li&gt;
  &lt;li&gt;增量计算&lt;/li&gt;
  &lt;li&gt;多样性与精确性的选择&lt;/li&gt;
  &lt;li&gt;推荐系统的脆弱性&lt;/li&gt;
  &lt;li&gt;用户行为的挖掘和利用&lt;/li&gt;
  &lt;li&gt;推荐系统的评估&lt;/li&gt;
  &lt;li&gt;用户界面及用户体验&lt;/li&gt;
  &lt;li&gt;多维数据交叉利用&lt;/li&gt;
  &lt;li&gt;社交网络推荐&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-9&quot;&gt;10、用户建模流程：&lt;/h1&gt;

&lt;p&gt;用户–&amp;gt;获取用户信息–&amp;gt;建立用户模型–&amp;gt;模型的更新–&amp;gt;利用模型进行推荐–&amp;gt;提供推荐结果–&amp;gt;用户反馈–&amp;gt;更新模型&lt;/p&gt;

&lt;h1 id=&quot;section-10&quot;&gt;11、创建一个推荐引擎过程：&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;1、创建输入。输入的数据是结构化的，包括用户、商品和喜好，或者还有时间戳，用于拆分数据&lt;/li&gt;
  &lt;li&gt;2、创建推荐程序。选择推荐算法编写推荐程序。&lt;/li&gt;
  &lt;li&gt;3、训练数据与评分。一般会将数据随机拆分为训练数据和测试数据，一般比例设置为8比2。&lt;/li&gt;
  &lt;li&gt;4、评估结果。使用&lt;code&gt;查全率&lt;/code&gt;和&lt;code&gt;查准率&lt;/code&gt;评估推荐程序，或者统计MAE、SMSE、UOC等&lt;/li&gt;
  &lt;li&gt;5、输出结果。取Top K个结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;输入的数据模型包括：用户、商品和偏好值，没有偏好值的关联成为布尔型偏好，这表示用户和物品的关联具备三种可能状态：喜好、不喜欢或无所谓。&lt;/p&gt;

&lt;h1 id=&quot;section-11&quot;&gt;12、推荐系统评测&lt;/h1&gt;

&lt;p&gt;主要有3种评测推荐效果的实验方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;离线实验&lt;/code&gt;：划分训练集和测试集，在训练集训练用户兴趣模型，在测试集预测&lt;/li&gt;
  &lt;li&gt;优点：快速方便&lt;/li&gt;
  &lt;li&gt;缺点：无法用真实的商业指标来衡量&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;用户调查&lt;/code&gt;：用抽样的方法找部分用户试验效果&lt;/li&gt;
  &lt;li&gt;优点：指标比较真实&lt;/li&gt;
  &lt;li&gt;缺点：规模受限，统计意义不够&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;在线实验&lt;/code&gt;：AB测试&lt;/li&gt;
  &lt;li&gt;优点：指标真实&lt;/li&gt;
  &lt;li&gt;缺点：测试时间长，设计复杂&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实际中，这三种方法在推荐算法上线前都要完成。&lt;/p&gt;

&lt;p&gt;评测指标：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;用户满意度&lt;/code&gt;。用户作为推荐系统的重要参与者，其满意度是评测推荐系统的最重要指标。但是，用户满意度没有办法离线计算，只能通过用户调查问卷或者在线实验获得。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;预测准确度&lt;/code&gt;。预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的推荐系统离线评测指标，可以通过离线实验计算。在计算该指标时需要有一个离线的数据集，该数据集包含用户的历史行为记录。然后，将该&lt;br /&gt;
数据集通过时间分成训练集和测试集。最后，通过在训练集上建立用户的行为和兴趣模型预测用户在测试集上的行为，并计算预测行为和测试集上实际行为的重合度作为预测准确度。预测用户对物品评分的行为称为评分预测。评分预测的预测准确度一般通过&lt;code&gt;均方根误差&lt;/code&gt;（RMSE）和&lt;code&gt;平均绝对误差&lt;/code&gt;（MAE）计算。TopN推荐的预测准确率一般通过&lt;code&gt;准确率&lt;/code&gt;（precision）/&lt;code&gt;召回率&lt;/code&gt;（recall）度量。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;覆盖率&lt;/code&gt;。覆盖率（coverage）描述一个推荐系统对物品长尾的发掘能力。覆盖率有不同的定义方法，最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。覆盖率是一个内容提供商会关心的指标。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;多样性&lt;/code&gt;。为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域，即推荐结果需要具有多样性。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;新颖性&lt;/code&gt;。新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。在一个网站中实现新颖性的最简单办法是，把那些用户之前在网站中对其有过行为的物品从推荐列表中过滤掉。评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。因此，如果推荐结果中物品的平均热门程度较低，那么推荐结果就可能有比较高的新颖性。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;惊喜度&lt;/code&gt;。如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;信任度&lt;/code&gt;。度量推荐系统的信任度只能通过问卷调查的方式，询问用户是否信任推荐系统的推荐结果。提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度，而增加推荐系统透明度的主要办法是提供推荐解释。只有让用户了解推荐系统的运行机制，让用户认同推荐系统的运行机制，才会提高用户对推荐系统的信任度。其次是考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。这是因为用户对他们的好友一般都比较信任，因此如果推荐的商品是好友购买过的，那么他们对推荐结果就会相对比较信任。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;实时性&lt;/code&gt;。推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;健壮性&lt;/code&gt;（反作弊）。算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;设计推荐系统时尽量使用代价比较高的用户行为。比如，如果有用户购买行为和用户浏览行为，那么主要应该使用用户购买行为，因为购买需要付费，所以攻击购买行为的代价远远大于攻击浏览行为。&lt;/li&gt;
  &lt;li&gt;在使用数据前，进行攻击检测，从而对数据进行清理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图直观地描述了准确率和召回率的含义:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ww2.sinaimg.cn/large/81b78497jw1efj1yg6uywj20kg0cm778.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一般来说，评测维度分为如下3种。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;用户维度。主要包括用户的人口统计学信息、活跃度以及是不是新用户等。&lt;/li&gt;
  &lt;li&gt;物品维度。包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。&lt;/li&gt;
  &lt;li&gt;时间维度。包括季节，是工作日还是周末，是白天还是晚上等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果能够在推荐系统评测报告中包含不同维度下的系统评测指标，就能帮我们全面地了解推荐系统性能，找到一个看上去比较弱的算法的优势，发现一个看上去比较强的算法的缺点。&lt;/p&gt;

&lt;h1 id=&quot;section-12&quot;&gt;13、协同过滤&lt;/h1&gt;

&lt;p&gt;协同过滤分为：基于用户的推荐和基于物品的推荐。&lt;/p&gt;

&lt;p&gt;基于用户的推荐是找出于该用户邻近的用户，然后将这些用户最感兴趣的物品推荐给该用户。通常包括以下几个组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据模型&lt;/li&gt;
  &lt;li&gt;用户间的相似性度量&lt;/li&gt;
  &lt;li&gt;用户邻域的定义：N个最相似用户构成的邻域和基于阈值的邻域&lt;/li&gt;
  &lt;li&gt;推荐引擎&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相似性度量的说明，请参考 &lt;a href=&quot;/2014/09/22/mahout-recommend-engine.html&quot;&gt;Mahout推荐引擎介绍&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;《Mahout实战》上提到基于用户的推荐，Mahout中最佳方案为：使用两个最近的邻域，欧式距离相似性度量。&lt;/p&gt;

&lt;p&gt;基于物品的推荐，就是给目标用户推荐与他喜欢的物品相似度较高的物品。&lt;/p&gt;

&lt;h1 id=&quot;section-13&quot;&gt;14、冷启动问题&lt;/h1&gt;

&lt;p&gt;冷启动分几种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;用户冷启动&lt;/code&gt;，即用户刚刚来，还没有对物品做出行为，比如你刚注册豆瓣电影，但没有标记过任何一部电影，所以豆瓣很难根据你的行为来做出推荐；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;物品冷启动&lt;/code&gt;，新的物品一进入网站，还没有用户给出过对它的行为，那么如何将新物品推荐给可能会对它感兴趣的用户；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;系统冷启动&lt;/code&gt;， 即新开发的网站如何让用户体验到个性化服务的问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决冷启动的方法：&lt;/p&gt;

&lt;p&gt;1、&lt;code&gt;利用用户注册信息&lt;/code&gt;。我们注册一个网站的账号的时候很可能会填写性别、年龄、职业等人口统计学特征；还有的网站会让用户描述兴趣；以及用户可能通过其他网站比如新浪微博、腾讯账号来登录，这时就可以采用用户的社交数据。利用的人口统计学特征越多，对用户的兴趣描述就越准确。&lt;/p&gt;

&lt;p&gt;2、&lt;code&gt;要求用户在注册时对一些物品进行反馈&lt;/code&gt;，通过这些反馈来推测用户兴趣。使用决策树来选择待测试的物品。&lt;/p&gt;

&lt;p&gt;对于一个物品i，用户们对i的行为可以分为3类（喜欢、不喜欢、无感觉），然后看这三类人的兴趣是不一致，如果这三类人兴趣都差不多，那么i的区分度可能就不那么高，也就是说如果喜欢i和不喜欢i的人是兴趣类似的，那么i就没啥区分度，不能够用来度量新用户的兴趣。这个算法会从区分度最高的物品开始，将用户分成3类，又在每类用户找找到区分度最高的物品继续询问，一层层下去直到最后的叶子节点。&lt;/p&gt;

&lt;p&gt;3、&lt;code&gt;利用物品的内容信息&lt;/code&gt;。通过向量空间模型来表示物品的内容，将其表示成一个关键词向量，计算权重，再通过计算相似度来计算物品的相似度。这里涉及到文本挖掘工作。&lt;/p&gt;

&lt;p&gt;4、&lt;code&gt;发挥专家的作用&lt;/code&gt;，即利用专家来对数据进行标注。这个对绝大多数情况并不现实。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;UserCF对物品冷启动问题并不敏感，因为总有用户会访问到新物品，此时给那些相似的用户推荐这个物品，就会有越来越多的人来访问这个物品，形成良性循环。在ItemCF中，物品冷启动就是比较严重的问题了，物品冷启动必须频繁更新物品相似度表，时间复杂度高。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
      <link>http://blog.javachen.com/2015/06/15/note-about-recommendation-system.html</link>
      <guid>http://blog.javachen.com/2015/06/15/note-about-recommendation-system.html</guid>
      <pubDate>2015-06-15T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Mahout实现协同过滤</title>
      <description>&lt;p&gt;Mahout算法框架自带的推荐器有下面这些：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GenericUserBasedRecommender：基于用户的推荐器，用户数量少时速度快；&lt;/li&gt;
  &lt;li&gt;GenericItemBasedRecommender：基于商品推荐器，商品数量少时速度快，尤其当外部提供了商品相似度数据后效率更好；&lt;/li&gt;
  &lt;li&gt;SlopeOneRecommender：基于slope-one算法的推荐器，在线推荐或更新较快，需要事先大量预处理运算，物品数量少时较好；&lt;/li&gt;
  &lt;li&gt;SVDRecommender：奇异值分解，推荐效果较好，但之前需要大量预处理运算；&lt;/li&gt;
  &lt;li&gt;KnnRecommender：基于k近邻算法(KNN)，适合于物品数量较小时；&lt;/li&gt;
  &lt;li&gt;TreeClusteringRecommender：基于聚类的推荐器，在线推荐较快，之前需要大量预处理运算，用户数量较少时效果好；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mahout最常用的三个推荐器是上述的前三个，本文主要讨论前两种的使用。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;接口相关介绍&lt;/h1&gt;

&lt;p&gt;基于用户或物品的推荐器主要包括以下几个接口：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;DataModel&lt;/code&gt; 是用户喜好信息的抽象接口，它的具体实现支持从任意类型的数据源抽取用户喜好信息。Taste 默认提供 JDBCDataModel 和 FileDataModel，分别支持从数据库和文件中读取用户的喜好信息。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;UserSimilarity&lt;/code&gt; 和 &lt;code&gt;ItemSimilarity&lt;/code&gt;。UserSimilarity 用于定义两个用户间的相似度，它是基于协同过滤的推荐引擎的核心部分，可以用来计算用户的“邻居”，这里我们将与当前用户口味相似的用户称为他的邻居。ItemSimilarity 类似的，计算内容之间的相似度。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;UserNeighborhood&lt;/code&gt; 用于基于用户相似度的推荐方法中，推荐的内容是基于找到与当前用户喜好相似的邻居用户的方式产生的。UserNeighborhood 定义了确定邻居用户的方法，具体实现一般是基于 UserSimilarity 计算得到的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Recommender&lt;/code&gt; 是推荐引擎的抽象接口，Taste 中的核心组件。程序中，为它提供一个 DataModel，它可以计算出对不同用户的推荐内容。实际应用中，主要使用它的实现类 GenericUserBasedRecommender 或者 GenericItemBasedRecommender，分别实现基于用户相似度的推荐引擎或者基于内容的推荐引擎。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;RecommenderEvaluator&lt;/code&gt;：评分器。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;RecommenderIRStatsEvaluator&lt;/code&gt;：搜集推荐性能相关的指标，包括准确率、召回率等等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前，Mahout为DataModel提供了以下几种实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.GenericDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.GenericBooleanPrefDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.PlusAnonymousUserDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.file.FileDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.jdbc.SQL92JDBCDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.jdbc.MySQLJDBCDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.jdbc.PostgreSQLJDBCDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.jdbc.GenericJDBCDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.jdbc.SQL92BooleanPrefJDBCDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.jdbc.MySQLBooleanPrefJDBCDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.jdbc.PostgreBooleanPrefSQLJDBCDataModel&lt;/li&gt;
  &lt;li&gt;org.apache.mahout.cf.taste.impl.model.jdbc.ReloadFromJDBCDataModel&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从类名上就可以大概猜出来每个DataModel的用途，奇怪的是竟然没有HDFS的DataModel，有人实现了一个，请参考&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1579&quot;&gt;MAHOUT-1579&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;UserSimilarity&lt;/code&gt; 和 &lt;code&gt;ItemSimilarity&lt;/code&gt; 相似度实现有以下几种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;CityBlockSimilarity&lt;/code&gt;：基于Manhattan距离相似度&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;EuclideanDistanceSimilarity&lt;/code&gt;：基于欧几里德距离计算相似度&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;LogLikelihoodSimilarity&lt;/code&gt;：基于对数似然比的相似度&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;PearsonCorrelationSimilarity&lt;/code&gt;：基于皮尔逊相关系数计算相似度&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;SpearmanCorrelationSimilarity&lt;/code&gt;：基于皮尔斯曼相关系数相似度&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;TanimotoCoefficientSimilarity&lt;/code&gt;：基于谷本系数计算相似度&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;UncenteredCosineSimilarity&lt;/code&gt;：计算 Cosine 相似度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上相似度的说明，请参考&lt;a href=&quot;/2014/09/22/mahout-recommend-engine.html&quot;&gt;Mahout推荐引擎介绍&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;UserNeighborhood 主要实现有两种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NearestNUserNeighborhood：对每个用户取固定数量N个最近邻居&lt;/li&gt;
  &lt;li&gt;ThresholdUserNeighborhood：对每个用户基于一定的限制，取落在相似度限制以内的所有用户为邻居&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recommender分为以下几种实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GenericUserBasedRecommender：基于用户的推荐引擎&lt;/li&gt;
  &lt;li&gt;GenericBooleanPrefUserBasedRecommender：基于用户的无偏好值推荐引擎&lt;/li&gt;
  &lt;li&gt;GenericItemBasedRecommender：基于物品的推荐引擎&lt;/li&gt;
  &lt;li&gt;GenericBooleanPrefItemBasedRecommender：基于物品的无偏好值推荐引擎&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RecommenderEvaluator有以下几种实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;AverageAbsoluteDifferenceRecommenderEvaluator&lt;/code&gt;：计算平均差值&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;RMSRecommenderEvaluator&lt;/code&gt;：计算均方根差&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RecommenderIRStatsEvaluator的实现类是GenericRecommenderIRStatsEvaluator。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;单机运行&lt;/h1&gt;

&lt;p&gt;首先，需要在maven中加入对mahout的依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.mahout&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;mahout-core&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.9&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;

&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.mahout&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;mahout-integration&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.9&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;

&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.mahout&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;mahout-math&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.9&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;

&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.mahout&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;mahout-examples&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.9&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基于用户的推荐，以FileDataModel为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;File modelFile modelFile = new File(&quot;intro.csv&quot;);

DataModel model = new FileDataModel(modelFile);

//用户相似度，使用基于皮尔逊相关系数计算相似度
UserSimilarity similarity = new PearsonCorrelationSimilarity(model);

//选择邻居用户，使用NearestNUserNeighborhood实现UserNeighborhood接口，选择邻近的4个用户
UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model);

Recommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity);

//给用户1推荐4个物品
List&amp;lt;RecommendedItem&amp;gt; recommendations = recommender.recommend(1, 4);

for (RecommendedItem recommendation : recommendations) {
    System.out.println(recommendation);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;br /&gt;
FileDataModel要求输入文件中的字段分隔符为逗号或者制表符，如果你想使用其他分隔符，你可以扩展一个FileDataModel的实现，例如，mahout中已经提供了一个解析MoiveLens的数据集（分隔符为&lt;code&gt;::&lt;/code&gt;）的实现GroupLensDataModel。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GenericUserBasedRecommender是基于用户的简单推荐器实现类，推荐主要参照传入的DataModel和UserNeighborhood，总体是三个步骤：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1) 从UserNeighborhood获取当前用户Ui最相似的K个用户集合{U1, U2, …Uk}；&lt;/li&gt;
  &lt;li&gt;(2) 从这K个用户集合排除Ui的偏好商品，剩下的Item集合为{Item0, Item1, …Itemm}；&lt;/li&gt;
  &lt;li&gt;(3) 对Item集合里每个Itemj计算Ui可能偏好程度值pref(Ui, Itemj)，并把Item按此数值从高到低排序，前N个item推荐给用户Ui。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对相同用户重复获得推荐结果，我们可以改用CachingRecommender来包装GenericUserBasedRecommender对象，将推荐结果缓存起来：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Recommender cachingRecommender = new CachingRecommender(recommender);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码可以在main方法中直接运行，然后，我们可以获取推荐模型的评分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//使用平均绝对差值获得评分
RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();
// 用RecommenderBuilder构建推荐引擎
RecommenderBuilder recommenderBuilder = new RecommenderBuilder() {
    @Override
    public Recommender buildRecommender(DataModel model) throws TasteException {
        UserSimilarity similarity = new PearsonCorrelationSimilarity(model);
        UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model);
        return new GenericUserBasedRecommender(model, neighborhood, similarity);
    }
};
// Use 70% of the data to train; test using the other 30%.
double score = evaluator.evaluate(recommenderBuilder, null, model, 0.7, 1.0);
System.out.println(score);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，可以获取推荐结果的查准率和召回率：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator();
// Build the same recommender for testing that we did last time:
RecommenderBuilder recommenderBuilder = new RecommenderBuilder() {
    @Override
    public Recommender buildRecommender(DataModel model) throws TasteException {
        UserSimilarity similarity = new PearsonCorrelationSimilarity(model);
        UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model);
        return new GenericUserBasedRecommender(model, neighborhood, similarity);
    }
};
// 计算推荐4个结果时的查准率和召回率
IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder,null, model, null, 4,
        GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD,1.0);
System.out.println(stats.getPrecision());
System.out.println(stats.getRecall());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是基于物品的推荐，代码大体相似，只是没有了UserNeighborhood，然后将上面代码中的User换成Item即可，完整代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;File modelFile modelFile = new File(&quot;intro.csv&quot;);

DataModel model = new FileDataModel(new File(file));

// Build the same recommender for testing that we did last time:
RecommenderBuilder recommenderBuilder = new RecommenderBuilder() {
    @Override
    public Recommender buildRecommender(DataModel model) throws TasteException {
        ItemSimilarity similarity = new PearsonCorrelationSimilarity(model);
        return new GenericItemBasedRecommender(model, similarity);
    }
};

//获取推荐结果
List&amp;lt;RecommendedItem&amp;gt; recommendations = recommenderBuilder.buildRecommender(model).recommend(1, 4);

for (RecommendedItem recommendation : recommendations) {
    System.out.println(recommendation);
}

//计算评分
RecommenderEvaluator evaluator =
        new AverageAbsoluteDifferenceRecommenderEvaluator();
// Use 70% of the data to train; test using the other 30%.
double score = evaluator.evaluate(recommenderBuilder, null, model, 0.7, 1.0);
System.out.println(score);

//计算查全率和查准率
RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator();

// Evaluate precision and recall &quot;at 2&quot;:
IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder,
        null, model, null, 4,
        GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD,
        1.0);
System.out.println(stats.getPrecision());
System.out.println(stats.getRecall());
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;spark&quot;&gt;在Spark中运行&lt;/h1&gt;

&lt;p&gt;在Spark中运行，需要将Mahout相关的jar添加到Spark的classpath中，修改/etc/spark/conf/spark-env.sh，添加下面两行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;SPARK_DIST_CLASSPATH=&quot;$SPARK_DIST_CLASSPATH:/usr/lib/mahout/lib/*&quot;
SPARK_DIST_CLASSPATH=&quot;$SPARK_DIST_CLASSPATH:/usr/lib/mahout/*&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，以本地模式在spark-shell中运行下面代码交互测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//注意：这里是本地目录
val model = new FileDataModel(new File(&quot;intro.csv&quot;))

val evaluator = new RMSRecommenderEvaluator()
val recommenderBuilder = new RecommenderBuilder {
  override def buildRecommender(dataModel: DataModel): Recommender = {
    val similarity = new LogLikelihoodSimilarity(dataModel)
    new GenericItemBasedRecommender(dataModel, similarity)
  }
}

val score = evaluator.evaluate(recommenderBuilder, null, model, 0.95, 0.05)
println(s&quot;Score=$score&quot;)

val recommender=recommenderBuilder.buildRecommender(model)
val users=trainingRatings.map(_.user).distinct().take(20)

import scala.collection.JavaConversions._

val result=users.par.map{user=&amp;gt;
  user+&quot;,&quot;+recommender.recommend(user,40).map(_.getItemID).mkString(&quot;,&quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/sujitpal/mia-scala-examples&quot;&gt;https://github.com/sujitpal/mia-scala-examples&lt;/a&gt;上面有一个评估基于物品或是用户的各种相似度下的评分的类，叫做 RecommenderEvaluator，供大家学习参考。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;分布式运行&lt;/h1&gt;

&lt;p&gt;Mahout提供了&lt;code&gt;org.apache.mahout.cf.taste.hadoop.item.RecommenderJob&lt;/code&gt;类以MapReduce的方式来实现基于物品的协同过滤，查看该类的使用说明：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop jar /usr/lib/mahout/mahout-examples-0.9-cdh5.4.0-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob
15/06/10 16:19:34 ERROR common.AbstractJob: Missing required option --similarityClassname
Missing required option --similarityClassname
Usage:
 [--input &amp;lt;input&amp;gt; --output &amp;lt;output&amp;gt; --numRecommendations &amp;lt;numRecommendations&amp;gt;
--usersFile &amp;lt;usersFile&amp;gt; --itemsFile &amp;lt;itemsFile&amp;gt; --filterFile &amp;lt;filterFile&amp;gt;
--booleanData &amp;lt;booleanData&amp;gt; --maxPrefsPerUser &amp;lt;maxPrefsPerUser&amp;gt;
--minPrefsPerUser &amp;lt;minPrefsPerUser&amp;gt; --maxSimilaritiesPerItem
&amp;lt;maxSimilaritiesPerItem&amp;gt; --maxPrefsInItemSimilarity &amp;lt;maxPrefsInItemSimilarity&amp;gt;
--similarityClassname &amp;lt;similarityClassname&amp;gt; --threshold &amp;lt;threshold&amp;gt;
--outputPathForSimilarityMatrix &amp;lt;outputPathForSimilarityMatrix&amp;gt; --randomSeed
&amp;lt;randomSeed&amp;gt; --sequencefileOutput --help --tempDir &amp;lt;tempDir&amp;gt; --startPhase
&amp;lt;startPhase&amp;gt; --endPhase &amp;lt;endPhase&amp;gt;]
--similarityClassname (-s) similarityClassname    Name of distributed
                                                  similarity measures class to
                                                  instantiate, alternatively
                                                  use one of the predefined
                                                  similarities
                                                  ([SIMILARITY_COOCCURRENCE,
                                                  SIMILARITY_LOGLIKELIHOOD,
                                                  SIMILARITY_TANIMOTO_COEFFICIEN
                                                  T, SIMILARITY_CITY_BLOCK,
                                                  SIMILARITY_COSINE,
                                                  SIMILARITY_PEARSON_CORRELATION
                                                  ,
                                                  SIMILARITY_EUCLIDEAN_DISTANCE]
                                                  )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，该类可以接收的命令行参数如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;--input(path)&lt;/code&gt;: 存储用户偏好数据的目录，该目录下可以包含一个或多个存储用户偏好数据的文本文件；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--output(path)&lt;/code&gt;: 结算结果的输出目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--numRecommendations (integer)&lt;/code&gt;: 为每个用户推荐的item数量，默认为10&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--usersFile (path)&lt;/code&gt;: 指定一个包含了一个或多个存储userID的文件路径，仅为该路径下所有文件包含的userID做推荐计算 (该选项可选)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--itemsFile (path)&lt;/code&gt;: 指定一个包含了一个或多个存储itemID的文件路径，仅为该路径下所有文件包含的itemID做推荐计算 (该选项可选)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--filterFile (path)&lt;/code&gt;: 指定一个路径，该路径下的文件包含了&lt;code&gt;[userID,itemID]&lt;/code&gt;值对，userID和itemID用逗号分隔。计算结果将不会为user推荐&lt;code&gt;[userID,itemID]&lt;/code&gt;值对中包含的item (该选项可选)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--booleanData (boolean)&lt;/code&gt;: 如果输入数据不包含偏好数值，则将该参数设置为true，默认为false&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--maxPrefsPerUser (integer)&lt;/code&gt;: 在最后计算推荐结果的阶段，针对每一个user使用的偏好数据的最大数量，默认为10&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--minPrefsPerUser (integer)&lt;/code&gt;: 在相似度计算中，忽略所有偏好数据量少于该值的用户，默认为1&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--maxSimilaritiesPerItem (integer)&lt;/code&gt;: 针对每个item的相似度最大值，默认为100&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--maxPrefsPerUserInItemSimilarity (integer)&lt;/code&gt;: 在item相似度计算阶段，针对每个用户考虑的偏好数据最大数量，默认为1000&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--similarityClassname (classname)&lt;/code&gt;: 向量相似度计算类&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;outputPathForSimilarityMatrix&lt;/code&gt;：SimilarityMatrix输出目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--randomSeed&lt;/code&gt;：随机种子&lt;br /&gt;
–&lt;code&gt;sequencefileOutput&lt;/code&gt;：序列文件输出路径&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--tempDir (path)&lt;/code&gt;: 存储临时文件的目录，默认为当前用户的home目录下的temp目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--startPhase&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--endPhase&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--threshold (double)&lt;/code&gt;: 忽略相似度低于该阀值的item对&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个例子如下，使用SIMILARITY_LOGLIKELIHOOD相似度推荐物品：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop jar /usr/lib/mahout/mahout-examples-0.9-cdh5.4.0-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob --input /tmp/mahout/part-00000 --output /tmp/mahout-out  -s SIMILARITY_LOGLIKELIHOOD
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，mahout使用的reduce数目为1，这样造成大数据处理时效率较低，可以通过参数mahout执行脚本中的&lt;code&gt;MAHOUT_OPTS&lt;/code&gt;中的&lt;code&gt;-Dmapred.reduce.tasks&lt;/code&gt;参数指定reduce数目。&lt;/p&gt;

&lt;p&gt;上面命令运行完成之后，会在当前用户的hdfs主目录生成temp目录，该目录可由&lt;code&gt;--tempDir (path)&lt;/code&gt;参数设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -ls temp
Found 10 items
-rw-r--r--   3 root hadoop          7 2015-06-10 14:42 temp/maxValues.bin
-rw-r--r--   3 root hadoop    5522717 2015-06-10 14:42 temp/norms.bin
drwxr-xr-x   - root hadoop          0 2015-06-10 14:41 temp/notUsed
-rw-r--r--   3 root hadoop          7 2015-06-10 14:42 temp/numNonZeroEntries.bin
-rw-r--r--   3 root hadoop    3452222 2015-06-10 14:41 temp/observationsPerColumn.bin
drwxr-xr-x   - root hadoop          0 2015-06-10 14:47 temp/pairwiseSimilarity
drwxr-xr-x   - root hadoop          0 2015-06-10 14:52 temp/partialMultiply
drwxr-xr-x   - root hadoop          0 2015-06-10 14:39 temp/preparePreferenceMatrix
drwxr-xr-x   - root hadoop          0 2015-06-10 14:50 temp/similarityMatrix
drwxr-xr-x   - root hadoop          0 2015-06-10 14:42 temp/weights
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;观察yarn的管理界面，该命令会生成9个任务，任务名称依次是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PreparePreferenceMatrixJob-ItemIDIndexMapper-Reducer&lt;/li&gt;
  &lt;li&gt;PreparePreferenceMatrixJob-ToItemPrefsMapper-Reducer&lt;/li&gt;
  &lt;li&gt;PreparePreferenceMatrixJob-ToItemVectorsMapper-Reducer&lt;/li&gt;
  &lt;li&gt;RowSimilarityJob-CountObservationsMapper-Reducer&lt;/li&gt;
  &lt;li&gt;RowSimilarityJob-VectorNormMapper-Reducer&lt;/li&gt;
  &lt;li&gt;RowSimilarityJob-CooccurrencesMapper-Reducer&lt;/li&gt;
  &lt;li&gt;RowSimilarityJob-UnsymmetrifyMapper-Reducer&lt;/li&gt;
  &lt;li&gt;partialMultiply&lt;/li&gt;
  &lt;li&gt;RecommenderJob-PartialMultiplyMapper-Reducer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从任务名称，大概可以知道每个任务在做什么，如果你的输入参数不一样，生成的任务数可能不一样，这个需要测试一下才能确认。&lt;/p&gt;

&lt;p&gt;在hdfs上查看输出的结果，用户和推荐结果用&lt;code&gt;\t&lt;/code&gt;分隔，推荐结果中物品之间用逗号分隔，物品后面通过冒号连接评分：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;843 [10709679:4.8334665,8389878:4.833426,9133835:4.7503786,10366169:4.7503185,9007487:4.750272,8149253:4.7501993,10366165:4.750115,9780049:4.750108,8581254:4.750071,10456307:4.7500467]
6253    [10117445:3.0375953,10340299:3.0340924,8321090:3.0340924,10086615:3.032164,10436801:3.0187714,9668385:3.0141575,8502110:3.013954,10476325:3.0074399,10318667:3.0004222,8320987:3.0003839]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用Java API方式执行，请参考&lt;a href=&quot;http://blog.fens.me/hadoop-mahout-mapreduce-itemcf/&quot;&gt;Mahout分步式程序开发 基于物品的协同过滤ItemCF&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在Scala或者Spark中，可以以Java API或者命令方式运行，最后还可以通过Spark来处理推荐的结果，例如：过滤、去重、补足数据，这部分内容不做介绍。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://matrix-lisp.github.io/blog/2013/12/20/mahout-taste-CF/&quot;&gt;协同过滤原理与Mahout实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/06/10/collaborative-filtering-using-mahout.html</link>
      <guid>http://blog.javachen.com/2015/06/10/collaborative-filtering-using-mahout.html</guid>
      <pubDate>2015-06-10T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spark On YARN内存分配</title>
      <description>&lt;p&gt;本文主要了解Spark On YARN部署模式下的内存分配情况，因为没有深入研究Spark的源代码，所以只能根据日志去看相关的源代码，从而了解“为什么会这样，为什么会那样”。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;说明&lt;/h1&gt;

&lt;p&gt;按照Spark应用程序中的driver分布方式不同，Spark on YARN有两种模式： &lt;code&gt;yarn-client&lt;/code&gt;模式、&lt;code&gt;yarn-cluster&lt;/code&gt;模式。&lt;/p&gt;

&lt;p&gt;当在YARN上运行Spark作业，每个Spark executor作为一个YARN容器运行。Spark可以使得多个Tasks在同一个容器里面运行。&lt;/p&gt;

&lt;p&gt;下图是yarn-cluster模式的作业执行图，图片来源于网络：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.guozhongxin.com/images/taobao.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于Spark On YARN相关的配置参数，请参考&lt;a href=&quot;/2015/06/07/spark-configuration.html&quot;&gt;Spark配置参数&lt;/a&gt;。本文主要讨论内存分配情况，所以只需要关注以下几个内心相关的参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;spark.driver.memory&lt;/code&gt;：默认值512m&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.executor.memory&lt;/code&gt;：默认值512m&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.yarn.am.memory&lt;/code&gt;：默认值512m&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.yarn.executor.memoryOverhead&lt;/code&gt;：值为&lt;code&gt;executorMemory * 0.07, with minimum of 384&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.yarn.driver.memoryOverhead&lt;/code&gt;：值为&lt;code&gt;driverMemory * 0.07, with minimum of 384&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.yarn.am.memoryOverhead&lt;/code&gt;：值为&lt;code&gt;AM memory * 0.07, with minimum of 384&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;--executor-memory/spark.executor.memory&lt;/code&gt; 控制 executor 的堆的大小，但是 JVM 本身也会占用一定的堆空间，比如内部的 String 或者直接 byte buffer，&lt;code&gt;spark.yarn.XXX.memoryOverhead&lt;/code&gt;属性决定向 YARN 请求的每个 executor 或dirver或am 的额外堆内存大小，默认值为 &lt;code&gt;max(384, 0.07 * spark.executor.memory&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;在 executor 执行的时候配置过大的 memory 经常会导致过长的GC延时，64G是推荐的一个 executor 内存大小的上限。&lt;/li&gt;
  &lt;li&gt;HDFS client 在大量并发线程时存在性能问题。大概的估计是每个 executor 中最多5个并行的 task 就可以占满写入带宽。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外，因为任务是提交到YARN上运行的，所以YARN中有几个关键参数，参考&lt;a href=&quot;/2015/06/05/yarn-memory-and-cpu-configuration.html&quot;&gt;YARN的内存和CPU配置&lt;/a&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;yarn.app.mapreduce.am.resource.mb&lt;/code&gt;：AM能够申请的最大内存，默认值为1536MB&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.nodemanager.resource.memory-mb&lt;/code&gt;：nodemanager能够申请的最大内存，默认值为8192MB&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.scheduler.minimum-allocation-mb&lt;/code&gt;：调度时一个container能够申请的最小资源，默认值为1024MB&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.scheduler.maximum-allocation-mb&lt;/code&gt;：调度时一个container能够申请的最大资源，默认值为8192MB&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;测试&lt;/h1&gt;

&lt;p&gt;Spark集群测试环境为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;master：64G内存，16核cpu&lt;/li&gt;
  &lt;li&gt;worker：128G内存，32核cpu&lt;/li&gt;
  &lt;li&gt;worker：128G内存，32核cpu&lt;/li&gt;
  &lt;li&gt;worker：128G内存，32核cpu&lt;/li&gt;
  &lt;li&gt;worker：128G内存，32核cpu&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：YARN集群部署在Spark集群之上的，每一个worker节点上同时部署了一个NodeManager，并且YARN集群中的配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.nodemanager.resource.memory-mb&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;106496&amp;lt;/value&amp;gt; &amp;lt;!-- 104G --&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.scheduler.minimum-allocation-mb&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;2048&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.scheduler.maximum-allocation-mb&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;106496&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.app.mapreduce.am.resource.mb&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;2048&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将spark的日志基本调为DEBUG，并将log4j.logger.org.apache.hadoop设置为WARN建设不必要的输出，修改/etc/spark/conf/log4j.properties：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;# Set everything to be logged to the console
log4j.rootCategory=DEBUG, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Settings to quiet third party logs that are too verbose
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.apache.hadoop=WARN
log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来是运行测试程序，以官方自带的SparkPi例子为例，&lt;code&gt;下面主要测试client模式，至于cluster模式请参考下面的过程&lt;/code&gt;。运行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn-client  \
    --num-executors 4 \
    --driver-memory 2g \
    --executor-memory 3g \
    --executor-cores 4 \
    /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar \
    100000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;观察输出日志（无关的日志被略去）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;15/06/08 13:57:01 INFO SparkContext: Running Spark version 1.3.0
15/06/08 13:57:02 INFO SecurityManager: Changing view acls to: root
15/06/08 13:57:02 INFO SecurityManager: Changing modify acls to: root

15/06/08 13:57:03 INFO MemoryStore: MemoryStore started with capacity 1060.3 MB

15/06/08 13:57:04 DEBUG YarnClientSchedulerBackend: ClientArguments called with: --arg bj03-bi-pro-hdpnamenn:51568 --num-executors 4 --num-executors 4 --executor-memory 3g --executor-memory 3g --executor-cores 4 --executor-cores 4 --name Spark Pi
15/06/08 13:57:04 DEBUG YarnClientSchedulerBackend: [actor] handled message (24.52531 ms) ReviveOffers from Actor[akka://sparkDriver/user/CoarseGrainedScheduler#864850679]
15/06/08 13:57:05 INFO Client: Requesting a new application from cluster with 4 NodeManagers
15/06/08 13:57:05 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (106496 MB per container)
15/06/08 13:57:05 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
15/06/08 13:57:05 INFO Client: Setting up container launch context for our AM

15/06/08 13:57:07 DEBUG Client: ===============================================================================
15/06/08 13:57:07 DEBUG Client: Yarn AM launch context:
15/06/08 13:57:07 DEBUG Client:     user class: N/A
15/06/08 13:57:07 DEBUG Client:     env:
15/06/08 13:57:07 DEBUG Client:         CLASSPATH -&amp;gt; &amp;lt;CPS&amp;gt;/__spark__.jar&amp;lt;CPS&amp;gt;$HADOOP_CONF_DIR&amp;lt;CPS&amp;gt;$HADOOP_COMMON_HOME/*&amp;lt;CPS&amp;gt;$HADOOP_COMMON_HOME/lib/*&amp;lt;CPS&amp;gt;$HADOOP_HDFS_HOME/*&amp;lt;CPS&amp;gt;$HADOOP_HDFS_HOME/lib/*&amp;lt;CPS&amp;gt;$HADOOP_MAPRED_HOME/*&amp;lt;CPS&amp;gt;$HADOOP_MAPRED_HOME/lib/*&amp;lt;CPS&amp;gt;$HADOOP_YARN_HOME/*&amp;lt;CPS&amp;gt;$HADOOP_YARN_HOME/lib/*&amp;lt;CPS&amp;gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*&amp;lt;CPS&amp;gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&amp;lt;CPS&amp;gt;:/usr/lib/spark/lib/spark-assembly.jar::/usr/lib/hadoop/lib/*:/usr/lib/hadoop/*:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-mapreduce/lib/*:/usr/lib/hadoop-mapreduce/*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/*:/usr/lib/hive/lib/*:/usr/lib/flume-ng/lib/*:/usr/lib/paquet/lib/*:/usr/lib/avro/lib/*
15/06/08 13:57:07 DEBUG Client:         SPARK_DIST_CLASSPATH -&amp;gt; :/usr/lib/spark/lib/spark-assembly.jar::/usr/lib/hadoop/lib/*:/usr/lib/hadoop/*:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-mapreduce/lib/*:/usr/lib/hadoop-mapreduce/*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/*:/usr/lib/hive/lib/*:/usr/lib/flume-ng/lib/*:/usr/lib/paquet/lib/*:/usr/lib/avro/lib/*
15/06/08 13:57:07 DEBUG Client:         SPARK_YARN_CACHE_FILES_FILE_SIZES -&amp;gt; 97237208
15/06/08 13:57:07 DEBUG Client:         SPARK_YARN_STAGING_DIR -&amp;gt; .sparkStaging/application_1433742899916_0001
15/06/08 13:57:07 DEBUG Client:         SPARK_YARN_CACHE_FILES_VISIBILITIES -&amp;gt; PRIVATE
15/06/08 13:57:07 DEBUG Client:         SPARK_USER -&amp;gt; root
15/06/08 13:57:07 DEBUG Client:         SPARK_YARN_MODE -&amp;gt; true
15/06/08 13:57:07 DEBUG Client:         SPARK_YARN_CACHE_FILES_TIME_STAMPS -&amp;gt; 1433743027399
15/06/08 13:57:07 DEBUG Client:         SPARK_YARN_CACHE_FILES -&amp;gt; hdfs://mycluster:8020/user/root/.sparkStaging/application_1433742899916_0001/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar#__spark__.jar
15/06/08 13:57:07 DEBUG Client:     resources:
15/06/08 13:57:07 DEBUG Client:         __spark__.jar -&amp;gt; resource { scheme: &quot;hdfs&quot; host: &quot;mycluster&quot; port: 8020 file: &quot;/user/root/.sparkStaging/application_1433742899916_0001/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar&quot; } size: 97237208 timestamp: 1433743027399 type: FILE visibility: PRIVATE
15/06/08 13:57:07 DEBUG Client:     command:
15/06/08 13:57:07 DEBUG Client:         /bin/java -server -Xmx512m -Djava.io.tmpdir=/tmp &#39;-Dspark.eventLog.enabled=true&#39; &#39;-Dspark.executor.instances=4&#39; &#39;-Dspark.executor.memory=3g&#39; &#39;-Dspark.executor.cores=4&#39; &#39;-Dspark.driver.port=51568&#39; &#39;-Dspark.serializer=org.apache.spark.serializer.KryoSerializer&#39; &#39;-Dspark.driver.appUIAddress=http://bj03-bi-pro-hdpnamenn:4040&#39; &#39;-Dspark.executor.id=&amp;lt;driver&amp;gt;&#39; &#39;-Dspark.kryo.classesToRegister=scala.collection.mutable.BitSet,scala.Tuple2,scala.Tuple1,org.apache.spark.mllib.recommendation.Rating&#39; &#39;-Dspark.driver.maxResultSize=8g&#39; &#39;-Dspark.jars=file:/usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar&#39; &#39;-Dspark.driver.memory=2g&#39; &#39;-Dspark.eventLog.dir=hdfs://mycluster:8020/user/spark/applicationHistory&#39; &#39;-Dspark.app.name=Spark Pi&#39; &#39;-Dspark.fileserver.uri=http://X.X.X.X:49172&#39; &#39;-Dspark.tachyonStore.folderName=spark-81ae0186-8325-40f2-867b-65ee7c922357&#39; -Dspark.yarn.app.container.log.dir=&amp;lt;LOG_DIR&amp;gt; org.apache.spark.deploy.yarn.ExecutorLauncher --arg &#39;bj03-bi-pro-hdpnamenn:51568&#39; --executor-memory 3072m --executor-cores 4 --num-executors  4 1&amp;gt; &amp;lt;LOG_DIR&amp;gt;/stdout 2&amp;gt; &amp;lt;LOG_DIR&amp;gt;/stderr
15/06/08 13:57:07 DEBUG Client: ===============================================================================
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从&lt;code&gt;Will allocate AM container, with 896 MB memory including 384 MB overhead&lt;/code&gt;日志可以看到，AM占用了&lt;code&gt;896 MB&lt;/code&gt;内存，除掉&lt;code&gt;384 MB&lt;/code&gt;的overhead内存，实际上只有&lt;code&gt;512 MB&lt;/code&gt;，即&lt;code&gt;spark.yarn.am.memory&lt;/code&gt;的默认值，另外可以看到YARN集群有4个NodeManager，每个container最多有106496 MB内存。&lt;/p&gt;

&lt;p&gt;Yarn AM launch context启动了一个Java进程，设置的JVM内存为&lt;code&gt;512m&lt;/code&gt;，见&lt;code&gt;/bin/java -server -Xmx512m&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;这里为什么会取默认值呢？查看打印上面这行日志的代码，见org.apache.spark.deploy.yarn.Client：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;  private def verifyClusterResources(newAppResponse: GetNewApplicationResponse): Unit = {
    val maxMem = newAppResponse.getMaximumResourceCapability().getMemory()
    logInfo(&quot;Verifying our application has not requested more than the maximum &quot; +
      s&quot;memory capability of the cluster ($maxMem MB per container)&quot;)
    val executorMem = args.executorMemory + executorMemoryOverhead
    if (executorMem &amp;gt; maxMem) {
      throw new IllegalArgumentException(s&quot;Required executor memory (${args.executorMemory}&quot; +
        s&quot;+$executorMemoryOverhead MB) is above the max threshold ($maxMem MB) of this cluster!&quot;)
    }
    val amMem = args.amMemory + amMemoryOverhead
    if (amMem &amp;gt; maxMem) {
      throw new IllegalArgumentException(s&quot;Required AM memory (${args.amMemory}&quot; +
        s&quot;+$amMemoryOverhead MB) is above the max threshold ($maxMem MB) of this cluster!&quot;)
    }
    logInfo(&quot;Will allocate AM container, with %d MB memory including %d MB overhead&quot;.format(
      amMem,
      amMemoryOverhead))
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;args.amMemory来自ClientArguments类，这个类中会校验输出参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;  private def validateArgs(): Unit = {
    if (numExecutors &amp;lt;= 0) {
      throw new IllegalArgumentException(
        &quot;You must specify at least 1 executor!\n&quot; + getUsageMessage())
    }
    if (executorCores &amp;lt; sparkConf.getInt(&quot;spark.task.cpus&quot;, 1)) {
      throw new SparkException(&quot;Executor cores must not be less than &quot; +
        &quot;spark.task.cpus.&quot;)
    }
    if (isClusterMode) {
      for (key &amp;lt;- Seq(amMemKey, amMemOverheadKey, amCoresKey)) {
        if (sparkConf.contains(key)) {
          println(s&quot;$key is set but does not apply in cluster mode.&quot;)
        }
      }
      amMemory = driverMemory
      amCores = driverCores
    } else {
      for (key &amp;lt;- Seq(driverMemOverheadKey, driverCoresKey)) {
        if (sparkConf.contains(key)) {
          println(s&quot;$key is set but does not apply in client mode.&quot;)
        }
      }
      sparkConf.getOption(amMemKey)
        .map(Utils.memoryStringToMb)
        .foreach { mem =&amp;gt; amMemory = mem }
      sparkConf.getOption(amCoresKey)
        .map(_.toInt)
        .foreach { cores =&amp;gt; amCores = cores }
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面代码可以看到当 isClusterMode 为true时，则args.amMemory值为driverMemory的值；否则，则从&lt;code&gt;spark.yarn.am.memory&lt;/code&gt;中取，如果没有设置该属性，则取默认值512m。isClusterMode 为true的条件是 userClass 不为空，&lt;code&gt;def isClusterMode: Boolean = userClass != null&lt;/code&gt;，即输出参数需要有&lt;code&gt;--class&lt;/code&gt;参数，而从下面日志可以看到ClientArguments的输出参数中并没有该参数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;15/06/08 13:57:04 DEBUG YarnClientSchedulerBackend: ClientArguments called with: --arg bj03-bi-pro-hdpnamenn:51568 --num-executors 4 --num-executors 4 --executor-memory 3g --executor-memory 3g --executor-cores 4 --executor-cores 4 --name Spark Pi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;故，要想设置AM申请的内存值，要么使用cluster模式，要么在client模式中，是有&lt;code&gt;--conf&lt;/code&gt;手动设置&lt;code&gt;spark.yarn.am.memory&lt;/code&gt;属性，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn-client  \
    --num-executors 4 \
    --driver-memory 2g \
    --executor-memory 3g \
    --executor-cores 4 \
    --conf spark.yarn.am.memory=1024m \
    /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar \
    100000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开YARN管理界面，可以看到：&lt;/p&gt;

&lt;p&gt;a. Spark Pi 应用启动了5个Container，使用了18G内存、5个CPU core&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/hadoop/memory-in-spark-on-yarn-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;b. YARN为AM启动了一个Container，占用内存为2048M&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/hadoop/memory-in-spark-on-yarn-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;c. YARN启动了4个Container运行任务，每一个Container占用内存为4096M&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/hadoop/memory-in-spark-on-yarn-3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为什么会是&lt;code&gt;2G +4G *4=18G&lt;/code&gt;呢？第一个Container只申请了2G内存，是因为我们的程序只为AM申请了512m内存，而&lt;code&gt;yarn.scheduler.minimum-allocation-mb&lt;/code&gt;参数决定了最少要申请2G内存。至于其余的Container，我们设置了executor-memory内存为3G，为什么每一个Container占用内存为4096M呢？&lt;/p&gt;

&lt;p&gt;为了找出规律，多测试几组数据，分别测试并收集executor-memory为3G、4G、5G、6G时每个executor对应的Container内存申请情况：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;executor-memory=3g：2G+4G * 4=18G&lt;/li&gt;
  &lt;li&gt;executor-memory=4g：2G+6G * 4=26G&lt;/li&gt;
  &lt;li&gt;executor-memory=5g：2G+6G * 4=26G&lt;/li&gt;
  &lt;li&gt;executor-memory=6g：2G+8G * 4=34G&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于这个问题，我是查看源代码，根据org.apache.spark.deploy.yarn.ApplicationMaster -&amp;gt; YarnRMClient -&amp;gt; YarnAllocator的类查找路径找到YarnAllocator中有这样一段代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;  // Executor memory in MB.
  protected val executorMemory = args.executorMemory
  // Additional memory overhead.
  protected val memoryOverhead: Int = sparkConf.getInt(&quot;spark.yarn.executor.memoryOverhead&quot;,
    math.max((MEMORY_OVERHEAD_FACTOR * executorMemory).toInt, MEMORY_OVERHEAD_MIN))
  // Number of cores per executor.
  protected val executorCores = args.executorCores
  // Resource capability requested for each executors
  private val resource = Resource.newInstance(executorMemory + memoryOverhead, executorCores)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为没有具体的去看YARN的源代码，所以这里猜测Container的大小是根据&lt;code&gt;executorMemory + memoryOverhead&lt;/code&gt;计算出来的，大概的规则是每一个Container的大小必须为&lt;code&gt;yarn.scheduler.minimum-allocation-mb&lt;/code&gt;值的整数倍，当&lt;code&gt;executor-memory=3g&lt;/code&gt;时，&lt;code&gt;executorMemory + memoryOverhead&lt;/code&gt;为3G+384M=3456M，需要申请的Container大小为&lt;code&gt;yarn.scheduler.minimum-allocation-mb&lt;/code&gt; * 2 =4096m=4G，其他依此类推。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Yarn always rounds up memory requirement to multiples of &lt;code&gt;yarn.scheduler.minimum-allocation-mb&lt;/code&gt;, which by default is 1024 or 1GB.&lt;/li&gt;
    &lt;li&gt;Spark adds an &lt;code&gt;overhead&lt;/code&gt; to &lt;code&gt;SPARK_EXECUTOR_MEMORY/SPARK_DRIVER_MEMORY&lt;/code&gt; before asking Yarn for the amount.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;另外，需要注意memoryOverhead的计算方法，当executorMemory的值很大时，memoryOverhead的值相应会变大，这个时候就不是384m了，相应的Container申请的内存值也变大了，例如：当executorMemory设置为90G时，memoryOverhead值为&lt;code&gt;math.max(0.07 * 90G, 384m)=6.3G&lt;/code&gt;，其对应的Container申请的内存为98G。&lt;/p&gt;

&lt;p&gt;回头看看给AM对应的Container分配2G内存原因，512+384=896，小于2G，故分配2G，你可以在设置&lt;code&gt;spark.yarn.am.memory&lt;/code&gt;的值之后再来观察。&lt;/p&gt;

&lt;p&gt;打开Spark的管理界面 &lt;a href=&quot;http://ip:4040&quot;&gt;http://ip:4040&lt;/a&gt; ，可以看到driver和Executor中内存的占用情况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/hadoop/memory-in-spark-on-yarn-4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上图可以看到Executor占用了1566.7 MB内存，这是怎样计算出来的？参考&lt;a href=&quot;http://www.wdong.org/wordpress/blog/2015/01/08/spark-on-yarn-where-have-all-my-memory-gone/&quot;&gt;Spark on Yarn: Where Have All the Memory Gone?&lt;/a&gt;这篇文章，totalExecutorMemory的计算方式为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//yarn/common/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala
  val MEMORY_OVERHEAD_FACTOR = 0.07
  val MEMORY_OVERHEAD_MIN = 384

//yarn/common/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala
  protected val memoryOverhead: Int = sparkConf.getInt(&quot;spark.yarn.executor.memoryOverhead&quot;,
    math.max((MEMORY_OVERHEAD_FACTOR * executorMemory).toInt, MEMORY_OVERHEAD_MIN))
......
      val totalExecutorMemory = executorMemory + memoryOverhead
      numPendingAllocate.addAndGet(missing)
      logInfo(s&quot;Will allocate $missing executor containers, each with $totalExecutorMemory MB &quot; +
        s&quot;memory including $memoryOverhead MB overhead&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们给executor-memory设置的3G内存，memoryOverhead的值为&lt;code&gt;math.max(0.07 * 3072, 384)=384&lt;/code&gt;，其最大可用内存通过下面代码来计算：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//core/src/main/scala/org/apache/spark/storage/BlockManager.scala
/** Return the total amount of storage memory available. */
private def getMaxMemory(conf: SparkConf): Long = {
  val memoryFraction = conf.getDouble(&quot;spark.storage.memoryFraction&quot;, 0.6)
  val safetyFraction = conf.getDouble(&quot;spark.storage.safetyFraction&quot;, 0.9)
  (Runtime.getRuntime.maxMemory * memoryFraction * safetyFraction).toLong
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即，对于executor-memory设置3G时，executor内存占用大约为 3072m * 0.6 * 0.9 = 1658.88m，注意：实际上是应该乘以&lt;code&gt;Runtime.getRuntime.maxMemory&lt;/code&gt;的值，该值小于3072m。&lt;/p&gt;

&lt;p&gt;上图中driver占用了1060.3 MB，此时driver-memory的值是位2G，故driver中存储内存占用为：2048m * 0.6 * 0.9 =1105.92m，注意：实际上是应该乘以&lt;code&gt;Runtime.getRuntime.maxMemory&lt;/code&gt;的值，该值小于2048m。&lt;/p&gt;

&lt;p&gt;这时候，查看worker节点CoarseGrainedExecutorBackend进程启动脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ jps
46841 Worker
21894 CoarseGrainedExecutorBackend
9345
21816 ExecutorLauncher
43369
24300 NodeManager
38012 JournalNode
36929 QuorumPeerMain
22909 Jps

$ ps -ef|grep 21894
nobody   21894 21892 99 17:28 ?        00:04:49 /usr/java/jdk1.7.0_71/bin/java -server -XX:OnOutOfMemoryError=kill %p -Xms3072m -Xmx3072m  -Djava.io.tmpdir=/data/yarn/local/usercache/root/appcache/application_1433742899916_0069/container_1433742899916_0069_01_000003/tmp -Dspark.driver.port=60235 -Dspark.yarn.app.container.log.dir=/data/yarn/logs/application_1433742899916_0069/container_1433742899916_0069_01_000003 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url akka.tcp://sparkDriver@bj03-bi-pro-hdpnamenn:60235/user/CoarseGrainedScheduler --executor-id 2 --hostname X.X.X.X --cores 4 --app-id application_1433742899916_0069 --user-class-path file:/data/yarn/local/usercache/root/appcache/application_1433742899916_0069/container_1433742899916_0069_01_000003/__app__.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到每个CoarseGrainedExecutorBackend进程分配的内存为3072m，如果我们想查看每个executor的jvm运行情况，可以开启jmx。在/etc/spark/conf/spark-defaults.conf中添加下面一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;spark.executor.extraJavaOptions -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，通过jconsole监控jvm堆内存运行情况，这样方便调试内存大小。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;由上可知，在client模式下，AM对应的Container内存由&lt;code&gt;spark.yarn.am.memory&lt;/code&gt;加上&lt;code&gt;spark.yarn.am.memoryOverhead&lt;/code&gt;来确定，executor加上spark.&lt;code&gt;yarn.executor.memoryOverhead&lt;/code&gt;的值之后确定对应Container需要申请的内存大小，driver和executor的内存加上&lt;code&gt;spark.yarn.driver.memoryOverhead&lt;/code&gt;或&lt;code&gt;spark.yarn.executor.memoryOverhead&lt;/code&gt;的值之后再乘以0.54确定storage memory内存大小。在YARN中，Container申请的内存大小必须为&lt;code&gt;yarn.scheduler.minimum-allocation-mb&lt;/code&gt;的整数倍。&lt;/p&gt;

&lt;p&gt;下面这张图展示了Spark on YARN 内存结构，图片来自&lt;a href=&quot;http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/&quot;&gt;How-to: Tune Your Apache Spark Jobs (Part 2)&lt;/a&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://blog.cloudera.com/wp-content/uploads/2015/03/spark-tuning2-f1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;至于cluster模式下的分析，请参考上面的过程。希望这篇文章对你有所帮助！&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/book_mmicky/article/details/25714287&quot;&gt;Spark1.0.0 on YARN 模式部署&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.wdong.org/wordpress/blog/2015/01/08/spark-on-yarn-where-have-all-my-memory-gone/&quot;&gt;Spark on Yarn: Where Have All the Memory Gone?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zybuluo.com/xiaop1987/note/102894&quot;&gt;Apache Spark Jobs 性能调优（二）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html</link>
      <guid>http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html</guid>
      <pubDate>2015-06-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spark配置参数</title>
      <description>&lt;p&gt;以下是整理的Spark中的一些配置参数，官方文档请参考&lt;a href=&quot;https://spark.apache.org/docs/latest/configuration.html&quot;&gt;Spark Configuration&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Spark提供三个位置用来配置系统：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spark属性：控制大部分的应用程序参数，可以用SparkConf对象或者Java系统属性设置&lt;/li&gt;
  &lt;li&gt;环境变量：可以通过每个节点的&lt;code&gt; conf/spark-env.sh&lt;/code&gt;脚本设置。例如IP地址、端口等信息&lt;/li&gt;
  &lt;li&gt;日志配置：可以通过log4j.properties配置&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;spark&quot;&gt;Spark属性&lt;/h1&gt;

&lt;p&gt;Spark属性控制大部分的应用程序设置，并且为每个应用程序分别配置它。这些属性可以直接在&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf&quot;&gt;SparkConf&lt;/a&gt;上配置，然后传递给&lt;code&gt;SparkContext&lt;/code&gt;。&lt;code&gt;SparkConf&lt;/code&gt;&lt;br /&gt;
允许你配置一些通用的属性（如master URL、应用程序名称等等）以及通过&lt;code&gt;set()&lt;/code&gt;方法设置的任意键值对。例如，我们可以用如下方式创建一个拥有两个线程的应用程序。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val conf = new SparkConf()
             .setMaster(&quot;local[2]&quot;)
             .setAppName(&quot;CountingSheep&quot;)
             .set(&quot;spark.executor.memory&quot;, &quot;1g&quot;)
val sc = new SparkContext(conf)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spark-1&quot;&gt;动态加载Spark属性&lt;/h2&gt;

&lt;p&gt;在一些情况下，你可能想在&lt;code&gt;SparkConf&lt;/code&gt;中避免硬编码确定的配置。例如，你想用不同的master或者不同的内存数运行相同的应用程序。Spark允许你简单地创建一个空conf。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val sc = new SparkContext(new SparkConf())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后你在运行时设置变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./bin/spark-submit --name &quot;My app&quot; --master local[4] --conf spark.shuffle.spill=false
  --conf &quot;spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps&quot; myApp.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spark shell和&lt;code&gt;spark-submit&lt;/code&gt;工具支持两种方式动态加载配置。第一种方式是命令行选项，例如&lt;code&gt;--master&lt;/code&gt;，如上面shell显示的那样。&lt;code&gt;spark-submit&lt;/code&gt;可以接受任何Spark属性，用&lt;code&gt;--conf&lt;/code&gt;参数表示。但是那些参与Spark应用程序启动的属性要用特定的参数表示。运行&lt;code&gt;./bin/spark-submit --help&lt;/code&gt;将会显示选项的整个列表。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bin/spark-submit&lt;/code&gt;也会从&lt;code&gt;conf/spark-defaults.conf&lt;/code&gt;中读取配置选项，这个配置文件中，每一行都包含一对以&lt;code&gt;空格&lt;/code&gt;或者&lt;code&gt;等号&lt;/code&gt;分开的键和值。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spark.master            spark://5.6.7.8:7077
spark.executor.memory   512m
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;任何标签指定的值或者在配置文件中的值将会传递给应用程序，并且通过&lt;code&gt;SparkConf&lt;/code&gt;合并这些值。在&lt;code&gt;SparkConf&lt;/code&gt;上设置的属性具有最高的优先级，其次是传递给&lt;code&gt;spark-submit&lt;/code&gt;或者&lt;code&gt;spark-shell&lt;/code&gt;的属性值，最后是&lt;code&gt;spark-defaults.conf&lt;/code&gt;文件中的属性值。&lt;/p&gt;

&lt;p&gt;优先级顺序：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SparkConf &amp;gt; CLI &amp;gt; spark-defaults.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spark-2&quot;&gt;查看Spark属性&lt;/h2&gt;

&lt;p&gt;在&lt;code&gt;http://&amp;lt;driver&amp;gt;:4040&lt;/code&gt;上的应用程序Web UI在&lt;code&gt;Environment&lt;/code&gt;标签中列出了所有的Spark属性。这对你确保设置的属性的正确性是很有用的。&lt;/p&gt;

&lt;p&gt;注意：&lt;code&gt;只有通过spark-defaults.conf, SparkConf以及命令行直接指定的值才会显示&lt;/code&gt;。对于其它的配置属性，你可以认为程序用到了默认的值。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;可用的属性&lt;/h2&gt;

&lt;p&gt;控制内部设置的大部分属性都有合理的默认值，一些最通用的选项设置如下：&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;应用程序属性&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.app.name&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;你的应用程序的名字。这将在UI和日志数据中出现&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.cores&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;driver程序运行需要的cpu内核数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.maxResultSize&lt;/td&gt;
      &lt;td&gt;1g&lt;/td&gt;
      &lt;td&gt;每个Spark action(如collect)所有分区的序列化结果的总大小限制。设置的值应该不小于1m，0代表没有限制。如果总大小超过这个限制，程序将会终止。大的限制值可能导致driver出现内存溢出错误（依赖于&lt;code&gt;spark.driver.memory&lt;/code&gt;和JVM中对象的内存消耗）。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.memory&lt;/td&gt;
      &lt;td&gt;512m&lt;/td&gt;
      &lt;td&gt;driver进程使用的内存数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.memory&lt;/td&gt;
      &lt;td&gt;512m&lt;/td&gt;
      &lt;td&gt;每个executor进程使用的内存数。和JVM内存串拥有相同的格式（如512m,2g）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.extraListeners&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;注册监听器，需要实现SparkListener&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.local.dir&lt;/td&gt;
      &lt;td&gt;/tmp&lt;/td&gt;
      &lt;td&gt;Spark中暂存空间的使用目录。在Spark1.0以及更高的版本中，这个属性被SPARK_LOCAL_DIRS(Standalone, Mesos)和LOCAL_DIRS(YARN)环境变量覆盖。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.logConf&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;当SparkContext启动时，将有效的SparkConf记录为INFO。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.master&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;集群管理器连接的地方&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-2&quot;&gt;运行环境&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.extraClassPath&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;附加到driver的classpath的额外的classpath实体。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.extraJavaOptions&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;传递给driver的JVM选项字符串。例如GC设置或者其它日志设置。注意，&lt;code&gt;在这个选项中设置Spark属性或者堆大小是不合法的&lt;/code&gt;。Spark属性需要用&lt;code&gt;--driver-class-path&lt;/code&gt;设置。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.extraLibraryPath&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;指定启动driver的JVM时用到的库路径&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.userClassPathFirst&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;(实验性)当在driver中加载类时，是否用户添加的jar比Spark自己的jar优先级高。这个属性可以降低Spark依赖和用户依赖的冲突。它现在还是一个实验性的特征。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.extraClassPath&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;附加到executors的classpath的额外的classpath实体。这个设置存在的主要目的是Spark与旧版本的向后兼容问题。用户一般不用设置这个选项&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.extraJavaOptions&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;传递给executors的JVM选项字符串。例如GC设置或者其它日志设置。注意，&lt;code&gt;在这个选项中设置Spark属性或者堆大小是不合法的&lt;/code&gt;。Spark属性需要用SparkConf对象或者&lt;code&gt;spark-submit&lt;/code&gt;脚本用到的&lt;code&gt;spark-defaults.conf&lt;/code&gt;文件设置。堆内存可以通过&lt;code&gt;spark.executor.memory&lt;/code&gt;设置&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.extraLibraryPath&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;指定启动executor的JVM时用到的库路径&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.logs.rolling.maxRetainedFiles&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;设置被系统保留的最近滚动日志文件的数量。更老的日志文件将被删除。默认没有开启。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.logs.rolling.size.maxBytes&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;executor日志的最大滚动大小。默认情况下没有开启。值设置为字节&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.logs.rolling.strategy&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;设置executor日志的滚动(rolling)策略。默认情况下没有开启。可以配置为&lt;code&gt;time&lt;/code&gt;和&lt;code&gt;size&lt;/code&gt;。对于&lt;code&gt;time&lt;/code&gt;，用&lt;code&gt;spark.executor.logs.rolling.time.interval&lt;/code&gt;设置滚动间隔；对于&lt;code&gt;size&lt;/code&gt;，用&lt;code&gt;spark.executor.logs.rolling.size.maxBytes&lt;/code&gt;设置最大的滚动大小&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.logs.rolling.time.interval&lt;/td&gt;
      &lt;td&gt;daily&lt;/td&gt;
      &lt;td&gt;executor日志滚动的时间间隔。默认情况下没有开启。合法的值是&lt;code&gt;daily&lt;/code&gt;, &lt;code&gt;hourly&lt;/code&gt;, &lt;code&gt;minutely&lt;/code&gt;以及任意的秒。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.files.userClassPathFirst&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;(实验性)当在Executors中加载类时，是否用户添加的jar比Spark自己的jar优先级高。这个属性可以降低Spark依赖和用户依赖的冲突。它现在还是一个实验性的特征。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.python.worker.memory&lt;/td&gt;
      &lt;td&gt;512m&lt;/td&gt;
      &lt;td&gt;在聚合期间，每个python worker进程使用的内存数。在聚合期间，如果内存超过了这个限制，它将会将数据塞进磁盘中&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.python.profile&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;在Python worker中开启profiling。通过&lt;code&gt;sc.show_profiles()&lt;/code&gt;展示分析结果。或者在driver退出前展示分析结果。可以通过&lt;code&gt;sc.dump_profiles(path)&lt;/code&gt;将结果dump到磁盘中。如果一些分析结果已经手动展示，那么在driver退出前，它们再不会自动展示&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.python.profile.dump&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;driver退出前保存分析结果的dump文件的目录。每个RDD都会分别dump一个文件。可以通过&lt;code&gt;ptats.Stats()&lt;/code&gt;加载这些文件。如果指定了这个属性，分析结果不会自动展示&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.python.worker.reuse&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;是否重用python worker。如果是，它将使用固定数量的Python workers，而不需要为每个任务&lt;code&gt;fork()&lt;/code&gt;一个Python进程。如果有一个非常大的广播，这个设置将非常有用。因为，广播不需要为每个任务从JVM到Python worker传递一次&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executorEnv.[EnvironmentVariableName]&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;通过&lt;code&gt;EnvironmentVariableName&lt;/code&gt;添加指定的环境变量到executor进程。用户可以指定多个&lt;code&gt;EnvironmentVariableName&lt;/code&gt;，设置多个环境变量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.mesos.executor.home&lt;/td&gt;
      &lt;td&gt;driver side SPARK_HOME&lt;/td&gt;
      &lt;td&gt;设置安装在Mesos的executor上的Spark的目录。默认情况下，executors将使用driver的Spark本地（home）目录，这个目录对它们不可见。注意，如果没有通过&lt;code&gt; spark.executor.uri&lt;/code&gt;指定Spark的二进制包，这个设置才起作用&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.mesos.executor.memoryOverhead&lt;/td&gt;
      &lt;td&gt;executor memory * 0.07, 最小384m&lt;/td&gt;
      &lt;td&gt;这个值是&lt;code&gt;spark.executor.memory&lt;/code&gt;的补充。它用来计算mesos任务的总内存。另外，有一个7%的硬编码设置。最后的值将选择&lt;code&gt;spark.mesos.executor.memoryOverhead&lt;/code&gt;或者&lt;code&gt;spark.executor.memory&lt;/code&gt;的7%二者之间的大者&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;shuffle&quot;&gt;Shuffle行为&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.reducer.maxMbInFlight&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;从递归任务中同时获取的map输出数据的最大大小（mb）。因为每一个输出都需要我们创建一个缓存用来接收，这个设置代表每个任务固定的内存上限，所以除非你有更大的内存，将其设置小一点&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.blockTransferService&lt;/td&gt;
      &lt;td&gt;netty&lt;/td&gt;
      &lt;td&gt;实现用来在executor直接传递shuffle和缓存块。有两种可用的实现：&lt;code&gt;netty&lt;/code&gt;和&lt;code&gt;nio&lt;/code&gt;。基于netty的块传递在具有相同的效率情况下更简单&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.compress&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;是否压缩map操作的输出文件。一般情况下，这是一个好的选择。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.consolidateFiles&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;如果设置为”true”，在shuffle期间，合并的中间文件将会被创建。创建更少的文件可以提供文件系统的shuffle的效率。这些shuffle都伴随着大量递归任务。当用ext4和dfs文件系统时，推荐设置为”true”。在ext3中，因为文件系统的限制，这个选项可能机器（大于8核）降低效率&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.file.buffer.kb&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;每个shuffle文件输出流内存内缓存的大小，单位是kb。这个缓存减少了创建只中间shuffle文件中磁盘搜索和系统访问的数量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.io.maxRetries&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Netty only，自动重试次数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.io.numConnectionsPerPeer&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Netty only&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.io.preferDirectBufs&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;Netty only&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.io.retryWait&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Netty only&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.manager&lt;/td&gt;
      &lt;td&gt;sort&lt;/td&gt;
      &lt;td&gt;它的实现用于shuffle数据。有两种可用的实现：&lt;code&gt;sort&lt;/code&gt;和&lt;code&gt;hash&lt;/code&gt;。基于sort的shuffle有更高的内存使用率&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.memoryFraction&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;如果&lt;code&gt;spark.shuffle.spill&lt;/code&gt;为true，shuffle中聚合和合并组操作使用的java堆内存占总内存的比重。在任何时候，shuffles使用的所有内存内maps的集合大小都受这个限制的约束。超过这个限制，spilling数据将会保存到磁盘上。如果spilling太过频繁，考虑增大这个值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.sort.bypassMergeThreshold&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;(Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no map-side aggregation and there are at most this many reduce partitions&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.spill&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;如果设置为”true”，通过将多出的数据写入磁盘来限制内存数。通过&lt;code&gt;spark.shuffle.memoryFraction&lt;/code&gt;来指定spilling的阈值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.shuffle.spill.compress&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;在shuffle时，是否将spilling的数据压缩。压缩算法通过&lt;code&gt;spark.io.compression.codec&lt;/code&gt;指定。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;spark-ui&quot;&gt;Spark UI&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.eventLog.compress&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;是否压缩事件日志。需要&lt;code&gt;spark.eventLog.enabled&lt;/code&gt;为true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.eventLog.dir&lt;/td&gt;
      &lt;td&gt;file:///tmp/spark-events&lt;/td&gt;
      &lt;td&gt;Spark事件日志记录的基本目录。在这个基本目录下，Spark为每个应用程序创建一个子目录。各个应用程序记录日志到直到的目录。用户可能想设置这为统一的地点，像HDFS一样，所以历史文件可以通过历史服务器读取&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.eventLog.enabled&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;是否记录Spark的事件日志。这在应用程序完成后，重新构造web UI是有用的&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ui.killEnabled&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;运行在web UI中杀死stage和相应的job&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ui.port&lt;/td&gt;
      &lt;td&gt;4040&lt;/td&gt;
      &lt;td&gt;你的应用程序dashboard的端口。显示内存和工作量数据&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ui.retainedJobs&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;在垃圾回收之前，Spark UI和状态API记住的job数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ui.retainedStages&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;在垃圾回收之前，Spark UI和状态API记住的stage数&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-3&quot;&gt;压缩和序列化&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.broadcast.compress&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;在发送广播变量之前是否压缩它&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.closure.serializer&lt;/td&gt;
      &lt;td&gt;org.apache.spark.serializer.JavaSerializer&lt;/td&gt;
      &lt;td&gt;闭包用到的序列化类。目前只支持java序列化器&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.io.compression.codec&lt;/td&gt;
      &lt;td&gt;snappy&lt;/td&gt;
      &lt;td&gt;压缩诸如RDD分区、广播变量、shuffle输出等内部数据的编码解码器。默认情况下，Spark提供了三种选择：lz4、lzf和snappy，你也可以用完整的类名来制定。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.io.compression.lz4.block.size&lt;/td&gt;
      &lt;td&gt;32768&lt;/td&gt;
      &lt;td&gt;LZ4压缩中用到的块大小。降低这个块的大小也会降低shuffle内存使用率&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.io.compression.snappy.block.size&lt;/td&gt;
      &lt;td&gt;32768&lt;/td&gt;
      &lt;td&gt;Snappy压缩中用到的块大小。降低这个块的大小也会降低shuffle内存使用率&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.kryo.classesToRegister&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;如果你用Kryo序列化，给定的用逗号分隔的自定义类名列表表示要注册的类&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.kryo.referenceTracking&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;当用Kryo序列化时，跟踪是否引用同一对象。如果你的对象图有环，这是必须的设置。如果他们包含相同对象的多个副本，这个设置对效率是有用的。如果你知道不在这两个场景，那么可以禁用它以提高效率&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.kryo.registrationRequired&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;是否需要注册为Kyro可用。如果设置为true，然后如果一个没有注册的类序列化，Kyro会抛出异常。如果设置为false，Kryo将会同时写每个对象和其非注册类名。写类名可能造成显著地性能瓶颈。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.kryo.registrator&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;如果你用Kryo序列化，设置这个类去注册你的自定义类。如果你需要用自定义的方式注册你的类，那么这个属性是有用的。否则&lt;code&gt;spark.kryo.classesToRegister&lt;/code&gt;会更简单。它应该设置一个继承自&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.serializer.KryoRegistrator&quot;&gt;KryoRegistrator&lt;/a&gt;的类&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.kryoserializer.buffer.max.mb&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;Kryo序列化缓存允许的最大值。这个值必须大于你尝试序列化的对象&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.kryoserializer.buffer.mb&lt;/td&gt;
      &lt;td&gt;0.064&lt;/td&gt;
      &lt;td&gt;Kyro序列化缓存的大小。这样worker上的每个核都有一个缓存。如果有需要，缓存会涨到&lt;code&gt;spark.kryoserializer.buffer.max.mb&lt;/code&gt;设置的值那么大。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.rdd.compress&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;是否压缩序列化的RDD分区。在花费一些额外的CPU时间的同时节省大量的空间&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.serializer&lt;/td&gt;
      &lt;td&gt;org.apache.spark.serializer.JavaSerializer&lt;/td&gt;
      &lt;td&gt;序列化对象使用的类。默认的Java序列化类可以序列化任何可序列化的java对象但是它很慢。所有我们建议用&lt;a href=&quot;http://spark.apache.org/docs/latest/tuning.html&quot;&gt;org.apache.spark.serializer.KryoSerializer&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.serializer.objectStreamReset&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;当用&lt;code&gt;org.apache.spark.serializer.JavaSerializer&lt;/code&gt;序列化时，序列化器通过缓存对象防止写多余的数据，然而这会造成这些对象的垃圾回收停止。通过请求’reset’，你从序列化器中flush这些信息并允许收集老的数据。为了关闭这个周期性的reset，你可以将值设为-1。默认情况下，每一百个对象reset一次&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-4&quot;&gt;运行时行为&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.broadcast.blockSize&lt;/td&gt;
      &lt;td&gt;4096&lt;/td&gt;
      &lt;td&gt;TorrentBroadcastFactory传输的块大小，太大值会降低并发，太小的值会出现性能瓶颈&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.broadcast.factory&lt;/td&gt;
      &lt;td&gt;org.apache.spark.broadcast.TorrentBroadcastFactory&lt;/td&gt;
      &lt;td&gt;broadcast实现类&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.cleaner.ttl&lt;/td&gt;
      &lt;td&gt;(infinite)&lt;/td&gt;
      &lt;td&gt;spark记录任何元数据（stages生成、task生成等）的持续时间。定期清理可以确保将超期的元数据丢弃，这在运行长时间任务是很有用的，如运行7*24的sparkstreaming任务。RDD持久化在内存中的超期数据也会被清理&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.default.parallelism&lt;/td&gt;
      &lt;td&gt;本地模式：机器核数；Mesos：8；其他：&lt;code&gt;max(executor的core，2)&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;如果用户不设置，系统使用集群中运行shuffle操作的默认任务数（groupByKey、 reduceByKey等）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.heartbeatInterval&lt;/td&gt;
      &lt;td&gt;10000&lt;/td&gt;
      &lt;td&gt;executor 向 the driver 汇报心跳的时间间隔，单位毫秒&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.files.fetchTimeout&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;driver 程序获取通过&lt;code&gt;SparkContext.addFile()&lt;/code&gt;添加的文件时的超时时间，单位秒&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.files.useFetchCache&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;获取文件时是否使用本地缓存&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.files.overwrite&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;调用&lt;code&gt;SparkContext.addFile()&lt;/code&gt;时候是否覆盖文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.hadoop.cloneConf&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;每个task是否克隆一份hadoop的配置文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.hadoop.validateOutputSpecs&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;是否校验输出&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.storage.memoryFraction&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;Spark内存缓存的堆大小占用总内存比例，该值不能大于老年代内存大小，默认值为0.6，但是，如果你手动设置老年代大小，你可以增加该值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.storage.memoryMapThreshold&lt;/td&gt;
      &lt;td&gt;2097152&lt;/td&gt;
      &lt;td&gt;内存块大小&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.storage.unrollFraction&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Fraction of spark.storage.memoryFraction to use for unrolling blocks in memory.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.tachyonStore.baseDir&lt;/td&gt;
      &lt;td&gt;System.getProperty(“java.io.tmpdir”)&lt;/td&gt;
      &lt;td&gt;Tachyon File System临时目录&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.tachyonStore.url&lt;/td&gt;
      &lt;td&gt;tachyon://localhost:19998&lt;/td&gt;
      &lt;td&gt;Tachyon File System URL&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-5&quot;&gt;网络&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.host&lt;/td&gt;
      &lt;td&gt;(local hostname)&lt;/td&gt;
      &lt;td&gt;driver监听的主机名或者IP地址。这用于和executors以及独立的master通信&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.port&lt;/td&gt;
      &lt;td&gt;(random)&lt;/td&gt;
      &lt;td&gt;driver监听的接口。这用于和executors以及独立的master通信&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.fileserver.port&lt;/td&gt;
      &lt;td&gt;(random)&lt;/td&gt;
      &lt;td&gt;driver的文件服务器监听的端口&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.broadcast.port&lt;/td&gt;
      &lt;td&gt;(random)&lt;/td&gt;
      &lt;td&gt;driver的HTTP广播服务器监听的端口&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.replClassServer.port&lt;/td&gt;
      &lt;td&gt;(random)&lt;/td&gt;
      &lt;td&gt;driver的HTTP类服务器监听的端口&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.blockManager.port&lt;/td&gt;
      &lt;td&gt;(random)&lt;/td&gt;
      &lt;td&gt;块管理器监听的端口。这些同时存在于driver和executors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.port&lt;/td&gt;
      &lt;td&gt;(random)&lt;/td&gt;
      &lt;td&gt;executor监听的端口。用于与driver通信&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.port.maxRetries&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;当绑定到一个端口，在放弃前重试的最大次数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.akka.frameSize&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;在”control plane”通信中允许的最大消息大小。如果你的任务需要发送大的结果到driver中，调大这个值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.akka.threads&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;通信的actor线程数。当driver有很多CPU核时，调大它是有用的&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.akka.timeout&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;Spark节点之间的通信超时。单位是秒&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.akka.heartbeat.pauses&lt;/td&gt;
      &lt;td&gt;6000&lt;/td&gt;
      &lt;td&gt;This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). Acceptable heart beat pause in seconds for akka. This can be used to control sensitivity to gc pauses. Tune this in combination of &lt;code&gt;spark.akka.heartbeat.interval&lt;/code&gt; and &lt;code&gt;spark.akka.failure-detector.threshold&lt;/code&gt; if you need to.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.akka.failure-detector.threshold&lt;/td&gt;
      &lt;td&gt;300.0&lt;/td&gt;
      &lt;td&gt;This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). This maps to akka’s &lt;code&gt;akka.remote.transport-failure-detector.threshold&lt;/code&gt;. Tune this in combination of &lt;code&gt;spark.akka.heartbeat.pauses&lt;/code&gt; and &lt;code&gt;spark.akka.heartbeat.interval&lt;/code&gt; if you need to.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.akka.heartbeat.interval&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). A larger interval value in seconds reduces network overhead and a smaller value ( ~ 1 s) might be more informative for akka’s failure detector. Tune this in combination of &lt;code&gt;spark.akka.heartbeat.pauses&lt;/code&gt; and &lt;code&gt;spark.akka.failure-detector.threshold&lt;/code&gt; if you need to. Only positive use case for using failure detector can be, a sensistive failure detector can help evict rogue executors really quick. However this is usually not the case as gc pauses and network lags are expected in a real Spark cluster. Apart from that enabling this leads to a lot of exchanges of heart beats between nodes leading to flooding the network with those.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-6&quot;&gt;调度相关属性&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.task.cpus&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;为每个任务分配的内核数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.task.maxFailures&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Task的最大重试次数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.scheduler.mode&lt;/td&gt;
      &lt;td&gt;FIFO&lt;/td&gt;
      &lt;td&gt;Spark的任务调度模式，还有一种Fair模式&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.cores.max&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;当应用程序运行在Standalone集群或者粗粒度共享模式Mesos集群时，应用程序向集群请求的最大CPU内核总数（不是指每台机器，而是整个集群）。如果不设置，对于Standalone集群将使用spark.deploy.defaultCores中数值，而Mesos将使用集群中可用的内核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.mesos.coarse&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;如果设置为true，在Mesos集群中运行时使用粗粒度共享模式&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.speculation&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;以下几个参数是关于Spark推测执行机制的相关参数。此参数设定是否使用推测执行机制，如果设置为true则spark使用推测执行机制，对于Stage中拖后腿的Task在其他节点中重新启动，并将最先完成的Task的计算结果最为最终结果&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.speculation.interval&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;Spark多长时间进行检查task运行状态用以推测，以毫秒为单位&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.speculation.quantile&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;推测启动前，Stage必须要完成总Task的百分比&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.speculation.multiplier&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;比已完成Task的运行速度中位数慢多少倍才启用推测&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.locality.wait&lt;/td&gt;
      &lt;td&gt;3000&lt;/td&gt;
      &lt;td&gt;以下几个参数是关于Spark数据本地性的。本参数是以毫秒为单位启动本地数据task的等待时间，如果超出就启动下一本地优先级别的task。该设置同样可以应用到各优先级别的本地性之间（本地进程 -&amp;gt; 本地节点 -&amp;gt; 本地机架 -&amp;gt; 任意节点 ），当然，也可以通过spark.locality.wait.node等参数设置不同优先级别的本地性&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.locality.wait.process&lt;/td&gt;
      &lt;td&gt;spark.locality.wait&lt;/td&gt;
      &lt;td&gt;本地进程级别的本地等待时间&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.locality.wait.node&lt;/td&gt;
      &lt;td&gt;spark.locality.wait&lt;/td&gt;
      &lt;td&gt;本地节点级别的本地等待时间&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.locality.wait.rack&lt;/td&gt;
      &lt;td&gt;spark.locality.wait&lt;/td&gt;
      &lt;td&gt;本地机架级别的本地等待时间&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.scheduler.revive.interval&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;复活重新获取资源的Task的最长时间间隔（毫秒），发生在Task因为本地资源不足而将资源分配给其他Task运行后进入等待时间，如果这个等待时间内重新获取足够的资源就继续计算&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;dynamic-allocation&quot;&gt;Dynamic Allocation&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.dynamicAllocation.enabled&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;是否开启动态资源搜集&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.dynamicAllocation.executorIdleTimeout&lt;/td&gt;
      &lt;td&gt;600&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.dynamicAllocation.initialExecutors&lt;/td&gt;
      &lt;td&gt;spark.dynamicAllocation.minExecutors&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.dynamicAllocation.maxExecutors&lt;/td&gt;
      &lt;td&gt;Integer.MAX_VALUE&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.dynamicAllocation.minExecutors&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.dynamicAllocation.schedulerBacklogTimeout&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.dynamicAllocation.sustainedSchedulerBacklogTimeout&lt;/td&gt;
      &lt;td&gt;schedulerBacklogTimeout&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-7&quot;&gt;安全&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.authenticate&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;是否Spark验证其内部连接。如果不是运行在YARN上，请看&lt;code&gt;spark.authenticate.secret&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.authenticate.secret&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;设置Spark两个组件之间的密匙验证。如果不是运行在YARN上，但是需要验证，这个选项必须设置&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.core.connection.auth.wait.timeout&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;连接时等待验证的实际。单位为秒&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.core.connection.ack.wait.timeout&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;连接等待回答的时间。单位为秒。为了避免不希望的超时，你可以设置更大的值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ui.filters&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;应用到Spark web UI的用于过滤类名的逗号分隔的列表。过滤器必须是标准的&lt;a href=&quot;http://docs.oracle.com/javaee/6/api/javax/servlet/Filter.html&quot;&gt;javax servlet Filter&lt;/a&gt;。通过设置java系统属性也可以指定每个过滤器的参数。&lt;code&gt;spark.&amp;lt;class name of filter&amp;gt;.params=&#39;param1=value1,param2=value2&#39;&lt;/code&gt;。例如&lt;code&gt;-Dspark.ui.filters=com.test.filter1&lt;/code&gt;、&lt;code&gt;-Dspark.com.test.filter1.params=&#39;param1=foo,param2=testing&#39;&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.acls.enable&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;是否开启Spark acls。如果开启了，它检查用户是否有权限去查看或修改job。UI利用使用过滤器验证和设置用户&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ui.view.acls&lt;/td&gt;
      &lt;td&gt;empty&lt;/td&gt;
      &lt;td&gt;逗号分隔的用户列表，列表中的用户有查看Spark web UI的权限。默认情况下，只有启动Spark job的用户有查看权限&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.modify.acls&lt;/td&gt;
      &lt;td&gt;empty&lt;/td&gt;
      &lt;td&gt;逗号分隔的用户列表，列表中的用户有修改Spark job的权限。默认情况下，只有启动Spark job的用户有修改权限&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.admin.acls&lt;/td&gt;
      &lt;td&gt;empty&lt;/td&gt;
      &lt;td&gt;逗号分隔的用户或者管理员列表，列表中的用户或管理员有查看和修改所有Spark job的权限。如果你运行在一个共享集群，有一组管理员或开发者帮助debug，这个选项有用&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-8&quot;&gt;加密&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ssl.enabled&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;是否开启ssl&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ssl.enabledAlgorithms&lt;/td&gt;
      &lt;td&gt;Empty&lt;/td&gt;
      &lt;td&gt;JVM支持的加密算法列表，逗号分隔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ssl.keyPassword&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ssl.keyStore&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ssl.keyStorePassword&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ssl.protocol&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ssl.trustStore&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.ssl.trustStorePassword&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;spark-streaming&quot;&gt;Spark Streaming&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.streaming.blockInterval&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;在这个时间间隔（ms）内，通过Spark Streaming receivers接收的数据在保存到Spark之前，chunk为数据块。推荐的最小值为50ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.streaming.receiver.maxRate&lt;/td&gt;
      &lt;td&gt;infinite&lt;/td&gt;
      &lt;td&gt;每秒钟每个receiver将接收的数据的最大记录数。有效的情况下，每个流将消耗至少这个数目的记录。设置这个配置为0或者-1将会不作限制&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.streaming.receiver.writeAheadLogs.enable&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Enable write ahead logs for receivers. All the input data received through receivers will be saved to write ahead logs that will allow it to be recovered after driver failures&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.streaming.unpersist&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
      &lt;td&gt;强制通过Spark Streaming生成并持久化的RDD自动从Spark内存中非持久化。通过Spark Streaming接收的原始输入数据也将清除。设置这个属性为false允许流应用程序访问原始数据和持久化RDD，因为它们没有被自动清除。但是它会造成更高的内存花费&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-9&quot;&gt;集群管理&lt;/h3&gt;

&lt;h4 id=&quot;spark-on-yarn&quot;&gt;Spark On YARN&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;默认值&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.am.memory&lt;/td&gt;
      &lt;td&gt;512m&lt;/td&gt;
      &lt;td&gt;client 模式时，am的内存大小；cluster模式时，使用&lt;code&gt;spark.driver.memory&lt;/code&gt;变量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.driver.cores&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;claster模式时，driver使用的cpu核数，这时候driver运行在am中，其实也就是am和核数；client模式时，使用&lt;code&gt;spark.yarn.am.cores&lt;/code&gt;变量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.am.cores&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;client 模式时，am的cpu核数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.am.waitTime&lt;/td&gt;
      &lt;td&gt;100000&lt;/td&gt;
      &lt;td&gt;启动时等待时间&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.submit.file.replication&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;应用程序上传到HDFS的文件的副本数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.preserve.staging.files&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;若为true，在job结束后，将stage相关的文件保留而不是删除&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.scheduler.heartbeat.interval-ms&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
      &lt;td&gt;Spark AppMaster发送心跳信息给YARN RM的时间间隔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.max.executor.failures&lt;/td&gt;
      &lt;td&gt;2倍于executor数，最小值3&lt;/td&gt;
      &lt;td&gt;导致应用程序宣告失败的最大executor失败次数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.applicationMaster.waitTries&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;RM等待Spark AppMaster启动重试次数，也就是SparkContext初始化次数。超过这个数值，启动失败&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.historyServer.address&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Spark history server的地址（不要加 &lt;code&gt;http://&lt;/code&gt;）。这个地址会在Spark应用程序完成后提交给YARN RM，然后RM将信息从RM UI写到history server UI上。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.dist.archives&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.dist.files&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.executor.instances&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;executor实例个数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.executor.memoryOverhead&lt;/td&gt;
      &lt;td&gt;executorMemory * 0.07, with minimum of 384&lt;/td&gt;
      &lt;td&gt;executor的堆内存大小设置&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.driver.memoryOverhead&lt;/td&gt;
      &lt;td&gt;driverMemory * 0.07, with minimum of 384&lt;/td&gt;
      &lt;td&gt;driver的堆内存大小设置&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.am.memoryOverhead&lt;/td&gt;
      &lt;td&gt;AM memory * 0.07, with minimum of 384&lt;/td&gt;
      &lt;td&gt;am的堆内存大小设置，在client模式时设置&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.queue&lt;/td&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;使用yarn的队列&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.jar&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.access.namenodes&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.appMasterEnv.[EnvironmentVariableName]&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt;设置am的环境变量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.containerLauncherMaxThreads&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;am启动executor的最大线程数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.am.extraJavaOptions&lt;/td&gt;
      &lt;td&gt;(none)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark.yarn.maxAppAttempts&lt;/td&gt;
      &lt;td&gt;yarn.resourcemanager.am.max-attempts in YARN&lt;/td&gt;
      &lt;td&gt;am重试次数&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;spark-on-mesos&quot;&gt;Spark on Mesos&lt;/h3&gt;

&lt;p&gt;使用较少，参考&lt;a href=&quot;https://spark.apache.org/docs/latest/running-on-mesos.html#configuration&quot;&gt;Running Spark on Mesos&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;spark-standalone-mode&quot;&gt;Spark Standalone Mode&lt;/h3&gt;

&lt;p&gt;参考&lt;a href=&quot;https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts&quot;&gt;Spark Standalone Mode&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;spark-history-server&quot;&gt;Spark History Server&lt;/h3&gt;

&lt;p&gt;当你运行Spark Standalone Mode或者Spark on Mesos模式时，你可以通过Spark History Server来查看job运行情况。&lt;/p&gt;

&lt;p&gt;Spark History Server的环境变量：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性名称&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;SPARK_DAEMON_MEMORY&lt;/td&gt;
      &lt;td&gt;Memory to allocate to the history server (default: 512m).&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SPARK_DAEMON_JAVA_OPTS&lt;/td&gt;
      &lt;td&gt;JVM options for the history server (default: none).&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SPARK_PUBLIC_DNS&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SPARK_HISTORY_OPTS&lt;/td&gt;
      &lt;td&gt;配置 spark.history.* 属性&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Spark History Server的属性：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;属性名称&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;默认&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.provider&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;org.apache.spark.deploy.history.FsHistoryProvide&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;应用历史后端实现的类名。 目前只有一个实现, 由Spark提供, 它查看存储在文件系统里面的应用日志&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.fs.logDirectory&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;file:/tmp/spark-events&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.updateInterval&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;以秒为单位，多长时间Spark history server显示的信息进行更新。每次更新都会检查持久层事件日志的任何变化。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.retainedApplications&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在Spark history server上显示的最大应用程序数量，如果超过这个值，旧的应用程序信息将被删除。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.ui.port&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;18080&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;官方版本中，Spark history server的默认访问端口&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.kerberos.enabled&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;false&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;是否使用kerberos方式登录访问history server，对于持久层位于安全集群的HDFS上是有用的。如果设置为true，就要配置下面的两个属性。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.kerberos.principal&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;空&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;用于Spark history server的kerberos主体名称&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.kerberos.keytab&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;空&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;用于Spark history server的kerberos keytab文件位置&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;spark.history.ui.acls.enable&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;false&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;授权用户查看应用程序信息的时候是否检查acl。如果启用，只有应用程序所有者和&lt;code&gt;spark.ui.view.acls&lt;/code&gt;指定的用户可以查看应用程序信息;如果禁用，不做任何检查。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section-10&quot;&gt;环境变量&lt;/h2&gt;

&lt;p&gt;通过环境变量配置确定的Spark设置。环境变量从Spark安装目录下的&lt;code&gt;conf/spark-env.sh&lt;/code&gt;脚本读取（或者windows的&lt;code&gt;conf/spark-env.cmd&lt;/code&gt;）。在独立的或者Mesos模式下，这个文件可以给机器确定的信息，如主机名。当运行本地应用程序或者提交脚本时，它也起作用。&lt;/p&gt;

&lt;p&gt;注意，当Spark安装时，&lt;code&gt;conf/spark-env.sh&lt;/code&gt;默认是不存在的。你可以复制&lt;code&gt;conf/spark-env.sh.template&lt;/code&gt;创建它。&lt;/p&gt;

&lt;p&gt;可以在&lt;code&gt;spark-env.sh&lt;/code&gt;中设置如下变量：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;环境变量&lt;/th&gt;
      &lt;th&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;JAVA_HOME&lt;/td&gt;
      &lt;td&gt;Java安装的路径&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PYSPARK_PYTHON&lt;/td&gt;
      &lt;td&gt;PySpark用到的Python二进制执行文件路径&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SPARK_LOCAL_IP&lt;/td&gt;
      &lt;td&gt;机器绑定的IP地址&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SPARK_PUBLIC_DNS&lt;/td&gt;
      &lt;td&gt;你Spark应用程序通知给其他机器的主机名&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;除了以上这些，Spark &lt;a href=&quot;http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts&quot;&gt;standalone cluster scripts&lt;/a&gt;也可以设置一些选项。例如每台机器使用的核数以及最大内存。&lt;/p&gt;

&lt;p&gt;因为&lt;code&gt;spark-env.sh&lt;/code&gt;是shell脚本，其中的一些可以以编程方式设置。例如，你可以通过特定的网络接口计算&lt;code&gt;SPARK_LOCAL_IP&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;配置日志&lt;/h2&gt;

&lt;p&gt;Spark用&lt;a href=&quot;http://logging.apache.org/log4j/&quot;&gt;log4j&lt;/a&gt; logging。你可以通过在conf目录下添加&lt;code&gt;log4j.properties&lt;/code&gt;文件来配置。一种方法是复制&lt;code&gt;log4j.properties.template&lt;/code&gt;文件。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2015/06/07/spark-configuration.html</link>
      <guid>http://blog.javachen.com/2015/06/07/spark-configuration.html</guid>
      <pubDate>2015-06-07T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>YARN的内存和CPU配置</title>
      <description>&lt;p&gt;Hadoop YARN同时支持内存和CPU两种资源的调度，本文介绍如何配置YARN对内存和CPU的使用。&lt;/p&gt;

&lt;p&gt;YARN作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据application申请的资源进行分配Container。Container是YARN里面资源分配的基本单位，具有一定的内存以及CPU资源。&lt;/p&gt;

&lt;p&gt;在YARN集群中，平衡内存、CPU、磁盘的资源的很重要的，根据经验，每两个container使用一块磁盘以及一个CPU核的时候可以使集群的资源得到一个比较好的利用。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;内存配置&lt;/h1&gt;

&lt;p&gt;关于&lt;em&gt;内存&lt;/em&gt;相关的配置可以参考hortonwork公司的文档&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html&quot;&gt;Determine HDP Memory Configuration Settings&lt;/a&gt;来配置你的集群。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;YARN以及MAPREDUCE所有可用的内存资源应该要除去系统运行需要的以及其他的hadoop的一些程序，总共保留的内存=系统内存+HBASE内存。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以参考下面的表格确定应该保留的内存：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;每台机子内存&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;系统需要的内存&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;HBase需要的内存&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;16GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;24GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;48GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;6GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;64GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;72GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;96GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;12GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;16GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;128GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;24GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;24GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;255GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;32GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;32GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;512GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;64GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;64GB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;计算每台机子最多可以拥有多少个container，可以使用下面的公式:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;containers = min (2*CORES, 1.8*DISKS, (Total available RAM) / MIN_CONTAINER_SIZE)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;CORES&lt;/code&gt;为机器CPU核数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;DISKS&lt;/code&gt;为机器上挂载的磁盘个数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Total available RAM&lt;/code&gt;为机器总内存&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;MIN_CONTAINER_SIZE&lt;/code&gt;是指container最小的容量大小，这需要根据具体情况去设置，可以参考下面的表格：&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;每台机子可用的RAM&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;container最小值&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;小于4GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;256MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4GB到8GB之间&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;512MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8GB到24GB之间&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1024MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;大于24GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2048MB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;每个container的平均使用内存大小计算方式为：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;通过上面的计算，YARN以及MAPREDUCE可以这样配置：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;配置文件&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;配置设置&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;默认值&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;计算值&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.nodemanager.resource.memory-mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8192 MB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= containers * RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.scheduler.minimum-allocation-mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1024MB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.scheduler.maximum-allocation-mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8192 MB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= containers * RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml (check)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.app.mapreduce.am.resource.mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1536 MB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 2 * RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml (check)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.app.mapreduce.am.command-opts&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-Xmx1024m&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 0.8 * 2 * RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapred-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapreduce.map.memory.mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1024 MB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapred-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapreduce.reduce.memory.mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1024 MB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 2 * RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapred-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapreduce.map.java.opts&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 0.8 * RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapred-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapreduce.reduce.java.opts&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 0.8 * 2 * RAM-per-container&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;举个例子：对于128G内存、32核CPU的机器，挂载了7个磁盘，根据上面的说明，系统保留内存为24G，不适应HBase情况下，系统剩余可用内存为104G，计算containers值如下：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;containers = min (2*32, 1.8* 7 , (128-24)/2) = min (64, 12.6 , 51) = 13&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;计算RAM-per-container值如下：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;你也可以使用脚本&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-9.html&quot;&gt;yarn-utils.py&lt;/a&gt;来计算上面的值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#!/usr/bin/env python
import optparse
from pprint import pprint
import logging
import sys
import math
import ast

&#39;&#39;&#39; Reserved for OS + DN + NM,  Map: Memory =&amp;gt; Reservation &#39;&#39;&#39;
reservedStack = { 4:1, 8:2, 16:2, 24:4, 48:6, 64:8, 72:8, 96:12, 
                   128:24, 256:32, 512:64}
&#39;&#39;&#39; Reserved for HBase. Map: Memory =&amp;gt; Reservation &#39;&#39;&#39;
  
reservedHBase = {4:1, 8:1, 16:2, 24:4, 48:8, 64:8, 72:8, 96:16, 
                   128:24, 256:32, 512:64}
GB = 1024

def getMinContainerSize(memory):
  if (memory &amp;lt;= 4):
    return 256
  elif (memory &amp;lt;= 8):
    return 512
  elif (memory &amp;lt;= 24):
    return 1024
  else:
    return 2048
  pass

def getReservedStackMemory(memory):
  if (reservedStack.has_key(memory)):
    return reservedStack[memory]
  if (memory &amp;lt;= 4):
    ret = 1
  elif (memory &amp;gt;= 512):
    ret = 64
  else:
    ret = 1
  return ret

def getReservedHBaseMem(memory):
  if (reservedHBase.has_key(memory)):
    return reservedHBase[memory]
  if (memory &amp;lt;= 4):
    ret = 1
  elif (memory &amp;gt;= 512):
    ret = 64
  else:
    ret = 2
  return ret
                    
def main():
  log = logging.getLogger(__name__)
  out_hdlr = logging.StreamHandler(sys.stdout)
  out_hdlr.setFormatter(logging.Formatter(&#39; %(message)s&#39;))
  out_hdlr.setLevel(logging.INFO)
  log.addHandler(out_hdlr)
  log.setLevel(logging.INFO)
  parser = optparse.OptionParser()
  memory = 0
  cores = 0
  disks = 0
  hbaseEnabled = True
  parser.add_option(&#39;-c&#39;, &#39;--cores&#39;, default = 16,
                     help = &#39;Number of cores on each host&#39;)
  parser.add_option(&#39;-m&#39;, &#39;--memory&#39;, default = 64, 
                    help = &#39;Amount of Memory on each host in GB&#39;)
  parser.add_option(&#39;-d&#39;, &#39;--disks&#39;, default = 4, 
                    help = &#39;Number of disks on each host&#39;)
  parser.add_option(&#39;-k&#39;, &#39;--hbase&#39;, default = &quot;True&quot;,
                    help = &#39;True if HBase is installed, False is not&#39;)
  (options, args) = parser.parse_args()
  
  cores = int (options.cores)
  memory = int (options.memory)
  disks = int (options.disks)
  hbaseEnabled = ast.literal_eval(options.hbase)
  
  log.info(&quot;Using cores=&quot; +  str(cores) + &quot; memory=&quot; + str(memory) + &quot;GB&quot; +
            &quot; disks=&quot; + str(disks) + &quot; hbase=&quot; + str(hbaseEnabled))
  minContainerSize = getMinContainerSize(memory)
  reservedStackMemory = getReservedStackMemory(memory)
  reservedHBaseMemory = 0
  if (hbaseEnabled):
    reservedHBaseMemory = getReservedHBaseMem(memory)
  reservedMem = reservedStackMemory + reservedHBaseMemory
  usableMem = memory - reservedMem
  memory -= (reservedMem)
  if (memory &amp;lt; 2):
    memory = 2
    reservedMem = max(0, memory - reservedMem)
    
  memory *= GB
  
  containers = int (min(2 * cores,
                         min(math.ceil(1.8 * float(disks)),
                              memory/minContainerSize)))
  if (containers &amp;lt;= 2):
    containers = 3

  log.info(&quot;Profile: cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;MB&quot;
           + &quot; reserved=&quot; + str(reservedMem) + &quot;GB&quot; + &quot; usableMem=&quot;
           + str(usableMem) + &quot;GB&quot; + &quot; disks=&quot; + str(disks))
    
  container_ram =  abs(memory/containers)
  if (container_ram &amp;gt; GB):
    container_ram = int(math.floor(container_ram / 512)) * 512
  log.info(&quot;Num Container=&quot; + str(containers))
  log.info(&quot;Container Ram=&quot; + str(container_ram) + &quot;MB&quot;)
  log.info(&quot;Used Ram=&quot; + str(int (containers*container_ram/float(GB))) + &quot;GB&quot;)
  log.info(&quot;Unused Ram=&quot; + str(reservedMem) + &quot;GB&quot;)
  log.info(&quot;yarn.scheduler.minimum-allocation-mb=&quot; + str(container_ram))
  log.info(&quot;yarn.scheduler.maximum-allocation-mb=&quot; + str(containers*container_ram))
  log.info(&quot;yarn.nodemanager.resource.memory-mb=&quot; + str(containers*container_ram))
  map_memory = container_ram
  reduce_memory = 2*container_ram if (container_ram &amp;lt;= 2048) else container_ram
  am_memory = max(map_memory, reduce_memory)
  log.info(&quot;mapreduce.map.memory.mb=&quot; + str(map_memory))
  log.info(&quot;mapreduce.map.java.opts=-Xmx&quot; + str(int(0.8 * map_memory)) +&quot;m&quot;)
  log.info(&quot;mapreduce.reduce.memory.mb=&quot; + str(reduce_memory))
  log.info(&quot;mapreduce.reduce.java.opts=-Xmx&quot; + str(int(0.8 * reduce_memory)) + &quot;m&quot;)
  log.info(&quot;yarn.app.mapreduce.am.resource.mb=&quot; + str(am_memory))
  log.info(&quot;yarn.app.mapreduce.am.command-opts=-Xmx&quot; + str(int(0.8*am_memory)) + &quot;m&quot;)
  log.info(&quot;mapreduce.task.io.sort.mb=&quot; + str(int(0.4 * map_memory)))
  pass

if __name__ == &#39;__main__&#39;:
  try:
    main()
  except(KeyboardInterrupt, EOFError):
    print(&quot;\nAborting ... Keyboard Interrupt.&quot;)
    sys.exit(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;python yarn-utils.py -c 32 -m 128 -d 7 -k False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt; Using cores=32 memory=128GB disks=7 hbase=False
 Profile: cores=32 memory=106496MB reserved=24GB usableMem=104GB disks=7
 Num Container=13
 Container Ram=8192MB
 Used Ram=104GB
 Unused Ram=24GB
 yarn.scheduler.minimum-allocation-mb=8192
 yarn.scheduler.maximum-allocation-mb=106496
 yarn.nodemanager.resource.memory-mb=106496
 mapreduce.map.memory.mb=8192
 mapreduce.map.java.opts=-Xmx6553m
 mapreduce.reduce.memory.mb=8192
 mapreduce.reduce.java.opts=-Xmx6553m
 yarn.app.mapreduce.am.resource.mb=8192
 yarn.app.mapreduce.am.command-opts=-Xmx6553m
 mapreduce.task.io.sort.mb=3276
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样的话，每个container内存为8G，似乎有点多，我更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;配置文件&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;配置设置&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;计算值&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.nodemanager.resource.memory-mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 52 * 2 =104 G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.scheduler.minimum-allocation-mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 2G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.scheduler.maximum-allocation-mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 52 * 2 = 104G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml (check)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.app.mapreduce.am.resource.mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 2 * 2=4G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn-site.xml (check)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yarn.app.mapreduce.am.command-opts&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 0.8 * 2 * 2=3.2G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapred-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapreduce.map.memory.mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 2G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapred-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapreduce.reduce.memory.mb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 2 * 2=4G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapred-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapreduce.map.java.opts&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;= 0.8 * 2=1.6G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapred-site.xml&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapreduce.reduce.java.opts&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;=  0.8 * 2 * 2=3.2G&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;对应的xml配置为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.nodemanager.resource.memory-mb&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;106496&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.scheduler.minimum-allocation-mb&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;2048&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.scheduler.maximum-allocation-mb&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;106496&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.app.mapreduce.am.resource.mb&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;4096&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.app.mapreduce.am.command-opts&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;-Xmx3276m&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，还有一下几个参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/code&gt;：任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.nodemanager.pmem-check-enabled&lt;/code&gt;：是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/code&gt;：是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第一个参数的意思是当一个map任务总共分配的物理内存为2G的时候，该任务的container最多内分配的堆内存为1.6G，可以分配的虚拟内存上限为2*2.1=4.2G。另外，照这样算下去，每个节点上YARN可以启动的Map数为104/2=52个。&lt;/p&gt;

&lt;h1 id=&quot;cpu&quot;&gt;CPU配置&lt;/h1&gt;

&lt;p&gt;YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟CPU个数。&lt;/p&gt;

&lt;p&gt;在YARN中，CPU相关配置参数如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;yarn.nodemanager.resource.cpu-vcores&lt;/code&gt;：表示该节点上YARN可使用的虚拟CPU个数，默认是8，注意，目前推荐将该值设值为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.scheduler.minimum-allocation-vcores&lt;/code&gt;：单个任务可申请的最小虚拟CPU个数，默认是1，如果一个任务申请的CPU个数少于该数，则该对应的值改为这个数。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.scheduler.maximum-allocation-vcores&lt;/code&gt;：单个任务可申请的最多虚拟CPU个数，默认是32。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于一个CPU核数较多的集群来说，上面的默认配置显然是不合适的，在我的测试集群中，4个节点每个机器CPU核数为31，留一个给操作系统，可以配置为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.nodemanager.resource.cpu-vcores&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;31&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;yarn.scheduler.maximum-allocation-vcores&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;124&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html&quot;&gt;Determine HDP Memory Configuration Settings&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-memory-cpu-scheduling/&quot;&gt;Hadoop YARN如何调度内存和CPU&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/06/05/yarn-memory-and-cpu-configuration.html</link>
      <guid>http://blog.javachen.com/2015/06/05/yarn-memory-and-cpu-configuration.html</guid>
      <pubDate>2015-06-05T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>如何使用Spark ALS实现协同过滤</title>
      <description>&lt;p&gt;本文主要记录最近一段时间学习和实现&lt;a href=&quot;/2015/04/17/spark-mllib-collaborative-filtering.html&quot;&gt;Spark MLlib中的协同过滤&lt;/a&gt;的一些总结，希望对大家熟悉Spark ALS算法有所帮助。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;更新：&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;【2016.06.12】Spark1.4.0中MatrixFactorizationModel提供了recommendForAll方法实现离线批量推荐，见&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3066&quot;&gt;SPARK-3066&lt;/a&gt;。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section&quot;&gt;测试环境&lt;/h1&gt;

&lt;p&gt;为了测试简单，在本地以local方式运行Spark，你需要做的是下载编译好的压缩包解压即可，可以参考&lt;a href=&quot;/2015/03/30/spark-test-in-local-mode.html&quot;&gt;Spark本地模式运行&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;测试数据使用&lt;a href=&quot;http://grouplens.org/datasets/movielens/&quot;&gt;MovieLens&lt;/a&gt;的&lt;a href=&quot;http://files.grouplens.org/datasets/movielens/ml-10m.zip&quot;&gt;MovieLens 10M数据集&lt;/a&gt;，下载之后解压到data目录。数据的格式请参考README中的说明，需要注意的是ratings.dat中的数据被处理过，&lt;code&gt;每个用户至少访问了20个商品&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;下面的代码均在spark-shell中运行，启动时候可以根据你的机器内存设置JVM参数，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bin/spark-shell --executor-memory 3g --driver-memory 3g --driver-java-options &#39;-Xms2g -Xmx2g -XX:+UseCompressedOops&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;预测评分&lt;/h1&gt;

&lt;p&gt;这个例子主要演示如何训练数据、评分并计算根均方差。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;准备工作&lt;/h2&gt;

&lt;p&gt;首先，启动spark-shell，然后引入mllib包，我们需要用到ALS算法类和Rating评分类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import org.apache.spark.mllib.recommendation.{ALS, Rating}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spark的日志级别默认为INFO，你可以手动设置为WARN级别，同样先引入log4j依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import org.apache.log4j.{Logger,Level}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，运行下面代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;加载数据&lt;/h2&gt;

&lt;p&gt;spark-shell启动成功之后，sc为内置变量，你可以通过它来加载测试数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val data = sc.textFile(&quot;data/ml-1m/ratings.dat&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来解析文件内容，获得用户对商品的评分记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val ratings = data.map(_.split(&quot;::&quot;) match { case Array(user, item, rate, ts) =&amp;gt;
  Rating(user.toInt, item.toInt, rate.toDouble)
}).cache()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看第一条记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; ratings.first
res81: org.apache.spark.mllib.recommendation.Rating = Rating(1,1193,5.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以统计文件中用户和商品数量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val users = ratings.map(_.user).distinct()
val products = ratings.map(_.product).distinct()
println(&quot;Got &quot;+ratings.count()+&quot; ratings from &quot;+users.count+&quot; users on &quot;+products.count+&quot; products.&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到如下输出：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//Got 1000209 ratings from 6040 users on 3706 products.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以对评分数据生成训练集和测试集，例如：训练集和测试集比例为8比2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val splits = ratings.randomSplit(Array(0.8, 0.2), seed = 111l)
val training = splits(0).repartition(numPartitions)
val test = splits(1).repartition(numPartitions)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里，我们是将评分数据全部当做训练集，并且也为测试集。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;训练模型&lt;/h2&gt;

&lt;p&gt;接下来调用&lt;code&gt;ALS.train()&lt;/code&gt;方法，进行模型训练：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val rank = 12
val lambda = 0.01
val numIterations = 20
val model = ALS.train(ratings, rank, numIterations, lambda)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;训练完后，我们看看model中的用户和商品特征向量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;model.userFeatures
//res82: org.apache.spark.rdd.RDD[(Int, Array[Double])] = users MapPartitionsRDD[400] at mapValues at ALS.scala:218

model.userFeatures.count
//res84: Long = 6040

model.productFeatures
//res85: org.apache.spark.rdd.RDD[(Int, Array[Double])] = products MapPartitionsRDD[401] at mapValues at ALS.scala:222

model.productFeatures.count
//res86: Long = 3706
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;评测&lt;/h2&gt;

&lt;p&gt;我们要对比一下预测的结果，注意：&lt;strong&gt;我们将训练集当作测试集&lt;/strong&gt;来进行对比测试。从训练集中获取用户和商品的映射：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val usersProducts= ratings.map { case Rating(user, product, rate) =&amp;gt;
  (user, product)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显然，测试集的记录数等于评分总记录数，验证一下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;usersProducts.count  //Long = 1000209
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用推荐模型对用户商品进行预测评分，得到预测评分的数据集：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;var predictions = model.predict(usersProducts).map { case Rating(user, product, rate) =&amp;gt;
    ((user, product), rate)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看其记录数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;predictions.count //Long = 1000209
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将真实评分数据集与预测评分数据集进行合并，这样得到用户对每一个商品的实际评分和预测评分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val ratesAndPreds = ratings.map { case Rating(user, product, rate) =&amp;gt;
  ((user, product), rate)
}.join(predictions)

ratesAndPreds.count  //Long = 1000209
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后计算根均方差：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val rmse= math.sqrt(ratesAndPreds.map { case ((user, product), (r1, r2)) =&amp;gt;
  val err = (r1 - r2)
  err * err
}.mean())

println(s&quot;RMSE = $rmse&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这段代码其实就是&lt;code&gt;对测试集进行评分预测并计算相似度&lt;/code&gt;，这段代码可以抽象为一个方法，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;/** Compute RMSE (Root Mean Squared Error). */
def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating]) = {
  val usersProducts = data.map { case Rating(user, product, rate) =&amp;gt;
    (user, product)
  }

  val predictions = model.predict(usersProducts).map { case Rating(user, product, rate) =&amp;gt;
    ((user, product), rate)
  }

  val ratesAndPreds = data.map { case Rating(user, product, rate) =&amp;gt;
    ((user, product), rate)
  }.join(predictions)

  math.sqrt(ratesAndPreds.map { case ((user, product), (r1, r2)) =&amp;gt;
    val err = (r1 - r2)
    err * err
  }.mean())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了RMSE指标，我们还可以及时AUC以及Mean average precision at K (MAPK)，关于AUC的计算方法，参考&lt;a href=&quot;https://github.com/sryza/aas/blob/master/ch03-recommender/src/main/scala/com/cloudera/datascience/recommender/RunRecommender.scala&quot;&gt;RunRecommender.scala&lt;/a&gt;，关于MAPK的计算方法可以参考《&lt;a href=&quot;http://f.dataguru.cn/thread-495493-1-1.html&quot;&gt;Packt.Machine Learning with Spark.2015.pdf&lt;/a&gt;》一书第四章节内容，或者你可以看本文后面内容。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;保存真实评分和预测评分&lt;/h2&gt;

&lt;p&gt;我们还可以保存用户对商品的真实评分和预测评分记录到本地文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;ratesAndPreds.sortByKey().repartition(1).sortBy(_._1).map({
  case ((user, product), (rate, pred)) =&amp;gt; (user + &quot;,&quot; + product + &quot;,&quot; + rate + &quot;,&quot; + pred)
}).saveAsTextFile(&quot;/tmp/result&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这段代码先按用户排序，然后重新分区确保目标目录中只生成一个文件。如果你重复运行这段代码，则需要先删除目标路径：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import scala.sys.process._
&quot;rm -r /tmp/result&quot;.!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们还可以对预测的评分结果按用户进行分组并按评分倒排序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;predictions.map { case ((user, product), rate) =&amp;gt;
  (user, (product, rate))
}.groupByKey(numPartitions).map{case (user_id,list)=&amp;gt;
  (user_id,list.toList.sortBy {case (goods_id,rate)=&amp;gt; - rate})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-7&quot;&gt;给一个用户推荐商品&lt;/h1&gt;

&lt;p&gt;这个例子主要是记录如何给一个或大量用户进行推荐商品，例如，对用户编号为384的用户进行推荐，查出该用户在测试集中评分过的商品。&lt;/p&gt;

&lt;p&gt;找出5个用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;users.take(5) 
//Array[Int] = Array(384, 1084, 4904, 3702, 5618)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看用户编号为384的用户的预测结果中预测评分排前10的商品：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val userId = users.take(1)(0) //384
val K = 10
val topKRecs = model.recommendProducts(userId, K)
println(topKRecs.mkString(&quot;\n&quot;))
//    Rating(384,2545,8.354966018818265)
//    Rating(384,129,8.113083736094676)
//    Rating(384,184,8.038113395650853)
//    Rating(384,811,7.983433591425284)
//    Rating(384,1421,7.912044967873945)
//    Rating(384,1313,7.719639594879865)
//    Rating(384,2892,7.53667094600392)
//    Rating(384,2483,7.295378004543803)
//    Rating(384,397,7.141158013610967)
//    Rating(384,97,7.071089782695754)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看该用户的评分记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val goodsForUser=ratings.keyBy(_.user).lookup(384)
// Seq[org.apache.spark.mllib.recommendation.Rating] = WrappedArray(Rating(384,2055,2.0), Rating(384,1197,4.0), Rating(384,593,5.0), Rating(384,599,3.0), Rating(384,673,2.0), Rating(384,3037,4.0), Rating(384,1381,2.0), Rating(384,1610,4.0), Rating(384,3074,4.0), Rating(384,204,4.0), Rating(384,3508,3.0), Rating(384,1007,3.0), Rating(384,260,4.0), Rating(384,3487,3.0), Rating(384,3494,3.0), Rating(384,1201,5.0), Rating(384,3671,5.0), Rating(384,1207,4.0), Rating(384,2947,4.0), Rating(384,2951,4.0), Rating(384,2896,2.0), Rating(384,1304,5.0))

productsForUser.size //Int = 22
productsForUser.sortBy(-_.rating).take(10).map(rating =&amp;gt; (rating.product, rating.rating)).foreach(println)
//    (593,5.0)
//    (1201,5.0)
//    (3671,5.0)
//    (1304,5.0)
//    (1197,4.0)
//    (3037,4.0)
//    (1610,4.0)
//    (3074,4.0)
//    (204,4.0)
//    (260,4.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到该用户对22个商品评过分以及浏览的商品是哪些。&lt;/p&gt;

&lt;p&gt;我们可以该用户对某一个商品的实际评分和预测评分方差为多少：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val actualRating = productsForUser.take(1)(0)
//actualRating: org.apache.spark.mllib.recommendation.Rating = Rating(384,2055,2.0)    val predictedRating = model.predict(789, actualRating.product)
val predictedRating = model.predict(384, actualRating.product)
//predictedRating: Double = 1.9426030777174637
val squaredError = math.pow(predictedRating - actualRating.rating, 2.0)
//squaredError: Double = 0.0032944066875075172
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如何找出和一个已知商品最相似的商品呢？这里，我们可以使用余弦相似度来计算：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import org.jblas.DoubleMatrix

/* Compute the cosine similarity between two vectors */
def cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = {
  vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以2055商品为例，计算实际评分和预测评分相似度&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val itemId = 2055
val itemFactor = model.productFeatures.lookup(itemId).head
//itemFactor: Array[Double] = Array(0.3660752773284912, 0.43573060631752014, -0.3421429991722107, 0.44382765889167786, -1.4875195026397705, 0.6274569630622864, -0.3264533579349518, -0.9939845204353333, -0.8710321187973022, -0.7578890323638916, -0.14621856808662415, -0.7254264950752258)
val itemVector = new DoubleMatrix(itemFactor)
//itemVector: org.jblas.DoubleMatrix = [0.366075; 0.435731; -0.342143; 0.443828; -1.487520; 0.627457; -0.326453; -0.993985; -0.871032; -0.757889; -0.146219; -0.725426]

cosineSimilarity(itemVector, itemVector)
// res99: Double = 0.9999999999999999
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到和该商品最相似的10个商品：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val sims = model.productFeatures.map{ case (id, factor) =&amp;gt;
  val factorVector = new DoubleMatrix(factor)
  val sim = cosineSimilarity(factorVector, itemVector)
  (id, sim)
}
val sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { case (id, similarity) =&amp;gt; similarity })
//sortedSims: Array[(Int, Double)] = Array((2055,0.9999999999999999), (2051,0.9138311231145874), (3520,0.8739823400539756), (2190,0.8718466671129721), (2050,0.8612639515847019), (1011,0.8466911667526461), (2903,0.8455764332511272), (3121,0.8227325520485377), (3674,0.8075743004357392), (2016,0.8063817280259447))
println(sortedSims.mkString(&quot;\n&quot;))
//    (2055,0.9999999999999999)
//    (2051,0.9138311231145874)
//    (3520,0.8739823400539756)
//    (2190,0.8718466671129721)
//    (2050,0.8612639515847019)
//    (1011,0.8466911667526461)
//    (2903,0.8455764332511272)
//    (3121,0.8227325520485377)
//    (3674,0.8075743004357392)
//    (2016,0.8063817280259447)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显然第一个最相似的商品即为该商品本身，即2055，我们可以修改下代码，取前k+1个商品，然后排除第一个：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] { case (id, similarity) =&amp;gt; similarity })
//sortedSims2: Array[(Int, Double)] = Array((2055,0.9999999999999999), (2051,0.9138311231145874), (3520,0.8739823400539756), (2190,0.8718466671129721), (2050,0.8612639515847019), (1011,0.8466911667526461), (2903,0.8455764332511272), (3121,0.8227325520485377), (3674,0.8075743004357392), (2016,0.8063817280259447), (3672,0.8016276723120674))

sortedSims2.slice(1, 11).map{ case (id, sim) =&amp;gt; (id, sim) }.mkString(&quot;\n&quot;)
//    (2051,0.9138311231145874)
//    (3520,0.8739823400539756)
//    (2190,0.8718466671129721)
//    (2050,0.8612639515847019)
//    (1011,0.8466911667526461)
//    (2903,0.8455764332511272)
//    (3121,0.8227325520485377)
//    (3674,0.8075743004357392)
//    (2016,0.8063817280259447)
//    (3672,0.8016276723120674)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，我们可以计算给该用户推荐的前K个商品的平均准确度MAPK，该算法定义如下（该算法是否正确还有待考证）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;/* Function to compute average precision given a set of actual and predicted ratings */
// Code for this function is based on: https://github.com/benhamner/Metrics
def avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int): Double = {
  val predK = predicted.take(k)
  var score = 0.0
  var numHits = 0.0
  for ((p, i) &amp;lt;- predK.zipWithIndex) {
    if (actual.contains(p)) {
      numHits += 1.0
      score += numHits / (i.toDouble + 1.0)
    }
  }
  if (actual.isEmpty) {
    1.0
  } else {
    score / scala.math.min(actual.size, k).toDouble
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给该用户推荐的商品为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val actualProducts = productsForUser.map(_.product)
//actualProducts: Seq[Int] = ArrayBuffer(2055, 1197, 593, 599, 673, 3037, 1381, 1610, 3074, 204, 3508, 1007, 260, 3487, 3494, 1201, 3671, 1207, 2947, 2951, 2896, 1304)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给该用户预测的商品为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt; val predictedProducts = topKRecs.map(_.product)
//predictedProducts: Array[Int] = Array(2545, 129, 184, 811, 1421, 1313, 2892, 2483, 397, 97)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后的准确度为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val apk10 = avgPrecisionK(actualProducts, predictedProducts, 10)
// apk10: Double = 0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-8&quot;&gt;批量推荐&lt;/h1&gt;

&lt;p&gt;你可以评分记录中获得所有用户然后依次给每个用户推荐：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val users = ratings.map(_.user).distinct()

users.collect.flatMap { user =&amp;gt;
  model.recommendProducts(user, 10)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式是遍历内存中的一个集合然后循环调用RDD的操作，运行会比较慢，另外一种方式是直接操作model中的userFeatures和productFeatures，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val itemFactors = model.productFeatures.map { case (id, factor) =&amp;gt; factor }.collect()
val itemMatrix = new DoubleMatrix(itemFactors)
println(itemMatrix.rows, itemMatrix.columns)
//(3706,12)

// broadcast the item factor matrix
val imBroadcast = sc.broadcast(itemMatrix)

//获取商品和索引的映射
var idxProducts=model.productFeatures.map { case (prodcut, factor) =&amp;gt; prodcut }.zipWithIndex().map{case (prodcut, idx) =&amp;gt; (idx,prodcut)}.collectAsMap()
val idxProductsBroadcast = sc.broadcast(idxProducts)

val allRecs = model.userFeatures.map{ case (user, array) =&amp;gt;
  val userVector = new DoubleMatrix(array)
  val scores = imBroadcast.value.mmul(userVector)
  val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)
  //根据索引取对应的商品id
  val recommendedProducts = sortedWithId.map(_._2).map{idx=&amp;gt;idxProductsBroadcast.value.get(idx).get}
  (user, recommendedProducts) 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式其实还不是最优方法，更好的方法可以参考&lt;a href=&quot;http://www.alesisnovik.com/?p=8&quot;&gt;Personalised recommendations using Spark&lt;/a&gt;，当然这篇文章中的代码还可以继续优化一下。我修改后的代码如下，供大家参考：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val productFeatures = model.productFeatures.collect()
var productArray = ArrayBuffer[Int]()
var productFeaturesArray = ArrayBuffer[Array[Double]]()
for ((product, features) &amp;lt;- productFeatures) {
  productArray += product
  productFeaturesArray += features
}

val productArrayBroadcast = sc.broadcast(productArray)
val productFeatureMatrixBroadcast = sc.broadcast(new DoubleMatrix(productFeaturesArray.toArray).transpose())

start = System.currentTimeMillis()
val allRecs = model.userFeatures.mapPartitions { iter =&amp;gt;
  // Build user feature matrix for jblas
  var userFeaturesArray = ArrayBuffer[Array[Double]]()
  var userArray = new ArrayBuffer[Int]()
  while (iter.hasNext) {
    val (user, features) = iter.next()
    userArray += user
    userFeaturesArray += features
  }

  var userFeatureMatrix = new DoubleMatrix(userFeaturesArray.toArray)
  var userRecommendationMatrix = userFeatureMatrix.mmul(productFeatureMatrixBroadcast.value)
  var productArray=productArrayBroadcast.value
  var mappedUserRecommendationArray = new ArrayBuffer[String](params.topk)

  // Extract ratings from the matrix
  for (i &amp;lt;- 0 until userArray.length) {
    var ratingSet =  mutable.TreeSet.empty(Ordering.fromLessThan[(Int,Double)](_._2 &amp;gt; _._2))
    for (j &amp;lt;- 0 until productArray.length) {
      var rating = (productArray(j), userRecommendationMatrix.get(i,j))
      ratingSet += rating
    }
    mappedUserRecommendationArray += userArray(i)+&quot;,&quot;+ratingSet.take(params.topk).mkString(&quot;,&quot;)
  }
  mappedUserRecommendationArray.iterator
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;2015.06.12 更新：&lt;/p&gt;

  &lt;p&gt;悲哀的是，上面的方法还是不能解决问题，因为矩阵相乘会撑爆集群内存；可喜的是，如果你关注Spark最新动态，你会发现Spark1.4.0中MatrixFactorizationModel提供了&lt;code&gt;recommendForAll&lt;/code&gt;方法实现离线批量推荐，详细说明见&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3066&quot;&gt;SPARK-3066&lt;/a&gt;。因为，我使用的Hadoop版本是CDH-5.4.0，其中Spark版本还是1.3.0，所以暂且不能在集群上测试Spark1.4.0中添加的新方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;如果上面结果跑出来了，就可以验证推荐结果是否正确&lt;/code&gt;。还是以384用户为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;allRecs.lookup(384).head.take(10)
//res50: Array[Int] = Array(1539, 219, 1520, 775, 3161, 2711, 2503, 771, 853, 759)
topKRecs.map(_.product)
//res49: Array[Int] = Array(1539, 219, 1520, 775, 3161, 2711, 2503, 771, 853, 759)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，我们可以计算所有推荐结果的准确度了，首先，得到每个用户评分过的所有商品：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val userProducts = ratings.map{ case Rating(user, product, rating) =&amp;gt; (user, product) }.groupBy(_._1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，预测的商品和实际商品关联求准确度：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// finally, compute the APK for each user, and average them to find MAPK
val MAPK = allRecs.join(userProducts).map{ case (userId, (predicted, actualWithIds)) =&amp;gt;
  val actual = actualWithIds.map(_._2).toSeq
  avgPrecisionK(actual, predicted, K)
}.reduce(_ + _) / allRecs.count
println(&quot;Mean Average Precision at K = &quot; + MAPK)
//Mean Average Precision at K = 0.018827551771260383
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实，我们也可以使用Spark内置的算法计算RMSE和MAE：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// MSE, RMSE and MAE
import org.apache.spark.mllib.evaluation.RegressionMetrics

val predictedAndTrue = ratesAndPreds.map { case ((user, product), (actual, predicted)) =&amp;gt; (actual, predicted) }
val regressionMetrics = new RegressionMetrics(predictedAndTrue)
println(&quot;Mean Squared Error = &quot; + regressionMetrics.meanSquaredError)
println(&quot;Root Mean Squared Error = &quot; + regressionMetrics.rootMeanSquaredError)
// Mean Squared Error = 0.5490153087908566
// Root Mean Squared Error = 0.7409556726220918

// MAPK
import org.apache.spark.mllib.evaluation.RankingMetrics
val predictedAndTrueForRanking = allRecs.join(userProducts).map{ case (userId, (predicted, actualWithIds)) =&amp;gt;
  val actual = actualWithIds.map(_._2)
  (predicted.toArray, actual.toArray)
}
val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)
println(&quot;Mean Average Precision = &quot; + rankingMetrics.meanAveragePrecision)
// Mean Average Precision = 0.04417535679520426
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;计算推荐2000个商品时的准确度为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val MAPK2000 = allRecs.join(userProducts).map{ case (userId, (predicted, actualWithIds)) =&amp;gt;
  val actual = actualWithIds.map(_._2).toSeq
  avgPrecisionK(actual, predicted, 2000)
}.reduce(_ + _) / allRecs.count
println(&quot;Mean Average Precision = &quot; + MAPK2000)
//Mean Average Precision = 0.025228311843069083
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-9&quot;&gt;保存和加载推荐模型&lt;/h1&gt;

&lt;p&gt;对与实时推荐，我们需要启动一个web server，在启动的时候生成或加载训练模型，然后提供API接口返回推荐接口，需要调用的相关方法为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;save(model: MatrixFactorizationModel, path: String)
load(sc: SparkContext, path: String)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;model中的userFeatures和productFeatures也可以保存起来：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val outputDir=&quot;/tmp&quot;
model.userFeatures.map{ case (id, vec) =&amp;gt; id + &quot;\t&quot; + vec.mkString(&quot;,&quot;) }.saveAsTextFile(outputDir + &quot;/userFeatures&quot;)
model.productFeatures.map{ case (id, vec) =&amp;gt; id + &quot;\t&quot; + vec.mkString(&quot;,&quot;) }.saveAsTextFile(outputDir + &quot;/productFeatures&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-10&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;本文主要记录如何使用ALS算法实现协同过滤并给用户推荐商品，以上代码在&lt;a href=&quot;https://github.com/javachen/learning-spark/tree/master/src/main/scala/com/javachen/spark/examples/mllib&quot;&gt;Github&lt;/a&gt;仓库中的ScalaLocalALS.scala文件。&lt;/p&gt;

&lt;p&gt;如果你想更加深入了解Spark MLlib算法的使用，可以看看&lt;a href=&quot;http://f.dataguru.cn/thread-495493-1-1.html&quot;&gt;Packt.Machine Learning with Spark.2015.pdf&lt;/a&gt;这本电子书并下载书中的源码，本文大部分代码参考自该电子书。&lt;/p&gt;

&lt;h1 id=&quot;section-11&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/04/17/spark-mllib-collaborative-filtering.html&quot;&gt;Spark MLlib中的协同过滤&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://f.dataguru.cn/thread-495493-1-1.html&quot;&gt;Packt.Machine Learning with Spark.2015.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3066&quot;&gt;SPARK-3066&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html</link>
      <guid>http://blog.javachen.com/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html</guid>
      <pubDate>2015-06-01T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>测试Hive集成Sentry</title>
      <description>&lt;p&gt;本文在&lt;a href=&quot;/2015/04/30/install-and-config-sentry.html&quot;&gt;安装和配置Sentry&lt;/a&gt;基础之上测试Hive集成Sentry。注意：&lt;strong&gt;这里Hive中并没有配置Kerberos认证&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;关于配置了Kerberos的Hive集群如何集成Sentry，请参考&lt;a href=&quot;/2014/11/14/config-secured-hive-with-sentry.html&quot;&gt;配置安全的Hive集群集成Sentry&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;sentry&quot;&gt;1. 配置Sentry&lt;/h1&gt;

&lt;p&gt;见&lt;a href=&quot;/2015/04/30/install-and-config-sentry.html&quot;&gt;安装和配置Sentry&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;hive&quot;&gt;2. 配置Hive&lt;/h1&gt;

&lt;h2 id=&quot;hive-metastoresentry&quot;&gt;Hive Metastore集成Sentry&lt;/h2&gt;

&lt;p&gt;需要在 /etc/hive/conf/hive-site.xml中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hive.metastore.pre.event.listeners&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;org.apache.sentry.binding.metastore.MetastoreAuthzBinding&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hive.metastore.event.listeners&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;org.apache.sentry.binding.metastore.SentryMetastorePostEventListener&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hive-server2sentry&quot;&gt;Hive-server2集成Sentry&lt;/h2&gt;

&lt;p&gt;修改 /etc/hive/conf/hive-site.xml，添加以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hive.server2.enable.impersonation&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.security.authorization.task.factory&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.binding.hive.SentryHiveAuthorizationTaskFactoryImpl&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.session.hook&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.binding.hive.HiveAuthzBindingSessionHook&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.sentry.conf.url&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///etc/hive/conf/sentry-site.xml&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考模板&lt;a href=&quot;https://github.com/cloudera/sentry/blob/cdh5-1.4.0_5.4.0/conf%2Fsentry-site.xml.hive-client.template&quot;&gt;sentry-site.xml.hive-client.template&lt;/a&gt;在 /etc/hive/conf/ 目录创建 sentry-site.xml：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.client.server.rpc-port&amp;lt;/name&amp;gt;
       &amp;lt;value&amp;gt;8038&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.client.server.rpc-address&amp;lt;/name&amp;gt;
       &amp;lt;value&amp;gt;cdh1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.client.server.rpc-connection-timeout&amp;lt;/name&amp;gt;
       &amp;lt;value&amp;gt;200000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!--以下是客户端配置--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.provider&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.provider.file.HadoopGroupResourceAuthorizationProvider&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.provider.backend&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.provider.db.SimpleDBProviderBackend&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.metastore.service.users&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;&amp;lt;!--queries made by hive user (beeline) skip meta store check--&amp;gt;
    &amp;lt;/property&amp;gt;
      &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.server&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;server1&amp;lt;/value&amp;gt;
      &amp;lt;/property&amp;gt;
     &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.testing.mode&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
     &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hive-1&quot;&gt;3. 重启Hive&lt;/h1&gt;

&lt;p&gt;在cdh1上启动或重启hiveserver2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /etc/init.d/hive-server2 restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;4. 准备测试数据&lt;/h1&gt;

&lt;p&gt;参考 &lt;a href=&quot;http://blog.evernote.com/tech/2014/06/09/securing-impala-for-analysts/&quot;&gt;Securing Impala for analysts&lt;/a&gt;，准备测试数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /tmp/events.csv
10.1.2.3,US,android,createNote
10.200.88.99,FR,windows,updateNote
10.1.2.3,US,android,updateNote
10.200.88.77,FR,ios,createNote
10.1.4.5,US,windows,updateTag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在hive中运行下面 sql 语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create database sensitive;

create table sensitive.events (
    ip STRING, country STRING, client STRING, action STRING
  ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;

load data local inpath &#39;/tmp/events.csv&#39; overwrite into table sensitive.events;
create database filtered;
create view filtered.events as select country, client, action from sensitive.events;
create view filtered.events_usonly as select * from filtered.events where country = &#39;US&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh1上通过 beeline 连接 hiveserver2，运行下面命令创建角色和组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 注意：我设置了hiveserver2的jdbc端口为10001
$  beeline -u &quot;jdbc:hive2://cdh1:10001/&quot; -n hive -p hive -d org.apache.hive.jdbc.HiveDriver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行下面的 sql 语句创建 role、group等：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create role admin_role;
GRANT ALL ON SERVER server1 TO ROLE admin_role;
GRANT ROLE admin_role TO GROUP admin;
GRANT ROLE admin_role TO GROUP hive;

create role test_role;
GRANT ALL ON DATABASE filtered TO ROLE test_role;
GRANT ROLE test_role TO GROUP test;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面创建了两个角色：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;admin_role，具有管理员权限，可以读写所有数据库，并授权给 admin 和 hive 组（对应操作系统上的组）&lt;/li&gt;
  &lt;li&gt;test_role，只能读写 filtered 数据库，并授权给 test 组。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为系统上没有test用户和组，所以需要手动创建：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ useradd test
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;5. 测试&lt;/h1&gt;

&lt;h2 id=&quot;adminrole&quot;&gt;测试admin_role角色&lt;/h2&gt;

&lt;p&gt;使用hive用户访问beeline：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ beeline -u &quot;jdbc:hive2://cdh1:10000/&quot; -n hive -p hive -d org.apache.hive.jdbc.HiveDriver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看当前系统用户是谁：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; set system:user.name;
+------------------------+--+
|          set           |
+------------------------+--+
| system:user.name=hive  |
+------------------------+--+
1 row selected (0.188 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hive属于admin_role组，具有管理员权限，可以查看所有角色：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; show roles;
+-------------+--+
|    role     |
+-------------+--+
| test_role   |
| admin_role  |
+-------------+--+
2 rows selected (0.199 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看所有权限：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; SHOW GRANT ROLE test_role;
+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+
| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  | grant_time | 
+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+
| filtered   |        |            |         | test_role       | ROLE            | *          | false       | 1430293474047000  | 
+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+

0: jdbc:hive2://cdh1:10000/&amp;gt; SHOW GRANT ROLE admin_role;
+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+
| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  | grant_time  | 
+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+
| *         |        |            |         | admin_role      | ROLE            | *          | false       | 1430293473308000  
+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+
1 row selected (0.16 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hive用户可以查看所有数据库、访问所有表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; show databases;
+----------------+--+
| database_name  |
+----------------+--+
| default        |
| filtered       |
| sensitive      |
+----------------+--+
3 rows selected (0.391 seconds)
0: jdbc:hive2://cdh1:10000/&amp;gt; use filtered;
No rows affected (0.101 seconds)
0: jdbc:hive2://cdh1:10000/&amp;gt; select * from filtered.events;
+-----------------+----------------+----------------+--+
| events.country  | events.client  | events.action  |
+-----------------+----------------+----------------+--+
| US              | android        | createNote     |
| FR              | windows        | updateNote     |
| US              | android        | updateNote     |
| FR              | ios            | createNote     |
| US              | windows        | updateTag      |
+-----------------+----------------+----------------+--+
5 rows selected (0.431 seconds)
0: jdbc:hive2://cdh1:10000/&amp;gt; select * from sensitive.events;
+---------------+-----------------+----------------+----------------+--+
|   events.ip   | events.country  | events.client  | events.action  |
+---------------+-----------------+----------------+----------------+--+
| 10.1.2.3      | US              | android        | createNote     |
| 10.200.88.99  | FR              | windows        | updateNote     |
| 10.1.2.3      | US              | android        | updateNote     |
| 10.200.88.77  | FR              | ios            | createNote     |
| 10.1.4.5      | US              | windows        | updateTag      |
+---------------+-----------------+----------------+----------------+--+
5 rows selected (0.247 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;testrole&quot;&gt;测试test_role角色&lt;/h2&gt;

&lt;p&gt;使用test用户访问beeline：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ beeline -u &quot;jdbc:hive2://cdh1:10000/&quot; -n test -p test -d org.apache.hive.jdbc.HiveDriver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看当前系统用户是谁：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; set system:user.name;
+------------------------+--+
|          set           |
+------------------------+--+
| system:user.name=hive  |
+------------------------+--+
1 row selected (0.188 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test用户不是管理员，是不能查看所有角色的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; show roles;
ERROR : Error processing Sentry command: Access denied to test. Server Stacktrace: org.apache.sentry.provider.db.SentryAccessDeniedException: Access denied to test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test用户可以列出所有数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; show databases;
+----------------+--+
| database_name  |
+----------------+--+
| default        |
| filtered       |
| sensitive      |
+----------------+--+
3 rows selected (0.079 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test用户可以filtered库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; use filtered;
No rows affected (0.206 seconds)
0: jdbc:hive2://cdh1:10000/&amp;gt; select * from events;
+-----------------+----------------+----------------+--+
| events.country  | events.client  | events.action  |
+-----------------+----------------+----------------+--+
| US              | android        | createNote     |
| FR              | windows        | updateNote     |
| US              | android        | updateNote     |
| FR              | ios            | createNote     |
| US              | windows        | updateTag      |
+-----------------+----------------+----------------+--+
5 rows selected (0.361 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是，test用户没有权限访问sensitive库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:hive2://cdh1:10000/&amp;gt; use sensitive;
Error: Error while compiling statement: FAILED: SemanticException No valid privileges
 Required privileges for this query: Server=server1-&amp;gt;Db=sensitive-&amp;gt;Table=*-&amp;gt;action=insert;Server=server1-&amp;gt;Db=sensitive-&amp;gt;Table=*-&amp;gt;action=select; (state=42000,code=40000)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;6. 排错&lt;/h1&gt;

&lt;p&gt;在CDH5的高版本中，hive cli 不建议使用，在hive集成sentry之后，再运行hive cli 会提示找不到sentry的类的遗产，解决办法是，将sentry相关的jar包链接到hive的home目录下的lib目录下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ln -s /usr/lib/sentry/lib/sentry-binding-hive.jar /usr/lib/hive/lib/
ln -s /usr/lib/sentry/lib/sentry-core-common.jar /usr/lib/hive/lib
ln -s /usr/lib/sentry/lib/sentry-core-common-db.jar /usr/lib/hive/lib
ln -s /usr/lib/sentry/lib/sentry-policy-common.jar /usr/lib/hive/lib
ln -s /usr/lib/sentry/lib/sentry-policy-db.jar /usr/lib/hive/lib
ln -s /usr/lib/sentry/lib/sentry-policy-cache.jar /usr/lib/hive/lib
ln -s /usr/lib/sentry/lib/sentry-provider-common.jar /usr/lib/hive/lib
ln -s /usr/lib/sentry/lib/sentry-provider-db.jar /usr/lib/hive/lib
ln -s /usr/lib/sentry/lib/sentry-provider-cache.jar /usr/lib/hive/lib
ln -s /usr/lib/sentry/lib/sentry-provider-file.jar /usr/lib/hive/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;7. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.evernote.com/tech/2014/06/09/securing-impala-for-analysts/&quot;&gt;Securing Impala for analysts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/04/30/test-hive-with-sentry.html</link>
      <guid>http://blog.javachen.com/2015/04/30/test-hive-with-sentry.html</guid>
      <pubDate>2015-04-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>安装和配置Sentry</title>
      <description>&lt;p&gt;本文主要记录安装和配置Sentry的过程，关于Sentry的介绍，请参考&lt;a href=&quot;/2015/04/29/apache-sentry-architecture.html&quot;&gt;Apache Sentry架构介绍&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里，我参考&lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;一文搭建了一个测试集群，并选择cdh1节点来安装sentry服务。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 安装&lt;/h1&gt;

&lt;p&gt;在cdh1节点上运行下面命令查看Sentry的相关组件有哪些:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum list sentry*

sentry.noarch                        1.4.0+cdh5.4.0+155-1.cdh5.4.0.p0.47.el6                            @cdh
sentry-hdfs-plugin.noarch        1.4.0+cdh5.4.0+155-1.cdh5.4.0.p0.47.el6                            @cdh
sentry-store.noarch                1.4.0+cdh5.4.0+155-1.cdh5.4.0.p0.47.el6                            @cdh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上组件说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;sentry&lt;/code&gt;：sentry的基本包&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sentry-hdfs-plugin&lt;/code&gt;：hdfs插件&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sentry-store&lt;/code&gt;：sentry store组件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里安装以上所有组件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install sentry* -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 配置&lt;/h1&gt;

&lt;p&gt;参考&lt;a href=&quot;https://github.com/cloudera/sentry/blob/cdh5-1.4.0_5.4.0/conf/sentry-site.xml.service.template&quot;&gt;sentry-site.xml.service.template&lt;/a&gt;，来修改Sentry的配置文件 /etc/sentry/conf/sentry-site.xml。&lt;/p&gt;

&lt;h2 id=&quot;sentry-service-&quot;&gt;配置 sentry service 相关的参数&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.admin.group&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;impala,hive,solr,hue&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.allow.connect&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;impala,hive,solr,hue&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.verify.schema.version&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.reporting&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;JMX&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.server.rpc-address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;cdh1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.server.rpc-port&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;8038&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.web.enable&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果需要使用kerberos认证，则还需要配置以下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.security.mode&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kerberos&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.server.principal&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.server.keytab&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sentry-store-&quot;&gt;配置 sentry store 相关参数&lt;/h2&gt;

&lt;p&gt;sentry store可以使用两种方式，如果使用基于SimpleDbProviderBackend的方式，则需要设置jdbc相关的参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.store.jdbc.url&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;jdbc:postgresql://cdh1:5432/sentry&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.store.jdbc.driver&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.postgresql.Driver&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.store.jdbc.user&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;sentry&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.store.jdbc.password&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;sentry&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sentry store的组映射&lt;code&gt;sentry.store.group.mapping&lt;/code&gt;有些两种配置方式：&lt;code&gt;org.apache.sentry.provider.common.HadoopGroupMappingService&lt;/code&gt;或者&lt;code&gt;org.apache.sentry.provider.file.LocalGroupMapping&lt;/code&gt;，当使用后者的时候，还需要配置&lt;code&gt;sentry.store.group.mapping.resource&lt;/code&gt;参数，即设置Policy file的路径。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.store.group.mapping&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.provider.common.HadoopGroupMappingService&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.store.group.mapping.resource&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt; &amp;lt;/value&amp;gt;
        &amp;lt;description&amp;gt; Policy file for group mapping. Policy file path for local group mapping, when sentry.store.group.mapping is set to LocalGroupMapping Service class.&amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;配置客户端的参数：&lt;/h2&gt;

&lt;p&gt;配置Sentry和hive集成时的服务名称，默认值为&lt;code&gt;HS2&lt;/code&gt;，这里设置为server1：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.server&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;server1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;初始化数据库&lt;/h2&gt;

&lt;p&gt;如果配置 sentry store 使用 &lt;code&gt;posrgres&lt;/code&gt; 数据库，当然你也可以使用其他的数据库，则需要创建并初始化数据库。数据库的创建过程，请参考 &lt;a href=&quot;/2013/08/02/hadoop-install-script/&quot;&gt;Hadoop自动化安装shell脚本&lt;/a&gt;，下面列出关键脚本。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;yum install postgresql-server postgresql-jdbc -y

ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jar
ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/sentry/lib/postgresql-jdbc.jar

su -c &quot;cd ; /usr/bin/pg_ctl start -w -m fast -D /var/lib/pgsql/data&quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;create user sentry with password &#39;sentry&#39;; \&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;drop database sentry;\&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;CREATE DATABASE sentry owner=sentry;\&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;GRANT ALL privileges ON DATABASE sentry TO sentry;\&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/pg_ctl restart -w -m fast -D /var/lib/pgsql/data&quot; postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，修改 /var/lib/pgsql/data/pg_hba.conf 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# TYPE  DATABASE    USER        CIDR-ADDRESS          METHOD

# &quot;local&quot; is for Unix domain socket connections only
local   all         all                               md5
# IPv4 local connections:
#host    all         all         0.0.0.0/0             trust
host    all         all         127.0.0.1/32          md5

# IPv6 local connections:
#host    all         all         ::1/128               nd5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是第一次安装，则初始化 sentry 的元数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sentry --command schema-tool --conffile /etc/sentry/conf/sentry-site.xml --dbType postgres --initSchema
Sentry store connection URL:     jdbc:postgresql://cdh1/sentry
Sentry store Connection Driver :     org.postgresql.Driver
Sentry store connection User:    sentry
Starting sentry store schema initialization to 1.4.0-cdh5-2
Initialization script sentry-postgres-1.4.0-cdh5-2.sql
Connecting to jdbc:postgresql://cdh1/sentry
Connected to: PostgreSQL (version 8.4.18)
Driver: PostgreSQL Native Driver (version PostgreSQL 9.0 JDBC4 (build 801))
Transaction isolation: TRANSACTION_REPEATABLE_READ
Autocommit status: true
1 row affected (0.002 seconds)
No rows affected (0.004 seconds)
Closing: 0: jdbc:postgresql://cdh1/sentry
Initialization script completed
Sentry schemaTool completed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是更新，则执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sentry --command schema-tool --conffile /etc/sentry/conf/sentry-site.xml --dbType postgres --upgradeSchema
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-5&quot;&gt;4. 启动&lt;/h1&gt;

&lt;p&gt;在cdh1上启动sentry-store服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /etc/init.d/sentry-store start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看日志：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /var/log/sentry/sentry-store.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看sentry的web监控界面&lt;a href=&quot;http://cdh1:51000/&quot;&gt;http://cdh1:51000/&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;5. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/cloudera-manager/v4-8-0/Cloudera-Manager-Managing-Clusters/cmmc_sentry_config.html&quot;&gt;Setting Up Hive Authorization with Sentry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/04/30/install-and-config-sentry.html</link>
      <guid>http://blog.javachen.com/2015/04/30/install-and-config-sentry.html</guid>
      <pubDate>2015-04-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Apache Sentry架构介绍</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;介绍&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://sentry.incubator.apache.org/&quot;&gt;Apache Sentry&lt;/a&gt;是Cloudera公司发布的一个Hadoop开源组件，截止目前还是Apache的孵化项目，它提供了细粒度级、基于角色的授权以及多租户的管理模式。Sentry当前可以和Hive/Hcatalog、Apache Solr 和Cloudera Impala集成，未来会扩展到其他的Hadoop组件，例如HDFS和HBase。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;特性&lt;/h1&gt;

&lt;p&gt;Apache Sentry为Hadoop使用者提供了以下便利：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;能够在Hadoop中存储更敏感的数据&lt;/li&gt;
  &lt;li&gt;使更多的终端用户拥有Hadoop数据访问权&lt;/li&gt;
  &lt;li&gt;创建更多的Hadoop使用案例&lt;/li&gt;
  &lt;li&gt;构建多用户应用程序&lt;/li&gt;
  &lt;li&gt;符合规范（例如SOX，PCI，HIPAA，EAL3）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在Sentry诞生之前，对于授权有两种备选解决方案：&lt;code&gt;粗粒度级的HDFS授权&lt;/code&gt;和&lt;code&gt;咨询授权&lt;/code&gt;，但它们并不符合典型的规范和数据安全需求，原因如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;粗粒度级的HDFS授权&lt;/code&gt;：安全访问和授权的基本机制被HDFS文件模型的粒度所限制。五级授权是粗粒度的，因为没有对文件内数据的访问控制：用户要么可以访问整个文件，要么什么都看不到。另外，HDFS权限模式不允许多个组对同一数据集有不同级别的访问权限。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;咨询授权&lt;/code&gt;：咨询授权在Hive中是一个很少使用的机制，旨在使用户能够自我监管，以防止意外删除或重写数据。这是一种“自服务”模式，因为用户可以为自己授予任何权限。因此，一旦恶意用户通过认证，它不能阻止其对敏感数据的访问。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过引进Sentry，Hadoop目前可在以下方面满足企业和政府用户的RBAC需求：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;安全授权&lt;/code&gt;：Sentry可以控制数据访问，并对已通过验证的用户提供数据访问特权。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;细粒度访问控制&lt;/code&gt;：Sentry支持细粒度的Hadoop数据和元数据访问控制。在Hive和Impala中Sentry的最初发行版本中，Sentry在服务器、数据库、表和视图范围提供了不同特权级别的访问控制，包括查找、插入等，允许管理员使用视图限制对行或列的访问。管理员也可以通过Sentry和带选择语句的视图或UDF，根据需要在文件内屏蔽数据。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;基于角色的管理&lt;/code&gt;：Sentry通过基于角色的授权简化了管理，你可以轻易将访问同一数据集的不同特权级别授予多个组。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;多租户管理&lt;/code&gt;：Sentry允许为委派给不同管理员的不同数据集设置权限。在Hive/Impala的情况下，Sentry可以在数据库/schema级别进行权限管理。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;统一平台&lt;/code&gt;：Sentry为确保数据安全，提供了一个统一平台，使用现有的Hadoop Kerberos实现安全认证。同时，通过Hive或Impala访问数据时可以使用同样的Sentry协议。未来，Sentry协议会被扩展到其它组件。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;如何工作&lt;/h1&gt;

&lt;p&gt;Apache Sentry的目标是实现授权管理，它是一个策略引擎，被数据处理工具用来验证访问权限。它也是一个高度扩展的模块，可以支持任何的数据模型。当前，它支持Apache Hive和Cloudera Impala的关系数据模型，以及Apache中的有继承关系的数据模型。&lt;/p&gt;

&lt;p&gt;Sentry提供了定义和持久化访问资源的策略的方法。目前，这些策略可以存储在文件里或者是能使用RPC服务访问的数据库后端存储里。数据访问工具，例如Hive，以一定的模式辨认用户访问数据的请求，例如从一个表读一行数据或者删除一个表。这个工具请求Sentry验证访问是否合理。Sentry构建请求用户被允许的权限的映射并判断给定的请求是否允许访问。请求工具这时候根据Sentry的判断结果来允许或者禁止用户的访问请求。&lt;/p&gt;

&lt;p&gt;Sentry授权包括以下几种角色：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;资源&lt;/code&gt;。可能是&lt;code&gt;Server&lt;/code&gt;、&lt;code&gt;Database&lt;/code&gt;、&lt;code&gt;Table&lt;/code&gt;、或者&lt;code&gt;URL&lt;/code&gt;（例如：HDFS或者本地路径）。Sentry1.5中支持对&lt;code&gt;列&lt;/code&gt;进行授权。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;权限&lt;/code&gt;。授权访问某一个资源的规则。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;角色&lt;/code&gt;。角色是一系列权限的集合。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;用户和组&lt;/code&gt;。一个组是一系列用户的集合。Sentry 的组映射是可以扩展的。默认情况下，Sentry使用Hadoop的组映射（可以是操作系统组或者LDAP中的组）。Sentry允许你将用户和组进行关联，你可以将一系列的用户放入到一个组中。Sentry不能直接给一个用户或组授权，需要将权限授权给角色，角色可以授权给一个组而不是一个用户。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;架构&lt;/h1&gt;

&lt;p&gt;下面是Sentry架构图，图片来自《&lt;a href=&quot;http://developer.51cto.com/art/201502/465091.htm&quot;&gt;Apache Sentry architecture overview&lt;/a&gt;》。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://blogs.apache.org/sentry/mediaresource/c07e8094-e79a-4c97-aa9e-cbb2b18fe9b2&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sentry的体系结构中有三个重要的组件：一是Binding；二是Policy Engine；三是Policy Provider。&lt;/p&gt;

&lt;h2 id=&quot;binding&quot;&gt;Binding&lt;/h2&gt;

&lt;p&gt;Binding实现了对不同的查询引擎授权，Sentry将自己的Hook函数插入到各SQL引擎的编译、执行的不同阶段。这些Hook函数起两大作用：一是起过滤器的作用，只放行具有相应数据对象访问权限的SQL查询；二是起授权接管的作用，使用了Sentry之后，grant/revoke管理的权限完全被Sentry接管，grant/revoke的执行也完全在Sentry中实现；对于所有引擎的授权信息也存储在由Sentry设定的统一的数据库中。这样所有引擎的权限就实现了集中管理。&lt;/p&gt;

&lt;h2 id=&quot;policy-engine&quot;&gt;Policy Engine&lt;/h2&gt;

&lt;p&gt;这是Sentry授权的核心组件。Policy Engine判定从binding层获取的输入的权限要求与服务提供层已保存的权限描述是否匹配。&lt;/p&gt;

&lt;h2 id=&quot;policy-provider&quot;&gt;Policy Provider&lt;/h2&gt;

&lt;p&gt;Policy Provider负责从文件或者数据库中读取出原先设定的访问权限。Policy Engine以及Policy Provider其实对于任何授权体系来说都是必须的，因此是公共模块，后续还可服务于别的查询引擎。&lt;/p&gt;

&lt;p&gt;基于文件的提供者使用的是ini格式的文件保存元数据信息，这个文件可以是一个本地文件或者HDFS文件。下面是一个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[groups]
# Assigns each Hadoop group to its set of roles
manager = analyst_role, junior_analyst_role
analyst = analyst_role

admin = admin_role
[roles]
analyst_role = server=server1-&amp;gt;db=analyst1, \
   server=server1-&amp;gt;db=jranalyst1-&amp;gt;table=*-&amp;gt;action=select, \
   server=server1-&amp;gt;uri=hdfs://ha-nn-uri/landing/analyst1, \
   server=server1-&amp;gt;db=default-&amp;gt;table=tab2
# Implies everything on server1.
admin_role = server=server1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基于文件的方式不易于使用程序修改，在修改过程中会存在资源竞争，不利于维护。Hive和Impala需要提供工业标准的SQL接口来管理授权策略，要求能够使用编程的方式进行管理。&lt;/p&gt;

&lt;p&gt;Sentry策略存储和服务将角色和权限以及组合角色的映射持久化到一个关系数据库并提供编程的API接口方便创建、查询、更新和删除。这允许Sentry的客户端并行和安全地获取和修改权限。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://blogs.apache.org/sentry/mediaresource/d9cc7fbc-dbf0-4dcb-a065-3f4d95878c00&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sentry策略存储可以使用很多后端的数据库，例如MySQL、Postgres等等，它使用ORM库DataNucleus来完成持久化操作。Sentry支持kerberos认证，也可以扩展代码支持其他认证方式。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;使用&lt;/h1&gt;

&lt;h2 id=&quot;hive&quot;&gt;和Hive集成&lt;/h2&gt;

&lt;p&gt;Sentry策略引擎通过hook的方式插入到hive中，hiveserver2在查询成功编译之后执行这个hook。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://blogs.apache.org/sentry/mediaresource/c7680848-aa39-46cc-8165-0fc27b8b12db&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个hook获得这个查询需要以读和写的方式访问的对象，然后Sentry的Hive binding基于SQL授权模型将他们转换为授权的请求。&lt;/p&gt;

&lt;p&gt;策略维护：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://blogs.apache.org/sentry/mediaresource/6b3b87ce-5054-40ab-90a2-0711dda06678&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;策略维护包括两个步骤。在查询编译期间，hive调用Sentry的授权任务工厂来生产会在查询过程中执行的Sentry的特定任务行。这个任务调用Sentry存储客户端发送RPC请求给Sentry服务要求改变授权策略。&lt;/p&gt;

&lt;h2 id=&quot;hcatalog&quot;&gt;和HCatalog集成&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://blogs.apache.org/sentry/mediaresource/eb8c8249-189f-404a-a348-8f5722b4d1ed&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sentry通过pre-listener hook集成到Hive Metastore。metastore在执行metadata维护请求之前执行这个hook。metastore binding为提交给metastore和HCatalog客户端的metadata修改请求创建一个Sentry授权请求。&lt;/p&gt;

&lt;p&gt;网上关于sentry配置和使用的例子：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.evernote.com/tech/2014/06/09/securing-impala-for-analysts/&quot;&gt;Securing Impala for analysts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/04/30/install-and-config-sentry.html&quot;&gt;安装和配置Sentry&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/04/30/test-hive-with-sentry.html&quot;&gt;测试Hive集成Sentry&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/14/config-secured-hive-with-sentry.html&quot;&gt;配置安全的Hive集群集成Sentry&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/14/config-secured-impala-with-sentry.html&quot;&gt;配置安全的Impala集群集成Sentry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-5&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sentry/tags/architecture&quot;&gt;Apache Sentry architecture overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.51cto.com/art/201502/465091.htm&quot;&gt;为什么Cloudera要创建Hadoop安全组件Sentry？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2013-08-14/2816575-with-sentry-cloudera-fills-hadoops-enterprise-security-gap&quot;&gt;Cloudera发布Hadoop开源组件Sentry：提供细粒度基于角色的安全控制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/04/29/apache-sentry-architecture.html</link>
      <guid>http://blog.javachen.com/2015/04/29/apache-sentry-architecture.html</guid>
      <pubDate>2015-04-29T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>编译CDH Spark源代码</title>
      <description>&lt;p&gt;本文以Cloudera维护的Spark分支项目为例，记录跟新Spark分支以及编译Spark源代码的过程。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;下载代码&lt;/h1&gt;

&lt;p&gt;在Github上fork Cloudera维护的&lt;a href=&quot;https://github.com/cloudera/spark&quot;&gt;Spark&lt;/a&gt;项目到自己的github账号里，对应的地址为&lt;a href=&quot;https://github.com/javachen/spark&quot;&gt;https://github.com/javachen/spark&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;下载代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone https://github.com/javachen/spark
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，切换到最新的分支，当前为 cdh5-1.3.0_5.4.0。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd spark
$ git checkout cdh5-1.3.0_5.4.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看当前分支：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;⇒  git branch
* cdh5-1.3.0_5.4.0
  master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果spark发布了新的版本，需要同步到我自己维护的spark项目中，可以按以下步骤进行操作:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 添加远程仓库地址
$ git remote add cdh git@github.com:cloudera/spark.git

# 抓取远程仓库更新：
$ git fetch cdh

# 假设cloudera发布了新的版本 cdh/cdh5-1.3.0_5.4.X
$ git checkout -b cdh5-1.3.0_5.4.X cdh/cdh5-1.3.0_5.4.X

# 切换到新下载的分支 
$ git checkout cdh5-1.3.0_5.4.X

# 将其提交到自己的远程仓库：
$ git push origin cdh5-1.3.0_5.4.X:cdh5-1.3.0_5.4.X
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;编译&lt;/h1&gt;

&lt;h2 id=&quot;zinc&quot;&gt;安装 zinc&lt;/h2&gt;

&lt;p&gt;在mac上安装&lt;a href=&quot;https://github.com/typesafehub/zinc&quot;&gt;zinc&lt;/a&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install zinc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;maven&quot;&gt;使用maven编译&lt;/h2&gt;

&lt;p&gt;指定hadoop版本为&lt;code&gt;2.6.0-cdh5.4.0&lt;/code&gt;，并集成yarn和hive：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot;
$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive -DskipTests clean package
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在CDH的spark中，要想集成&lt;code&gt;hive-thriftserver&lt;/code&gt;进行编译，需要修改 pom.xml 文件，添加一行 &lt;module&gt;sql/hive-thriftserver&lt;/module&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;modules&amp;gt;
    &amp;lt;module&amp;gt;core&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;bagel&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;graphx&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;mllib&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;tools&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;streaming&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;sql/catalyst&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;sql/core&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;sql/hive&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;sql/hive-thriftserver&amp;lt;/module&amp;gt; &amp;lt;!--添加的一行--&amp;gt;
    &amp;lt;module&amp;gt;repl&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;assembly&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/twitter&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/kafka&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/flume&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/flume-sink&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/zeromq&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/mqtt&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;examples&amp;lt;/module&amp;gt;
  &amp;lt;/modules&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，再执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot;
$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive -Phive-thriftserver -DskipTests clean package
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行测试用例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive  test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行java8测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn install -DskipTests -Pjava8-tests
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sbt&quot;&gt;使用sbt编译&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ build/sbt -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive assembly
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;生成压缩包&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./make-distribution.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;排错&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt; Unable to find configuration file at location scalastyle-config.xml&lt;/code&gt; 异常&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在idea中使用maven对examples模块运行&lt;code&gt;package&lt;/code&gt;或者&lt;code&gt;install&lt;/code&gt;命令会出现&lt;code&gt; Unable to find configuration file at location scalastyle-config.xml&lt;/code&gt;异常，解决办法是将根目录下的scalastyle-config.xml拷贝到examples目录下去，这是因为pom.xml中定义的是scalastyle-maven-plugin插件从maven运行的当前目录查找该文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;plugin&amp;gt;
    &amp;lt;groupId&amp;gt;org.scalastyle&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;scalastyle-maven-plugin&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.4.0&amp;lt;/version&amp;gt;
    &amp;lt;configuration&amp;gt;
      &amp;lt;verbose&amp;gt;false&amp;lt;/verbose&amp;gt;
      &amp;lt;failOnViolation&amp;gt;true&amp;lt;/failOnViolation&amp;gt;
      &amp;lt;includeTestSourceDirectory&amp;gt;false&amp;lt;/includeTestSourceDirectory&amp;gt;
      &amp;lt;failOnWarning&amp;gt;false&amp;lt;/failOnWarning&amp;gt;
      &amp;lt;sourceDirectory&amp;gt;${basedir}/src/main/scala&amp;lt;/sourceDirectory&amp;gt;
      &amp;lt;testSourceDirectory&amp;gt;${basedir}/src/test/scala&amp;lt;/testSourceDirectory&amp;gt;
      &amp;lt;configLocation&amp;gt;scalastyle-config.xml&amp;lt;/configLocation&amp;gt;
      &amp;lt;outputFile&amp;gt;scalastyle-output.xml&amp;lt;/outputFile&amp;gt;
      &amp;lt;outputEncoding&amp;gt;UTF-8&amp;lt;/outputEncoding&amp;gt;
    &amp;lt;/configuration&amp;gt;
    &amp;lt;executions&amp;gt;
      &amp;lt;execution&amp;gt;
        &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
        &amp;lt;goals&amp;gt;
          &amp;lt;goal&amp;gt;check&amp;lt;/goal&amp;gt;
        &amp;lt;/goals&amp;gt;
      &amp;lt;/execution&amp;gt;
    &amp;lt;/executions&amp;gt;
&amp;lt;/plugin&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;参考&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/building-spark.html&quot;&gt;Building Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/04/28/compile-cdh-spark-source-code.html</link>
      <guid>http://blog.javachen.com/2015/04/28/compile-cdh-spark-source-code.html</guid>
      <pubDate>2015-04-28T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Scala中下划线的用途</title>
      <description>&lt;p&gt;存在性类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def foo(l: List[Option[_]]) = 

def f(m: M[_]) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;高阶类型参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;case class A[K[_],T](a: K[T])

def f[M[_]] 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;临时变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val _ = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;临时参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;List(1, 2, 3) foreach { _ =&amp;gt; println(&quot;Hi&quot;) }    //List(1, 2, 3) foreach { t =&amp;gt; println(&quot;Hi&quot;) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通配模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;Some(5) match { case Some(_) =&amp;gt; println(&quot;Yes&quot;) }

match {
     case List(1,_,_) =&amp;gt; &quot; a list with three element and the first element is 1&quot;
     case List(_*)  =&amp;gt; &quot; a list with zero or more elements &quot;
     case Map[_,_] =&amp;gt; &quot; matches a map with any key type and any value type &quot;
     case _ =&amp;gt;
 }

val (a, _) = (1, 2)
for (_ &amp;lt;- 1 to 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通配导入：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// Imports all the classes in the package matching
import scala.util.matching._

// Imports all the members of the object Fun (static import in Java).
import com.test.Fun._
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;隐藏导入：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// Imports all the members of the object Fun but renames Foo to Bar
import com.test.Fun.{ Foo =&amp;gt; Bar , _ }

// Imports all the members except Foo. To exclude a member rename it to _
import com.test.Fun.{ Foo =&amp;gt; _ , _ }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;连接字母和标点符号：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def bang_!(x: Int) = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;占位符：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;( (_: Int) + (_: Int) )(2,3)

val nums = List(1,2,3,4,5,6,7,8,9,10)

nums map (_ + 2)
nums sortWith(_&amp;gt;_)
nums filter (_ % 2 == 0)
nums reduceLeft(_+_)
nums reduce (_ + _)
nums reduceLeft(_ max _)
nums.exists(_ &amp;gt; 5)
nums.takeWhile(_ &amp;lt; 8)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;偏函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def fun = {
    // Some code
}
val funLike = fun _

List(1, 2, 3) foreach println _

1 to 5 map (10 * _)

//List(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;).map(_.toUpperCase())
List(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;).map(n =&amp;gt; n.toUpperCase())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化默认值:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;var d:Double = _ 
var i:Int = _
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数序列：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//Range转换为List
List(1 to 5:_*)

//Range转换为Vector
Vector(1 to 5: _*)

//可变参数中
def capitalizeAll(args: String*) = {
  args.map { arg =&amp;gt;
    arg.capitalize
  }
}

val arr = Array(&quot;what&#39;s&quot;, &quot;up&quot;, &quot;doc?&quot;)
capitalizeAll(arr: _*)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;作为参数名：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//访问map
var m3 = Map((1,100), (2,200))
for(e&amp;lt;-m3) println(e._1 + &quot;: &quot; + e._2)
m3 filter (e=&amp;gt;e._1&amp;gt;1)
m3 filterKeys (_&amp;gt;1)
m3.map(e=&amp;gt;(e._1*10, e._2))
m3 map (e=&amp;gt;e._2)

//元组
(1,2)._2
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala&quot;&gt;What are all the uses of an underscore in Scala?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/04/23/all-the-uses-of-an-underscore-in-scala.html</link>
      <guid>http://blog.javachen.com/2015/04/23/all-the-uses-of-an-underscore-in-scala.html</guid>
      <pubDate>2015-04-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Scala集合</title>
      <description>&lt;p&gt;Scala有一个非常通用，丰富，强大，可组合的集合库；集合是高阶的(high level)并暴露了一大套操作方法。很多集合的处理和转换可以被表达的简洁又可读，但不审慎地用它们的功能也会导致相反的结果。每个Scala程序员应该阅读 集合设计文档；通过它可以很好地洞察集合库，并了解设计动机。&lt;/p&gt;

&lt;p&gt;scala集合API：&lt;a href=&quot;http://www.scala-lang.org/docu/files/collections-api/collections.html&quot;&gt;http://www.scala-lang.org/docu/files/collections-api/collections.html&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;怎样使用集合，请参考 &lt;a href=&quot;http://twitter.github.io/effectivescala/index-cn.html#集合&quot;&gt;Effective Scala&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;架构&lt;/h1&gt;

&lt;p&gt;Scala的所有的集合类都可以在包 &lt;code&gt;scala.collection&lt;/code&gt; 包中找到，其中的集合类都是高级抽象类或特性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/2015/scala.collection.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scala 集合类系统地区分了可变的和不可变的集合。可变集合可以在适当的地方被更新或扩展。这意味着你可以修改，添加，移除一个集合的元素。而不可变集合类，相比之下，永远不会改变。不过，你仍然可以模拟添加，移除或更新操作。但是这些操作将在每一种情况下都返回一个新的集合，同时使原来的集合不发生改变。&lt;/p&gt;

&lt;p&gt;可变的集合类位于 &lt;code&gt;scala.collection.mutable&lt;/code&gt; 包中，而不可变的集合位于 &lt;code&gt;scala.collection.immutable&lt;/code&gt; 。&lt;code&gt;scala.collection&lt;/code&gt; 包中的集合，既可以是可变的，也可以是不可变的。例如：&lt;a href=&quot;http://www.scala-lang.org/api/current/scala/collection/IndexedSeq.html&quot;&gt;collection.IndexedSeq[T]&lt;/a&gt; 就是 &lt;a href=&quot;http://www.scala-lang.org/api/current/scala/collection/immutable/IndexedSeq.html&quot;&gt;collection.immutable.IndexedSeq[T]&lt;/a&gt; 和 &lt;a href=&quot;http://www.scala-lang.org/api/current/scala/collection/mutable/IndexedSeq.html&quot;&gt;collection.mutable.IndexedSeq[T]&lt;/a&gt; 这两类的超类。&lt;code&gt;scala.collection&lt;/code&gt; 包中的根集合类中定义了相同的接口作为不可变集合类，同时，&lt;code&gt;scala.collection.mutable&lt;/code&gt; 包中的可变集合类代表性的添加了一些有辅助作用的修改操作到这个 immutable 接口。&lt;/p&gt;

&lt;p&gt;下面的图表显示 &lt;code&gt;scala.collection.immutable&lt;/code&gt; 中的所有集合类。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/2015/scala.collection.immutable.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面的图表显示 &lt;code&gt;scala.collection.mutable&lt;/code&gt; 中的所有集合类。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/2015/scala.collection.mutable.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;默认情况下，Scala 一直采用不可变集合类&lt;/code&gt;。例如，如果你仅写了 &lt;code&gt;Set&lt;/code&gt; 而没有任何加前缀也没有从其它地方导入 &lt;code&gt;Set&lt;/code&gt;，你会得到一个不可变的 set，另外如果你写迭代，你也会得到一个不可变的迭代集合类，这是由于这些类在从 scala 中导入的时候都是默认绑定的。为了得到可变的默认版本，你需要显式的声明&lt;code&gt;collection.mutable.Set&lt;/code&gt;或&lt;code&gt;collection.mutable.Iterable&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;一个有用的约定，如果你想要同时使用可变和不可变集合类，只导入 &lt;code&gt;collection.mutable&lt;/code&gt; 包即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import scala.collection.mutable  //导入包scala.collection.mutable 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然而，像没有前缀的 &lt;code&gt;Set&lt;/code&gt; 这样的关键字， 仍然指的是一个不可变集合，然而 &lt;code&gt;mutable.Set&lt;/code&gt; 指的是可变的副本（可变集合）。&lt;/p&gt;

&lt;p&gt;为了方便和向后兼容性，一些导入类型在包 scala 中有别名，所以你能通过简单的名字使用它们而不需要 import。这有一个例子是 &lt;code&gt;List&lt;/code&gt;类型，它可以用以下两种方法使用，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala.collection.immutable.List // 这是它的定义位置
scala.List //通过scala 包中的别名
List // 因为scala._ 总是是被自动导入。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其它类型的别名有： Traversable, Iterable, Seq, IndexedSeq, Iterator, Stream, Vector, StringBuilder, Range。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;不可变（collection.immutable._）&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;可变（collection.mutable._）&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Array&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ArrayBuffer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;List&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ListBuffer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;String&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;StringBuilder&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;/&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;LinkedList, DoubleLinkedList&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;List&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;MutableList&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;/&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Queue&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Array&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ArraySeq&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Stack&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ArrayStack&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HashMap HashSet&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HashMap HashSet&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;traversable&quot;&gt;Traversable&lt;/h1&gt;

&lt;p&gt;Traversable 是容器类的最高级别特性，它唯一的抽象操作是 foreach：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def foreach[U](f: Elem =&amp;gt; U) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Traversable 同时定义的很多具体方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;相加操作++&lt;/li&gt;
  &lt;li&gt;Map 操作有 map，flatMap 和 collect
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs map f&lt;/code&gt;  通过函数xs中的每一个元素调用函数f来生成一个容器。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs flatMap f&lt;/code&gt;  通过对容器xs中的每一个元素调用作为容器的值函数f，在把所得的结果连接起来作为一个新的容器。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs collect f&lt;/code&gt;  通过对每个xs中的符合定义的元素调用偏函数f，并把结果收集起来生成一个集合。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;转换操作包括 toArray，toList，toIterable，toSeq，toIndexedSeq，toStream，toSet，和 toMap
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.toArray&lt;/code&gt;  把容器转换为一个数组&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.toList&lt;/code&gt; 把容器转换为一个list&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.toIterable&lt;/code&gt; 把容器转换为一个迭代器。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.toSeq&lt;/code&gt;  把容器转换为一个序列&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.toIndexedSeq&lt;/code&gt; 把容器转换为一个索引序列&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.toStream&lt;/code&gt; 把容器转换为一个延迟计算的流。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.toSet&lt;/code&gt;  把容器转换为一个Set。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.toMap&lt;/code&gt;  把由键/值对组成的容器转换为一个映射表。如果该容器并不是以键/值对作为元素的，那么调用这个操作将会导致一个静态类型的错误。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;拷贝操作有 copyToBuffer 和 copyToArray
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs copyToBuffer buf&lt;/code&gt; 把容器的所有元素拷贝到buf缓冲区。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs copyToArray(arr, s, n)&lt;/code&gt; 拷贝最多n个元素到数组arr的坐标s处。参数s，n是可选项。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Size 操作包括有 isEmpty，nonEmpty，size 和 hasDefiniteSize
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.isEmpty&lt;/code&gt;  测试容器是否为空。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.nonEmpty&lt;/code&gt; 测试容器是否包含元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.size&lt;/code&gt; 计算容器内元素的个数。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.hasDefiniteSize&lt;/code&gt;  如果xs的大小是有限的，则为true。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;元素检索操作有 head，last，headOption，lastOption 和 find
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.head&lt;/code&gt; 返回容器内第一个元素（或其他元素，若当前的容器无序）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.headOption&lt;/code&gt; xs选项值中的第一个元素，若xs为空则为None。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.last&lt;/code&gt; 返回容器的最后一个元素（或某个元素，如果当前的容器无序的话）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.lastOption&lt;/code&gt; xs选项值中的最后一个元素，如果xs为空则为None。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs find p&lt;/code&gt; 查找xs中满足p条件的元素，若存在则返回第一个元素；若不存在，则为空。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;子容器检索操作有 tail，init，slice，take，drop，takeWhilte，dropWhile，filter，filteNot 和 withFilter
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.tail&lt;/code&gt; 返回由除了xs.head外的其余部分。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.init&lt;/code&gt; 返回除xs.last外的其余部分。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs slice (from, to)&lt;/code&gt; 返回由xs的一个片段索引中的元素组成的容器（从from到to，但不包括to）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs take n&lt;/code&gt; 由xs的第一个到第n个元素（或当xs无序时任意的n个元素）组成的容器。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs drop n&lt;/code&gt; 由除了xs take n以外的元素组成的容器。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs takeWhile p&lt;/code&gt;  容器xs中最长能够满足断言p的前缀。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs dropWhile p&lt;/code&gt;  容器xs中除了xs takeWhile p以外的全部元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs filter p&lt;/code&gt; 由xs中满足条件p的元素组成的容器。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs withFilter p&lt;/code&gt; 这个容器是一个不太严格的过滤器。子容器调用map，flatMap，foreach和withFilter只适用于xs中那些的满足条件p的元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs filterNot p&lt;/code&gt;  由xs中不满足条件p的元素组成的容器。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;拆分操作有 splitAt，span，partition 和 groupBy
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs splitAt n&lt;/code&gt;  把xs从指定位置的拆分成两个容器（&lt;code&gt;xs take n&lt;/code&gt;和&lt;code&gt;xs drop n&lt;/code&gt;）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs span p&lt;/code&gt; 根据一个断言p将xs拆分为两个容器（&lt;code&gt;xs takeWhile p&lt;/code&gt;, &lt;code&gt;xs.dropWhile p&lt;/code&gt;）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs partition p&lt;/code&gt;  把xs分割为两个容器，符合断言p的元素赋给一个容器，其余的赋给另一个(&lt;code&gt;xs filter p&lt;/code&gt;, &lt;code&gt;xs.filterNot p&lt;/code&gt;)。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs groupBy f&lt;/code&gt;  根据判别函数f把xs拆分一个到容器的map中。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;元素测试包括有 exists，forall 和 count
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs forall p&lt;/code&gt; 返回一个布尔值表示用于表示断言p是否适用xs中的所有元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs exists p&lt;/code&gt; 返回一个布尔值判断xs中是否有部分元素满足断言p。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs count p&lt;/code&gt;  返回xs中符合断言p条件的元素个数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;折叠操作有 foldLeft，foldRight，/:，:\，reduceLeft 和 reduceRight
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;(z /: xs)(op)&lt;/code&gt; 在xs中，对由z开始从左到右的连续元素应用二进制运算op。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;(xs :\ z)(op)&lt;/code&gt; 在xs中，对由z开始从右到左的连续元素应用二进制运算op&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.foldLeft(z)(op)&lt;/code&gt; 与 &lt;code&gt;(z /: xs)(op)&lt;/code&gt;相同。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.foldRight(z)(op)&lt;/code&gt; 与 &lt;code&gt;(xs :\ z)(op)&lt;/code&gt;相同。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs reduceLeft op&lt;/code&gt;  非空容器xs中的连续元素从左至右调用二进制运算op。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs reduceRight op&lt;/code&gt; 非空容器xs中的连续元素从右至左调用二进制运算op。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;特殊折叠包括 sum, product, min, max
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.sum&lt;/code&gt;  返回容器xs中数字元素的和。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.product&lt;/code&gt;  xs返回容器xs中数字元素的积。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.min&lt;/code&gt;  容器xs中有序元素值中的最小值。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.max&lt;/code&gt;  容器xs中有序元素值中的最大值。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;字符串操作有 mkString，addString 和 stringPrefix
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs addString (b, start, sep, end)&lt;/code&gt; 把一个字符串加到StringBuilder对象b中，该字符串显示为将xs中所有元素用分隔符sep连接起来并封装在start和end之间。其中start，end和sep都是可选的。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs mkString (start, sep, end)&lt;/code&gt; 把容器xs转换为一个字符串，该字符串显示为将xs中所有元素用分隔符sep连接起来并封装在start和end之间。其中start，end和sep都是可选的。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.stringPrefix&lt;/code&gt; 返回一个字符串，该字符串是以容器名开头的 xs.toString。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;视图操作包含两个view方法的重载体
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.view&lt;/code&gt; 通过容器xs生成一个视图。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs view (from, to)&lt;/code&gt;  生成一个表示在指定索引范围内的xs元素的视图。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;iterable&quot;&gt;Iterable&lt;/h1&gt;

&lt;p&gt;继承 Traversable 的特性是 Iterable，该类实现了 foreach 方法，定义了一个迭代器。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def foreach[U](f: Elem =&amp;gt; U): Unit = {
  val it = iterator
  while (it.hasNext) f(it.next())
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Iterable 有两个方法返回迭代器：grouped 和 sliding。grouped 方法返回元素的增量分块，sliding 方法生成一个滑动元素的窗口。两者的差异见下面代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val xs = List(1, 2, 3, 4, 5)
xs: List[Int] = List(1, 2, 3, 4, 5)
scala&amp;gt; val git = xs grouped 3
git: Iterator[List[Int]] = non-empty iterator
scala&amp;gt; git.next()
res3: List[Int] = List(1, 2, 3)
scala&amp;gt; git.next()
res4: List[Int] = List(4, 5)
scala&amp;gt; val sit = xs sliding 3
sit: Iterator[List[Int]] = non-empty iterator
scala&amp;gt; sit.next()
res5: List[Int] = List(1, 2, 3)
scala&amp;gt; sit.next()
res6: List[Int] = List(2, 3, 4)
scala&amp;gt; sit.next()
res7: List[Int] = List(3, 4, 5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Iterable 增加了一些其他方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;xs takeRight n&lt;/code&gt;  一个容器由xs的最后n个元素组成（若定义的元素是无序，则由任意的n个元素组成）。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;xs dropRight n&lt;/code&gt;  一个容器由除了xs 被取走的（执行过takeRight方法）n个元素外的其余元素组成。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;xs zip ys&lt;/code&gt; 把一对容器 xs和ys的包含的元素合成到一个iterabale。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;xs zipAll (ys, x, y)&lt;/code&gt;  一对容器 xs 和ys的相应的元素合并到一个iterable ，实现方式是通过附加的元素x或y，把短的序列被延展到相对更长的一个上。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;xs.zip WithIndex&lt;/code&gt;  把一对容器xs和它的序列，所包含的元素组成一个iterable 。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;xs sameElements ys&lt;/code&gt;  测试 xs 和 ys 是否以相同的顺序包含相同的元素。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;seq&quot;&gt;Seq&lt;/h1&gt;

&lt;p&gt;序列，指的是一类具有一定长度的可迭代访问的对象，其中每个元素均带有一个从0开始计数的固定索引位置。&lt;/p&gt;

&lt;p&gt;序列的操作有以下几种，如下表所示：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;索引和长度
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs(i)&lt;/code&gt; (或者为&lt;code&gt;xs apply i&lt;/code&gt;)。xs的第i个元素&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs isDefinedAt i&lt;/code&gt;  测试xs.indices中是否包含i。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.length&lt;/code&gt; 序列的长度（同size）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.lengthCompare ys&lt;/code&gt; 如果xs的长度小于ys的长度，则返回-1。如果xs的长度大于ys的长度，则返回+1，如果它们长度相等，则返回0。即使其中一个序列是无限的，也可以使用此方法。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.indices&lt;/code&gt;  xs的索引范围，从0到xs.length - 1。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;索引搜索
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs indexOf x&lt;/code&gt;  返回序列xs中等于x的第一个元素的索引（存在多种变体）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs lastIndexOf x&lt;/code&gt;  返回序列xs中等于x的最后一个元素的索引（存在多种变体）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs indexOfSlice ys&lt;/code&gt;  查找子序列ys，返回xs中匹配的第一个索引。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs indexOfSlice ys&lt;/code&gt;  查找子序列ys，返回xs中匹配的倒数一个索引。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs indexWhere p&lt;/code&gt; xs序列中满足p的第一个元素。（有多种形式）&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs segmentLength (p, i)&lt;/code&gt; xs中，从xs(i)开始并满足条件p的元素的最长连续片段的长度。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs prefixLength p&lt;/code&gt; xs序列中满足p条件的先头元素的最大个数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;加法
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;x +: xs&lt;/code&gt; 由序列xs的前方添加x所得的新序列。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs :+ x&lt;/code&gt; 由序列xs的后方追加x所得的新序列。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs padTo (len, x)&lt;/code&gt; 在xs后方追加x，直到长度达到len后得到的序列。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;更新
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs patch (i, ys, r)&lt;/code&gt; 将xs中第i个元素开始的r个元素，替换为ys所得的序列。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs updated (i, x)&lt;/code&gt; 将xs中第i个元素替换为x后所得的xs的副本。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs(i) = x&lt;/code&gt; （或写作 &lt;code&gt;xs.update(i, x)&lt;/code&gt;，仅适用于可变序列）将xs序列中第i个元素修改为x。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;排序
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.sorted&lt;/code&gt; 通过使用xs中元素类型的标准顺序，将xs元素进行排序后得到的新序列。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs sortWith lt &lt;/code&gt; 将lt作为比较操作，并以此将xs中的元素进行排序后得到的新序列。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs sortBy f&lt;/code&gt; 将序列xs的元素进行排序后得到的新序列。参与比较的两个元素各自经f函数映射后得到一个结果，通过比较它们的结果来进行排序。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;反转
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.reverse&lt;/code&gt;  与xs序列元素顺序相反的一个新序列。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.reverseIterator&lt;/code&gt;  产生序列xs中元素的反序迭代器。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs reverseMap f&lt;/code&gt; 以xs的相反顺序，通过f映射xs序列中的元素得到的新序列。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;比较
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs startsWith ys&lt;/code&gt;  测试序列xs是否以序列ys开头（存在多种形式）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs endsWith ys&lt;/code&gt;  测试序列xs是否以序列ys结束（存在多种形式）。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs contains x&lt;/code&gt; 测试xs序列中是否存在一个与x相等的元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs containsSlice ys&lt;/code&gt; 测试xs序列中是否存在一个与ys相同的连续子序列。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;(xs corresponds ys)(p)&lt;/code&gt;  测试序列xs与序列ys中对应的元素是否满足二元的判断式p。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;多集操作
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs intersect ys&lt;/code&gt; 序列xs和ys的交集，并保留序列xs中的顺序。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs diff ys&lt;/code&gt;  序列xs和ys的差集，并保留序列xs中的顺序。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs union ys&lt;/code&gt; 并集；同xs ++ ys。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.distinct &lt;/code&gt;不含重复元素的xs的子序列。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Seq 具有两个子特征 &lt;a href=&quot;http://www.scala-lang.org/api/current/scala/collection/IndexedSeq.html&quot;&gt;LinearSeq&lt;/a&gt; 和 &lt;a href=&quot;http://www.scala-lang.org/api/current/scala/collection/IndexedSeq.html&quot;&gt;IndexedSeq&lt;/a&gt;。它们不添加任何新的操作，但都提供不同的性能特点：线性序列具有高效的 head 和 tail 操作，而索引序列具有高效的apply, length, 和 (如果可变) update操作。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;缓冲器&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Buffers是可变序列一个重要的种类&lt;/code&gt;。它们不仅允许更新现有的元素，而且允许元素的插入、移除和在buffer尾部高效地添加新元素。buffer 支持的主要新方法有：用于在尾部添加元素的 &lt;code&gt;+=&lt;/code&gt; 和 &lt;code&gt;++=&lt;/code&gt;；用于在前方添加元素的 &lt;code&gt;+=:&lt;/code&gt; 和 &lt;code&gt;++=:&lt;/code&gt;；用于插入元素的 insert 和 insertAll；以及用于删除元素的 remove 和 &lt;code&gt;-=&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Buffer类的操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;加法
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;buf += x &lt;/code&gt; 将元素x追加到buffer，并将buf自身作为结果返回。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf += (x, y, z)&lt;/code&gt;  将给定的元素追加到buffer。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf ++= xs&lt;/code&gt;  将xs中的所有元素追加到buffer。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;x +=: buf&lt;/code&gt; 将元素x添加到buffer的前方。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs ++=: buf&lt;/code&gt; 将xs中的所有元素都添加到buffer的前方。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf insert (i, x)&lt;/code&gt; 将元素x插入到buffer中索引为i的位置。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf insertAll (i, xs)&lt;/code&gt; 将xs的所有元素都插入到buffer中索引为i的位置。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;移除
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;buf -= x&lt;/code&gt;  将元素x从buffer中移除。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf remove i&lt;/code&gt;  将buffer中索引为i的元素移除。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf remove (i, n)&lt;/code&gt; 将buffer中从索引i开始的n个元素移除。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf trimStart n&lt;/code&gt; 移除buffer中的前n个元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf trimEnd n&lt;/code&gt; 移除buffer中的后n个元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;buf.clear()&lt;/code&gt; 移除buffer中的所有元素。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;克隆
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;buf.clone&lt;/code&gt; 与buf具有相同元素的新buffer。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ListBuffer 和 ArrayBuffer 是常用的 buffer 实现 。顾名思义，ListBuffe r依赖列表，支持高效地将它的元素转换成列表。而ArrayBuffer依赖数组，能快速地转换成数组。&lt;/p&gt;

&lt;h1 id=&quot;set&quot;&gt;Set&lt;/h1&gt;

&lt;p&gt;Set 是不包含重复元素的可迭代对象。&lt;/p&gt;

&lt;p&gt;不可变 Set 类的操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;测试
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs contains x&lt;/code&gt; 测试x是否是xs的元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs(x)&lt;/code&gt; 与&lt;code&gt;xs contains x&lt;/code&gt;相同。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs subsetOf ys&lt;/code&gt;  测试xs是否是ys的子集。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;加法：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs + x&lt;/code&gt;  包含xs中所有元素以及x的集合。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs + (x, y, z)&lt;/code&gt;  包含xs中所有元素及附加元素的集合&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs ++ ys&lt;/code&gt;  包含xs中所有元素及ys中所有元素的集合&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;减法：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs - x&lt;/code&gt;  包含xs中除x以外的所有元素的集合。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs - x&lt;/code&gt;  包含xs中除去给定元素以外的所有元素的集合。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs -- ys&lt;/code&gt;  集合内容为：xs中所有元素，去掉ys中所有元素后剩下的部分。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.empty&lt;/code&gt;  与xs同类的空集合。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;二进制操作：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs &amp;amp; ys&lt;/code&gt; 集合xs和ys的交集。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs intersect ys&lt;/code&gt; 等同于 xs &amp;amp; ys。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs union ys&lt;/code&gt; 等同于xs&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs &amp;amp;~ ys&lt;/code&gt;  集合xs和ys的差集。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs diff ys&lt;/code&gt;  等同于 &lt;code&gt;xs &amp;amp;~ ys&lt;/code&gt;。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可变 Set 类的操作&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;加法：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs += x&lt;/code&gt; 把元素x添加到集合xs中。该操作有副作用，它会返回左操作符，这里是xs自身。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs += (x, y, z)&lt;/code&gt; 添加指定的元素到集合xs中，并返回xs本身。（同样有副作用）&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs ++= ys&lt;/code&gt; 添加集合ys中的所有元素到集合xs中，并返回xs本身。（表达式有副作用）&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs add x&lt;/code&gt;  把元素x添加到集合xs中，如集合xs之前没有包含x，该操作返回true，否则返回false。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;移除：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs -= x&lt;/code&gt; 从集合xs中删除元素x，并返回xs本身。（表达式有副作用）&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs -= (x, y, z)&lt;/code&gt; 从集合xs中删除指定的元素，并返回xs本身。（表达式有副作用）&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs --= ys&lt;/code&gt; 从集合xs中删除所有属于集合ys的元素，并返回xs本身。（表达式有副作用）&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs remove x&lt;/code&gt; 从集合xs中删除元素x。如之前xs中包含了x元素，返回true，否则返回false。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs retain p&lt;/code&gt; 只保留集合xs中满足条件p的元素。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;xs.clear()&lt;/code&gt;  删除集合xs中的所有元素。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;更新：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs(x) = b&lt;/code&gt; （ 同 &lt;code&gt;xs.update(x, b)&lt;/code&gt; ）参数b为布尔类型，如果值为true就把元素x加入集合xs，否则从集合xs中删除x。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;克隆：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;xs.clone&lt;/code&gt;  产生一个与xs具有相同元素的可变集合。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;与不变集合一样，可变集合也提供了&lt;code&gt;+&lt;/code&gt;和&lt;code&gt;++&lt;/code&gt;操作符来添加元素，&lt;code&gt;-&lt;/code&gt;和&lt;code&gt;--&lt;/code&gt;用来删除元素。但是这些操作在可变集合中通常很少使用，因为这些操作都要通过集合的拷贝来实现。可变集合提供了更有效率的更新方法，&lt;code&gt;+=&lt;/code&gt;和&lt;code&gt;-=&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;s += elem&lt;/code&gt;，添加元素elem到集合s中，并返回产生变化后的集合作为运算结果。同样的，&lt;code&gt;s -= elem&lt;/code&gt;执行从集合s中删除元素elem的操作，并返回产生变化后的集合作为运算结果。除了&lt;code&gt;+=&lt;/code&gt;和&lt;code&gt;-=&lt;/code&gt;之外还有从可遍历对象集合或迭代器集合中添加和删除所有元素的批量操作符&lt;code&gt;++=&lt;/code&gt;和&lt;code&gt;--=&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Set集合的两个特质是SortedSet和 BitSet。&lt;/p&gt;

&lt;h2 id=&quot;sortedset&quot;&gt;SortedSet&lt;/h2&gt;

&lt;p&gt;SortedSet 是指以特定的顺序（这一顺序可以在创建集合之初自由的选定）排列其元素（使用iterator或foreach）的集合。 SortedSet 的默认表示是有序二叉树，即左子树上的元素小于所有右子树上的元素。这样，一次简单的顺序遍历能按增序返回集合中的所有元素。Scala的类 &lt;code&gt;immutable.TreeSet&lt;/code&gt; 使用红黑树实现，它在维护元素顺序的同时，也会保证二叉树的平衡，即叶节点的深度差最多为1。&lt;/p&gt;

&lt;p&gt;创建一个空的 TreeSet ，可以先定义排序规则：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val myOrdering = Ordering.fromLessThan[String](_ &amp;gt; _)
myOrdering: scala.math.Ordering[String] = scala.math.Ordering$$anon$9@6bd5a0fa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，用这一排序规则创建一个空的树集：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; TreeSet.empty(myOrdering)
res1: scala.collection.immutable.TreeSet[String] = TreeSet()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者，你也可以不指定排序规则参数，只需要给定一个元素类型或空集合。在这种情况下，将使用此元素类型默认的排序规则。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; TreeSet.empty[String]
res2: scala.collection.immutable.TreeSet[String] = TreeSet()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果通过已有的TreeSet来创建新的集合（例如，通过串联或过滤操作），这些集合将和原集合保持相同的排序规则。例如，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; res2 + (&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;)
res3: scala.collection.immutable.TreeSet[String] = TreeSet(four, one, three, two)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有序集合同样支持元素的范围操作。例如，range方法返回从指定起始位置到结束位置（不含结束元素）的所有元素，from方法返回大于等于某个元素的所有元素。调用这两种方法的返回值依然是有序集合。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; res3 range (&quot;one&quot;, &quot;two&quot;)
res4: scala.collection.immutable.TreeSet[String] = TreeSet(one, three)
scala&amp;gt; res3 from &quot;three&quot;
res5: scala.collection.immutable.TreeSet[String] = TreeSet(three, two)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;bitset&quot;&gt;Bitset&lt;/h2&gt;

&lt;p&gt;位集合是由单字或多字的紧凑位实现的非负整数的集合。其内部使用Long型数组来表示。第一个Long元素表示的范围为0到63，第二个范围为64到127，以此类推（值为0到127的非可变位集合通过直接将值存储到第一个或第两个Long字段的方式，优化掉了数组处理的消耗）。对于每个Long，如果有相应的值包含于集合中则它对应的位设置为1，否则该位为0。这里遵循的规律是，位集合的大小取决于存储在该集合的最大整数的值的大小。假如N是为集合所要表示的最大整数，则集合的大小就是N/64个长整形字，或者N/8个字节，再加上少量额外的状态信息字节。&lt;/p&gt;

&lt;p&gt;因此当位集合包含的元素值都比较小时，它比其他的集合类型更紧凑。位集合的另一个优点是它的contains方法（成员测试）、+=运算（添加元素）、-=运算（删除元素）都非常的高效。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val bs = collection.mutable.BitSet()
bs += (1,3,5) // BitSet(1, 5, 3)
bs ++= List(7,9) // BitSet(1, 9, 7, 5, 3)
bs.clear // BitSet()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;map&quot;&gt;Map&lt;/h1&gt;

&lt;p&gt;Map是一种可迭代的键值对结构（也称映射或关联）。Scala的Predef类提供了隐式转换，允许使用另一种语法：&lt;code&gt;key -&amp;gt; value&lt;/code&gt;，来代替&lt;code&gt;(key, value)&lt;/code&gt;。如：&lt;code&gt;Map(&quot;x&quot; -&amp;gt; 24, &quot;y&quot; -&amp;gt; 25, &quot;z&quot; -&amp;gt; 26)&lt;/code&gt; 等同于 &lt;code&gt;Map((&quot;x&quot;, 24), (&quot;y&quot;, 25), (&quot;z&quot;, 26))&lt;/code&gt;，却更易于阅读。&lt;/p&gt;

&lt;p&gt;不可变Map类的操作:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;查询：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms get k&lt;/code&gt;  返回一个Option，其中包含和键k关联的值。若k不存在，则返回None。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms(k)&lt;/code&gt; （完整写法是&lt;code&gt;ms apply k&lt;/code&gt;）返回和键k关联的值。若k不存在，则抛出异常。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms getOrElse (k, d) &lt;/code&gt;返回和键k关联的值。若k不存在，则返回默认值d。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms contains k&lt;/code&gt; 检查ms是否包含与键k相关联的映射。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms isDefinedAt k&lt;/code&gt;  同contains。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;添加及更新:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms + (k -&amp;gt; v)&lt;/code&gt; 返回一个同时包含ms中所有键值对及从k到v的键值对k -&amp;gt; v的新映射。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms + (k -&amp;gt; v, l -&amp;gt; w)&lt;/code&gt; 返回一个同时包含ms中所有键值对及所有给定的键值对的新映射。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms ++ kvs&lt;/code&gt; 返回一个同时包含ms中所有键值对及kvs中的所有键值对的新映射。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms updated (k, v)&lt;/code&gt; 同&lt;code&gt;ms + (k -&amp;gt; v)&lt;/code&gt;。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;移除：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms - k&lt;/code&gt;  返回一个包含ms中除键k以外的所有映射关系的映射。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms - (k, 1, m)&lt;/code&gt;  返回一个滤除了ms中与所有给定的键相关联的映射关系的新映射。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms -- ks&lt;/code&gt;  返回一个滤除了ms中与ks中给出的键相关联的映射关系的新映射。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;子容器：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms.keys&lt;/code&gt; 返回一个用于包含ms中所有键的iterable对象&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms.keySet&lt;/code&gt; 返回一个包含ms中所有的键的集合。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms.keyIterator&lt;/code&gt;  返回一个用于遍历ms中所有键的迭代器。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms.values&lt;/code&gt; 返回一个包含ms中所有值的iterable对象。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms.valuesIterator&lt;/code&gt; 返回一个用于遍历ms中所有值的迭代器。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;变换：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms filterKeys p&lt;/code&gt; 一个映射视图，其包含一些ms中的映射，且这些映射的键满足条件p。用条件谓词p过滤ms中所有的键，返回一个仅包含与过滤出的键值对的映射视图。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms mapValues f&lt;/code&gt;  用f将ms中每一个键值对的值转换成一个新的值，进而返回一个包含所有新键值对的映射视图。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可变Map类中的操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;添加及更新：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms(k) = v&lt;/code&gt; （完整形式为&lt;code&gt;ms.update(x, v)&lt;/code&gt;）。向映射ms中新增一个以k为键、以v为值的映射关系，ms先前包含的以k为值的映射关系将被覆盖。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms += (k -&amp;gt; v)&lt;/code&gt;  向映射ms增加一个以k为键、以v为值的映射关系，并返回ms自身。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms += (k -&amp;gt; v, l -&amp;gt; w) &lt;/code&gt; 向映射ms中增加给定的多个映射关系，并返回ms自身。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms ++= kvs&lt;/code&gt;  向映射ms增加kvs中的所有映射关系，并返回ms自身。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms put (k, v)&lt;/code&gt; 向映射ms增加一个以k为键、以v为值的映射，并返回一个Option，其中可能包含此前与k相关联的值。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms getOrElseUpdate (k, d)&lt;/code&gt; 如果ms中存在键k，则返回键k的值。否则向ms中新增映射关系&lt;code&gt;k -&amp;gt; v&lt;/code&gt;并返回d。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;移除：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms -= k&lt;/code&gt; 从映射ms中删除以k为键的映射关系，并返回ms自身。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms -= (k, l, m)&lt;/code&gt; 从映射ms中删除与给定的各个键相关联的映射关系，并返回ms自身。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms --= ks&lt;/code&gt; 从映射ms中删除与ks给定的各个键相关联的映射关系，并返回ms自身。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms remove k&lt;/code&gt; 从ms中移除以k为键的映射关系，并返回一个Option，其可能包含之前与k相关联的值。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms retain p&lt;/code&gt; 仅保留ms中键满足条件谓词p的映射关系。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ms.clear()&lt;/code&gt;  删除ms中的所有映射关系&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;变换：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms transform f&lt;/code&gt;  以函数f转换ms中所有键值对，transform中参数f的类型是&lt;code&gt;(A, B) =&amp;gt; B&lt;/code&gt;，即对ms中的所有键值对调用f，得到一个新的值，并用该值替换原键值对中的值。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;克隆：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;ms.clone&lt;/code&gt;  返回一个新的可变映射，其中包含与ms相同的映射关系。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Map的添加和删除操作与Set的相关操作相同。同Set操作一样，可变映射也支持非破坏性修改操作&lt;code&gt;+&lt;/code&gt;、&lt;code&gt;-&lt;/code&gt;、和 &lt;code&gt;updated&lt;/code&gt;。但是这些操作涉及到可变映射的复制，因此较少被使用。而利用两种变形&lt;code&gt;m(key) = value&lt;/code&gt;和&lt;code&gt;m += (key -&amp;gt; value)&lt;/code&gt;， 我们可以“原地”修改可变映射m。此外，存还有一种变形&lt;code&gt;m put (key, value)&lt;/code&gt;，该调用返回一个Option值，其中包含此前与键相关联的值，如果不存在这样的值，则返回None。&lt;/p&gt;

&lt;p&gt;同步的Map，使用SychronizedMap，同步Set，使用SynchronizedSet。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;不可变Map&lt;/code&gt;的定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//创建map并指定类型
scala&amp;gt; var m1 = Map[Int, Int]()
m: scala.collection.immutable.Map[Int,Int] = Map()  //缺醒是不可变map

//创建map并初始化
scala&amp;gt; var m2 = Map(1-&amp;gt;100, 2-&amp;gt;200)
m: scala.collection.immutable.Map[Int,Int] = Map(1 -&amp;gt; 100, 2 -&amp;gt; 200)

scala&amp;gt; var m3 = Map((1,100), (2,200))
m: scala.collection.immutable.Map[Int,Int] = Map(1 -&amp;gt; 100, 2 -&amp;gt; 200)

//创建map并指定类型、初始化
scala&amp;gt; val m4:Map[Int,String] = Map(1-&amp;gt;&quot;a&quot;,2-&amp;gt;&quot;b&quot;)
m4: Map[Int,String] = Map(1 -&amp;gt; a, 2 -&amp;gt; b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读取元素：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; m3(1)
res0: Int = 100

scala&amp;gt; m3.get(1)
res1: Option[Int] = Some(100)

scala&amp;gt; m3.getOrElse(4, -1)
res2: Int = -1

//读取所有元素
scala&amp;gt; for(e&amp;lt;-m3) println(e._1 + &quot;: &quot; + e._2)
1: 100
2: 200
3: 300

scala&amp;gt; m3.foreach(e=&amp;gt;println(e._1 + &quot;: &quot; + e._2))
1: 100
2: 200
3: 300

scala&amp;gt; for ((k,v)&amp;lt;-m3) println(k + &quot;: &quot; + v)
1: 100
2: 200
3: 300
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以进行filter、map操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; m3 filter (e=&amp;gt;e._1&amp;gt;1)
res46: scala.collection.immutable.Map[Int,Int] = Map(2 -&amp;gt; 200, 3 -&amp;gt; 300)

scala&amp;gt; m3 filterKeys (_&amp;gt;1)
res47: scala.collection.immutable.Map[Int,Int] = Map(2 -&amp;gt; 200, 3 -&amp;gt; 300)

scala&amp;gt; m3.map(e=&amp;gt;(e._1*10, e._2))
res48: scala.collection.immutable.Map[Int,Int] = Map(10 -&amp;gt; 100, 20 -&amp;gt; 200, 30 -&amp;gt; 300)

scala&amp;gt; m3 map (e=&amp;gt;e._2)
res49: scala.collection.immutable.Iterable[Int] = List(100, 200, 300)

//相当于：
scala&amp;gt; m3.values.toList
res50: List[Int] = List(100, 200, 300)

//按照key来取对应的value值：
scala&amp;gt; 2 to 100 flatMap m3.get
res52: scala.collection.immutable.IndexedSeq[Int] = Vector(200, 300)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;增加、删除、更新：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//Map本身不可改变，即使定义为var，更新操作也是返回一个新的不可变Map
scala&amp;gt; var m4 = Map(1-&amp;gt;100)
m4: scala.collection.immutable.Map[Int,Int] = Map(1 -&amp;gt; 100)

scala&amp;gt; m4 += (2-&amp;gt;200)  // m4指向新的(1-&amp;gt;100,2-&amp;gt;200), (1-&amp;gt;100)应该被回收

//另一种更新方式
scala&amp;gt; m4.updated(1,1000)
res7: scala.collection.immutable.Map[Int,Int] = Map(1 -&amp;gt; 1000, 2 -&amp;gt; 200)

//增加多个元素：
scala&amp;gt; Map(1-&amp;gt;100,2-&amp;gt;200) + (3-&amp;gt;300, 4-&amp;gt;400)
res8: scala.collection.immutable.Map[Int,Int] = Map(1 -&amp;gt; 100, 2 -&amp;gt; 200, 3 -&amp;gt; 300, 4 -&amp;gt; 400)

//删除元素：
scala&amp;gt; Map(1-&amp;gt;100,2-&amp;gt;200,3-&amp;gt;300) - (2,3) 
res9: scala.collection.immutable.Map[Int,Int] = Map(1 -&amp;gt; 100)

scala&amp;gt; Map(1-&amp;gt;100,2-&amp;gt;200,3-&amp;gt;300) -- List(2,3) 
res10: scala.collection.immutable.Map[Int,Int] = Map(1 -&amp;gt; 100)

//合并Map：
scala&amp;gt; Map(1-&amp;gt;100,2-&amp;gt;200) ++ Map(3-&amp;gt;300) 
res11: scala.collection.immutable.Map[Int,Int] = Map(1 -&amp;gt; 100, 2 -&amp;gt; 200, 3 -&amp;gt; 300)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;对于可变Map&lt;/code&gt;的定义和操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val map = scala.collection.mutable.Map[String, Any]()
map: scala.collection.mutable.Map[String,Any] = Map()

// 增加元素
scala&amp;gt; map(&quot;k1&quot;)=100

// 增加元素
scala&amp;gt; map += &quot;k2&quot;-&amp;gt;&quot;v2&quot;
res13: map.type = Map(k2 -&amp;gt; v2, k1 -&amp;gt; 100)

//判断元素值
scala&amp;gt; map(&quot;k2&quot;)==&quot;v2&quot;
res14: Boolean = true

scala&amp;gt; map.get(&quot;k2&quot;)==Some(&quot;v2&quot;)
res15: Boolean = true

scala&amp;gt; map.get(&quot;k3&quot;)==None
res16: Boolean = true

scala&amp;gt; val mm = collection.mutable.Map(1-&amp;gt;100,2-&amp;gt;200,3-&amp;gt;300)
mm: scala.collection.mutable.Map[Int,Int] = Map(2 -&amp;gt; 200, 1 -&amp;gt; 100, 3 -&amp;gt; 300)

//有则取之，无则加之
scala&amp;gt; mm getOrElseUpdate (3,-1)
res17: Int = 300

scala&amp;gt; mm getOrElseUpdate (4,-1)
res18: Int = -1

//删除元素
scala&amp;gt; mm -= 1
res19: mm.type = Map(2 -&amp;gt; 200, 4 -&amp;gt; -1, 3 -&amp;gt; 300)

//删除元素
scala&amp;gt; mm -= (2,3)
res20: mm.type = Map(4 -&amp;gt; -1)

//添加一个Map
scala&amp;gt; mm += (1-&amp;gt;100,2-&amp;gt;200,3-&amp;gt;300)
res21: mm.type = Map(2 -&amp;gt; 200, 4 -&amp;gt; -1, 1 -&amp;gt; 100, 3 -&amp;gt; 300)

//删除元素
scala&amp;gt; mm --= List(1,2)
res22: mm.type = Map(4 -&amp;gt; -1, 3 -&amp;gt; 300)

//删除元素
scala&amp;gt; mm remove 1
res23: Option[Int] = None

scala&amp;gt; mm += (1-&amp;gt;100,2-&amp;gt;200,3-&amp;gt;300)
res24: mm.type = Map(2 -&amp;gt; 200, 4 -&amp;gt; -1, 1 -&amp;gt; 100, 3 -&amp;gt; 300)

scala&amp;gt; mm.retain((x,y) =&amp;gt; x&amp;gt;1)
res25: mm.type = Map(2 -&amp;gt; 200, 4 -&amp;gt; -1, 3 -&amp;gt; 300)

//转换操作
scala&amp;gt; mm transform ((x,y)=&amp;gt; 0)
res26: mm.type = Map(2 -&amp;gt; 0, 4 -&amp;gt; 0, 3 -&amp;gt; 0)

scala&amp;gt; mm transform ((x,y)=&amp;gt; x*10)
res27: mm.type = Map(2 -&amp;gt; 20, 4 -&amp;gt; 40, 3 -&amp;gt; 30)

scala&amp;gt; mm transform ((x,y)=&amp;gt; y+3)
res28: mm.type = Map(2 -&amp;gt; 23, 4 -&amp;gt; 43, 3 -&amp;gt; 33)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;listmap&quot;&gt;ListMap&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;ListMap被用来表示一个保存键-值映射的链表&lt;/code&gt;。一般情况下，ListMap操作都&lt;strong&gt;需要遍历整个列表&lt;/strong&gt;，所以操作的运行时间也同列表长度成线性关系。实际上ListMap在Scala中很少使用，因为标准的不可变映射通常速度会更快。唯一的例外是，在构造映射时由于某种原因，链表中靠前的元素被访问的频率大大高于其他的元素。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val map = scala.collection.immutable.ListMap(1-&amp;gt;&quot;one&quot;, 2-&amp;gt;&quot;two&quot;)
map: scala.collection.immutable.ListMap[Int,java.lang.String] = 
   Map(1 -&amp;gt; one, 2 -&amp;gt; two)
scala&amp;gt; map(2)
res30: String = &quot;two&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;seq-1&quot;&gt;不可变Seq实体类&lt;/h1&gt;

&lt;h2 id=&quot;list&quot;&gt;List&lt;/h2&gt;

&lt;p&gt;列表List是一种有限的不可变序列式。&lt;/p&gt;

&lt;p&gt;列表定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val list:List[Int] = List(1,3,4,5,6) // 或者 List(1 to 6:_*)
val list1 = List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;) // 或者 List(&#39;a&#39; to &#39;d&#39;:_*) map (_.toString)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;合并：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val list2 = &quot;a&quot;::&quot;b&quot;::&quot;c&quot;::Nil // Nil是必须的
val list3 = &quot;begin&quot; :: list2 // list2不变，只能加在头，不能加在尾

//多个List合并用++，也可以用:::(不如++)
val list4 = list2 ++ &quot;end&quot; ++ Nil
val list4 = list2 ::: &quot;end&quot; :: Nil // 相当于 list2 ::: List(&quot;end&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;建议定义方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val head::body = List(4,&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;)
// head: Any = 4
// body: List[Any] = List(a, b, c, d)
val a::b::c = List(1,2,3)
// a: Int = 1
// b: Int = 2
// c: List[Int] = List(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListBuffer是可变的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val lb = collection.mutable.ListBuffer[Int]()
lb += (1,3,5,7)
lb ++= List(9,11) // ListBuffer(1, 3, 5, 7, 9, 11)
lb.toList // List(1, 3, 5, 7, 9, 11)
lb.clear // ListBuffer()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;stream&quot;&gt;Stream&lt;/h2&gt;

&lt;p&gt;流Stream与List很相似，只不过其中的每一个元素都经过了一些简单的计算处理。也正是因为如此，stream结构可以无限长。只有那些被要求的元素才会经过计算处理，除此以外stream结构的性能特性与List基本相同。&lt;/p&gt;

&lt;p&gt;鉴于List通常使用&lt;code&gt;::&lt;/code&gt;运算符来进行构造，stream使用外观上很相像的&lt;code&gt;#::&lt;/code&gt;。这里用一个包含整数1，2和3的stream来做一个简单的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val str = 1 #:: 2 #:: 3 #:: Stream.empty   //同List的构造，最后一个必须为空
str: scala.collection.immutable.Stream[Int] = Stream(1, ?)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该stream的头结点是1，尾是2和3，尾部并没有被打印出来，因为还没有被计算。stream被特别定义为懒惰计算，并且&lt;strong&gt;stream的toString方法很谨慎的设计为不去做任何额外的计算&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;下面给出一个稍复杂些的例子。这里讲一个以两个给定的数字为起始的斐波那契数列转换成stream。斐波那契数列的定义是，序列中的每个元素等于序列中在它之前的两个元素之和。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def fibFrom(a: Int, b: Int): Stream[Int] = a #:: fibFrom(b, a + b)
fibFrom: (a: Int,b: Int)Stream[Int]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数看起来比较简单。序列中的第一个元素显然是a，其余部分是以b和位于其后的a+b为开始斐波那契数列。这段程序最大的亮点是在对序列进行计算的时候避免了无限递归。如果函数中使用&lt;code&gt;::&lt;/code&gt;来替换&lt;code&gt;#::&lt;/code&gt;，那么之后的每次调用都会产生另一次新的调用，从而导致无限递归。在此例中，由于使用了&lt;code&gt;#::&lt;/code&gt;，等式右值中的调用在需要求值之前都不会被展开。这里尝试着打印出以1，1开头的斐波那契数列的前几个元素：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val fibs = fibFrom(1, 1).take(7)
fibs: scala.collection.immutable.Stream[Int] = Stream(1, ?)
scala&amp;gt; fibs.toList
res9: List[Int] = List(1, 1, 2, 3, 5, 8, 13)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stream相当于lazy List，避免在中间过程中生成不必要的集合。&lt;/p&gt;

&lt;p&gt;例子1：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;Range(1,50000000).filter (_ % 13==0)(1) // 26, 但很慢，需要大量内存
Stream.range(1,50000000).filter(_%13==0)(1) // 26，很快，只计算最终结果需要的内容
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;第一个版本在filter后生成一个中间集合，大小为50000000/13；而后者不生成此中间集合，只计算到26即可。&lt;/p&gt;

&lt;p&gt;例子2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;(1 to 100).map(i=&amp;gt; i*3+7).filter(i=&amp;gt; (i%10)==0).sum // map和filter生成两个中间collection
(1 to 100).toStream.map(i=&amp;gt; i*3+7).filter(i=&amp;gt; (i%10)==0).sum
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;vector&quot;&gt;Vector&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;向量Vector是用来解决列表不能高效的随机访问的一种结构&lt;/code&gt;。Vector结构能够在“更高效”的固定时间内&lt;strong&gt;访问到列表中的任意元素&lt;/strong&gt;。虽然这个时间会比访问头结点或者访问某数组元素所需的时间长一些，但至少这个时间也是个常量。因此，使用Vector的算法不必仅是小心的处理数据结构的头结点。由于可以快速修改和访问任意位置的元素，所以对Vector结构做写操作很方便。&lt;/p&gt;

&lt;p&gt;Seq的缺省实现是List：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; Seq(1,2,3)
res84: Seq[Int] = List(1, 2, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IndexSeq的缺省实现是Vector:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; IndexedSeq(1,2,3)
res85: IndexedSeq[Int] = Vector(1, 2, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vector类型的构建和修改与其他的序列结构基本一样。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val vec = scala.collection.immutable.Vector.empty
vec: scala.collection.immutable.Vector[Nothing] = Vector()
scala&amp;gt; val vec2 = vec :+ 1 :+ 2
vec2: scala.collection.immutable.Vector[Int] = Vector(1, 2)
scala&amp;gt; val vec3 = 100 +: vec2
vec3: scala.collection.immutable.Vector[Int] = Vector(100, 1, 2)
scala&amp;gt; vec3(0)
res1: Int = 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vector结构通常被表示成具有高分支因子的树（树或者图的分支因子是指数据结构中每个节点的子节点数目）。每一个树节点包含最多32个vector元素或者至多32个子树节点。包含最多32个元素的vector可以表示为一个单一节点，而一个间接引用则可以用来表示一个包含至多32*32=1024个元素的vector。从树的根节点经过两跳到达叶节点足够存下有2的15次方个元素的vector结构，经过3跳可以存2的20次方个，4跳2的25次方个，5跳2的30次方个。所以对于一般大小的vector数据结构，一般经过至多5次数组访问就可以访问到指定的元素。这也就是我们之前所提及的随机数据访问时“运行时间的相对高效”。&lt;/p&gt;

&lt;p&gt;由于Vectors结构是不可变的，所以您不能通过修改vector中元素的方法来返回一个新的vector。尽管如此，您仍可以通过update方法从一个单独的元素中创建出区别于给定数据结构的新vector结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val vec = Vector(1, 2, 3)
vec: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3)
scala&amp;gt; vec updated (2, 4)
res0: scala.collection.immutable.Vector[Int] = Vector(1, 2, 4)
scala&amp;gt; vec
res1: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面例子的最后一行我们可以看出，update方法的调用并不会改变vec的原始值。与元素访问类似，vector的update方法的运行时间也是“相对高效的固定时间”。对vector中的某一元素进行update操作可以通过从树的根节点开始拷贝该节点以及每一个指向该节点的节点中的元素来实现。这就意味着一次update操作能够创建1到5个包含至多32个元素或者子树的树节点。当然，这样做会比就地更新一个可变数组败家很多，但比起拷贝整个vector结构还是绿色环保了不少。&lt;/p&gt;

&lt;p&gt;由于vector在快速随机选择和快速随机更新的性能方面做到很好的平衡，所以&lt;strong&gt;它目前正被用作不可变索引序列的默认实现方式&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; collection.immutable.IndexedSeq(1, 2, 3)
res2: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;stack&quot;&gt;Stack&lt;/h2&gt;

&lt;p&gt;如果您想要实现一个后入先出的序列，那您可以使用Stack。您可以使用push向栈中压入一个元素，用pop从栈中弹出一个元素，用top查看栈顶元素而不用删除它。所有的这些操作都仅仅耗费固定的运行时间。&lt;/p&gt;

&lt;p&gt;这里提供几个简单的stack操作的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val stack = scala.collection.immutable.Stack.empty
stack: scala.collection.immutable.Stack[Nothing] = Stack()
scala&amp;gt; val hasOne = stack.push(1)
hasOne: scala.collection.immutable.Stack[Int] = Stack(1)
scala&amp;gt; stack
stack: scala.collection.immutable.Stack[Nothing] = Stack()
scala&amp;gt; hasOne.top
res20: Int = 1
scala&amp;gt; hasOne.pop
res21: scala.collection.immutable.Stack[Int] = Stack()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不可变stack一般很少用在Scala编程中，因为List结构已经能够覆盖到它的功能：push操作同List中的&lt;code&gt;::&lt;/code&gt;基本相同，pop则对应着tail。&lt;/p&gt;

&lt;h2 id=&quot;queue&quot;&gt;Queue&lt;/h2&gt;

&lt;p&gt;Queue是一种与stack很相似的数据结构，除了与stack的后入先出不同，Queue结构的是先入先出的。&lt;/p&gt;

&lt;h2 id=&quot;range&quot;&gt;Range&lt;/h2&gt;

&lt;p&gt;Range表示的是一个有序的等差整数数列。&lt;/p&gt;

&lt;p&gt;创建 Range：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; Range(0, 5)
res58: scala.collection.immutable.Range = Range(0, 1, 2, 3, 4)

//等同于：
scala&amp;gt; 0 until 5
res59: scala.collection.immutable.Range = Range(0, 1, 2, 3, 4)

//等同于：
scala&amp;gt; 0 to 4
res60: scala.collection.immutable.Range.Inclusive = Range(0, 1, 2, 3, 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;两个Range相加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; (&#39;0&#39; to &#39;9&#39;) ++ (&#39;A&#39; to &#39;Z&#39;)
res61: scala.collection.immutable.IndexedSeq[Char] = Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Range和List、Vector转换：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; 1 to 5 toList
warning: there were 1 feature warning(s); re-run with -feature for details
res62: List[Int] = List(1, 2, 3, 4, 5)

//相当与：
scala&amp;gt; List(1 to 5:_*)
res63: List[Int] = List(1, 2, 3, 4, 5)

//或者：
scala&amp;gt; Vector(1 to 5: _*)
res64: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3, 4, 5)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;array&quot;&gt;Array&lt;/h1&gt;

&lt;p&gt;数组定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val list1 = new Array[String](0) // Array()
val list2 = new Array[String](3) // Array(null, null, null)
val list3:Array[String] = new Array(3) // // Array(null, null, null)
val list1 = Array(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;) // 相当于Array.apply(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;)

//定义一个类型为Any的Array：
val aa = Array[Any](1, 2)
val aa: Array[Any] = Array(1, 2)
val aa: Array[_] = Array(1, 2)
 
Array (1,3,5,7,9,11)
Array[Int](1 to 11 by 2:_*)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与Array对应的可变ArrayBuffer：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val ab = collection.mutable.ArrayBuffer[Int]()
ab += (1,3,5,7)
ab ++= List(9,11) // ArrayBuffer(1, 3, 5, 7, 9, 11)
ab toArray // Array (1, 3, 5, 7, 9, 11)
ab clear // ArrayBuffer()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;tuple&quot;&gt;Tuple&lt;/h1&gt;

&lt;p&gt;定义方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val t1 = (&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)
var t2 = (&quot;a&quot;, 123, 3.14, new Date())
val (a,b,c) = (2,4,6)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最简单的Tuple：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;1-&amp;gt;&quot;hello world&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和下面的写法是等价的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;(1, &quot;hello world&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://twitter.github.io/scala_school/zh_cn/index.html&quot;&gt;Scala 课堂&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://qiujj.com/static/Scala-Handbook.htm&quot;&gt;Scala 2.8+ Handbook&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://code.csdn.net/DOC_Scala/chinese_scala_offical_document/file/Introduction.md#anchor_0&quot;&gt;CSDN CODE翻译的Scala容器库(Scala’s Collections Library)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/04/22/scala-collections.html</link>
      <guid>http://blog.javachen.com/2015/04/22/scala-collections.html</guid>
      <pubDate>2015-04-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Scala基本语法和概念</title>
      <description>&lt;p&gt;本文主要包括Scala的安装过程并理解Scala的基本语法和概念，包括表达式、变量、基本类型、函数、流程控制等相关内容。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装&lt;/h1&gt;

&lt;p&gt;从&lt;a href=&quot;http://www.scala-lang.org/download/all.html&quot;&gt;All Versions Scala&lt;/a&gt;下载所需版本Scala安装包，解压到指定目录之后，配置环境变量并使其生效。&lt;/p&gt;

&lt;p&gt;如果你使用Mac，则可以使用&lt;code&gt;brew&lt;/code&gt;安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;⇒  brew install scala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在终端键入&lt;code&gt;scala&lt;/code&gt;查看Scala的版本，并进入Scala的解释器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;⇒  scala
Welcome to Scala version 2.11.6 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60).
Type in expressions to have them evaluated.
Type :help for more information.

scala&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 表达式&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; 1 + 1
res0: Int = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;res0是解释器自动创建的变量名称，用来指代表达式的计算结果。它是Int类型，值为2。&lt;/p&gt;

&lt;p&gt;resX识别符还将用在后续的代码行中。例如,既然res0已在之前设为3，res0 * 3就是 9:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; res0 * 3
res1: Int = 9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打印 “Hello, world!’’  :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; println(&quot;Hello, world!&quot;)
Hello, world!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;println&lt;/code&gt;函数在标准输出上打印传给它的字串，就跟Java里的&lt;code&gt;System.out.println&lt;/code&gt;一样。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 变量&lt;/h1&gt;

&lt;p&gt;Scala 有两种变量：&lt;code&gt;val&lt;/code&gt; 和 &lt;code&gt;var&lt;/code&gt;。val 类似于 Java 里的 final 变量。一旦初始化了，val 就不能再赋值了。与之对应的，var 如同 Java 里面的非 final 变量。var 可以在它生命周期 中被多次赋值。下面是一个 val 的定义:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val msg = &quot;Hello, world!&quot;
msg: String = Hello, world!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个语句引入了msg当作字串”Hello, world!”的名字。类型是 java.lang.String，因为 Scala 的字串是由 Java 的 String 类实现的。&lt;/p&gt;

&lt;p&gt;这里定义的msg变量并没有指定类型，Scala解释器会自动推断出其类型，当然你也可以显示的标注类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val msg2: java.lang.String = &quot;Hello again, world!&quot;
msg2: String = Hello again, world!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为在Scala程序里&lt;code&gt;java.lang&lt;/code&gt;类型的简化名也是可见的，所以可以简化为:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val msg3: String = &quot;Hello yet again, world!&quot;
msg3: String = Hello yet again, world!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你想定义一个变量并修改它的值，你可以选择使用&lt;code&gt;var&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; var greeting = &quot;Hello, world!&quot;
greeting: String = Hello, world!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 greeting 是 var 而不是 val,你可以在之后对它重新赋值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; greeting = &quot;Leave me alone, world!&quot;
greeting: String = Leave me alone, world!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要输入一些能跨越多行的东西,只要一行行输进去就行。如果输到行尾还没结束,解释器 将在下一行回应一个竖线。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val multiLine =
     | &quot;This is the next line.&quot;
multiLine: java.lang.String = This is the next line.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你意识到你输入了一些错误的东西,而解释器仍在等着你更多的输入,你可以通过按 两次回车取消掉:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val oops =
     |
     |
You typed two blank lines. Starting a new command.
scala&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;var 变量可重新赋值，如果赋值为&lt;code&gt;_&lt;/code&gt;，则表示使用缺省值(0、false、null)，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;var d:Double = _ // d = 0.0
var i:Int = _ // i = 0
var s:String = _ // s = null
var t:T = _  // 泛型T对应的默认值
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scala还能像Python一样方便的赋值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// 给多个变量赋同一初始值
scala&amp;gt; val x,y=0
x: Int = 0
y: Int = 0

// 同时定义多个变量，注意：val x,y=10,&quot;hello&quot; 是错误的
scala&amp;gt; val (x,y) = (10, &quot;hello&quot;)
x: Int = 10
y: String = hello

// x = 1, y = List(2,3,4)
scala&amp;gt; val x::y = List(1,2,3,4)
x: Int = 1
y: List[Int] = List(2, 3, 4)

// a = 1, b = 2, c = 3
scala&amp;gt; val List(a,b,c) = List(1,2,3)
a: Int = 1
b: Int = 2
c: Int = 3

// 也可以用List，Seq
scala&amp;gt; val Array(a, b, _, _, c @ _*) = Array(1, 2, 3, 4, 5, 6, 7)
a: Int = 1
b: Int = 2
c: Seq[Int] = Vector(5, 6, 7) // Array(5, 6, 7), _*匹配0个到多个
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用正则表达式赋值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val regex = &quot;(\\d+)/(\\d+)/(\\d+)&quot;.r
regex: scala.util.matching.Regex = (\d+)/(\d+)/(\d+)

scala&amp;gt; val regex(year, month, day) = &quot;2015/04/20&quot;
year: String = 2015
month: String = 04
day: String = 20
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了&lt;code&gt;val&lt;/code&gt;，你还可以使用&lt;code&gt;lazy&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;val&lt;/code&gt;：定义时就一次求值完成，保持不变&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;lazy&lt;/code&gt;：定义时不求值，第一次使用时完成求值，保持不变&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scala是严格意义上的静态类型语言，由于其采用了先进的类型推断技术，程序员不需要在写程序时显式指定类型，编译器会根据上下文推断出类型信息。&lt;/li&gt;
  &lt;li&gt;Scala程序语句结尾没有分号，这也是 Scala中约定俗成的编程习惯。大多数情况下分号都是可省的，如果你需要将两条语句写在同一行，则需要用分号分开它们。&lt;/li&gt;
  &lt;li&gt;val用于定义不能修改的变量，var定义的变量可以修改和赋值，赋值为&lt;code&gt;_&lt;/code&gt;表示使用默认值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4. 基本类型和操作&lt;/h1&gt;

&lt;h2 id=&quot;section-4&quot;&gt;一些基本类型&lt;/h2&gt;

&lt;p&gt;Scala 中的一些基本类型和其实例值域范围如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;值&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;类型&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Byte&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8 位有符号补码整数(-27~27-1)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Short&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;16 位有符号补码整数(-215~215-1)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Int&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;32 位有符号补码整数(-231~231-1)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Long&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;64 位有符号补码整数(-263~263-1)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Char&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;16 位无符号Unicode字符(0~216-1)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;String&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;字符序列&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Float&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;32 位 IEEE754 单精度浮点数&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Double&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;64 位 IEEE754 单精度浮点数&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Boolean&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;true 或 false&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;除了String归于&lt;code&gt;java.lang&lt;/code&gt;包之外，其余所有的基本类型都是包scala的成员。如&lt;code&gt;Int&lt;/code&gt;的全名是&lt;code&gt;scala.Int&lt;/code&gt;。然而，由于包&lt;code&gt;scala&lt;/code&gt;和&lt;code&gt;java.lang&lt;/code&gt;的所有成员都被每个Scala源文件自动引用，你可以在任何地方只用简化名。&lt;/p&gt;

&lt;p&gt;Scala的基本类型与Java的对应类型范围完全一样，这让Scala编译器能直接把Scala的值类型在它产生的 字节码里转译成 Java 原始类型。&lt;/p&gt;

&lt;p&gt;Scala用&lt;code&gt;Any&lt;/code&gt;统一了原生类型和引用类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Any
    AnyRef
        java String
        其他Java类型
        ScalaObject
    AnyVal
        scala Double
        scala Float
        scala Long
        scala Int
        scala Short
        scala Unit
        scala Boolean
        scala Char
        scala Byte
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于基本类型，可以用&lt;code&gt;asInstanseOf[T]&lt;/code&gt;方法来强制转换类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def i = 10.asInstanceOf[Double]
i: Double

scala&amp;gt; List(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;).map(c=&amp;gt;(c+32).asInstanceOf[Char])
res1: List[Char] = List(a, b, c)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用&lt;code&gt;isInstanceOf[T]&lt;/code&gt;方法来判断类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val b = 10.isInstanceOf[Int]
b: Boolean = true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而在&lt;code&gt;match ... case&lt;/code&gt;中可以直接判断而不用此方法。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;操作符和方法&lt;/h2&gt;

&lt;p&gt;Scala为它的基本类型提供了丰富的操作符集。例如，&lt;code&gt;1 + 2&lt;/code&gt;与&lt;code&gt;(1).+(2)&lt;/code&gt;其实是一回事。换句话说，就是 Int 类包含了叫做&lt;code&gt;+&lt;/code&gt;的方法，它带一个 Int 参数并返回一个 Int 结果。这个&lt;code&gt;+&lt;/code&gt;方法在两 个 Int 相加时被调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val sum = 1 + 2 // Scala调用了(1).+(2) 
sum: Int = 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;想要证实这点，可以把表达式显式地写成方法调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val sumMore = (1).+(2)
sumMore: Int = 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而真正的事实是，Int包含了许多带不同的参数类型的重载的&lt;code&gt;+&lt;/code&gt;方法。&lt;/p&gt;

&lt;p&gt;符号&lt;code&gt;+&lt;/code&gt;是操作符——更明确地说，是中缀操作符。操作符标注不仅限于像&lt;code&gt;+&lt;/code&gt;这种其他语言里 看上去像操作符一样的东西。你可以把任何方法都当作操作符来标注。例如，类 String 有一个方法 &lt;code&gt;indexOf&lt;/code&gt; 带一个 Char 参数。&lt;code&gt;indexOf&lt;/code&gt; 方法搜索 String 里第一次出现的指定字符，并返回它的索引或 -1 如果没有找到。你可以把 &lt;code&gt;indexOf&lt;/code&gt; 当作中缀操作符使用，就像这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val s = &quot;Hello, world!&quot;
s: String = Hello, world!
scala&amp;gt; s indexOf &#39;o&#39; // Scala调用了s.indexOf(’o’) 
res30: Int = 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;String 提供一个重载的 indexOf 方法，带两个参数，分别是要搜索的字符和从哪个索引开始搜索。尽管 这个 indexOf 方法带两个参数,你仍然可以用操作符标注的方式使用它。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; s indexOf (&#39;o&#39;, 5) // Scala调用了s.indexOf(’o’, 5) 
res31: Int = 8
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;任何方法都可以是操作符。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Scala 还有另外两种操作符标注：前缀和后缀。前缀标注中，方法名被放在调用的对象之前，如，&lt;code&gt;-7&lt;/code&gt; 里的&lt;code&gt;-&lt;/code&gt;。后缀标注在方法放在对象之后，如&lt;code&gt;7 toLong&lt;/code&gt;里的&lt;code&gt;toLong&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;与中缀操作符——-操作符带后两个操作数，一个在左一个在右，相反，前缀和后缀操作符都是一元 unary 的：它们仅带一个操作数。前缀方式中，操作数在操作符的右边。前缀操作符的例子有 &lt;code&gt;-2.0&lt;/code&gt;、&lt;code&gt;!found&lt;/code&gt;和&lt;code&gt;~0xFF&lt;/code&gt;。与中缀操作符一致，这些前缀操作符是在值类型对象上调用方法的简写方式。然而这种情况下，方法名在操作符字符上前缀了&lt;code&gt;unary_&lt;/code&gt;。 例如，Scala 会把表达式 &lt;code&gt;-2.0&lt;/code&gt; 转换成方法调用&lt;code&gt;(2.0).unary_-&lt;/code&gt;。你可以输入通过操作符和显式方法名两种方式对方法的调用来演示这一点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; -2.0 // Scala调用了(2.0).unary_- res2: Double = -2.0
scala&amp;gt; (2.0).unary_-
res32: Double = -2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;可以当作前缀操作符用的标识符只有+,-,!和~&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;后缀操作符是不用点或括号调用的不带任何参数的方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val s = &quot;Hello, world!&quot;
s: String = Hello, world!
scala&amp;gt; s.toLowerCase
res33: String = hello, world!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;后面的这个例子里，方法没带参数，或者还可以去掉点，采用后缀操作符标注方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; s toLowerCase
res34: String = hello, world!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;数学运算&lt;/h2&gt;

&lt;p&gt;Int 无&lt;code&gt;++&lt;/code&gt;、&lt;code&gt;--&lt;/code&gt;操作，但可以&lt;code&gt;+=&lt;/code&gt;、&lt;code&gt;-=&lt;/code&gt;, 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;var i = 0
i++  // 报错，无此操作
i+=1 // 1
i--  // 报错，无此操作
i-=1 // 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-7&quot;&gt;对象相等性&lt;/h2&gt;

&lt;p&gt;如果你想比较一下看看两个对象是否相等，可以使用&lt;code&gt;==&lt;/code&gt;，或它的反义&lt;code&gt;!=&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//比较基本类型
scala&amp;gt; 1 == 2
res36: Boolean = false

scala&amp;gt; 1 != 2
res37: Boolean = true

scala&amp;gt; 2 == 2
res38: Boolean = true

//比较对象
scala&amp;gt; List(1, 2, 3) == List(1, 2, 3)
res39: Boolean = true

scala&amp;gt; List(1, 2, 3) == List(4, 5, 6)
res40: Boolean = false

//比较不同类型
scala&amp;gt; 1 == 1.0
res41: Boolean = true

scala&amp;gt; List(1, 2, 3) == &quot;hello&quot;
res42: Boolean = false

//和null进行比较，不会有任何异常抛出
scala&amp;gt; List(1, 2, 3) == null
res43: Boolean = false

scala&amp;gt; null == List(1, 2, 3)
res44: Boolean = false

scala&amp;gt; null == List(1, 2, 3)
res45: Boolean = false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scala的&lt;code&gt;==&lt;/code&gt;很智能，他知道对于数值类型要调用Java中的&lt;code&gt;==&lt;/code&gt;，引用类型要调用Java的&lt;code&gt;equals()&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; &quot;hello&quot;==&quot;Hello&quot;.toLowerCase()
res46: Boolean = truescala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在java中为false，在scala中为true。&lt;/p&gt;

&lt;p&gt;Scala的&lt;code&gt;==&lt;/code&gt;总是内容对比，&lt;code&gt;eq&lt;/code&gt;才是引用对比，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val s1,s2 = &quot;hello&quot;
val s3 = new String(&quot;hello&quot;)
s1==s2 // true
s1 eq s2 // true
s1==s3 // true 值相同
s1 eq s3 // false 不是同一个引用
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-8&quot;&gt;富包装器&lt;/h2&gt;

&lt;p&gt;Scala的每一个基本类型都有一个富包装类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Byte：scala.runtime.RichByte&lt;/li&gt;
  &lt;li&gt;Short：scala.runtime.RichShort&lt;/li&gt;
  &lt;li&gt;Int：scala.runtime.RichInt&lt;/li&gt;
  &lt;li&gt;Long：scala.runtime.RichLong&lt;/li&gt;
  &lt;li&gt;Char：scala.runtime.RichChar&lt;/li&gt;
  &lt;li&gt;String：scala.runtime.RichString&lt;/li&gt;
  &lt;li&gt;Float：scala.runtime.RichFloat&lt;/li&gt;
  &lt;li&gt;Double：scala.runtime.RichDouble&lt;/li&gt;
  &lt;li&gt;Boolean：scala.runtime.RichBoolean&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一些富操作的例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;0 max 5
0 min 5
-2.7 abs
-2.7 round
1.5 isInfinity
(1.0 / 0) isInfinity
4 to 6
&quot;bob&quot; capitalize
&quot;robert&quot; drop 2
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-9&quot;&gt;5. 函数&lt;/h1&gt;

&lt;p&gt;函数的地位和一般的变量是同等的，可以作为函数的参数，可以作为返回值。传入函数的任何输入是只读的，比如一个字符串，不会被改变，只会返回一个新的字符串。&lt;/p&gt;

&lt;p&gt;Java里面的一个问题就是很多只用到一次的private方法，没有和使用它的方法紧密结合；Scala可以在函数里面定义函数，很好地解决了这个问题。&lt;/p&gt;

&lt;h2 id=&quot;section-10&quot;&gt;函数定义&lt;/h2&gt;

&lt;p&gt;函数和方法一般用&lt;code&gt;def&lt;/code&gt;定义。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def max(x: Int, y: Int): Int = {
     |   if (x &amp;gt; y) x
     |   else y }
max: (x: Int, y: Int)Int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数的基本结构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/2015/scala-function-defined.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有时候Scala编译器会需要你定义函数的结果类型。比方说，如果函数是递归的，你就必须显式地定义函数结果类型。然而在max的例子里，你可以不用写结果类型，编译器也能够推断它。同样，如果函数仅由一个句子组成，你可以可选地不写大括号。这样，你就可以把max函数写成这样:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def max2(x: Int, y: Int) = if (x &amp;gt; y) x else y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一旦你定义了函数，你就可以用它的名字调用它，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; max(3, 5)
res25: Int = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数不带参数，调用时括号可以省略：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def three() = 1 + 2
three: ()Int

scala&amp;gt; three()
res26: Int = 3

scala&amp;gt; three
res27: Int = 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有既不带参数也不返回有用结果的函数定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def greet() = println(&quot;Hello, world!&quot;)
greet: ()Unit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当你定义了&lt;code&gt;greet()&lt;/code&gt;函数，解释器会回应一个&lt;code&gt;greet: ()Unit&lt;/code&gt;。空白的括号说明函数不带参数。Unit 是 greet 的结果类型，Unit 的结果类型指的是函数没有返回有用的值。Scala 的 Unit 类型比较接近 Java 的 void 类型，而且实际上 Java 里 每一个返回 void 的方法都被映射为 Scala 里返回 Unit 的方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;函数体没有像Java那样放在&lt;code&gt;{}&lt;/code&gt;里，Scala 中的一条语句其实是一个表达式&lt;/li&gt;
  &lt;li&gt;如果函数体只包含一条表达式，则可以省略&lt;code&gt;{}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;函数体没有显示的return语句，最后一条表达式的值会自动返回给函数的调用者&lt;/li&gt;
  &lt;li&gt;没有参数的函数调用时，括号可以省略&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-11&quot;&gt;映射式定义&lt;/h2&gt;

&lt;p&gt;一种特殊的定义：映射式定义（直接相当于数学中的映射关系）；其实也可以看成是没有参数的函数，返回一个匿名函数；调用的时候是调用这个返回的匿名函数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def f:Int=&amp;gt;Double = { 
    case 1 =&amp;gt; 0.1
    case 2 =&amp;gt; 0.2
    case _ =&amp;gt; 0.0
}
f(1) // 0.1
f(3) // 0.0

def m:Option[User]=&amp;gt;User = {
    case Some(x) =&amp;gt; x
    case None =&amp;gt; null
}
m(o).getOrElse(&quot;none...&quot;)

def m:(Int,Int)=&amp;gt;Int = _+_
m(2,3) // 5

def m:Int=&amp;gt;Int = 30+  // 相当于30+_,如果唯一的&quot;_&quot;在最后,可以省略
m(5) // 35
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-12&quot;&gt;特殊函数名 + - * /&lt;/h2&gt;

&lt;p&gt;方法名可以是&lt;code&gt;+&lt;/code&gt;、&lt;code&gt;-&lt;/code&gt;、&lt;code&gt;*&lt;/code&gt;、&lt;code&gt;/&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def *(x:Int, y:Int) = { x*y }
*(10,20) // = 200
1+2  //相当于1.+(2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定义一元操作符（置前）可用&lt;code&gt;unary_&lt;/code&gt;：一元的，单一元素的，单一构成的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;-2  //相当于：(2).unary_- 
+2 //相当于：(2).unary_+ 
!true //相当于：(true).unary_! 
~0  //相当于 (0).unary_~  
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-13&quot;&gt;函数的调用&lt;/h2&gt;

&lt;p&gt;正常调用，不传参数时候可以省略括号：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def f(s: String = &quot;default&quot;) = { s }
f // &quot;hello world&quot;
f() // &quot;hello world&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对象的无参数方法的调用，可以省略&lt;code&gt;.&lt;/code&gt;和&lt;code&gt;()&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;&quot;hello world&quot; toUpperCase // &quot;HELLO WORLD&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对象的1个参数方法的调用，可以省略&lt;code&gt;.&lt;/code&gt;和&lt;code&gt;()&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;&quot;hello world&quot; indexOf w // 6
&quot;hello world&quot; substring 5 // &quot;world&quot;
Console print 10 // 但不能写 print 10，只能print(10)，省略Console.
1 + 2 // 相当于 (1).+(2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对象的多个参数方法的调用,也可省略&lt;code&gt;.&lt;/code&gt;但不能省略&lt;code&gt;()&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;&quot;hello world&quot; substring (0, 5) // &quot;hello&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不在class或者object中的函数不能如此调用：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def m(i:Int) = i*i
m 10 // 错误
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;但在class或者object中可以使用&lt;code&gt;this&lt;/code&gt;调用：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object method {
   def m(i:Int) = i*i
   def main(args: Array[String]) = {
        val ii = this m 15  // 等同于 m(15), this 不能省略
       println(ii)
   }
}   
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-14&quot;&gt;匿名函数&lt;/h2&gt;

&lt;p&gt;形式：&lt;code&gt;((命名参数列表)=&amp;gt;函数实现)(参数列表)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;特殊地：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;无参数： &lt;code&gt;(()=&amp;gt;函数实现)()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;有一个参数且在最后： &lt;code&gt;(函数实现)(参数)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;无返回值： &lt;code&gt;((命名参数列表)=&amp;gt;Unit)(参数列表)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用&lt;code&gt;=&amp;gt;&lt;/code&gt;创建匿名函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;scala&amp;gt; (x: Int)=&amp;gt; x+1 
res23: Int =&amp;gt; Int = &amp;lt;function1&amp;gt;

scala&amp;gt; x:Int =&amp;gt; x+1 // 没有大括号时，()是必须的
&amp;lt;console&amp;gt;:1: error: &#39;;&#39; expected but &#39;=&amp;gt;&#39; found.
       x:Int =&amp;gt; x+1

scala&amp;gt; {(x: Int)=&amp;gt; x+1 } 
res24: Int =&amp;gt; Int = &amp;lt;function1&amp;gt;

scala&amp;gt; {x:Int =&amp;gt; x+1} // 有大括号时，()可以去掉
res25: Int =&amp;gt; Int = &amp;lt;function1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数值是对象，所以如果你愿意可以把它们存入变量。它们也是函数，所以你可以使用通常的括&lt;br /&gt;
号函数调用写法调用它们：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val m1 = (x:Int)=&amp;gt; x+1
m1: Int =&amp;gt; Int = &amp;lt;function1&amp;gt;

scala&amp;gt; val m2 = {x:Int=&amp;gt; x+1} // 不用(), 用{}
m2: Int =&amp;gt; Int = &amp;lt;function1&amp;gt;

scala&amp;gt; m1(10)
res26: Int = 11
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有参数的匿名函数的调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; ((i:Int)=&amp;gt; i*i)(3)
res25: Int = 9

scala&amp;gt; ((i:Int, j:Int) =&amp;gt; i+j)(3, 4)
res26: Int = 7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有一个参数且在最后的匿名函数的调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; (10*)(2) // 20, 相当于 ((x:Int)=&amp;gt;10*x)(2)
res27: Int = 20

scala&amp;gt; (10+)(2) // 12, 相当于 ((x:Int)=&amp;gt;10+x)(2)
res28: Int = 12

scala&amp;gt; (List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) mkString)(&quot;=&quot;) // a=b=c
res29: String = a=b=c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;无参数的匿名函数的调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; (()=&amp;gt; 10)() // 10
res30: Int = 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;无参数无返回值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; (() =&amp;gt; Unit)
res31: () =&amp;gt; Unit.type = &amp;lt;function0&amp;gt;

scala&amp;gt; ( ()=&amp;gt; {println(&quot;hello&quot;); 20*10} )()
hello
res32: Int = 200

//相当于调用一段方法
scala&amp;gt; { println(&quot;hello&quot;); 20*10 }
hello
res33: Int = 200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;匿名函数的两个例子：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;例子1：直接使用匿名函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; List(1,2,3,4).map( (i:Int)=&amp;gt; i*i)
res34: List[Int] = List(1, 4, 9, 16)

//这里对变量 i 使用了类型推断
scala&amp;gt; List(1,2,3,4).map( i=&amp;gt; i*i)
res35: List[Int] = List(1, 4, 9, 16)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;例子2：无参数的匿名函数&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//定义一个普通的函数，参数为函数，返回值为空
scala&amp;gt; def times3(m:()=&amp;gt; Unit) = { m();m();m() }
times3: (m: () =&amp;gt; Unit)Unit

//传入一个无参数的匿名函数
scala&amp;gt; times3 ( ()=&amp;gt; println(&quot;hello world&quot;) )
hello world
hello world
hello world
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于是无参数的匿名函数，可进一步简化：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def times3(m: =&amp;gt;Unit) = { m;m;m }  // 参见“lazy参数”
times3: (m: =&amp;gt; Unit)Unit

scala&amp;gt; times3 ( println(&quot;hello world&quot;) )
hello world
hello world
hello world
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;partial-application&quot;&gt;偏应用函数（Partial application）&lt;/h2&gt;

&lt;p&gt;用下划线代替一个或多个参数的函数叫偏应用函数（partially applied function），例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def sum(a: Int, b: Int, c: Int) = a + b + c
sum: (Int,Int,Int)Int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你就可以把函数 sum 应用到参数 1、2 和 3 上，如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; sum(1, 2, 3)
res1: Int = 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;偏应用函数是一种表达式，你不需要提供函数需要的所有参数。代之以仅提供部分，或不提供所需参数。比如，要创建不提供任何三个所需参数的调用 sum 的偏应用表达式，只要在“sum”之后放一个下划线即可，然后可以把得到的函数存入变量。举例如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val a = sum _
a: (Int, Int, Int) =&amp;gt; Int = &amp;lt;function&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有了这个代码，Scala 编译器以偏应用函数表达式&lt;code&gt;sum _&lt;/code&gt;，实例化一个带三个缺失整数参数的函数值，并把这个新的函数值的索引赋给变量 a。当你把这个新函数值应用于三个参数之上时，它就转回头调用 &lt;code&gt;sum&lt;/code&gt;，并传入这三个参数:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; a(1, 2, 3)
res2: Int = 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际发生的事情是这样的：名为a的变量指向一个函数值对象。这个函数值是由Scala编译器依照偏应用函数表达式&lt;code&gt;sum _&lt;/code&gt;，自动产生的类的一个实例。编译器产生的类有一个&lt;code&gt;apply&lt;/code&gt;方法带三个参数。之所以带三个参数是因为&lt;code&gt;sum _&lt;/code&gt;表达式缺少的参数数量为三。Scala编译器把表达式&lt;code&gt;a(1,2,3)&lt;/code&gt;翻译成对函数值的&lt;code&gt;apply&lt;/code&gt;方法的调用，传入三个参数 1、2、3。因此 &lt;code&gt;a(1,2,3)&lt;/code&gt;是下列代码的短格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; a.apply(1, 2, 3)
res3: Int = 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可针对部分参数使用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val b = sum(1, _: Int, 3)
b: (Int) =&amp;gt; Int = &amp;lt;function&amp;gt;
scala&amp;gt; b(2)
res4: Int = 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;_&lt;/code&gt; 在Scala中的使用场景，见 &lt;a href=&quot;/2015/04/23/all-the-uses-of-an-underscore-in-scala.html&quot;&gt;Scala中下划线的用途&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;如果&lt;code&gt;_&lt;/code&gt;在最后，则可以省略：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;1 to 5 foreach println
1 to 5 map (10*)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-15&quot;&gt;柯里化函数&lt;/h2&gt;

&lt;p&gt;有时会有这样的需求：允许别人一会在你的函数上应用一些参数，然后又应用另外的一些参数。&lt;/p&gt;

&lt;p&gt;例如一个乘法函数，在一个场景需要选择乘数，而另一个场景需要选择被乘数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; def multiply(m: Int)(n: Int): Int = m * n
multiply: (m: Int)(n: Int)Int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以直接传入两个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; multiply(2)(3)
res0: Int = 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以填上第一个参数并且部分应用第二个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val timesTwo = multiply(2) _
timesTwo: (Int) =&amp;gt; Int = &amp;lt;function1&amp;gt;

scala&amp;gt; timesTwo(3)
res1: Int = 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以对任何多参数函数执行柯里化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; (multiply _).curried
res1: (Int) =&amp;gt; (Int) =&amp;gt; Int = &amp;lt;function1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-16&quot;&gt;可变长度参数&lt;/h2&gt;

&lt;p&gt;这是一个特殊的语法，可以向方法传入任意多个同类型的参数。例如要在多个字符串上执行String的&lt;code&gt;capitalize&lt;/code&gt;函数，可以这样写：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def capitalizeAll(args: String*) = {
  args.map { arg =&amp;gt;
    arg.capitalize
  }
}

scala&amp;gt; capitalizeAll(&quot;rarity&quot;, &quot;applejack&quot;)
res1: Seq[String] = ArrayBuffer(Rarity, Applejack)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数内部，重复参数的类型是声明参数类型的数组。因此，capitalizeAll函数里被声明为类型“String*” 的args的类型实际上是 Array[String]。然而，如果你有一个合适类型的数组，并尝试把它当作重复参数传入，你会得到一个编译器错误:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val arr = Array(&quot;what&#39;s&quot;, &quot;up&quot;, &quot;doc?&quot;)
scala&amp;gt; capitalizeAll(arr)
&amp;lt;console&amp;gt;:31: error: type mismatch;
 found   : Array[String]
 required: String
              capitalizeAll(arr)
                            ^
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要实现这个做法，你需要在数组参数后添加一个冒号和一个&lt;code&gt;_*&lt;/code&gt;符号，像这样:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; capitalizeAll(arr: _*)
res2: Seq[String] = ArrayBuffer(What&#39;s, Up, Doc?)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个标注告诉编译器把arr的每个元素当作参数，而不是当作单一的参数传给capitalizeAll。&lt;/p&gt;

&lt;h2 id=&quot;lazy&quot;&gt;lazy参数&lt;/h2&gt;

&lt;p&gt;就是调用时用到函数的该参数，每次都重新计算。&lt;/p&gt;

&lt;p&gt;lazy参数是变量或值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//一般参数
def f1(x: Long) = {
  val (a,b) = (x,x)
  println(&quot;a=&quot;+a+&quot;,b=&quot;+b)
}
f1(System.nanoTime)
//a=1429501936753731000,b=1429501936753731000

//lazy参数
def f2(x: =&amp;gt;Long) = {
  val (a,b) = (x,x)
  println(&quot;a=&quot;+a+&quot;,b=&quot;+b)
}
f2(System.nanoTime)
//a=1429502007512014000,b=1429502007512015000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;lazy参数是函数时：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//一般参数，打印一次
scala&amp;gt; def times1(m: Unit) = { m;m;m }
times1: (m: Unit)Unit

scala&amp;gt; times1 ( println(&quot;hello world&quot;) )
hello world

//lazy参数，打印三次
scala&amp;gt; def times2(m: =&amp;gt;Unit) = { m;m;m }
times2: (m: =&amp;gt; Unit)Unit

scala&amp;gt; times2 ( println(&quot;hello world&quot;) )
hello world
hello world
hello world
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-17&quot;&gt;6. 流程控制&lt;/h1&gt;

&lt;h2 id=&quot;ifelse&quot;&gt;if..else&lt;/h2&gt;

&lt;p&gt;使用 if else 表达式:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;if (x&amp;gt;y) 100 else -1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;while&quot;&gt;while&lt;/h2&gt;

&lt;p&gt;使用 while 为列表求和:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def sum(xs: List[Int]) = { 
    var total = 0 
    var index = 0 
    while (index &amp;lt; xs.size) { 
        total += xs(index) 
        index += 1 
    } 
    total 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scala 也有 do-while 循环。除了把状态测试从前面移到后面之外,与 while 循环没有区别。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;var line = &quot;&quot;
do {
  line = readLine()
  println(&quot;Read: &quot; + line)
} while (line != null)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-18&quot;&gt;循环操作&lt;/h2&gt;

&lt;p&gt;for：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//循环中的变量不用定义，如：
scala&amp;gt; for(i&amp;lt;-1 to 3; j=i*i) println(j)  // 包含3
1
4
9

scala&amp;gt; for (i &amp;lt;- 1 until 3) println(i)  // 不包含3
1
2

//如果for条件是多行，不能用()，要用{}
scala&amp;gt; for{i&amp;lt;-0 to 5
     | j&amp;lt;-0 to 2} yield i+j
res3: scala.collection.immutable.IndexedSeq[Int] = Vector(0, 1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7)

//带有过滤条件，可以有多个过滤条件，逗号分隔
scala&amp;gt; for (i &amp;lt;- 1 until 10 if i%2 == 0) println(i)
2
4
6
8

//for中的嵌套循环，包括多个 &amp;lt;-
scala&amp;gt; for {i &amp;lt;- 1 until 3; j &amp;lt;- 1 until 3} println(i*j)
1
2
2
4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;for yield：把每次循环的结果&lt;code&gt;移进&lt;/code&gt;一个集合（类型和循环内的一致），格式：&lt;code&gt;for {子句} yield {循环体}&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;for (e&amp;lt;-List(1,2,3)) yield (e*e) // List(1,4,9)
for {e&amp;lt;-List(1,2,3)} yield { e*e } // List(1,4,9)
for {e&amp;lt;-List(1,2,3)} yield e*e // List(1,4,9)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;foreach:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; List(1,2,3).foreach(println)
1
2
3

scala&amp;gt; (1 to 3).foreach(println)
1
2
3

scala&amp;gt; (1 until 4) foreach println
1
2
3

scala&amp;gt; Range(1,4) foreach println
1
2
3

//可以写步长
scala&amp;gt; 1 to (11,2)
res9: scala.collection.immutable.Range.Inclusive = Range(1, 3, 5, 7, 9, 11)

scala&amp;gt; 1 to 11 by 2
res10: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9, 11)

scala&amp;gt; 1 until (11,2)
res11: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9)

scala&amp;gt; 1 until 11 by 2
res12: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9)

scala&amp;gt; val r = (1 to 10 by 4)
r: scala.collection.immutable.Range = Range(1, 5, 9)

//也可以是BigInt
scala&amp;gt; (1:BigInt) to 3
res13: scala.collection.immutable.NumericRange.Inclusive[BigInt] = NumericRange(1, 2, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;forall：判断是否所有都符合。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; (1 to 3) forall (0&amp;lt;)
res0: Boolean = true

scala&amp;gt; (-1 to 3) forall (0&amp;lt;)
res1: Boolean = false

scala&amp;gt; def isPrime(n:Int) = 2 until n forall (n%_!=0)
isPrime: (n: Int)Boolean

scala&amp;gt; for (i&amp;lt;-1 to 10 if isPrime(i)) println(i)
1
2
3
5
7

scala&amp;gt; (2 to 20) partition (isPrime _)
res4: (scala.collection.immutable.IndexedSeq[Int], scala.collection.immutable.IndexedSeq[Int]) = (Vector(2, 3, 5, 7, 11, 13, 17, 19),Vector(4, 6, 8, 9, 10, 12, 14, 15, 16, 18, 20))
//也可直接调用BigInt的内部方法
scala&amp;gt; (2 to 20) partition (BigInt(_) isProbablePrime(10))
res5: (scala.collection.immutable.IndexedSeq[Int], scala.collection.immutable.IndexedSeq[Int]) = (Vector(2, 3, 5, 7, 11, 13, 17, 19),Vector(4, 6, 8, 9, 10, 12, 14, 15, 16, 18, 20))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;reduceLeft方法首先应用于前两个元素，然后再应用于第一次应用的结果和接下去的一个元素，等等，直至整个列表。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//计算阶乘
scala&amp;gt; def fac(n: Int) = 1 to n reduceLeft(_*_)
fac: (n: Int)Int

scala&amp;gt; fac(5) // 5*4*3*2 = 120
res6: Int = 120

//求和
scala&amp;gt; List(2,4,6).reduceLeft(_+_)
res7: Int = 12

//取max：
scala&amp;gt; List(1,4,9,6,7).reduceLeft( (x,y)=&amp;gt; if (x&amp;gt;y) x else y )
res8: Int = 9
//或者简化为：  
scala&amp;gt; List(1,4,9,6,7).reduceLeft(_ max _)
res9: Int = 9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;foldLeft：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//累加
scala&amp;gt; def sum(L: Seq[Int]) = L.foldLeft(0)((a, b) =&amp;gt; a + b)
sum: (L: Seq[Int])Int

scala&amp;gt; def sum(L: Seq[Int]) = L.foldLeft(0)(_ + _)
sum: (L: Seq[Int])Int

scala&amp;gt; def sum(L: List[Int]) = (0/:L){_ + _}
sum: (L: List[Int])Int

scala&amp;gt; sum(List(1,3,5,7))
res10: Int = 16

//乘法：
scala&amp;gt; def multiply(L: Seq[Int]) = L.foldLeft(1)(_ * _)
multiply: (L: Seq[Int])Int

scala&amp;gt; multiply(Seq(1,2,3,4,5))
res11: Int = 120

scala&amp;gt; multiply(1 until 5+1)
res12: Int = 120
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;scanLeft:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; List(1,2,3,4,5).scanLeft(0)(_+_)
res13: List[Int] = List(0, 1, 3, 6, 10, 15)
//相当于 (0,(0+1),(0+1+2),(0+1+2+3),(0+1+2+3+4),(0+1+2+3+4+5))

scala&amp;gt; List(1,2,3,4,5).scanLeft(1)(_*_)
res14: List[Int] = List(1, 1, 2, 6, 24, 120)
//相当于 (1, 1*1, 1*1*2, 1*1*2*3, 1*1*2*3*4, 1*1*2*3*4*5)

scala&amp;gt; List(1,2,3,4,5).scanLeft(2)(_*_)
res16: List[Int] = List(2, 2, 4, 12, 48, 240)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;take、drop、splitAt:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;1 to 10 by 2 take 3 // Range(1, 3, 5)
1 to 10 by 2 drop 3 // Range(7, 9)
1 to 10 by 2 splitAt 2 // (Range(1, 3),Range(5, 7, 9))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;takeWhile、dropWhile、span：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;1 to 10 takeWhile (_&amp;lt;5) // (1,2,3,4)
1 to 10 takeWhile (_&amp;gt;5) // ()
10 to (1,-1) takeWhile(_&amp;gt;6) // (10,9,8,7)

1 to 10 takeWhile ( n=&amp;gt; n*n&amp;lt;25)  // (1, 2, 3, 4)

1 to 10 dropWhile (_&amp;lt;5) // (5,6,7,8,9,10)
1 to 10 dropWhile (n=&amp;gt;n*n&amp;lt;25) // (5,6,7,8,9,10)
 
1 to 10 span (_&amp;lt;5) // ((1,2,3,4),(5,6,7,8)
List(1,0,1,0) span (_&amp;gt;0) // ((1), (0,1,0))
// 注意，partition是和span完全不同的操作
List(1,0,1,0) partition (_&amp;gt;0) // ((1,1),(0,0))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;match-&quot;&gt;match 表达式&lt;/h2&gt;

&lt;p&gt;Scala 的匹配表达式允许你在许多可选项中做选择，就好象其它语言中的 switch 语句。通常说来 match 表达式可以让你使用任意的模式。&lt;/p&gt;

&lt;p&gt;匹配值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val times = 1

times match {
  case -1|0 =&amp;gt; &quot;zore&quot;
  case 1 =&amp;gt; &quot;one&quot;
  case 2 =&amp;gt; &quot;two&quot;
  case _ =&amp;gt; &quot;some other number&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用守卫进行匹配:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;times match {
  case i if i == 1 =&amp;gt; &quot;one&quot;
  case i if i == 2 =&amp;gt; &quot;two&quot;
  case _ =&amp;gt; &quot;some other number&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;匹配类型:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def bigger(o: Any): Any = {
  o match {
    case i: Int if i &amp;lt; 0 =&amp;gt; i - 1
    case i: Int =&amp;gt; i + 1
    case d: Double if d &amp;lt; 0.0 =&amp;gt; d - 0.1
    case d: Double =&amp;gt; d + 0.1
    case text: String =&amp;gt; text + &quot;s&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;匹配 Option 类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; map.get(1) match {
     |  case Some(i) =&amp;gt; println(&quot;Got something&quot;)
     |  case None =&amp;gt; println(&quot;Got nothing&quot;)
     | }
Got something
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;匹配类成员:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def calcType(calc: Calculator) = calc match {
  case _ if calc.brand == &quot;hp&quot; &amp;amp;&amp;amp; calc.model == &quot;20B&quot; =&amp;gt; &quot;financial&quot;
  case _ if calc.brand == &quot;hp&quot; &amp;amp;&amp;amp; calc.model == &quot;48G&quot; =&amp;gt; &quot;scientific&quot;
  case _ if calc.brand == &quot;hp&quot; &amp;amp;&amp;amp; calc.model == &quot;30B&quot; =&amp;gt; &quot;business&quot;
  case _ =&amp;gt; &quot;unknown&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与 Java 的 switch 语句比，匹配表达式还有一些重要的差别：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;case 后面可以是任意类型。&lt;/li&gt;
  &lt;li&gt;每个可选项的最后并没有 break，break 是隐含的。&lt;/li&gt;
  &lt;li&gt;match 表达式也能产生值。&lt;/li&gt;
  &lt;li&gt;在最后一行指令中的&lt;code&gt;_&lt;/code&gt;是一个通配符，它保证了我们可以处理所有的情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考 Effective Scala 对&lt;a href=&quot;http://twitter.github.com/effectivescala/#Functional programming-Pattern matching&quot;&gt;什么时候使用模式匹配&lt;/a&gt;和 &lt;a href=&quot;http://twitter.github.com/effectivescala/#Formatting-Pattern matching&quot;&gt;模式匹配格式化&lt;/a&gt;的建议。A Tour of Scala 也描述了&lt;a href=&quot;http://www.scala-lang.org/node/120&quot;&gt;模式匹配&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;case-if-&quot;&gt;case if 表达式&lt;/h2&gt;

&lt;p&gt;写法1：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;(1 to 20) foreach {                          
    case x if (x % 15 == 0) =&amp;gt; printf(&quot;%2d:15n\n&quot;,x)
    case x if (x % 3 == 0)  =&amp;gt; printf(&quot;%2d:3n\n&quot;,x)
    case x if (x % 5 == 0)  =&amp;gt; printf(&quot;%2d:5n\n&quot;,x)
    case x =&amp;gt; printf(&quot;%2d\n&quot;,x)                          
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写法2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;(1 to 20) map (x=&amp;gt; (x%3,x%5) match {
  case (0,0) =&amp;gt; printf(&quot;%2d:15n\n&quot;,x)
  case (0,_) =&amp;gt; printf(&quot;%2d:3n\n&quot;,x)
  case (_,0) =&amp;gt; printf(&quot;%2d:5n\n&quot;,x)
  case (_,_) =&amp;gt; printf(&quot;%2d\n&quot;,x)
})
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;breakcontinue&quot;&gt;break、continue&lt;/h2&gt;

&lt;p&gt;Scala中没有break和continue语法，需要break得加辅助boolean变量，或者用库（continue没有）。&lt;/p&gt;

&lt;p&gt;例子1：打印’a’到’z’的前10个&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;var i=0; val rt = for(e&amp;lt;-(&#39;a&#39; to &#39;z&#39;) if {i=i+1;i&amp;lt;=10}) printf(&quot;%d:%s\n&quot;,i,e)

(&#39;a&#39; to &#39;z&#39;).slice(0,10).foreach(println)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子2：1 到 100 和小于1000的数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;var (n,sum)=(0,0); for(i&amp;lt;-0 to 100 if (sum+i&amp;lt;1000)) { n=i; sum+=i }
// n = 44, sum = 990
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子3：使用库来实现break&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import scala.util.control.Breaks._
for(e&amp;lt;-1 to 10) { val e2 = e*e; if (e2&amp;gt;10) break; println(e) }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;try-catch-finally&quot;&gt;try catch finally&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;var f = openFile()
try {
  f = new FileReader(&quot;input.txt&quot;)
} catch {
  case ex: FileNotFoundException =&amp;gt; // Handle missing file
  case ex: IOException =&amp;gt; // Handle other I/O error
} finally {
  f.close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;捕获异常使用的是&lt;code&gt;模式匹配&lt;/code&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-19&quot;&gt;7. 其他&lt;/h1&gt;

&lt;h2 id=&quot;null-none-nil-nothing&quot;&gt;Null, None, Nil, Nothing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Null&lt;/code&gt;： &lt;code&gt;Trait&lt;/code&gt;，其唯一实例为null，是&lt;code&gt;AnyRef&lt;/code&gt;的子类，&lt;em&gt;不是&lt;/em&gt; &lt;code&gt;AnyVal&lt;/code&gt;的子类&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Nothing&lt;/code&gt;： &lt;code&gt;Trait&lt;/code&gt;，所有类型（包括&lt;code&gt;AnyRef&lt;/code&gt;和&lt;code&gt;AnyVal&lt;/code&gt;）的子类，没有实例&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;None&lt;/code&gt;： &lt;code&gt;Option&lt;/code&gt;的两个子类之一，另一个是&lt;code&gt;Some&lt;/code&gt;，用于安全的函数返回值&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Unit&lt;/code&gt;： 无返回值的函数的类型，和java的void对应&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Nil&lt;/code&gt;： 长度为0的List&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-20&quot;&gt;区分&amp;lt;-,=&amp;gt;,-&amp;gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;-&lt;/code&gt;用于for循环，符号&lt;code&gt;∈&lt;/code&gt;的象形:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;for (i &amp;lt;- 0 until 100)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;=&amp;gt;&lt;/code&gt;用于匿名函数，也可用在import中定义别名：&lt;code&gt;import javax.swing.{JFrame=&amp;gt;jf}&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;List(1,2,3).map(x=&amp;gt; x*x)
((i:Int)=&amp;gt;i*i)(5) // 25
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;-&amp;gt;&lt;/code&gt;用于Map初始化&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;Map(1-&amp;gt;&quot;a&quot;,2-&amp;gt;&quot;b&quot;) // (1:&quot;a&quot;,2:&quot;b&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：在scala中任何对象都能调用&lt;code&gt;-&amp;gt;&lt;/code&gt;方法（隐式转换），返回包含键值对的二元组!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section-21&quot;&gt;8. 参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.scala-lang.org/&quot;&gt;http://www.scala-lang.org/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://twitter.github.io/scala_school/zh_cn/index.html&quot;&gt;Scala 课堂&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://qiujj.com/static/Scala-Handbook.htm&quot;&gt;Scala 2.8+ Handbook&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ibm.com/developerworks/cn/java/j-scala/&quot;&gt;面向 Java 开发人员的 Scala 指南系列&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2015/04/20/basic-of-scala.html</link>
      <guid>http://blog.javachen.com/2015/04/20/basic-of-scala.html</guid>
      <pubDate>2015-04-20T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spark MLlib中的协同过滤</title>
      <description>&lt;p&gt;本文主要通过Spark官方的例子理解ALS协同过滤算法的原理和编码过程，然后通过对电影进行推荐来熟悉一个完整的推荐过程。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;协同过滤&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering&quot;&gt;协同过滤&lt;/a&gt;常被应用于推荐系统，旨在补充用户-商品关联矩阵中所缺失的部分。MLlib当前支持基于模型的协同过滤，其中用户和商品通过一小组隐语义因子进行表达，并且这些因子也用于预测缺失的元素。Spark MLlib实现了&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1608614&quot;&gt;交替最小二乘法&lt;/a&gt;(ALS) 来学习这些隐性语义因子。&lt;/p&gt;

&lt;p&gt;在 MLlib 中的实现类为org.apache.spark.mllib.recommendation.ALS.scala，其有如下的参数:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;numUserBlocks&lt;/code&gt;：是用于并行化计算的分块个数 (设置为-1，为自动配置)。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;numProductBlocks&lt;/code&gt;：是用于并行化计算的分块个数 (设置为-1，为自动配置)。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;rank&lt;/code&gt;：是模型中隐语义因子的个数。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;iterations&lt;/code&gt;：是迭代的次数，推荐值：10-20。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;lambda&lt;/code&gt;：惩罚函数的因数，是ALS的正则化参数，推荐值：0.01。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;implicitPrefs&lt;/code&gt;：决定了是用显性反馈ALS的版本还是用适用隐性反馈数据集的版本。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;：是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;seed&lt;/code&gt;：随机种子&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以调整这些参数，不断优化结果，使均方差变小。比如：iterations越多，lambda较小，均方差会较小，推荐结果较优。&lt;/p&gt;

&lt;p&gt;提供以下方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def run(ratings: RDD[Rating]): MatrixFactorizationModel

def train(ratings: RDD[Rating],rank: Int,iterations: Int,lambda: Double,blocks: Int,seed: Long): MatrixFactorizationModel
def train(ratings: RDD[Rating],rank: Int,iterations: Int,lambda: Double,blocks: Int): MatrixFactorizationModel
def train(ratings: RDD[Rating], rank: Int, iterations: Int, lambda: Double): MatrixFactorizationModel
def train(ratings: RDD[Rating], rank: Int, iterations: Int): MatrixFactorizationModel

def trainImplicit(ratings: RDD[Rating],rank: Int,iterations: Int,lambda: Double,blocks: Int,alpha: Double,seed: Long): MatrixFactorizationModel
def trainImplicit(ratings: RDD[Rating],rank: Int,iterations: Int,lambda: Double,blocks: Int,alpha: Double): MatrixFactorizationModel
def trainImplicit(ratings: RDD[Rating], rank: Int, iterations: Int, lambda: Double, alpha: Double): MatrixFactorizationModel
def trainImplicit(ratings: RDD[Rating], rank: Int, iterations: Int): MatrixFactorizationModel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上所有方法需要一个参数Rating，其为一个包括三个元素的 case class：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;case class Rating(user: Int, product: Int, rating: Double)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，以上方法均返回MatrixFactorizationModel类型的对象，提供以下方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;/** Predict the rating of one user for one product. */
def predict(user: Int, product: Int): Double

/**Predict the rating of many users for many products.*/
def predict(usersProducts: RDD[(Int, Int)]): RDD[Rating]

// Recommends products to a user.
def recommendProducts(user: Int, num: Int): Array[Rating]

//Recommends users to a product.
def recommendUsers(product: Int, num: Int): Array[Rating]

def save(sc: SparkContext, path: String): Unit

def load(sc: SparkContext, path: String): MatrixFactorizationModel =
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;vs-&quot;&gt;隐性反馈 vs 显性反馈&lt;/h2&gt;

&lt;p&gt;基于矩阵分解的协同过滤的标准方法一般将用户商品矩阵中的元素作为用户对商品的显性偏好。&lt;br /&gt;
在许多的现实生活中的很多场景中，我们常常只能接触到隐性的反馈（例如游览，点击，购买，喜欢，分享等等）在 MLlib 中所用到的处理这种数据的方法来源于文献： &lt;a href=&quot;http://labs.yahoo.com/files/HuKorenVolinsky-ICDM08.pdf&quot;&gt;Collaborative Filtering for Implicit Feedback Datasets&lt;/a&gt;。 本质上，这个方法将数据作为二元偏好值和偏好强度的一个结合，而不是对评分矩阵直接进行建模。因此，评价就不是与用户对商品的显性评分而是和所观察到的用户偏好强度关联了起来。然后，这个模型将尝试找到隐语义因子来预估一个用户对一个商品的偏好。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;代码示例&lt;/h1&gt;

&lt;p&gt;下面例子来自&lt;a href=&quot;http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html&quot;&gt;http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html&lt;/a&gt;，并做了稍许修改。&lt;/p&gt;

&lt;h2 id=&quot;scala-&quot;&gt;Scala 示例&lt;/h2&gt;

&lt;p&gt;为了测试简单，使用Spark本地运行模式进行测试。下面代码可以在spark-shell中运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating

// Load and parse the data
val data = sc.textFile(&quot;data/mllib/als/test.data&quot;)
val ratings = data.map(_.split(&#39;,&#39;) match { case Array(user, item, rate) =&amp;gt;
    Rating(user.toInt, item.toInt, rate.toDouble)
  })

// Build the recommendation model using ALS
val rank = 10
val numIterations = 20
val model = ALS.train(ratings, rank, numIterations, 0.01)

// Evaluate the model on rating data
val usersProducts = ratings.map { case Rating(user, product, rate) =&amp;gt;
  (user, product)
}
val predictions = 
  model.predict(usersProducts).map { case Rating(user, product, rate) =&amp;gt; 
    ((user, product), rate)
  }
val ratesAndPreds = ratings.map { case Rating(user, product, rate) =&amp;gt; 
  ((user, product), rate)
}.join(predictions)
val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =&amp;gt; 
  val err = (r1 - r2)
  err * err
}.mean()
println(&quot;Mean Squared Error = &quot; + MSE)

// Save and load model
model.save(sc, &quot;myModelPath&quot;)
val sameModel = MatrixFactorizationModel.load(sc, &quot;myModelPath&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;java&quot;&gt;Java示例&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import scala.Tuple2;

import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.mllib.recommendation.ALS;
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel;
import org.apache.spark.mllib.recommendation.Rating;
import org.apache.spark.SparkConf;

public class JavaALS {
  public static void main(String[] args) {
    SparkConf conf = new SparkConf().setAppName(&quot;Collaborative Filtering Example&quot;);
    JavaSparkContext sc = new JavaSparkContext(conf);

    // Load and parse the data
    String path = &quot;data/mllib/als/test.data&quot;;
    JavaRDD&amp;lt;String&amp;gt; data = sc.textFile(path);
    JavaRDD&amp;lt;Rating&amp;gt; ratings = data.map(
      new Function&amp;lt;String, Rating&amp;gt;() {
        public Rating call(String s) {
          String[] sarray = s.split(&quot;,&quot;);
          return new Rating(Integer.parseInt(sarray[0]), Integer.parseInt(sarray[1]), 
                            Double.parseDouble(sarray[2]));
        }
      }
    );

    // Build the recommendation model using ALS
    int rank = 10;
    int numIterations = 20;
    float lambda = 0.01;
    MatrixFactorizationModel model = ALS.train(JavaRDD.toRDD(ratings), rank, numIterations, lambda); 

    // Evaluate the model on rating data
    JavaRDD&amp;lt;Tuple2&amp;lt;Object, Object&amp;gt;&amp;gt; userProducts = ratings.map(
      new Function&amp;lt;Rating, Tuple2&amp;lt;Object, Object&amp;gt;&amp;gt;() {
        public Tuple2&amp;lt;Object, Object&amp;gt; call(Rating r) {
          return new Tuple2&amp;lt;Object, Object&amp;gt;(r.user(), r.product());
        }
      }
    );
    JavaPairRDD&amp;lt;Tuple2&amp;lt;Integer, Integer&amp;gt;, Double&amp;gt; predictions = JavaPairRDD.fromJavaRDD(
      model.predict(JavaRDD.toRDD(userProducts)).toJavaRDD().map(
        new Function&amp;lt;Rating, Tuple2&amp;lt;Tuple2&amp;lt;Integer, Integer&amp;gt;, Double&amp;gt;&amp;gt;() {
          public Tuple2&amp;lt;Tuple2&amp;lt;Integer, Integer&amp;gt;, Double&amp;gt; call(Rating r){
            return new Tuple2&amp;lt;Tuple2&amp;lt;Integer, Integer&amp;gt;, Double&amp;gt;(
              new Tuple2&amp;lt;Integer, Integer&amp;gt;(r.user(), r.product()), r.rating());
          }
        }
    ));
    JavaRDD&amp;lt;Tuple2&amp;lt;Double, Double&amp;gt;&amp;gt; ratesAndPreds = 
      JavaPairRDD.fromJavaRDD(ratings.map(
        new Function&amp;lt;Rating, Tuple2&amp;lt;Tuple2&amp;lt;Integer, Integer&amp;gt;, Double&amp;gt;&amp;gt;() {
          public Tuple2&amp;lt;Tuple2&amp;lt;Integer, Integer&amp;gt;, Double&amp;gt; call(Rating r){
            return new Tuple2&amp;lt;Tuple2&amp;lt;Integer, Integer&amp;gt;, Double&amp;gt;(
              new Tuple2&amp;lt;Integer, Integer&amp;gt;(r.user(), r.product()), r.rating());
          }
        }
    )).join(predictions).values();
    double MSE = JavaDoubleRDD.fromRDD(ratesAndPreds.map(
      new Function&amp;lt;Tuple2&amp;lt;Double, Double&amp;gt;, Object&amp;gt;() {
        public Object call(Tuple2&amp;lt;Double, Double&amp;gt; pair) {
          Double err = pair._1() - pair._2();
          return err * err;
        }
      }
    ).rdd()).mean();
    System.out.println(&quot;Mean Squared Error = &quot; + MSE);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;python&quot;&gt;Python示例&lt;/h2&gt;

&lt;p&gt;下面代码可以在 pyspark 中运行下面代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pyspark.mllib.recommendation import ALS
from numpy import array

# Load and parse the data
data = sc.textFile(&quot;data/mllib/als/test.data&quot;)
ratings = data.map(lambda line: array([float(x) for x in line.split(&#39;,&#39;)]))

# Build the recommendation model using Alternating Least Squares
rank = 10
numIterations = 20
model = ALS.train(ratings, rank, numIterations)

# Evaluate the model on training data
testdata = ratings.map(lambda p: (int(p[0]), int(p[1])))
predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))
ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)
MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).reduce(lambda x, y: x + y)/ratesAndPreds.count()
print(&quot;Mean Squared Error = &quot; + str(MSE))
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;使用Spark MLlib的ALS算法进行协同过滤，首先需要了解推荐的过程，然后需要根据测试不断修改训练测试，建立合理的模型，最后再给用户进行推荐商品，保存推荐结果。&lt;/p&gt;

&lt;p&gt;另外，在网上找到一些Spark做推荐的项目：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提供Restfull接口的实时推荐：&lt;a href=&quot;https://github.com/OndraFiedler/spark-recommender&quot;&gt;https://github.com/OndraFiedler/spark-recommender&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;spark-elasticsearch-mllib：&lt;a href=&quot;https://github.com/ebiznext/spark-elasticsearch-mllib&quot;&gt;https://github.com/ebiznext/spark-elasticsearch-mllib&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Beyond Piwik Web Analytics：&lt;a href=&quot;https://github.com/skrusche63/spark-piwik&quot;&gt;https://github.com/skrusche63/spark-piwik&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Serves predictions via a REST API：&lt;a href=&quot;https://github.com/SeldonIO/seldon-server&quot;&gt;https://github.com/SeldonIO/seldon-server&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zhuixun/learningspark&quot;&gt;https://github.com/zhuixun/learningspark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark-Movie-Recommendation：&lt;a href=&quot;https://github.com/yuriy-voderatskiy/Spark-Movie-Recommendation&quot;&gt;https://github.com/yuriy-voderatskiy/Spark-Movie-Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多关于推荐相关的资源可以参考&lt;a href=&quot;/2015/03/30/reading-list-2015-03.html&quot;&gt;Reading List 2015-03&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html&quot;&gt;Movie Recommendation with MLlib&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/shifenglov/article/details/43795597&quot;&gt;Spark MLlib系列(二):基于协同过滤的电影推荐系统&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gaapt/als_recom/blob/master/ALSBenchmarkSpark%2Fsrc%2Fmain%2Fscala%2FALSBenchmark.scala&quot;&gt;ALSBenchmark.scala&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/04/17/spark-mllib-collaborative-filtering.html</link>
      <guid>http://blog.javachen.com/2015/04/17/spark-mllib-collaborative-filtering.html</guid>
      <pubDate>2015-04-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spark SQL中的数据源</title>
      <description>&lt;p&gt;Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、cassandra 等等。&lt;/p&gt;

&lt;p&gt;本文测试环境为 Spark 1.3。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;加载和保存文件&lt;/h1&gt;

&lt;p&gt;最简单的方式是调用 load 方法加载文件，默认的格式为 parquet，你可以修改 &lt;code&gt;spark.sql.sources.default&lt;/code&gt; 指定默认的格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val df = sqlContext.load(&quot;people.parquet&quot;)
scala&amp;gt; df.select(&quot;name&quot;, &quot;age&quot;).save(&quot;namesAndAges.parquet&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以收到指定数据源，使用全路径名称，如：&lt;code&gt;org.apache.spark.sql.parquet&lt;/code&gt;，对于内置的数据源，你也可以使用简称，如：&lt;code&gt;json&lt;/code&gt;、&lt;code&gt;parquet&lt;/code&gt;、&lt;code&gt;jdbc&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val df = sqlContext.load(&quot;people.json&quot;, &quot;json&quot;)
scala&amp;gt; df.select(&quot;name&quot;, &quot;age&quot;).save(&quot;namesAndAges.parquet&quot;, &quot;parquet&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保存操作还可以指定保存模式，用于处理文件已经存在的情况下如何操作。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Scala/Java&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Python&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SaveMode.ErrorIfExists (default)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;“error” (default)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;如果存在，则报错&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SaveMode.Append&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;“append”&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;追加模式&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SaveMode.Overwrite&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;“overwrite”&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;覆盖模式&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SaveMode.Ignore&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;“ignore”&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;忽略，类似 SQL 中的 &lt;code&gt;CREATE TABLE IF NOT EXISTS&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;parquet-&quot;&gt;Parquet 数据源&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;加载数据&lt;/h2&gt;

&lt;p&gt;Spark SQL 支持读写 Parquet文件。&lt;/p&gt;

&lt;p&gt;Scala:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// sqlContext from the previous example is used in this example.
// This is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

val people: RDD[Person] = ... // An RDD of case class objects, from the previous example.

// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.
people.saveAsParquetFile(&quot;people.parquet&quot;)

// Read in the parquet file created above.  Parquet files are self-describing so the schema is preserved.
// The result of loading a Parquet file is also a DataFrame.
val parquetFile = sqlContext.parquetFile(&quot;people.parquet&quot;)

//Parquet files can also be registered as tables and then used in SQL statements.
parquetFile.registerTempTable(&quot;parquetFile&quot;)
val teenagers = sqlContext.sql(&quot;SELECT name FROM parquetFile WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;)
teenagers.map(t =&amp;gt; &quot;Name: &quot; + t(0)).collect().foreach(println)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// sqlContext from the previous example is used in this example.

DataFrame schemaPeople = ... // The DataFrame from the previous example.

// DataFrames can be saved as Parquet files, maintaining the schema information.
schemaPeople.saveAsParquetFile(&quot;people.parquet&quot;);

// Read in the Parquet file created above.  Parquet files are self-describing so the schema is preserved.
// The result of loading a parquet file is also a DataFrame.
DataFrame parquetFile = sqlContext.parquetFile(&quot;people.parquet&quot;);

//Parquet files can also be registered as tables and then used in SQL statements.
parquetFile.registerTempTable(&quot;parquetFile&quot;);
DataFrame teenagers = sqlContext.sql(&quot;SELECT name FROM parquetFile WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;);
List&amp;lt;String&amp;gt; teenagerNames = teenagers.map(new Function&amp;lt;Row, String&amp;gt;() {
  public String call(Row row) {
    return &quot;Name: &quot; + row.getString(0);
  }
}).collect();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# sqlContext from the previous example is used in this example.

schemaPeople # The DataFrame from the previous example.

# DataFrames can be saved as Parquet files, maintaining the schema information.
schemaPeople.saveAsParquetFile(&quot;people.parquet&quot;)

# Read in the Parquet file created above.  Parquet files are self-describing so the schema is preserved.
# The result of loading a parquet file is also a DataFrame.
parquetFile = sqlContext.parquetFile(&quot;people.parquet&quot;)

# Parquet files can also be registered as tables and then used in SQL statements.
parquetFile.registerTempTable(&quot;parquetFile&quot;);
teenagers = sqlContext.sql(&quot;SELECT name FROM parquetFile WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;)
teenNames = teenagers.map(lambda p: &quot;Name: &quot; + p.name)
for teenName in teenNames.collect():
  print teenName
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SQL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;CREATE TEMPORARY TABLE parquetTable
USING org.apache.spark.sql.parquet
OPTIONS (
  path &quot;examples/src/main/resources/people.parquet&quot;
)

SELECT * FROM parquetTable
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;自动发现分区&lt;/h2&gt;

&lt;p&gt;Parquet 数据源可以自动识别分区目录以及分区列的类型，目前支持数据类型和字符串类型。&lt;/p&gt;

&lt;p&gt;例如，对于这样一个目录结构，有两个分区字段：gender、country。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;path
└── to
    └── table
        ├── gender=male
        │   ├── ...
        │   │
        │   ├── country=US
        │   │   └── data.parquet
        │   ├── country=CN
        │   │   └── data.parquet
        │   └── ...
        └── gender=female
            ├── ...
            │
            ├── country=US
            │   └── data.parquet
            ├── country=CN
            │   └── data.parquet
            └── ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 path/to/table 路径传递给 SQLContext.parquetFile 或 SQLContext.load 时，Spark SQL 将会字段获取分区信息，并返回 DataFrame 的 schema 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;schema-&quot;&gt;schema 自动扩展&lt;/h2&gt;

&lt;p&gt;Parquet 还支持 schema 自动扩展。&lt;/p&gt;

&lt;p&gt;Scala:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// sqlContext from the previous example is used in this example.
// This is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

// Create a simple DataFrame, stored into a partition directory
val df1 = sparkContext.makeRDD(1 to 5).map(i =&amp;gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)
df1.saveAsParquetFile(&quot;data/test_table/key=1&quot;)

// Create another DataFrame in a new partition directory,
// adding a new column and dropping an existing column
val df2 = sparkContext.makeRDD(6 to 10).map(i =&amp;gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)
df2.saveAsParquetFile(&quot;data/test_table/key=2&quot;)

// Read the partitioned table
val df3 = sqlContext.parquetFile(&quot;data/test_table&quot;)
df3.printSchema()

// The final schema consists of all 3 columns in the Parquet files together
// with the partiioning column appeared in the partition directory paths.
// root
// |-- single: int (nullable = true)
// |-- double: int (nullable = true)
// |-- triple: int (nullable = true)
// |-- key : int (nullable = true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# sqlContext from the previous example is used in this example.

# Create a simple DataFrame, stored into a partition directory
df1 = sqlContext.createDataFrame(sc.parallelize(range(1, 6))\
                                   .map(lambda i: Row(single=i, double=i * 2)))
df1.save(&quot;data/test_table/key=1&quot;, &quot;parquet&quot;)

# Create another DataFrame in a new partition directory,
# adding a new column and dropping an existing column
df2 = sqlContext.createDataFrame(sc.parallelize(range(6, 11))
                                   .map(lambda i: Row(single=i, triple=i * 3)))
df2.save(&quot;data/test_table/key=2&quot;, &quot;parquet&quot;)

# Read the partitioned table
df3 = sqlContext.parquetFile(&quot;data/test_table&quot;)
df3.printSchema()

# The final schema consists of all 3 columns in the Parquet files together
# with the partiioning column appeared in the partition directory paths.
# root
# |-- single: int (nullable = true)
# |-- double: int (nullable = true)
# |-- triple: int (nullable = true)
# |-- key : int (nullable = true)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;配置参数&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;spark.sql.parquet.binaryAsString&lt;/code&gt;：默认为 false，是否将 binary 当做字符串处理&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.sql.parquet.int96AsTimestamp&lt;/code&gt;：默认为 true&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.sql.parquet.cacheMetadata&lt;/code&gt; ：默认为 true，是否缓存元数据&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.sql.parquet.compression.codec&lt;/code&gt;：默认为 gzip，支持的值：uncompressed, snappy, gzip, lzo&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.sql.parquet.filterPushdown&lt;/code&gt;：默认为 false&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark.sql.hive.convertMetastoreParquet&lt;/code&gt;：默认为 false&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;json-&quot;&gt;JSON 数据源&lt;/h1&gt;

&lt;p&gt;Spark SQL 能够自动识别 JSON 数据的 schema ，SQLContext 中有两个方法处理 JSON：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;jsonFile&lt;/code&gt;：从一个 JSON 目录中加载数据，JSON 文件中每一行为一个 JSON 对象。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;jsonRDD&lt;/code&gt;：从一个 RDD 中加载数据，RDD 的每一个元素为一个 JSON 对象的字符串。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个 Scala 的例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// A JSON dataset is pointed to by path.
// The path can be either a single text file or a directory storing text files.
val path = &quot;people.json&quot;
// Create a DataFrame from the file(s) pointed to by path
val people = sqlContext.jsonFile(path)

// The inferred schema can be visualized using the printSchema() method.
people.printSchema()
// root
//  |-- age: integer (nullable = true)
//  |-- name: string (nullable = true)

// Register this DataFrame as a table.
people.registerTempTable(&quot;people&quot;)

// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sqlContext.sql(&quot;SELECT name FROM people WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;)

// Alternatively, a DataFrame can be created for a JSON dataset represented by
// an RDD[String] storing one JSON object per string.
val anotherPeopleRDD = sc.parallelize(
  &quot;&quot;&quot;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot; :: Nil)
val anotherPeople = sqlContext.jsonRDD(anotherPeopleRDD)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hive-&quot;&gt;Hive 数据源&lt;/h1&gt;

&lt;p&gt;Spark SQL 支持读和写 Hive 中的数据。Spark  源码本身不包括 Hive，故编译时候需要添加  &lt;code&gt;-Phive&lt;/code&gt; 和 &lt;code&gt;-Phive-thriftserver&lt;/code&gt; 开启对 Hive 的支持。另外，Hive assembly jar 需要存在于每一个 worker 节点上，因为他们需要 SerDes 去访问存在于 Hive 中的数据。&lt;/p&gt;

&lt;p&gt;Scala:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)

sqlContext.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)
sqlContext.sql(&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;)

// Queries are expressed in HiveQL
sqlContext.sql(&quot;FROM src SELECT key, value&quot;).collect().foreach(println)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// sc is an existing JavaSparkContext.
HiveContext sqlContext = new org.apache.spark.sql.hive.HiveContext(sc);

sqlContext.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;);
sqlContext.sql(&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;);

// Queries are expressed in HiveQL.
Row[] results = sqlContext.sql(&quot;FROM src SELECT key, value&quot;).collect();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# sc is an existing SparkContext.
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)

sqlContext.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)
sqlContext.sql(&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;)

# Queries can be expressed in HiveQL.
results = sqlContext.sql(&quot;FROM src SELECT key, value&quot;).collect()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;jdbc-&quot;&gt;JDBC 数据源&lt;/h1&gt;

&lt;p&gt;Spark SQL 支持通过 JDBC 访问关系数据库，这需要用到 &lt;a href=&quot;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD&quot;&gt;JdbcRDD&lt;/a&gt;。为了访问某一个关系数据库，需要将其驱动添加到 classpath，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问 jdbc 数据源需要提供以下参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;url&lt;/li&gt;
  &lt;li&gt;dbtable&lt;/li&gt;
  &lt;li&gt;driver&lt;/li&gt;
  &lt;li&gt;partitionColumn, lowerBound, upperBound, numPartitions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scala 示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val jdbcDF = sqlContext.load(&quot;jdbc&quot;, Map(
  &quot;url&quot; -&amp;gt; &quot;jdbc:postgresql:dbserver&quot;,
  &quot;dbtable&quot; -&amp;gt; &quot;schema.tablename&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Map&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();
options.put(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;);
options.put(&quot;dbtable&quot;, &quot;schema.tablename&quot;);

DataFrame jdbcDF = sqlContext.load(&quot;jdbc&quot;, options)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df = sqlContext.load(&quot;jdbc&quot;, url=&quot;jdbc:postgresql:dbserver&quot;, dbtable=&quot;schema.tablename&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SQL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;CREATE TEMPORARY TABLE jdbcTable
USING org.apache.spark.sql.jdbc
OPTIONS (
  url &quot;jdbc:postgresql:dbserver&quot;,
  dbtable &quot;schema.tablename&quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;avro&quot;&gt;访问 Avro&lt;/h1&gt;

&lt;p&gt;这不是 Spark 内置的数据源，要想访问 Avro 数据源 ，需要做些处理。这部分内容可以参考 &lt;a href=&quot;http://blog.javachen.com/2015/03/24/how-to-load-some-avro-data-into-spark.html&quot;&gt;如何将Avro数据加载到Spark&lt;/a&gt; 和 &lt;a href=&quot;http://www.infoobjects.com/spark-with-avro.html&quot;&gt;Spark with Avro&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;cassandra&quot;&gt;访问 Cassandra&lt;/h1&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;测试&lt;/h1&gt;

&lt;h2 id=&quot;spark--parquet&quot;&gt;Spark 和 Parquet&lt;/h2&gt;

&lt;p&gt;参考上面的例子，将 people.txt 文件加载到 Spark：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; import sqlContext.implicits._
scala&amp;gt; case class People(name: String, age: Int)
scala&amp;gt; val people = sc.textFile(&quot;people.txt&quot;).map(_.split(&quot;,&quot;)).map(p =&amp;gt; People(p(0), p(1).trim.toInt)).toDF()
scala&amp;gt; people.registerTempTable(&quot;people&quot;)
scala&amp;gt; val teenagers = sqlContext.sql(&quot;SELECT name FROM people WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;)
scala&amp;gt; teenagers.map(t =&amp;gt; &quot;Name: &quot; + t(0)).collect().foreach(println)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，将 people 这个 DataFrame 转换为 parquet 格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; people.saveAsParquetFile(&quot;people.parquet&quot;)
scala&amp;gt; val parquetFile = sqlContext.parquetFile(&quot;people.parquet&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，也可以从 hive 中加载 parquet 格式的文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; create table people_parquet like people stored as parquet;
hive&amp;gt; insert overwrite table people_parquet select * from people;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 HiveContext 来从 hive 中加载 parquet 文件，这里不再需要定义一个 case class ，因为 parquet 中已经包含了文件的 schema。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val hc = new org.apache.spark.sql.hive.HiveContext(sc)
scala&amp;gt; import hc.implicits._
scala&amp;gt;val peopleRDD = hc.parquetFile(&quot;people.parquet&quot;)
scala&amp;gt; peopleRDD.registerAsTempTable(&quot;pp&quot;)
scala&amp;gt;val teenagers = hc.sql(&quot;SELECT name FROM pp WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;)
scala&amp;gt;teenagers.collect.foreach(println)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意到 impala 中处理 parquet 文件时，会将字符串保存为 Binary，为了修正这个问题，可以添加下面一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; sqlContext.setConf(&quot;spark.sql.parquet.binaryAsString&quot;,&quot;true&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sparksql-join&quot;&gt;SparkSql Join&lt;/h2&gt;

&lt;p&gt;下面是两个表左外连接的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt;import sqlContext.implicits._
scala&amp;gt;import org.apache.spark.sql.catalyst.plans._

scala&amp;gt; case class Dept(dept_id:String,dept_name:String)
scala&amp;gt; val dept = sc.parallelize(List( (&quot;DEPT01&quot;,&quot;Information Technology&quot;), (&quot;DEPT02&quot;,&quot;WHITE HOUSE&quot;),(&quot;DEPT03&quot;,&quot;EX-PRESIDENTS OFFICE&quot;),(&quot;DEPT04&quot;,&quot;SALES&quot;))).map( d =&amp;gt; Dept(d._1,d._2)).toDF.as( &quot;dept&quot; )

scala&amp;gt; case class Emp(first_name:String,last_name:String,dept_id:String)
scala&amp;gt; val emp = sc.parallelize(List( (&quot;Rishi&quot;,&quot;Yadav&quot;,&quot;DEPT01&quot;),(&quot;Barack&quot;,&quot;Obama&quot;,&quot;DEPT02&quot;),(&quot;Bill&quot;,&quot;Clinton&quot;,&quot;DEPT04&quot;))).map( e =&amp;gt; Emp(e._1,e._2,e._3)).toDF.as(&quot;emp&quot;)

scala&amp;gt; val alldepts = dept.join(emp,dept(&quot;dept_id&quot;) === emp(&quot;dept_id&quot;), &quot;left_outer&quot;).select(&quot;dept.dept_id&quot;,&quot;dept_name&quot;,&quot;first_name&quot;,&quot;last_name&quot;)

scala&amp;gt; alldepts.foreach(println)
[DEPT01,Information Technology,Rishi,Yadav]
[DEPT02,WHITE HOUSE,Barack,Obama]
[DEPT04,SALES,Bill,Clinton]
[DEPT03,EX-PRESIDENTS OFFICE,null,null]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;支持的连接类型有：&lt;code&gt;inner&lt;/code&gt;、&lt;code&gt;outer&lt;/code&gt;、&lt;code&gt;left_outer&lt;/code&gt;、&lt;code&gt;right_outer&lt;/code&gt;、&lt;code&gt;semijoin&lt;/code&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes&quot;&gt;Spark SQL and DataFrame Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://endymecy.gitbooks.io/spark-programming-guide-zh-cn/content/spark-sql/README.html&quot;&gt;Spark 编程指南简体中文版-Spark SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.infoobjects.com/spark-cookbook/&quot;&gt;spark-cookbook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/04/03/spark-sql-datasource.html</link>
      <guid>http://blog.javachen.com/2015/04/03/spark-sql-datasource.html</guid>
      <pubDate>2015-04-03T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spark本地模式运行</title>
      <description>&lt;p&gt;Spark的安装分为几种模式，其中一种是本地运行模式，只需要在单节点上解压即可运行，这种模式不需要依赖Hadoop 环境。在本地运行模式中，master和worker都运行在一个jvm进程中，通过该模式，可以快速的测试Spark的功能。&lt;/p&gt;

&lt;h1 id=&quot;spark&quot;&gt;下载 Spark&lt;/h1&gt;

&lt;p&gt;下载地址为&lt;a href=&quot;http://spark.apache.org/downloads.html&quot;&gt;http://spark.apache.org/downloads.html&lt;/a&gt;，根据页面提示选择一个合适的版本下载，这里我下载的是 &lt;a href=&quot;http://mirror.bit.edu.cn/apache/spark/spark-1.3.0/spark-1.3.0-bin-cdh4.tgz&quot;&gt;spark-1.3.0-bin-cdh4.tgz&lt;/a&gt;。下载之后解压：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt; cd ~
 wget http://mirror.bit.edu.cn/apache/spark/spark-1.3.0/spark-1.3.0-bin-cdh4.tgz
 tar -xf spark-1.3.0-bin-cdh4.tgz
 cd spark-1.3.0-bin-cdh4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载之后的目录为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;⇒  tree -L 1
.
├── CHANGES.txt
├── LICENSE
├── NOTICE
├── README.md
├── RELEASE
├── bin
├── conf
├── data
├── ec2
├── examples
├── lib
├── python
└── sbin
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;spark-shell&quot;&gt;运行 spark-shell&lt;/h1&gt;

&lt;p&gt;本地模式运行spark-shell非常简单，只要运行以下命令即可，假设当前目录是$SPARK_HOME&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ MASTER=local 
$ bin/spark-shell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;MASTER=local&lt;/code&gt;就是表明当前运行在单机模式。如果一切顺利，将看到下面的提示信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Created spark context..
Spark context available as sc.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这表明spark-shell中已经内置了Spark context的变量，名称为sc，我们可以直接使用该变量进行后续的操作。&lt;/p&gt;

&lt;p&gt;spark-shell 后面设置 master 参数，可以支持更多的模式，请参考 &lt;a href=&quot;http://spark.apache.org/docs/latest/submitting-applications.html#master-urls&quot;&gt;http://spark.apache.org/docs/latest/submitting-applications.html#master-urls&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;我们在sparkshell中运行一下最简单的例子，统计在README.md中含有Spark的行数有多少，在spark-shell中输入如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;scala&amp;gt;sc.textFile(&quot;README.md&quot;).filter(_.contains(&quot;Spark&quot;)).count
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你觉得输出的日志太多，你可以从模板文件创建  conf/log4j.properties ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mv conf/log4j.properties.template conf/log4j.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后修改日志输出级别为&lt;code&gt;WARN&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log4j.rootCategory=WARN, console
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你设置的 log4j 日志等级为 INFO，则你可以看到这样的一行日志 &lt;code&gt;INFO SparkUI: Started SparkUI at http://10.9.4.165:4040&lt;/code&gt;，意思是 Spark 启动了一个 web 服务器，你可以通过浏览器访问&lt;a href=&quot;http://10.9.4.165:4040&quot;&gt;http://10.9.4.165:4040&lt;/a&gt;来查看 Spark 的任务运行状态等信息。&lt;/p&gt;

&lt;h1 id=&quot;pyspark&quot;&gt;pyspark&lt;/h1&gt;

&lt;p&gt;运行 bin/pyspark 的输出为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/pyspark
Python 2.7.6 (default, Sep  9 2014, 15:04:36)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
15/03/30 15:19:07 WARN Utils: Your hostname, june-mac resolves to a loopback address: 127.0.0.1; using 10.9.4.165 instead (on interface utun0)
15/03/30 15:19:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
15/03/30 15:19:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ / __/  _/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.0
      /_/

Using Python version 2.7.6 (default, Sep  9 2014 15:04:36)
SparkContext available as sc, HiveContext available as sqlCtx.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以使用 IPython 来运行 Spark：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IPYTHON=1  ./bin/pyspark
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果要使用 IPython NoteBook，则运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IPYTHON_OPTS=&quot;notebook&quot;  ./bin/pyspark
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从日志可以看到，不管是 bin/pyspark 还是 bin/spark-shell，他们都有两个内置的变量：sc 和 sqlCtx。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;SparkContext available as sc, HiveContext available as sqlCtx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sc 代表着 Spark 的上下文，通过该变量可以执行 Spark 的一些操作，而 sqlCtx 代表着 HiveContext 的上下文。&lt;/p&gt;

&lt;h1 id=&quot;spark-submit&quot;&gt;spark-submit&lt;/h1&gt;

&lt;p&gt;在Spark1.0之后提供了一个统一的脚本spark-submit来提交任务。&lt;/p&gt;

&lt;p&gt;对于 python 程序，我们可以直接使用 spark-submit：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p /usr/lib/spark/examples/python
$ tar zxvf /usr/lib/spark/lib/python.tar.gz -C /usr/lib/spark/examples/python

$ ./bin/spark-submit examples/python/pi.py 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 Java 程序，我们需要先编译代码然后打包运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit --class &quot;SimpleApp&quot; --master local[4] simple-project-1.0.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;rdd&quot;&gt;测试 RDD&lt;/h1&gt;

&lt;p&gt;在 Spark 中，我们操作的集合被称为 RDD，他们被并行拷贝到集群各个节点上。我们可以通过 sc 来创建 RDD 。&lt;/p&gt;

&lt;p&gt;创建 RDD 有两种方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;sc.parallelize()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sc.textFile()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用 Scala 对 RDD 的一些操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val rdd1=sc.parallelize(List(1,2,3,3))
val rdd2=sc.parallelize(List(3,4,5))

//转换操作
rdd1.map(2*).collect //等同于：rdd1.map(t=&amp;gt;2*t).collect
//Array[Int] = Array(2, 4, 6, 6)

rdd1.filter(_&amp;gt;2).collect
//Array[Int] = Array(3, 3)

rdd1.flatMap(_ to 4).collect
//Array[Int] = Array(1, 2, 3, 4, 2, 3, 4, 3, 4, 3, 4)

rdd1.sample(false, 0.3, 4).collect
//Array[Int] = Array(3, 3)

rdd1.sample(true, 0.3, 4).collect
//Array[Int] = Array(3)

rdd1.union(rdd2).collect
//Array[Int] = Array(1, 2, 3, 3, 3, 4, 5)

rdd1.distinct().collect
//Array[Int] = Array(1, 2, 3)

rdd1.map(i=&amp;gt;(i,i)).groupByKey.collect
//Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(1)), (2,CompactBuffer(2)), (3,CompactBuffer(3, 3)))

rdd1.map(i=&amp;gt;(i,i)).reduceByKey(_ + _).collect
//Array[(Int, Int)] = Array((1,1), (2,2), (3,6))

rdd1.map(i=&amp;gt;(i,i)).sortByKey(false).collect
//Array[(Int, Int)] = Array((3,3), (3,3), (2,2), (1,1))

rdd1.map(i=&amp;gt;(i,i)).join(rdd2.map(i=&amp;gt;(i,i))).collect
//Array[(Int, (Int, Int))] = Array((3,(3,3)), (3,(3,3)))

rdd1.map(i=&amp;gt;(i,i)).cogroup(rdd2.map(i=&amp;gt;(i,i))).collect
//Array[(Int, (Iterable[Int], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(1),CompactBuffer())), (5,(CompactBuffer(),CompactBuffer(5))), (2,(CompactBuffer(2),CompactBuffer())), (3,(CompactBuffer(3, 3),CompactBuffer(3))))

rdd1.cartesian(rdd2).collect()
//Array[(Int, Int)] = Array((1,3), (1,4), (1,5), (2,3), (2,4), (2,5), (3,3), (3,4), (3,5), (3,3), (3,4), (3,5))

rdd1.pipe(&quot;head -n 1&quot;).collect
//Array[String] = Array(1, 2, 3, 3)

//动作操作
rdd1.reduce(_ + _)
//Int = 9

rdd1.collect
//Array[Int] = Array(1, 2, 3, 3)

rdd1.first()
//Int = 1

rdd1.take(2)
//Array[Int] = Array(1, 2)

rdd1.top(2)
//Array[Int] = Array(3, 3)

rdd1.takeOrdered(2)
//Array[Int] = Array(1, 2)

rdd1.map(i=&amp;gt;(i,i)).countByKey()
//scala.collection.Map[Int,Long] = Map(1 -&amp;gt; 1, 2 -&amp;gt; 1, 3 -&amp;gt; 2)

rdd1.countByValue()
//scala.collection.Map[Int,Long] = Map(1 -&amp;gt; 1, 2 -&amp;gt; 1, 3 -&amp;gt; 2)

rdd1.intersection(rdd2).collect()
//Array[Int] = Array(3)

rdd1.subtract(rdd2).collect()
//Array[Int] = Array(1, 2)

rdd1.foreach(println)
//3
//2
//3
//1

rdd1.foreachPartition(x =&amp;gt; println(x.reduce(_ + _)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多例子，参考&lt;a href=&quot;http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html&quot;&gt;http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html&lt;/a&gt;。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2015/03/30/spark-test-in-local-mode.html</link>
      <guid>http://blog.javachen.com/2015/03/30/spark-test-in-local-mode.html</guid>
      <pubDate>2015-03-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Reading List 2015-03</title>
      <description>&lt;p&gt;这个月主要在关注流式处理和推荐系统方面的技术。如何从零构建一个推荐系统？网上能找到的有指导意义的资料太少，只能一点点摸索？&lt;/p&gt;

&lt;h3 id=&quot;spark&quot;&gt;Spark&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://forum.leancloud.cn/t/ji-zhu-xiang-jie-leancloud-chi-xian-shu-ju-fen-xi-gong-neng-jie-shao/281&quot;&gt;LeanCloud 离线数据分析功能介绍&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark在腾讯数据仓库TDW的应用 &lt;a href=&quot;http://www.biaodianfu.com/spark-tdw.html&quot;&gt;http://www.biaodianfu.com/spark-tdw.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark on Yarn：小火花照亮大数据 &lt;a href=&quot;http://rdc.taobao.org/?p=512&quot;&gt;http://rdc.taobao.org/?p=512&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark on Yarn：性能调优 &lt;a href=&quot;http://rdc.taobao.org/?p=533&quot;&gt;http://rdc.taobao.org/?p=533&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;spark-&quot;&gt;Spark 教程&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Spark Shell Examples &lt;a href=&quot;https://altiscale.zendesk.com/hc/en-us/articles/202627136-Spark-Shell-Examples&quot;&gt;https://altiscale.zendesk.com/hc/en-us/articles/202627136-Spark-Shell-Examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.javacodegeeks.com//?s=spark&quot;&gt;http://www.javacodegeeks.com//?s=spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark SQL join 的例子：&lt;a href=&quot;https://gist.github.com/ceteri/11381941&quot;&gt;https://gist.github.com/ceteri/11381941&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark Cook Book：&lt;a href=&quot;http://www.infoobjects.com/spark-cookbook/&quot;&gt;http://www.infoobjects.com/spark-cookbook/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;spark-1&quot;&gt;Spark做推荐系统&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;spark机器学习 &lt;a href=&quot;http://blog.selfup.cn/category/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&quot;&gt;http://blog.selfup.cn/category/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark MLlib系列(一)：入门介绍 &lt;a href=&quot;http://blog.csdn.net/shifenglov/article/details/43762705&quot;&gt;http://blog.csdn.net/shifenglov/article/details/43762705&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark MLlib系列(二)：基于协同过滤的电影推荐系统 &lt;a href=&quot;http://blog.csdn.net/shifenglov/article/details/43795597&quot;&gt;http://blog.csdn.net/shifenglov/article/details/43795597&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark机器学习库mllib之协同过滤  &lt;a href=&quot;http://blog.csdn.net/oopsoom/article/details/34462329&quot;&gt;http://blog.csdn.net/oopsoom/article/details/34462329&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;快刀初试：Spark GraphX在淘宝的实践 &lt;a href=&quot;http://www.csdn.net/article/2014-08-07/2821097&quot;&gt;http://www.csdn.net/article/2014-08-07/2821097&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;“Spark上流式机器学习算法实现”终期检查报告 &lt;a href=&quot;http://blog.csdn.net/zhangyuming010/article/details/38364867&quot;&gt;http://blog.csdn.net/zhangyuming010/article/details/38364867&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark 0.9.1 MLLib 机器学习库简介 &lt;a href=&quot;http://rdc.taobao.org/?p=2163&quot;&gt;http://rdc.taobao.org/?p=2163&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark MLlib 概念 6：ALS（Alternating Least Squares） or (ALS-WR) &lt;a href=&quot;http://www.cnblogs.com/zwCHAN/p/4269027.html&quot;&gt;http://www.cnblogs.com/zwCHAN/p/4269027.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MLlib实践经验(1) &lt;a href=&quot;http://yanbohappy.sinaapp.com/?p=498&quot;&gt;http://yanbohappy.sinaapp.com/?p=498&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;研究机器学习之MLlib实践经验 &lt;a href=&quot;http://www.hengha.info/blog/home/view/id/31003&quot;&gt;http://www.hengha.info/blog/home/view/id/31003&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;协同过滤算法在MapReduce与Spark上实现对比 &lt;a href=&quot;http://data.qq.com/article?id=823&quot;&gt;http://data.qq.com/article?id=823&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ItemBased with Spark &lt;a href=&quot;http://weikey.me/articles/187.html&quot;&gt;http://weikey.me/articles/187.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Collaborative Filtering with Spark &lt;a href=&quot;http://www.slideshare.net/MrChrisJohnson/collaborative-filtering-with-spark&quot;&gt;http://www.slideshare.net/MrChrisJohnson/collaborative-filtering-with-spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Movie Recommendations and More With Spark  &lt;a href=&quot;http://mlnick.github.io/blog/2013/04/01/movie-recommendations-and-more-with-spark/&quot;&gt;http://mlnick.github.io/blog/2013/04/01/movie-recommendations-and-more-with-spark/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Movie Recommendation with Mllib &lt;a href=&quot;https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html&quot;&gt;https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;基于ALS算法的简易在线推荐系统 &lt;a href=&quot;http://ju.outofmemory.cn/entry/110756&quot;&gt;http://ju.outofmemory.cn/entry/110756&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ALS 在 Spark MLlib 中的实现 &lt;a href=&quot;http://dataunion.org/16856.html&quot;&gt;http://dataunion.org/16856.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;阿基米德项目ALS矩阵分解算法应用案例  &lt;a href=&quot;https://github.com/ceys/jdml/wiki/ALS&quot;&gt;https://github.com/ceys/jdml/wiki/ALS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ALS矩阵分解推荐模型 &lt;a href=&quot;http://www.wfuyu.com/server/22823.html&quot;&gt;http://www.wfuyu.com/server/22823.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;协同过滤之ALS-WR算法 &lt;a href=&quot;http://www.fuqingchuan.com/2015/03/812.html&quot;&gt;http://www.fuqingchuan.com/2015/03/812.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spark上矩阵运算库 &lt;a href=&quot;http://blog.csdn.net/u014252240/article/category/2384017&quot;&gt;http://blog.csdn.net/u014252240/article/category/2384017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;用MongoDB和Spark实现电影推荐 &lt;a href=&quot;http://www.infoq.com/cn/news/2014/12/mongdb-spark-movie-recommend&quot;&gt;http://www.infoq.com/cn/news/2014/12/mongdb-spark-movie-recommend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;基于Spark构建推荐引擎之一：基于物品的协同过滤推荐 &lt;a href=&quot;http://blog.csdn.net/sunbow0/article/details/42737541&quot;&gt;http://blog.csdn.net/sunbow0/article/details/42737541&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;2015的 spark-summit ，使用 Spark 实时推荐系统：&lt;a href=&quot;http://spark-summit.org/wp-content/uploads/2015/03/SSE15-18-Neumann-Alla.pdf&quot;&gt;http://spark-summit.org/wp-content/uploads/2015/03/SSE15-18-Neumann-Alla.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;基于PredictionIO的推荐引擎打造，及大规模多标签分类探索 &lt;a href=&quot;http://www.uml.org.cn/yunjisuan/2015041410.asp&quot;&gt;http://www.uml.org.cn/yunjisuan/2015041410.asp&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PDF：MLlib: Scalable Machine Learning on Spark &lt;a href=&quot;http://stanford.edu/~rezab/sparkworkshop/slides/xiangrui.pdf&quot;&gt;http://stanford.edu/~rezab/sparkworkshop/slides/xiangrui.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;使用Spark的MLlib、Hbase作为模型、Hive作数据清洗的核心推荐引擎,在Spark on Yarn测试通过 &lt;a href=&quot;https://github.com/wbj0110/spark_resrecomend&quot;&gt;https://github.com/wbj0110/spark_resrecomend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section&quot;&gt;推荐系统&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;推荐算法总结Recommendation &lt;a href=&quot;http://blog.csdn.net/oopsoom/article/details/33740799&quot;&gt;http://blog.csdn.net/oopsoom/article/details/33740799&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Collaborative Filtering and Recommender Systems By Navisro Analytics，里面有推荐系统的步骤   &lt;a href=&quot;http://www.slideshare.net/navisro/recommender-system-navisroanalytics?related=1&quot;&gt;http://www.slideshare.net/navisro/recommender-system-navisroanalytics?related=1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Item Based Collaborative Filtering Recommendation Algorithms &lt;a href=&quot;http://www.slideshare.net/nextlib/item-based-collaborative-filtering-recommendation-algorithms?related=2&quot;&gt;http://www.slideshare.net/nextlib/item-based-collaborative-filtering-recommendation-algorithms?related=2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;协同过滤CF推荐介绍 &lt;a href=&quot;http://blog.sina.com.cn/s/blog_6e0035ad0102v26h.html&quot;&gt;http://blog.sina.com.cn/s/blog_6e0035ad0102v26h.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Python 实现的机器学习库 scikit-learn：&lt;a href=&quot;http://scikit-learn.org/stable/index.html&quot;&gt;http://scikit-learn.org/stable/index.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://weibo.com/fly51fly?from=feed&amp;amp;loc=nickname&quot;&gt;@爱可可-爱生活&lt;/a&gt; 免费好书！《Practical Machine Learning: Innovations in Recommendation》机器学习&amp;amp;推荐系统：简单构建有效的推荐系统；借搜索技术创新应用部署大规模推荐系统；从实时数据中提取信息改进推荐系统的方法和技巧。超赞&amp;amp;推荐！ &lt;a href=&quot;https://www.mapr.com/practical-machine-learning&quot;&gt;https://www.mapr.com/practical-machine-learning&lt;/a&gt;  另:讨论推荐系统设计模式的文章: &lt;a href=&quot;https://www.mapr.com/blog/design-patterns-recommendation-systems-%E2%80%93-everyone-wants-pony&quot;&gt;https://www.mapr.com/blog/design-patterns-recommendation-systems-%E2%80%93-everyone-wants-pony&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://weibo.com/infoqchina?from=feed&amp;amp;loc=nickname&quot;&gt;@InfoQ&lt;/a&gt;推荐系统中最核心的数据之一是 user profile 数据。我们需要从大量历史用户行为中分析和挖掘各种维度的特征，来刻画用户的兴趣偏好。在QCon北京2015 @今日头条 架构师丁海峰，将分享中会介绍今日头条 user profile 系统的现状，面临的问题，系统演进，以及技术架构中的关键问题。&lt;a href=&quot;http://www.qconbeijing.com/track/2509&quot;&gt;http://www.qconbeijing.com/track/2509&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://vip.weibo.com/prividesc?priv=1006&amp;amp;from=feed&quot;&gt;@陈志武zwchen&lt;/a&gt; 电商网站用户，可分为两类：有购买欲望及明确购买目标，有购买欲望但无明确购买目标。前者为主动用户，决策较独立；后者为被动用户，需要被引导和刺激，协助其明确购买目标，如亚马逊强大的推荐系统，听说贡献了30%以上销售额。针对主动用户和被动用户，网站该如何设计呢？&lt;a href=&quot;http://zwchen.iteye.com/blog/1439259&quot;&gt;http://zwchen.iteye.com/blog/1439259&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;【重磅！大数据工程师 &lt;a href=&quot;http://weibo.com/n/%E9%A3%9E%E6%9E%97%E6%B2%99?from=feed&amp;amp;loc=at&quot;&gt;@飞林沙&lt;/a&gt; 的年终总结&amp;amp;算法数据的思考】一个优秀的推荐算法，一个优秀的推荐系统的确可以为企业创造很多价值，曾经和某知名电商网站的数据总监交流，他们的推荐系统实实在在地把销售额增加了15%，但是过于神话迷恋推荐算法和过于看扁推荐算法都是一种偏激的行为 &lt;a href=&quot;http://www.36dsj.com/archives/18821&quot;&gt;http://www.36dsj.com/archives/18821&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://weibo.com/1222789964/CaRadr8eR?from=page_1005051222789964_profile&amp;amp;wvr=6&amp;amp;mod=weibotime&quot;&gt;@AixinSG&lt;/a&gt; 今天读了两篇关于微博推荐的文章。对推荐系统了解不深，感觉微博推荐应该是个非常困难的问题。推荐的不是普通的item而是”人”, 一个人有多重身份，一般需要一个较长的熟悉过程，还有线上线下两个不同的交际圈子，增加了信息不对等。关注一个人也有累加的时间成本，得到的信息是否能抵消成本也是一个问题。 &lt;a href=&quot;http://www.weibo.com/p/1001603824878045290430&quot;&gt;也谈谈新浪微博可能感兴趣的人&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://weibo.com/3204614242/BfJye9tX5&quot;&gt;@朝花夕拾录&lt;/a&gt;：最近看了几个推荐系统的文章，&lt;a href=&quot;http://bigdata.memect.com/?tag=recommendationsystems&quot;&gt;http://bigdata.memect.com/?tag=recommendationsystems&lt;/a&gt; 有入门级的教程，讲解推荐系统的经典解决方案，还有进阶体验，介绍如何在大数据平台（Hadoop，spark, mogodb)上形成实时推荐。还有两个搞笑的文案调侃推荐系统的用户体验。&lt;/li&gt;
  &lt;li&gt;使用Oryx和CDH进行个性化推荐 &lt;a href=&quot;http://weikey.me/articles/222.html&quot;&gt;http://weikey.me/articles/222.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Myrrix 分布式推荐 &lt;a href=&quot;http://weikey.me/articles/197.html&quot;&gt;http://weikey.me/articles/197.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;使用Mahout Kmeans算法进行中文聚类 &lt;a href=&quot;http://weikey.me/articles/133.html&quot;&gt;http://weikey.me/articles/133.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;漫谈“推荐系统” &lt;a href=&quot;http://youngfor.me/post/recsys/man-tan-tui-jian-xi-tong&quot;&gt;http://youngfor.me/post/recsys/man-tan-tui-jian-xi-tong&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://semocean.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E6%96%87%E7%8C%AE%E5%8F%8A%E8%B5%84%E6%96%99/&quot;&gt;推荐系统经典论文文献及业界应用&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;自己动手写一个推荐系统 &lt;a href=&quot;http://www.cnblogs.com/flclain/archive/2013/03/03/2941397.html&quot;&gt;http://www.cnblogs.com/flclain/archive/2013/03/03/2941397.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;什么是好的推荐系统 &lt;a href=&quot;http://guoze.me/2015/01/29/good-recommendation/&quot;&gt;http://guoze.me/2015/01/29/good-recommendation/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;58同城的大数据环境下实现一个O2O通用推荐引擎的实践 &lt;a href=&quot;http://www.tuicool.com/articles/3MFBfq&quot;&gt;http://www.tuicool.com/articles/3MFBfq&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;58同城推荐系统架构设计与实现 &lt;a href=&quot;http://chuansong.me/n/949426&quot;&gt;http://chuansong.me/n/949426&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;构建一个基于del.icio.us的链接推荐系统 &lt;a href=&quot;http://lazynight.me/2740.html&quot;&gt;http://lazynight.me/2740.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;基于协同过滤构建简单推荐系统 &lt;a href=&quot;http://blog.csdn.net/database_zbye/article/details/8664516&quot;&gt;http://blog.csdn.net/database_zbye/article/details/8664516&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;使用Python简易推荐系统的构建 &lt;a href=&quot;http://linux.readthedocs.org/zh_CN/latest/docsource/publication/alpha2/#id7&quot;&gt;http://linux.readthedocs.org/zh_CN/latest/docsource/publication/alpha2/#id7&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;微博推荐算法简述 &lt;a href=&quot;http://www.wbrecom.com/?p=80&quot;&gt;http://www.wbrecom.com/?p=80&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;使用 Azure、Hadoop 和 Mahout 构建一个推荐系统 &lt;a href=&quot;http://www.oschina.net/translate/building-a-recommendation-engine-machine-learning&quot;&gt;http://www.oschina.net/translate/building-a-recommendation-engine-machine-learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Netflix的推荐和个性化系统架构  &lt;a href=&quot;http://blog.sina.com.cn/s/blog_7ebae53b0101bnuy.html&quot;&gt;http://blog.sina.com.cn/s/blog_7ebae53b0101bnuy.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;探索推荐引擎内部的秘密 &lt;a href=&quot;http://blog.sae.sina.com.cn/archives/2706&quot;&gt;http://blog.sae.sina.com.cn/archives/2706&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;美团推荐算法实践 &lt;a href=&quot;http://tech.meituan.com/mt-recommend-practice.html&quot;&gt;http://tech.meituan.com/mt-recommend-practice.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;百分点推荐引擎——从需求到架构 &lt;a href=&quot;http://www.infoq.com/cn/articles/baifendian-recommendation-engine&quot;&gt;http://www.infoq.com/cn/articles/baifendian-recommendation-engine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;QConShanghai2013-杨浩-360推荐系统实践.pdf &lt;a href=&quot;http://vdisk.weibo.com/s/A0GI9rYhL4I2&quot;&gt;http://vdisk.weibo.com/s/A0GI9rYhL4I2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;大规模电商推荐系统应用经验分享 &lt;a href=&quot;http://vdisk.weibo.com/s/udVGwtAAA4xz4&quot;&gt;http://vdisk.weibo.com/s/udVGwtAAA4xz4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;打造最适合产品的推荐系统 &lt;a href=&quot;http://vdisk.weibo.com/s/BPySloeWUaiZ4&quot;&gt;http://vdisk.weibo.com/s/BPySloeWUaiZ4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;爱奇艺推荐系统的整体技术架构 &lt;a href=&quot;http://edu.51cto.com/lesson/id-56393.html&quot;&gt;http://edu.51cto.com/lesson/id-56393.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;推荐系统架构小结 &lt;a href=&quot;http://blog.csdn.net/idonot/article/details/7996733&quot;&gt;http://blog.csdn.net/idonot/article/details/7996733&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;淘宝推荐系统的学习 &lt;a href=&quot;http://www.biaodianfu.com/taobao-recommendation-system.html&quot;&gt;http://www.biaodianfu.com/taobao-recommendation-system.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;协同过滤算法：在线推荐系统如何工作？ &lt;a href=&quot;http://www.csdn.net/article/2013-01-30/2814014-Collaborative-Filtering&quot;&gt;http://www.csdn.net/article/2013-01-30/2814014-Collaborative-Filtering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://semocean.com/%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0-%E4%BB%A5%E7%99%BE%E5%BA%A6%E5%85%B3%E9%94%AE%E8%AF%8D%E6%90%9C%E7%B4%A2%E6%8E%A8/&quot;&gt;一个完整推荐系统的设计实现-以百度关键词搜索推荐为例&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;活用您的 Big Data，實現線上服務行銷的精準推薦 &lt;a href=&quot;http://www.slideshare.net/etusolution/big-data-13084872&quot;&gt;http://www.slideshare.net/etusolution/big-data-13084872&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;推荐系统规划 &lt;a href=&quot;http://www.slideshare.net/2005000613/ss-16169751?qid=4f0d8f9f-8b65-4d60-9685-5464a4b7b731&quot;&gt;http://www.slideshare.net/2005000613/ss-16169751?qid=4f0d8f9f-8b65-4d60-9685-5464a4b7b731&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Github 上大数据/数据挖掘/推荐系统/机器学习相关资源 &lt;a href=&quot;https://github.com/Flowerowl/Big-Data-Resources&quot;&gt;https://github.com/Flowerowl/Big-Data-Resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nysrsp.cqvip.com/bitstream/123456789/153941/1/%E5%9F%BA%E4%BA%8E%E9%A1%B9%E7%9B%AE%E6%B5%81%E8%A1%8C%E5%BA%A6%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4TopN%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95&quot;&gt;基于项目流行度的协同过滤TopN推荐算法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;计算准确率、召回率、覆盖率 &lt;a href=&quot;http://wuchong.me/blog/2014/04/19/recsys-cf-study/   http://my.oschina.net/zhangjiawen/blog/185625&quot;&gt;http://wuchong.me/blog/2014/04/19/recsys-cf-study/   http://my.oschina.net/zhangjiawen/blog/185625&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;个性化推荐&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://weibo.com/fly51fly?from=feed&amp;amp;loc=nickname&quot;&gt;@爱可可-爱生活&lt;/a&gt;  [文章]《Personalized Recommendations at Etsy》&lt;a href=&quot;https://codeascraft.com/2014/11/17/personalized-recommendations-at-etsy/ &quot;&gt;https://codeascraft.com/2014/11/17/personalized-recommendations-at-etsy/ &lt;/a&gt;介绍Etsy采用的个性化推荐算法，包括矩阵分解、交替最小二乘、随机SVD和局部敏感哈希等&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://weibo.com/fly51fly?from=feed&amp;amp;loc=nickname&quot;&gt;@爱可可-爱生活&lt;/a&gt;  [文章]《Pinnability: Machine learning in the home feed》&lt;a href=&quot;http://engineering.pinterest.com/post/114138410669/pinnability-machine-learning-in-the-home-feed&quot;&gt;http://engineering.pinterest.com/post/114138410669/pinnability-machine-learning-in-the-home-feed&lt;/a&gt; 介绍Pinterest的Pinnability，基于机器学习提供个性化内容(推荐)列表&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://weibo.com/1403039654/C9D96rH4i&quot;&gt;@付聪_BenFrank&lt;/a&gt;：主流商品往往代表了绝大多数用户的需求，而长尾商品往往代表了一小部分用户推荐系统的个性化需求。因此，如果要通过发掘长尾提高销售额，就必须充分研究用户的兴趣，而这正是个性化推荐系统主要解决的问题。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://weibo.com/1869647165/Cadidojtl&quot;&gt;@6弗恩er&lt;/a&gt;：今日头条是一款基于数据化挖掘的个性化信息推荐引擎。根据微博行为、阅读行为、地理位置、职业年龄等挖掘出兴趣。用户每次动作后，10秒内更新用户模型。对每条信息提取几十个到几百个高维特征进行降维、相似计算、聚类等去重；通过大数据的处理进行个性化推荐，使用户无需设置，即可享受高质量信息&lt;/li&gt;
  &lt;li&gt;大数据系列文章第2篇——大数据之“用户行为分析”：&lt;a href=&quot;http://36kr.com/p/205901.html&quot;&gt;http://36kr.com/p/205901.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;个性化推荐技术的十大挑战：&lt;a href=&quot;http://www.programmer.com.cn/13824/&quot;&gt;http://www.programmer.com.cn/13824/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;个性化推荐系统的简单实现：&lt;a href=&quot;http://www.slideshare.net/ssusera62527/ss-36914732&quot;&gt;http://www.slideshare.net/ssusera62527/ss-36914732&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;用Kiji构建实时、个性化推荐系统：&lt;a href=&quot;http://www.infoq.com/cn/articles/kiji&quot;&gt;http://www.infoq.com/cn/articles/kiji&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;一种基于LBS的移动个性化推荐系统：&lt;a href=&quot;http://wenku.baidu.com/view/7f6bb028482fb4daa58d4b39.html&quot;&gt;http://wenku.baidu.com/view/7f6bb028482fb4daa58d4b39.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;基于大规模隐式反馈的个性化推荐  &lt;a href=&quot;http://www.jos.org.cn/html/2014/9/4648.htm&quot;&gt;http://www.jos.org.cn/html/2014/9/4648.htm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;流式处理&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;江南白衣Calvin 写的《Storm笔记》，非常详细：&lt;a href=&quot;http://calvin1978.blogcn.com/articles/stormnotes.html&quot;&gt;http://calvin1978.blogcn.com/articles/stormnotes.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2015/03/how-edmunds-com-used-spark-streaming-to-build-a-near-real-time-dashboard/&quot;&gt;How Edmunds.com Used Spark Streaming to Build a Near Real-Time Dashboard&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/panfeng412/tag/%E5%AE%9E%E6%97%B6%E6%B5%81%E8%AE%A1%E7%AE%97/&quot;&gt;Storm常见模式&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hadoop Streaming程序基础 &lt;a href=&quot;http://blog.pureisle.net/archives/1760.html&quot;&gt;http://blog.pureisle.net/archives/1760.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hadoop Streaming 实战： 输出文件分割 &lt;a href=&quot;http://blog.csdn.net/yfkiss/article/details/6406432&quot;&gt;http://blog.csdn.net/yfkiss/article/details/6406432&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hadoop Streaming原理及实践 &lt;a href=&quot;http://shiyanjun.cn/archives/336.html&quot;&gt;http://shiyanjun.cn/archives/336.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hadoop-Streaming实战经验及问题解决方法总结 &lt;a href=&quot;http://www.crazyant.net/1122.html&quot;&gt;http://www.crazyant.net/1122.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;总结：&lt;br /&gt;
作为一个程序员，最重要的能力是自我学习、归纳、总结，知识在于总结而不是分享。如何把大量看到的、听到的信息、知识、笔记等转化为自己的经验值，是需要认真考虑的一件事情。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
      <link>http://blog.javachen.com/2015/03/30/reading-list-2015-03.html</link>
      <guid>http://blog.javachen.com/2015/03/30/reading-list-2015-03.html</guid>
      <pubDate>2015-03-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spark SQL中的DataFrame</title>
      <description>&lt;p&gt;在2014年7月1日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上。在会议上，Databricks 表示，Shark 更多是对 Hive 的改造，替换了 Hive 的物理执行引擎，因此会有一个很快的速度。然而，不容忽视的是，Shark 继承了大量的 Hive 代码，因此给优化和维护带来了大量的麻烦。随着性能优化和先进分析整合的进一步加深，基于 MapReduce 设计的部分无疑成为了整个项目的瓶颈。 详细内容请参看 &lt;a href=&quot;http://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html&quot;&gt;Shark, Spark SQL, Hive on Spark, and the future of SQL on Spark&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Spark SQL 允许 Spark 执行用 SQL, HiveQL 或者 Scala 表示的关系查询。在 Spark 1.3 之前，这个模块的核心是一个新类型的 RDD-&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD&quot;&gt;SchemaRDD&lt;/a&gt;。 SchemaRDDs 由&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package@Row:org.apache.spark.sql.catalyst.expressions.Row.type&quot;&gt;行&lt;/a&gt;对象组成，行对象拥有一个模式（scheme） 来描述行中每一列的数据类型。SchemaRDD 与关系型数据库中的表很相似，可以通过存在的 RDD、一个 &lt;a href=&quot;http://parquet.io/&quot;&gt;Parquet&lt;/a&gt; 文件、结构化的文件、外部数据库、或者对存储在 Apache Hive 中的数据执行 HiveSQL 查询中创建。&lt;/p&gt;

&lt;p&gt;当前 Spark SQL 还处于 alpha 阶段，一些 API 在将将来的版本中可能会有所改变。例如，&lt;a href=&quot;http://www.infoq.com/cn/news/2015/03/apache-spark-1.3-released&quot;&gt;Apache Spark 1.3发布，新增Data Frames API，改进Spark SQL和MLlib&lt;/a&gt;。在 Spark 1.3 中，SchemaRDD 改为叫做 DataFrame。&lt;/p&gt;

&lt;p&gt;本文是基于 Spark 1.3 写成，特此说明。&lt;/p&gt;

&lt;h1 id=&quot;sqlcontext&quot;&gt;创建 SQLContext&lt;/h1&gt;

&lt;p&gt;Spark SQL 中所有相关功能的入口点是 &lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext&quot;&gt;SQLContext&lt;/a&gt; 类或者它的子类， 创建一个 SQLContext 的所有需要仅仅是一个 SparkContext。&lt;/p&gt;

&lt;p&gt;使用 Scala 创建方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 Java 创建方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;JavaSparkContext sc = ...; // An existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 Python 创建方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了一个基本的 SQLContext，你也能够创建一个 HiveContext，它支持基本 SQLContext 所支持功能的一个超集。它的额外的功能包括用更完整的 HiveQL 分析器写查询去访问 HiveUDFs 的能力、 从 Hive 表读取数据的能力。用 HiveContext 你不需要一个已经存在的 Hive 开启，SQLContext 可用的数据源对 HiveContext 也可用。HiveContext 分开打包是为了避免在 Spark 构建时包含了所有 的 Hive 依赖。如果对你的应用程序来说，这些依赖不存在问题，Spark 1.3 推荐使用 HiveContext。以后的稳定版本将专注于为 SQLContext 提供与 HiveContext 等价的功能。&lt;/p&gt;

&lt;p&gt;用来解析查询语句的特定 SQL 变种语言可以通过 &lt;code&gt;spark.sql.dialect&lt;/code&gt; 选项来选择。这个参数可以通过两种方式改变，一种方式是通过 &lt;code&gt;setConf&lt;/code&gt; 方法设定，另一种方式是在 SQL 命令中通过 &lt;code&gt;SET key=value&lt;/code&gt; 来设定。对于 SQLContext，唯一可用的方言是 “sql”，它是 Spark SQL 提供的一个简单的 SQL 解析器。在 HiveContext 中，虽然也支持”sql”，但默认的方言是 “hiveql”，这是因为 HiveQL 解析器更完整。&lt;/p&gt;

&lt;h1 id=&quot;dataframe&quot;&gt;创建 DataFrame&lt;/h1&gt;

&lt;p&gt;使用 SQLContext，应用可以从一个存在的 RDD、Hive 表或者数据源中创建 DataFrame。&lt;/p&gt;

&lt;p&gt;下载测试数据 &lt;a href=&quot;https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json&quot;&gt;people.json&lt;/a&gt;，并将其上传到 HDFS 上：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json
$ hadoop fs -put people.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是使用 Scala 创建方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val sqlContext = new org.apache.spark.sql.SQLContext(sc)

val df = sqlContext.jsonFile(&quot;people.json&quot;)

// Displays the content of the DataFrame to stdout
df.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是使用 Java 创建方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;JavaSparkContext sc = ...; // An existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

DataFrame df = sqlContext.jsonFile(&quot;people.json&quot;);

// Displays the content of the DataFrame to stdout
df.show();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是使用 Python 创建方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

df = sqlContext.jsonFile(&quot;people.json&quot;)

# Displays the content of the DataFrame to stdout
df.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DataFrame API 请参考 &lt;a href=&quot;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame&quot;&gt;Scala&lt;/a&gt;、&lt;a href=&quot;https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/DataFrame.html&quot;&gt;Java&lt;/a&gt; 以及 &lt;a href=&quot;https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame&quot;&gt;Python&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;dataframe-&quot;&gt;DataFrame 操作&lt;/h2&gt;

&lt;p&gt;运行 spark-shell 执行下面代码进行测试，运行的代码和输出结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;$ spark-shell
Spark context available as sc.
SQL context available as sqlContext.

// Create the DataFrame
scala&amp;gt; val df = sqlContext.jsonFile(&quot;people.json&quot;)

scala&amp;gt; df.count()
res1: Long = 3

scala&amp;gt; df.first()
res2: org.apache.spark.sql.Row = [null,Michael]

scala&amp;gt; df.head()
res3: org.apache.spark.sql.Row = [null,Michael]

scala&amp;gt; df.collect()
res4: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy], [19,Justin])

scala&amp;gt; df.collectAsList()
res5: java.util.List[org.apache.spark.sql.Row] = [[null,Michael], [30,Andy], [19,Justin]]

// Show the content of the DataFrame
scala&amp;gt; df.show()
age  name
null Michael
30   Andy
19   Justin

scala&amp;gt; df.take(2)
res6: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy])

scala&amp;gt; df.columns
res7: Array[String] = Array(age, name)

scala&amp;gt; df.dtypes
res8: Array[(String, String)] = Array((age,LongType), (name,StringType))

// Print the schema in a tree format
scala&amp;gt; df.printSchema()
root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)

scala&amp;gt; df.explain()
== Physical Plan ==
PhysicalRDD [age#0L,name#1], MapPartitionsRDD[96] at map at JsonRDD.scala:41

// age column
scala&amp;gt; val ageCol = df(&quot;age&quot;)  

// The following creates a new column that increases everybody&#39;s age by 10.
scala&amp;gt; df(&quot;age&quot;) + 10 

// Select only the &quot;name&quot; column
scala&amp;gt; df.select(&quot;name&quot;).show()
name
Michael
Andy
Justin

// Select everybody, but increment the age by 1
scala&amp;gt; df.select(df(&quot;name&quot;), df(&quot;age&quot;)+1).show()
name    (age + 1)
Michael null
Andy    31
Justin  20

// Select people older than 21
scala&amp;gt; df.filter(df(&quot;age&quot;) &amp;gt; 21).show()
age name
30  Andy

// Count people by age
scala&amp;gt; df.groupBy(&quot;age&quot;).count().show()
age  count
null 1
19   1
30   1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sql-&quot;&gt;运行 SQL 查询&lt;/h2&gt;

&lt;p&gt;SQLContext 有一个 sql 方法，可以运行 SQL 查询。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;sqlContext.sql(&quot;SELECT * FROM table&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spark SQL 支持两种方法将存在的 RDD 转换为 DataFrame 。第一种方法使用反射来推断包含特定对象类型的 RDD 的模式。在你写 spark 程序的同时，当你已经知道了模式，这种基于反射的方法可以使代码更简洁并且程序工作得更好。&lt;/p&gt;

&lt;p&gt;第二种方法是通过一个编程接口来实现，这个接口允许你构造一个模式，然后在存在的 RDD 上使用它。虽然这种方法更冗长，但是它允许你在运行期之前不知道列以及列的类型的情况下构造 DataFrame。&lt;/p&gt;

&lt;p&gt;SQLContext 的 API 见 &lt;a href=&quot;https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.SQLContext&quot;&gt;SQLContext&lt;/a&gt; 。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;利用反射推断模式&lt;/h3&gt;

&lt;p&gt;Spark SQL的 Scala 接口支持将包含样本类的 RDD 自动转换为 DataFrame。这个样本类定义了表的模式。样本类的参数名字通过反射来读取，然后作为列的名字。样本类可以嵌套或者包含复杂的类型如序列或者数组。这个 RDD 可以隐式转化为一个 DataFrame，然后注册为一个表，表可以在后续的 sql 语句中使用。&lt;/p&gt;

&lt;p&gt;以 &lt;a href=&quot;https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.txt&quot;&gt;people.txt&lt;/a&gt; 作为测试数据，使用 Scala 语言来创建 DataFrame：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

// Define the schema using a case class.
// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
// you can use custom classes that implement the Product interface.
case class People(name: String, age: Int)

// Create an RDD of Person objects and register it as a table.
val people = sc.textFile(&quot;people.txt&quot;).map(_.split(&quot;,&quot;)).map(p =&amp;gt; People(p(0), p(1).trim.toInt)).toDF()
people.registerTempTable(&quot;people&quot;)

// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sqlContext.sql(&quot;SELECT name FROM people WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;)

// The results of SQL queries are DataFrames and support all the normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
teenagers.map(t =&amp;gt; &quot;Name: &quot; + t(0)).collect().foreach(println)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 Java 语言，需要创建一个 JavaBean，然后在将数据映射到它上面：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public static class People implements Serializable {
  private String name;
  private int age;

  public String getName() {
    return name;
  }

  public void setName(String name) {
    this.name = name;
  }

  public int getAge() {
    return age;
  }

  public void setAge(int age) {
    this.age = age;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，使用 sqlContext 的 createDataFrame 方法，从 JavaBean 和数据上创建一个 DataFrame 并注册一个表，下面是一个比较完整的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;

import java.io.Serializable;
import java.util.Arrays;
import java.util.List;

public class JavaSparkSQLByReflection {
    public static void main(String[] args) throws Exception {
        SparkConf sparkConf = new SparkConf().setAppName(&quot;JavaSparkSQLByReflection&quot;);
        JavaSparkContext ctx = new JavaSparkContext(sparkConf);
        SQLContext sqlCtx = new SQLContext(ctx);

        System.out.println(&quot;=== Data source: RDD ===&quot;);
        // Load a text file and convert each line to a Java Bean.
        JavaRDD&amp;lt;People&amp;gt; people = ctx.textFile(&quot;people.txt&quot;).map(
                new Function&amp;lt;String, People&amp;gt;() {
                    @Override
                    public People call(String line) {
                        String[] parts = line.split(&quot;,&quot;);

                        People people = new People();
                        people.setName(parts[0]);
                        people.setAge(Integer.parseInt(parts[1].trim()));
                        return people;
                    }
                });

        // Apply a schema to an RDD of Java Beans and register it as a table.
        DataFrame schemaPeople = sqlCtx.createDataFrame(people, People.class);
        schemaPeople.registerTempTable(&quot;people&quot;);

        // SQL can be run over RDDs that have been registered as tables.
        DataFrame teenagers = sqlCtx.sql(&quot;SELECT name FROM people WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;);

        // The results of SQL queries are DataFrames and support all the normal RDD operations.
        // The columns of a row in the result can be accessed by ordinal.
        List&amp;lt;String&amp;gt; teenagerNames = teenagers.toJavaRDD().map(new Function&amp;lt;Row, String&amp;gt;() {
            @Override
            public String call(Row row) {
                return &quot;Name: &quot; + row.getString(0);
            }
        }).collect();

        for (String name : teenagerNames) {
            System.out.println(name);
        }


        System.out.println(&quot;=== Data source: Parquet File ===&quot;);
        // DataFrames can be saved as parquet files, maintaining the schema information.
        schemaPeople.saveAsParquetFile(&quot;people.parquet&quot;);

        // Read in the parquet file created above.
        // Parquet files are self-describing so the schema is preserved.
        // The result of loading a parquet file is also a DataFrame.
        DataFrame parquetFile = sqlCtx.parquetFile(&quot;people.parquet&quot;);

        //Parquet files can also be registered as tables and then used in SQL statements.
        parquetFile.registerTempTable(&quot;parquetFile&quot;);
        DataFrame teenagers2 =
                sqlCtx.sql(&quot;SELECT name FROM parquetFile WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;);
        teenagerNames = teenagers2.toJavaRDD().map(new Function&amp;lt;Row, String&amp;gt;() {
            @Override
            public String call(Row row) {
                return &quot;Name: &quot; + row.getString(0);
            }
        }).collect();
        for (String name : teenagerNames) {
            System.out.println(name);
        }

        System.out.println(&quot;=== Data source: JSON Dataset ===&quot;);
        // A JSON dataset is pointed by path.
        // The path can be either a single text file or a directory storing text files.
        String path = &quot;people.json&quot;;
        // Create a DataFrame from the file(s) pointed by path
        DataFrame peopleFromJsonFile = sqlCtx.jsonFile(path);

        // Because the schema of a JSON dataset is automatically inferred, to write queries,
        // it is better to take a look at what is the schema.
        peopleFromJsonFile.printSchema();
        // The schema of people is ...
        // root
        //  |-- age: IntegerType
        //  |-- name: StringType

        // Register this DataFrame as a table.
        peopleFromJsonFile.registerTempTable(&quot;people&quot;);

        // SQL statements can be run by using the sql methods provided by sqlCtx.
        DataFrame teenagers3 = sqlCtx.sql(&quot;SELECT name FROM people WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;);

        // The results of SQL queries are DataFrame and support all the normal RDD operations.
        // The columns of a row in the result can be accessed by ordinal.
        teenagerNames = teenagers3.toJavaRDD().map(new Function&amp;lt;Row, String&amp;gt;() {
            @Override
            public String call(Row row) {
                return &quot;Name: &quot; + row.getString(0);
            }
        }).collect();
        for (String name : teenagerNames) {
            System.out.println(name);
        }

        // Alternatively, a DataFrame can be created for a JSON dataset represented by
        // a RDD[String] storing one JSON object per string.
        List&amp;lt;String&amp;gt; jsonData = Arrays.asList(
                &quot;{\&quot;name\&quot;:\&quot;Yin\&quot;,\&quot;address\&quot;:{\&quot;city\&quot;:\&quot;Columbus\&quot;,\&quot;state\&quot;:\&quot;Ohio\&quot;}}&quot;);
        JavaRDD&amp;lt;String&amp;gt; anotherPeopleRDD = ctx.parallelize(jsonData);
        DataFrame peopleFromJsonRDD = sqlCtx.jsonRDD(anotherPeopleRDD.rdd());


        // Take a look at the schema of this new DataFrame.
        peopleFromJsonRDD.printSchema();
        // The schema of anotherPeople is ...
        // root
        //  |-- address: StructType
        //  |    |-- city: StringType
        //  |    |-- state: StringType
        //  |-- name: StringType
        peopleFromJsonRDD.registerTempTable(&quot;people2&quot;);

        DataFrame peopleWithCity = sqlCtx.sql(&quot;SELECT name, address.city FROM people2&quot;);
        List&amp;lt;String&amp;gt; nameAndCity = peopleWithCity.toJavaRDD().map(new Function&amp;lt;Row, String&amp;gt;() {
            @Override
            public String call(Row row) {
                return &quot;Name: &quot; + row.getString(0) + &quot;, City: &quot; + row.getString(1);
            }
        }).collect();
        for (String name : nameAndCity) {
            System.out.println(name);
        }

        ctx.stop();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 Python 语言则需要用到 sqlContext 的 inferSchema 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# sc is an existing SparkContext.
from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a Row.
lines = sc.textFile(&quot;people.txt&quot;)
parts = lines.map(lambda l: l.split(&quot;,&quot;))
people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))

# Infer the schema, and register the DataFrame as a table.
schemaPeople = sqlContext.inferSchema(people)
schemaPeople.registerTempTable(&quot;people&quot;)

# SQL can be run over DataFrames that have been registered as a table.
teenagers = sqlContext.sql(&quot;SELECT name FROM people WHERE age &amp;gt;= 13 AND age &amp;lt;= 19&quot;)

# The results of SQL queries are RDDs and support all the normal RDD operations.
teenNames = teenagers.map(lambda p: &quot;Name: &quot; + p.name)
for teenName in teenNames.collect():
  print teenName
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-1&quot;&gt;编程指定模式&lt;/h3&gt;

&lt;p&gt;当样本类不能提前确定（例如，记录的结构是经过编码的字符串，或者一个文本集合将会被解析，不同的字段投影给不同的用户），一个 DataFrame 可以通过三步来创建。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;从原来的 RDD 创建一个行的 RDD&lt;/li&gt;
  &lt;li&gt;创建由一个 StructType 表示的模式与第一步创建的 RDD 的行结构相匹配&lt;/li&gt;
  &lt;li&gt;在行 RDD 上通过 applySchema 方法应用模式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;直接贴出代码，Scala 语言创建方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val sc = new SparkContext(new SparkConf().setAppName(&quot;ScalaSparkSQL&quot;))
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// Create an RDD
val people = sc.textFile(&quot;people.txt&quot;)

// The schema is encoded in a string
val schemaString = &quot;name age&quot;

// Import Spark SQL data types and Row.
import org.apache.spark.sql._

// Generate the schema based on the string of schema
val schema =
  StructType(
    schemaString.split(&quot; &quot;).map(fieldName =&amp;gt; StructField(fieldName, StringType, true)))

// Convert records of the RDD (people) to Rows.
val rowRDD = people.map(_.split(&quot;,&quot;)).map(p =&amp;gt; Row(p(0), p(1).trim))

// Apply the schema to the RDD.
val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)

// Register the DataFrames as a table.
peopleDataFrame.registerTempTable(&quot;people&quot;)

// SQL statements can be run by using the sql methods provided by sqlContext.
val results = sqlContext.sql(&quot;SELECT name FROM people&quot;)

// The results of SQL queries are DataFrames and support all the normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
results.map(t =&amp;gt; &quot;Name: &quot; + t(0)).collect().foreach(println)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java 创建的方式或许对一个 Java 程序员来说，更容易理解：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;

import java.util.List;

public class JavaSparkSQLBySchema {
    public static void main(String[] args) throws Exception {
        SparkConf sparkConf = new SparkConf().setAppName(&quot;JavaSparkSQLBySchema&quot;);
        JavaSparkContext ctx = new JavaSparkContext(sparkConf);
        SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

        // Load a text file and convert each line to a JavaBean.
        JavaRDD&amp;lt;String&amp;gt; people = sc.textFile(&quot;people.txt&quot;);

        // The schema is encoded in a string
        String schemaString = &quot;name age&quot;;

        // Generate the schema based on the string of schema
        List&amp;lt;StructField&amp;gt; fields = new ArrayList&amp;lt;StructField&amp;gt;();
        for (String fieldName : schemaString.split(&quot; &quot;)) {
            fields.add(DataType.createStructField(fieldName, DataType.StringType, true));
        }
        StructType schema = DataType.createStructType(fields);

        // Convert records of the RDD (people) to Rows.
        JavaRDD&amp;lt;Row&amp;gt; rowRDD = people.map(
                new Function&amp;lt;String, Row&amp;gt;() {
                    public Row call(String record) throws Exception {
                        String[] fields = record.split(&quot;,&quot;);
                        return Row.create(fields[0], fields[1].trim());
                    }
                });

        // Apply the schema to the RDD.
        DataFrame peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema);

        // Register the DataFrame as a table.
        peopleDataFrame.registerTempTable(&quot;people&quot;);

        // SQL can be run over RDDs that have been registered as tables.
        DataFrame results = sqlContext.sql(&quot;SELECT name FROM people&quot;);

        // The results of SQL queries are DataFrames and support all the normal RDD operations.
        // The columns of a row in the result can be accessed by ordinal.
        List&amp;lt;String&amp;gt; names = results.map(new Function&amp;lt;Row, String&amp;gt;() {
            public String call(Row row) {
                return &quot;Name: &quot; + row.getString(0);
            }
        }).collect();
    }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python 语言的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Import SQLContext and data types
from pyspark.sql import *

# sc is an existing SparkContext.
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a tuple.
lines = sc.textFile(&quot;people.txt&quot;)
parts = lines.map(lambda l: l.split(&quot;,&quot;))
people = parts.map(lambda p: (p[0], p[1].strip()))

# The schema is encoded in a string.
schemaString = &quot;name age&quot;

fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields)

# Apply the schema to the RDD.
schemaPeople = sqlContext.createDataFrame(people, schema)

# Register the DataFrame as a table.
schemaPeople.registerTempTable(&quot;people&quot;)

# SQL can be run over DataFrames that have been registered as a table.
results = sqlContext.sql(&quot;SELECT name FROM people&quot;)

# The results of SQL queries are RDDs and support all the normal RDD operations.
names = results.map(lambda p: &quot;Name: &quot; + p.name)
for name in names.collect():
  print name
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;本文主要介绍了 DataFrame 是什么以及两种从 RDD 创建 DataFrame 的方法，完整的代码见 &lt;a href=&quot;https://github.com/javachen/spark-examples&quot;&gt;Github&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes&quot;&gt;Spark SQL and DataFrame Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://endymecy.gitbooks.io/spark-programming-guide-zh-cn/content/spark-sql/README.html&quot;&gt;Spark 编程指南简体中文版-Spark SQL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/03/26/spark-sql-dataframe.html</link>
      <guid>http://blog.javachen.com/2015/03/26/spark-sql-dataframe.html</guid>
      <pubDate>2015-03-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>将Avro数据转换为Parquet格式</title>
      <description>&lt;p&gt;本文主要测试将Avro数据转换为Parquet格式的过程并查看 Parquet 文件的 schema 和元数据。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;准备&lt;/h1&gt;

&lt;p&gt;将文本数据转换为 Parquet 格式并读取内容，可以参考 Cloudera 的 MapReduce 例子：&lt;a href=&quot;https://github.com/cloudera/parquet-examples&quot;&gt;https://github.com/cloudera/parquet-examples&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;准备文本数据 a.txt 为 CSV 格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1,2
3,4
4,5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;准备 Avro 测试数据，可以参考 &lt;a href=&quot;/2015/03/25/how-to-load-some-avro-data-into-spark.html&quot;&gt;将Avro数据加载到Spark&lt;/a&gt; 一文。&lt;/p&gt;

&lt;p&gt;本文测试环境为：CDH 5.2，并且 Avro、Parquet 组件已经通过 YUM 源安装。&lt;/p&gt;

&lt;h1 id=&quot;csv--parquet&quot;&gt;将 CSV 转换为 Parquet&lt;/h1&gt;

&lt;p&gt;在 Hive 中创建一个表并导入数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create table mycsvtable (x int, y int)
row format delimited
FIELDS TERMINATED BY &#39;,&#39;
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH &#39;a.txt&#39; OVERWRITE INTO TABLE mycsvtable;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 Parquet 表并转换数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create table myparquettable (a INT, b INT)
STORED AS PARQUET
LOCATION &#39;/tmp/data&#39;;

insert overwrite table myparquettable select * from mycsvtable;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 hdfs 上生成的 myparquettable 表的数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hadoop fs -ls /tmp/data
Found 1 items
-rwxrwxrwx   3 hive hadoop        331 2015-03-25 15:50 /tmp/data/000000_0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 hive 中查看 myparquettable 表的数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive (default)&amp;gt; select * from myparquettable;
OK
myparquettable.a  myparquettable.b
1 2
3 4
4 5
Time taken: 0.149 seconds, Fetched: 3 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 /tmp/data/000000_0 文件的 schema ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop parquet.tools.Main schema /tmp/data/000000_0
message hive_schema {
  optional int32 a;
  optional int32 b;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 /tmp/data/000000_0 文件的元数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop parquet.tools.Main meta /tmp/data/000000_0
creator:     parquet-mr version 1.5.0-cdh5.2.0 (build 8e266e052e423af5 [more]...

file schema: hive_schema
--------------------------------------------------------------------------------
a:           OPTIONAL INT32 R:0 D:1
b:           OPTIONAL INT32 R:0 D:1

row group 1: RC:3 TS:102
--------------------------------------------------------------------------------
a:            INT32 UNCOMPRESSED DO:0 FPO:4 SZ:51/51/1.00 VC:3 ENC:BIT [more]...
b:            INT32 UNCOMPRESSED DO:0 FPO:55 SZ:51/51/1.00 VC:3 ENC:BI [more]...
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;avro--parquet&quot;&gt;将 Avro 转换为 Parquet&lt;/h1&gt;

&lt;p&gt;使用 &lt;a href=&quot;/2015/03/25/how-to-load-some-avro-data-into-spark.html&quot;&gt;将Avro数据加载到Spark&lt;/a&gt;  中的 schema 和 json 数据，从 json 数据生成 avro 数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar /usr/lib/avro/avro-tools.jar fromjson --schema-file twitter.avsc twitter.json &amp;gt; twitter.avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 twitter.avsc 和 twitter.avro 上传到 hdfs：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -put twitter.avsc
$ hadoop fs -put twitter.avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 https://github.com/laserson/avro2parquet 将 avro 转换为 parquet 格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop jar avro2parquet.jar twitter.avsc  twitter.avro /tmp/out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在 hive 中创建表并导入数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create table tweets_parquet (username string, tweet string, timestamp bigint) 
STORED AS PARQUET;

load data inpath &#39;/tmp/out/part-m-00000.snappy.parquet&#39; overwrite into table tweets_parquet;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，可以查询数据并查看 parquet 文件的 schema 和元数据，方法同上文。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.bigdatatidbits.cc/2015/03/converting-avro-data-to-parquet-format.html&quot;&gt;Converting Avro data to Parquet format in Hadoop&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/03/25/converting-avro-data-to-parquet-format.html</link>
      <guid>http://blog.javachen.com/2015/03/25/converting-avro-data-to-parquet-format.html</guid>
      <pubDate>2015-03-25T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>如何将Avro数据加载到Spark</title>
      <description>&lt;p&gt;这是一篇翻译，原文来自：&lt;a href=&quot;http://www.bigdatatidbits.cc/2015/01/how-to-load-some-avro-data-into-spark.html&quot;&gt;How to load some Avro data into Spark&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;avro-&quot;&gt;首先，为什么使用 Avro ？&lt;/h1&gt;

&lt;p&gt;最基本的格式是 CSV ，其廉价并且不需要顶一个一个 schema 和数据关联。&lt;/p&gt;

&lt;p&gt;随后流行起来的一个通用的格式是 XML，其有一个 schema 和 数据关联，XML 广泛的使用于 Web Services 和 SOA 架构中。不幸的是，其非常冗长，并且解析 XML 需要消耗内存。&lt;/p&gt;

&lt;p&gt;另外一种格式是 JSON，其非常流行易于使用因为它非常方便易于理解。&lt;/p&gt;

&lt;p&gt;这些格式在 Big Data 环境中都是不可拆分的，这使得他们难于使用。在他们之上使用一个压缩机制（Snappy，Gzip）并不能解决这个问题。&lt;/p&gt;

&lt;p&gt;因此不同的数据格式出现了。Avro 作为一种序列化平台被广泛使用，因为它能跨语言，提供了一个小巧紧凑的快速的二进制格式，支持动态 schema 发现（通过它的泛型）和 schema 演变，并且是可压缩和拆分的。它还提供了复杂的数据结构，例如嵌套类型。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;例子&lt;/h1&gt;

&lt;p&gt;让我们来看一个例子，创建一个 Avro schema 并生成一些数据。在一个真实案例的例子中，组织机构通常有一些更加普通的格式，例如 XML，的数据，并且他们需要通过一些工具例如  &lt;a href=&quot;http://www.infoq.com/articles/AVROSchemaJAXB&quot;&gt;JAXB&lt;/a&gt; 将他们的数据转换成 Avro。我们来使用&lt;a href=&quot;http://www.michael-noll.com/blog/2013/03/17/reading-and-writing-avro-files-from-the-command-line/&quot;&gt;这个例子&lt;/a&gt;，其中 twitter.avsc 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
   &quot;type&quot; : &quot;record&quot;,
   &quot;name&quot; : &quot;twitter_schema&quot;,
   &quot;namespace&quot; : &quot;com.miguno.avro&quot;,
   &quot;fields&quot; : [
        {     &quot;name&quot; : &quot;username&quot;,
               &quot;type&quot; : &quot;string&quot;,
              &quot;doc&quot;  : &quot;Name of the user account on Twitter.com&quot;   },
         {
             &quot;name&quot; : &quot;tweet&quot;,
             &quot;type&quot; : &quot;string&quot;,
             &quot;doc&quot;  : &quot;The content of the user&#39;s Twitter message&quot;   },
         {
             &quot;name&quot; : &quot;timestamp&quot;,
             &quot;type&quot; : &quot;long&quot;,
             &quot;doc&quot;  : &quot;Unix epoch time in seconds&quot;   } 
    ],
   &quot;doc:&quot; : &quot;A basic schema for storing Twitter messages&quot; 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;twitter.json 中有一些数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;username&quot;:&quot;miguno&quot;,&quot;tweet&quot;:&quot;Rock: Nerf paper, scissors is fine.&quot;,&quot;timestamp&quot;: 1366150681 } 
{&quot;username&quot;:&quot;BlizzardCS&quot;,&quot;tweet&quot;:&quot;Works as intended.  Terran is IMBA.&quot;,&quot;timestamp&quot;: 1366154481 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们将这些数据转换成二进制的 Avro 格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar ~/avro-tools-1.7.7.jar fromjson --schema-file twitter.avsc twitter.json &amp;gt; twitter.avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，我们将 Avro 数据转换为 Java：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar /app/avro/avro-tools-1.7.7.jar compile schema /app/avro/data/twitter.avsc /app/avro/data/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，我们编译这些类并将其打包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ CLASSPATH=/app/avro/avro-1.7.7-javadoc.jar:/app/avro/avro-mapred-1.7.7-hadoop1.jar:/app/avro/avro-tools-1.7.7.jar
$ javac -classpath $CLASSPATH /app/avro/data/com/miguno/avro/twitter_schema.java
$ jar cvf Twitter.jar com/miguno/avro/*.class
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们启动 Spark，并将上面创建的 Jar 和一些需要的库（Hadoop 和 Avro）传递给 Spark 程序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./bin/spark-shell --jars /app/avro/avro-mapred-1.7.7-hadoop1.jar,/avro/avro-1.7.7.jar,/app/avro/data/Twitter.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 REPL 中，我们获取数据并创建一个 RDD：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt;
import com.miguno.avro.twitter_schema
import org.apache.avro.file.DataFileReader;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.io.DatumReader;
import org.apache.avro.io.DatumWriter;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.mapreduce.AvroKeyInputFormat
import org.apache.avro.mapred.AvroKey
import org.apache.hadoop.io.NullWritable
import org.apache.avro.mapred.AvroInputFormat
import org.apache.avro.mapred.AvroWrapper
import org.apache.avro.generic.GenericRecord
import org.apache.avro.mapred.{AvroInputFormat, AvroWrapper}
import org.apache.hadoop.io.NullWritable


val path = &quot;/app/avro/data/twitter.avro&quot;
val avroRDD = sc.hadoopFile[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](path)
avroRDD.map(l =&amp;gt; new String(l._1.datum.get(&quot;username&quot;).toString() ) ).first
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;res2: String = miguno
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一些注意事项：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;我们在使用 MR1 的类，但是 MR2的类同样能够运行。&lt;/li&gt;
  &lt;li&gt;我们使用GenericRecord 而不是 Specific ，因为我们生成了 Avro schema（并且导入了它）。更多内容参见 &lt;a href=&quot;http://avro.apache.org/docs/current/gettingstartedjava.html&quot;&gt;http://avro.apache.org/docs/current/gettingstartedjava.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;注意到即使 Avro 类是用 Java 编译的，你还是可以在 Spark 中导入他们，因为 Scala 也是运行在 JVM 之上。&lt;/li&gt;
  &lt;li&gt;Avro 允许你定义一个可选的方式去定义 schema 中每个节点的反序列化类型，即通过 key/value 的键值对，这是方式非常方便。参考 &lt;a href=&quot;http://stackoverflow.com/questions/27827649/trying-to-deserialize-avro-in-spark-with-specific-type/27859980?noredirect=1%23comment44240726_27859980&quot;&gt;http://stackoverflow.com/questions/27827649/trying-to-deserialize-avro-in-spark-with-specific-type/27859980?noredirect=1%23comment44240726_27859980&lt;/a&gt; 。&lt;/li&gt;
  &lt;li&gt;还有大量的其他方式来实现这个功能，一种是使用 Kryo，另一种是使用 Spark SQL。然而，这需要你创建一个 Spark SQL 的上下文（见 &lt;a href=&quot;https://github.com/databricks/spark-avro&quot;&gt;https://github.com/databricks/spark-avro&lt;/a&gt; ），而不是一个纯粹的 Spark/Scala  方式。然而，也许这在将来会是一种最佳方式？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;翻译结束。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;接下来，我将上述过程在 CDH 5.3 集群中测试一遍。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;验证&lt;/h1&gt;

&lt;p&gt;首先，在集群一个节点创建 twitter.avsc 和 twitter.json 两个文件。&lt;/p&gt;

&lt;p&gt;然后，使用 avro-tools 将这些数据转换成二进制的 Avro 格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -jar /usr/lib/avro/avro-tools.jar fromjson --schema-file twitter.avsc twitter.json &amp;gt; twitter.avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候会生成 avro 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ll
总用量 12
-rw-r--r-- 1 root root 543  3月 25 15:13 twitter.avro
-rw-r--r-- 1 root root 590  3月 25 15:12 twitter.avsc
-rw-r--r-- 1 root root 191  3月 25 15:12 twitter.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 Avro 数据转换为 Java：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -jar /usr/lib/avro/avro-tools.jar compile schema twitter.avsc .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候会生成 twitter_schema.java 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tree
.
├── com
│   └── miguno
│       └── avro
│           └── twitter_schema.java
├── twitter.avro
├── twitter.avsc
└── twitter.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候会生成一个 Twitter.jar 的 jar 包。&lt;/p&gt;

&lt;p&gt;编译这些类并将其打包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ CLASSPATH=/usr/lib/avro/avro-mapred-hadoop2.jar:/usr/lib/avro/avro-tools.jar
$ javac -classpath $CLASSPATH com/miguno/avro/twitter_schema.java
$ jar cvf Twitter.jar com/miguno/avro/*.class
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在当前目录，运行 spark-shell:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;spark-shell --jars /usr/lib/avro/avro-mapred-hadoop2.jar,/usr/lib/avro/avro.jar,Twitter.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 twitter.avro 上传到 hdfs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop fs -put twitter.avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 REPL 中，我们创建一个 RDD 并查看结果是否和上面一致：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt;
import com.miguno.avro.twitter_schema;
import org.apache.avro.file.DataFileReader;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.io.DatumReader;
import org.apache.avro.io.DatumWriter;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.mapreduce.AvroKeyInputFormat
import org.apache.avro.mapred.AvroKey
import org.apache.hadoop.io.NullWritable
import org.apache.avro.mapred.AvroInputFormat
import org.apache.avro.mapred.AvroWrapper
import org.apache.avro.generic.GenericRecord
import org.apache.avro.mapred.{AvroInputFormat, AvroWrapper}
import org.apache.hadoop.io.NullWritable


val path = &quot;twitter.avro&quot;
val avroRDD = sc.hadoopFile[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](path)
avroRDD.map(l =&amp;gt; new String(l._1.datum.get(&quot;username&quot;).toString() ) ).first
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多的 Avro Tools 用法，可以参考 &lt;a href=&quot;/2015/03/20/about-avro.html&quot;&gt;Avro 介绍&lt;/a&gt;。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2015/03/24/how-to-load-some-avro-data-into-spark.html</link>
      <guid>http://blog.javachen.com/2015/03/24/how-to-load-some-avro-data-into-spark.html</guid>
      <pubDate>2015-03-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Avro介绍</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 介绍&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://avro.apache.org/&quot;&gt;Avro&lt;/a&gt; 是 Hadoop 中的一个子项目，也是 Apache 中一个独立的项目，Avro 是一个基于二进制数据传输高性能的中间件。在 Hadoop 的其他项目中，例如 HBase 和 Hive 的 Client 端与服务端的数据传输也采用了这个工具。Avro 是一个数据序列化的系统，它可以提供：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、丰富的数据结构类型&lt;/li&gt;
  &lt;li&gt;2、快速可压缩的二进制数据形式&lt;/li&gt;
  &lt;li&gt;3、存储持久数据的文件容器&lt;/li&gt;
  &lt;li&gt;4、远程过程调用 RPC&lt;/li&gt;
  &lt;li&gt;5、简单的动态语言结合功能，Avro 和动态语言结合后，读写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态类型语言中实现。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Avro 支持跨编程语言实现（C, C++, C#，Java, Python, Ruby, PHP），Avro 提供着与诸如 Thrift 和 Protocol Buffers 等系统相似的功能，但是在一些基础方面还是有区别的，主要是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、动态类型：Avro 并不需要生成代码，模式和数据存放在一起，而模式使得整个数据的处理过程并不生成代码、静态数据类型等等。这方便了数据处理系统和语言的构造。&lt;/li&gt;
  &lt;li&gt;2、未标记的数据：由于读取数据的时候模式是已知的，那么需要和数据一起编码的类型信息就很少了，这样序列化的规模也就小了。&lt;/li&gt;
  &lt;li&gt;3、不需要用户指定字段号：即使模式改变，处理数据时新旧模式都是已知的，所以通过使用字段名称可以解决差异问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Avro 和动态语言结合后，读/写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只需要在静态类型语言中实现。&lt;/p&gt;

&lt;p&gt;当在 RPC 中使用 Avro 时，服务器和客户端可以在握手连接时交换模式。服务器和客户端有着彼此全部的模式，因此相同命名字段、缺失字段和多余字段等信息之间通信中需要解决的一致性问题就可以容易解决。&lt;/p&gt;

&lt;p&gt;还有，Avro 模式是用 JSON（一种轻量级的数据交换模式）定义的，这样对于已经拥有 JSON 库的语言可以容易实现。&lt;/p&gt;

&lt;h1 id=&quot;schema&quot;&gt;2. Schema&lt;/h1&gt;

&lt;p&gt;Schema 通过 JSON 对象表示。Schema 定义了简单数据类型和复杂数据类型，其中复杂数据类型包含不同属性。通过各种数据类型用户可以自定义丰富的数据结构。&lt;/p&gt;

&lt;p&gt;基本类型有：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;类型&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;null&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;no value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;boolean&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;a binary value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;int&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;32-bit signed integer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;long&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;64-bit signed integer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;float&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;single precision (32-bit) IEEE 754 floating-point number&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;double&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;double precision (64-bit) IEEE 754 floating-point number&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;bytes&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;sequence of 8-bit unsigned bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;string&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;unicode character sequence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Avro定义了六种复杂数据类型：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Record：record 类型，任意类型的一个命名字段集合，JSON对象表示。支持以下属性：
    &lt;ul&gt;
      &lt;li&gt;name：名称，必须&lt;/li&gt;
      &lt;li&gt;namespace&lt;/li&gt;
      &lt;li&gt;doc&lt;/li&gt;
      &lt;li&gt;aliases&lt;/li&gt;
      &lt;li&gt;fields：一个 JSON 数组，必须
        &lt;ul&gt;
          &lt;li&gt;name&lt;/li&gt;
          &lt;li&gt;doc&lt;/li&gt;
          &lt;li&gt;type&lt;/li&gt;
          &lt;li&gt;default&lt;/li&gt;
          &lt;li&gt;order&lt;/li&gt;
          &lt;li&gt;aliases&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Enum：enum 类型，支持以下属性：
    &lt;ul&gt;
      &lt;li&gt;name：名称，必须&lt;/li&gt;
      &lt;li&gt;namespace&lt;/li&gt;
      &lt;li&gt;doc&lt;/li&gt;
      &lt;li&gt;aliases&lt;/li&gt;
      &lt;li&gt;symbols：枚举值，必须&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Array：array 类型，未排序的对象集合，对象的模式必须相同。支持以下属性：
    &lt;ul&gt;
      &lt;li&gt;items&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Map：map 类型，未排序的对象键/值对。键必须是字符串，值可以是任何类型，但必须模式相同。支持以下属性：
    &lt;ul&gt;
      &lt;li&gt;values&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fixed：fixed 类型，一组固定数量的8位无符号字节。支持以下属性：
    &lt;ul&gt;
      &lt;li&gt;name：名称，必须&lt;/li&gt;
      &lt;li&gt;namespace&lt;/li&gt;
      &lt;li&gt;size：每个值的 byte 长度&lt;/li&gt;
      &lt;li&gt;aliases&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Union：union 类型，模式的并集，可以用JSON数组表示，每个元素为一个模式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每一种复杂数据类型都含有各自的一些属性，其中部分属性是必需的，部分是可选的。&lt;/p&gt;

&lt;p&gt;举例，一个 linked-list of 64-bit 的值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
  &quot;type&quot;: &quot;record&quot;, 
  &quot;name&quot;: &quot;LongList&quot;,
  &quot;aliases&quot;: [&quot;LinkedLongs&quot;],                      // old name for this
  &quot;fields&quot; : [
    {&quot;name&quot;: &quot;value&quot;, &quot;type&quot;: &quot;long&quot;},             // each element has a long
    {&quot;name&quot;: &quot;next&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;LongList&quot;]} // optional next element
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个 enum 类型的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{ &quot;type&quot;: &quot;enum&quot;,
  &quot;name&quot;: &quot;Suit&quot;,
  &quot;symbols&quot; : [&quot;SPADES&quot;, &quot;HEARTS&quot;, &quot;DIAMONDS&quot;, &quot;CLUBS&quot;]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;array 类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;map 类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;long&quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;fixed 类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;type&quot;: &quot;fixed&quot;, &quot;size&quot;: 16, &quot;name&quot;: &quot;md5&quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里需要说明Record类型中field属性的默认值，当Record Schema实例数据中某个field属性没有提供实例数据时，则由默认值提供，具体值见下表。Union的field默认值由Union定义中的第一个Schema决定。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;avro type&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;json type&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;example&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;null&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;null&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;null&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;boolean&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;boolean&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;int,long&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;integer&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;float,double&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;number&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;bytes&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;string&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;“\u00FF”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;string&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;string&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;“foo”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;record&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;object&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;{“a”: 1}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;enum&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;string&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;“FOO”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;array&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;array&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;[1]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;map&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;object&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;{“a”: 1}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;fixed&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;string&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;“\u00ff”&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;section-1&quot;&gt;3.  序列化/反序列化&lt;/h1&gt;

&lt;p&gt;Avro 指定两种数据序列化编码方式：binary encoding 和 Json encoding。使用二进制编码会高效序列化，并且序列化后得到的结果会比较小；而 JSON 一般用于调试系统或是基于 WEB 的应用。&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h1 id=&quot;avro-tools&quot;&gt;4. Avro Tools&lt;/h1&gt;

&lt;p&gt;Avro Tools 不加参数时:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -jar /usr/lib/avro/avro-tools.jar
Version 1.7.6-cdh5.2.0 of Apache Avro
Copyright 2010 The Apache Software Foundation

This product includes software developed at
The Apache Software Foundation (http://www.apache.org/).

C JSON parsing provided by Jansson and
written by Petri Lehtinen. The original software is
available from http://www.digip.org/jansson/.
----------------
Available tools:
          cat  extracts samples from files
      compile  Generates Java code for the given schema.
       concat  Concatenates avro files without re-compressing.
   fragtojson  Renders a binary-encoded Avro datum as JSON.
     fromjson  Reads JSON records and writes an Avro data file.
     fromtext  Imports a text file into an avro data file.
      getmeta  Prints out the metadata of an Avro data file.
    getschema  Prints out schema of an Avro data file.
          idl  Generates a JSON schema from an Avro IDL file
 idl2schemata  Extract JSON schemata of the types from an Avro IDL file
       induce  Induce schema/protocol from Java class/interface via reflection.
   jsontofrag  Renders a JSON-encoded Avro datum as binary.
       random  Creates a file with randomly generated instances of a schema.
      recodec  Alters the codec of a data file.
  rpcprotocol  Output the protocol of a RPC service
   rpcreceive  Opens an RPC Server and listens for one message.
      rpcsend  Sends a single RPC message.
       tether  Run a tethered mapreduce job.
       tojson  Dumps an Avro data file as JSON, record per line or pretty.
       totext  Converts an Avro data file to a text file.
     totrevni  Converts an Avro data file to a Trevni file.
  trevni_meta  Dumps a Trevni file&#39;s metadata as JSON.
trevni_random  Create a Trevni file filled with random instances of a schema.
trevni_tojson  Dumps a Trevni file as JSON.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;fromjson 命令语法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar /usr/lib/avro/avro-tools.jar fromjson
Expected 1 arg: input_file
Option                                  Description
------                                  -----------
--codec                                 Compression codec (default: null)
--level &amp;lt;Integer&amp;gt;                       Compression level (only applies to
                                          deflate and xz) (default: -1)
--schema                                Schema
--schema-file                           Schema File
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以 &lt;a href=&quot;/2015/03/24/how-to-load-some-avro-data-into-spark.html&quot;&gt;将Avro数据加载到Spark&lt;/a&gt; 为例，将 json 数据转换为 avro 数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar /usr/lib/avro/avro-tools.jar fromjson --schema-file twitter.avsc twitter.json &amp;gt; twitter.avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置压缩格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar /usr/lib/avro/avro-tools.jar fromjson --codec snappy --schema-file twitter.avsc twitter.json &amp;gt; twitter.snappy.avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 avro 转换为 json：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar /usr/lib/avro/avro-tools.jar tojson twitter.avro &amp;gt; twitter.json
$ java -jar /usr/lib/avro/avro-tools.jar tojson twitter.snappy.avro &amp;gt; twitter.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取 avro 文件的 schema：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar /usr/lib/avro/avro-tools.jar getschema twitter.avro &amp;gt; twitter.avsc
$ java -jar /usr/lib/avro/avro-tools.jar getschema twitter.snappy.avro &amp;gt; twitter.avsc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 Avro 数据编译为 Java：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar /usr/lib/avro/avro-tools.jar compile schema twitter.avsc .
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;5. 文件结构&lt;/h1&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;6. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/xyw_blog/article/details/8967362&quot;&gt;Avro简介&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.michael-noll.com/blog/2013/03/17/reading-and-writing-avro-files-from-the-command-line/&quot;&gt;Reading and Writing Avro Files From the Command Line&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/03/20/about-avro.html</link>
      <guid>http://blog.javachen.com/2015/03/20/about-avro.html</guid>
      <pubDate>2015-03-20T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>安装和测试Kafka</title>
      <description>&lt;p&gt;本文主要介绍如何在单节点上安装 Kafka 并测试 broker、producer 和 consumer 功能。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;下载&lt;/h1&gt;

&lt;p&gt;进入下载页面：&lt;a href=&quot;http://kafka.apache.org/downloads.html&quot;&gt;http://kafka.apache.org/downloads.html&lt;/a&gt; ，选择 Binary downloads下载 （Source download需要编译才能使用），这里我下载 &lt;code&gt;kafka_2.11-0.8.2.1&lt;/code&gt;，其对应的 Scala 版本为 &lt;code&gt;2.11&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://apache.fayea.com/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解压并进入目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tar -xzvf kafka_2.11-0.8.2.1.tgz
$ cd kafka_2.11-0.8.2.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看目录结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tree -L 2
.
├── bin
│   ├── kafka-console-consumer.sh
│   ├── kafka-console-producer.sh
│   ├── kafka-consumer-offset-checker.sh
│   ├── kafka-consumer-perf-test.sh
│   ├── kafka-mirror-maker.sh
│   ├── kafka-preferred-replica-election.sh
│   ├── kafka-producer-perf-test.sh
│   ├── kafka-reassign-partitions.sh
│   ├── kafka-replay-log-producer.sh
│   ├── kafka-replica-verification.sh
│   ├── kafka-run-class.sh
│   ├── kafka-server-start.sh
│   ├── kafka-server-stop.sh
│   ├── kafka-simple-consumer-shell.sh
│   ├── kafka-topics.sh
│   ├── windows
│   ├── zookeeper-server-start.sh
│   ├── zookeeper-server-stop.sh
│   └── zookeeper-shell.sh
├── config
│   ├── consumer.properties
│   ├── log4j.properties
│   ├── producer.properties
│   ├── server.properties
│   ├── test-log4j.properties
│   ├── tools-log4j.properties
│   └── zookeeper.properties
├── libs
│   ├── jopt-simple-3.2.jar
│   ├── kafka_2.11-0.8.2.1.jar
│   ├── kafka_2.11-0.8.2.1-javadoc.jar
│   ├── kafka_2.11-0.8.2.1-scaladoc.jar
│   ├── kafka_2.11-0.8.2.1-sources.jar
│   ├── kafka_2.11-0.8.2.1-test.jar
│   ├── kafka-clients-0.8.2.1.jar
│   ├── log4j-1.2.16.jar
│   ├── lz4-1.2.0.jar
│   ├── metrics-core-2.2.0.jar
│   ├── scala-library-2.11.5.jar
│   ├── scala-parser-combinators_2.11-1.0.2.jar
│   ├── scala-xml_2.11-1.0.2.jar
│   ├── slf4j-api-1.7.6.jar
│   ├── slf4j-log4j12-1.6.1.jar
│   ├── snappy-java-1.1.1.6.jar
│   ├── zkclient-0.3.jar
│   └── zookeeper-3.4.6.jar
├── LICENSE
└── NOTICE

4 directories, 45 files
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;启动和停止&lt;/h1&gt;

&lt;p&gt;运行 kafka ，需要依赖 zookeeper，你可以使用已有的 zookeeper 集群或者利用 kafka 提供的脚本启动一个 zookeeper 实例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/zookeeper-server-start.sh config/zookeeper.properties &amp;amp;  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认的，zookeeper 会监听在 &lt;code&gt;*:2181/tcp&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;停止刚才启动的 zookeeper 实例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/zookeeper-server-stop.sh  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动Kafka server:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-server-start.sh config/server.properties &amp;amp;  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;config/server.properties 中有一些默认的配置参数，这里仅仅列出参数，不做解释：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;broker.id=0

port=9092

#host.name=localhost

#advertised.host.name=&amp;lt;hostname routable by clients&amp;gt;

#advertised.port=&amp;lt;port accessible by clients&amp;gt;

num.network.threads=3

num.io.threads=8

socket.send.buffer.bytes=102400

socket.receive.buffer.bytes=102400

socket.request.max.bytes=104857600

log.dirs=/tmp/kafka-logs

num.partitions=1

num.recovery.threads.per.data.dir=1

#log.flush.interval.messages=10000

#log.flush.interval.ms=1000

log.retention.hours=168

#log.retention.bytes=1073741824

log.segment.bytes=1073741824

log.retention.check.interval.ms=300000

log.cleaner.enable=false

zookeeper.connect=localhost:2181

zookeeper.connection.timeout.ms=6000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你像我一样是在虚拟机中测试 kafka，那么你需要修改 kafka 启动参数中 JVM 内存大小。查看 kafka-server-start.sh 脚本，修改 &lt;code&gt;KAFKA_HEAP_OPTS&lt;/code&gt; 处 &lt;code&gt;-Xmx&lt;/code&gt; 和 &lt;code&gt;-Xms&lt;/code&gt; 的值。&lt;/p&gt;

&lt;p&gt;启动成功之后，会看到如下日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2015-03-17 11:19:30,528] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2015-03-17 11:19:30,604] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2015-03-17 11:19:30,605] INFO [Socket Server on Broker 0], Started (kafka.network.SocketServer)
[2015-03-17 11:19:30,687] INFO Will not load MX4J, mx4j-tools.jar is not in the classpath (kafka.utils.Mx4jLoader$)
[2015-03-17 11:19:30,756] INFO 0 successfully elected as leader (kafka.server.ZookeeperLeaderElector)
[2015-03-17 11:19:30,887] INFO Registered broker 0 at path  /brokers/ids/0 with address cdh1:9092. (kafka.utils.ZkUtils$)
[2015-03-17 11:19:30,928] INFO [Kafka Server 0], started (kafka.server.KafkaServer)
[2015-03-17 11:19:31,048] INFO New leader is 0 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从日志可以看到：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;log flusher 有一个默认的周期值&lt;/li&gt;
  &lt;li&gt;kafka server 监听在9092端口&lt;/li&gt;
  &lt;li&gt;在 cdh1:9092 上注册了一个 broker 0 ，路径为 /brokers/ids/0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;停止 Kafka server :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-server-stop.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;broker-&quot;&gt;单 broker 测试&lt;/h1&gt;

&lt;p&gt;在启动  kafka-server 之后启动，运行producer：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在另一个终端运行 consumer：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 producer 端输入字符串并回车，查看 consumer 端是否显示。&lt;/p&gt;

&lt;h1 id=&quot;broker--1&quot;&gt;多 broker 测试&lt;/h1&gt;

&lt;h2 id=&quot;kafka-broker&quot;&gt;配置和启动 Kafka broker&lt;/h2&gt;

&lt;p&gt;接下来参考 &lt;a href=&quot;http://www.michael-noll.com/blog/2013/03/13/running-a-multi-broker-apache-kafka-cluster-on-a-single-node/&quot;&gt;Running a Multi-Broker Apache Kafka 0.8 Cluster on a Single Node&lt;/a&gt; 这篇文章，基于 config/server.properties 配置文件创建多个 broker 的 kafka 集群。&lt;/p&gt;

&lt;p&gt;创建第一个 broker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cp config/server.properties config/server1.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编写 config/server1.properties 并修改下面配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;broker.id=1
port=9092
log.dir=/tmp/kafka-logs-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建第二个 broker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cp config/server.properties config/server2.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编写 config/server2.properties 并修改下面配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;broker.id=2
port=9093
log.dir=/tmp/kafka-logs-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建第三个 broker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cp config/server.properties config/server3.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编写 config/server3.properties 并修改下面配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;broker.id=3
port=9094
log.dir=/tmp/kafka-logs-3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来分别启动这三个 broker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ JMX_PORT=9999  ; nohup bin/kafka-server-start.sh config/server1.properties &amp;amp;
$ JMX_PORT=10000  ; nohup bin/kafka-server-start.sh config/server2.properties &amp;amp;
$ JMX_PORT=10001  ; nohup bin/kafka-server-start.sh config/server3.properties &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是三个 broker 监听的网络接口和端口列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        Broker 1     Broker 2      Broker 3
----------------------------------------------
Kafka   *:9092/tcp   *:9093/tcp    *:9094/tcp
JMX     *:9999/tcp   *:10000/tcp   *:10001/tcp
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;kafka-topic&quot;&gt;创建 Kafka topic&lt;/h2&gt;

&lt;p&gt;在 Kafka 0.8 中有两种方式创建一个新的 topic：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 broker 上开启 &lt;code&gt;auto.create.topics.enable&lt;/code&gt; 参数，当 broker 接收到一个新的 topic 上的消息时候，会通过 &lt;code&gt;num.partitions&lt;/code&gt; 和 &lt;code&gt;default.replication.factor&lt;/code&gt; 两个参数自动创建 topic。&lt;/li&gt;
  &lt;li&gt;使用 &lt;code&gt;bin/kafka-topics.sh&lt;/code&gt; 命令&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;创建一个名称为 &lt;code&gt;zerg.hydra&lt;/code&gt;  的 topic：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic zerg.hydra --partitions 3 --replication-factor 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用下面查看创建的 topic:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-topics.sh --zookeeper localhost:2181 --list
test
zerg.hydra
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以查看更详细的信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic zerg.hydra
Topic:zerg.hydra    PartitionCount:3    ReplicationFactor:2 Configs:
    Topic: zerg.hydra   Partition: 0    Leader: 2   Replicas: 2,3   Isr: 2,3
    Topic: zerg.hydra   Partition: 1    Leader: 3   Replicas: 3,0   Isr: 3,0
    Topic: zerg.hydra   Partition: 2    Leader: 0   Replicas: 0,2   Isr: 0,2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认的，Kafka 持久化 topic 到 &lt;code&gt;log.dir&lt;/code&gt; 参数定义的目录。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tree /tmp/kafka-logs-{1,2,3}
/tmp/kafka-logs-1                   # first broker (broker.id = 1)
├── zerg.hydra-0                    # replica of partition 0 of topic &quot;zerg.hydra&quot; (this broker is leader)
│   ├── 00000000000000000000.index
│   └── 00000000000000000000.log
├── zerg.hydra-2                    # replica of partition 2 of topic &quot;zerg.hydra&quot;
│   ├── 00000000000000000000.index
│   └── 00000000000000000000.log
└── replication-offset-checkpoint

/tmp/kafka-logs-2                   # second broker (broker.id = 2)
├── zerg.hydra-0                    # replica of partition 0 of topic &quot;zerg.hydra&quot;
│   ├── 00000000000000000000.index
│   └── 00000000000000000000.log
├── zerg.hydra-1                    # replica of partition 1 of topic &quot;zerg.hydra&quot; (this broker is leader)
│   ├── 00000000000000000000.index
│   └── 00000000000000000000.log
└── replication-offset-checkpoint

/tmp/kafka-logs-3                   # third broker (broker.id = 3)
├── zerg.hydra-1                    # replica of partition 1 of topic &quot;zerg.hydra&quot;
│   ├── 00000000000000000000.index
│   └── 00000000000000000000.log
├── zerg.hydra-2                    # replica of partition 2 of topic &quot;zerg.hydra&quot; (this broker is leader)
│   ├── 00000000000000000000.index
│   └── 00000000000000000000.log
└── replication-offset-checkpoint

6 directories, 15 files
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;producer&quot;&gt;启动一个 producer&lt;/h2&gt;

&lt;p&gt;以 &lt;code&gt;sync&lt;/code&gt; 模式启动一个 producer：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --sync --topic zerg.hydra
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，输入以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Hello, world!
Rock: Nerf Paper. Scissors is fine.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;consumer&quot;&gt;启动一个 consumer&lt;/h2&gt;

&lt;p&gt;在另一个终端运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic zerg.hydra --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，生产环境通常不会添加 &lt;code&gt;--from-beginning&lt;/code&gt; 参数。&lt;/p&gt;

&lt;p&gt;观察输出，你会看到下面内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Hello, world!
Rock: Nerf Paper. Scissors is fine.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把 consumer 停掉再启动，你还会看到相同的输出结果。&lt;/p&gt;

&lt;h1 id=&quot;kafka&quot;&gt;将日志推送到 kafka&lt;/h1&gt;

&lt;p&gt;例如，将 apache 或者 nginx 或者 tomcat 等产生的日志 push 到 kafka，只需要执行下面代码即可：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tail -n 0 -f  /var/log/nginx/access.log | bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --sync --topic zerg.hydra
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.michael-noll.com/blog/2013/03/13/running-a-multi-broker-apache-kafka-cluster-on-a-single-node/&quot;&gt;Running a Multi-Broker Apache Kafka 0.8 Cluster on a Single Node&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://yangqijun.com/archives/227&quot;&gt;spark读取 kafka nginx网站日志消息 并写入HDFS中&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/03/17/install-and-test-kafka.html</link>
      <guid>http://blog.javachen.com/2015/03/17/install-and-test-kafka.html</guid>
      <pubDate>2015-03-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spring Boot特性</title>
      <description>&lt;h1 id=&quot;springapplication&quot;&gt;SpringApplication&lt;/h1&gt;

&lt;p&gt;SpringApplication 类是启动 Spring Boot 应用的入口类，你可以创建一个包含 &lt;code&gt;main()&lt;/code&gt; 方法的类，来运行 &lt;code&gt;SpringApplication.run&lt;/code&gt; 这个静态方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public static void main(String[] args) {
    SpringApplication.run(MySpringConfiguration.class, args);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行该类会有如下输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  .   ____          _            __ _ _
 /\\ / ___&#39;_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | &#39;_ | &#39;_| | &#39;_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  &#39;  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::   v1.2.2.RELEASE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的输出的图案可以通过 banner.txt 文件进行定制。在你的 classpath 中添加 banner.txt 文件，可以在该文件内设置如下变量：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;${application.version}&lt;/code&gt;：MANIFEST.MF 文件中的应用版本号&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;${application.formatted-version}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;${spring-boot.version}&lt;/code&gt;：你正在使用的 Spring Boot 版本号&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;${spring-boot.formatted-version}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面这些变量也可以通过 application.properties  来设置，后面再作介绍。&lt;/p&gt;

&lt;p&gt;此外，你还可以通过代码进行设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public static void main(String[] args) {
    SpringApplication app = new SpringApplication(MySpringConfiguration.class);
    app.setShowBanner(false);
    app.run(args);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者使用流式 API:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;new SpringApplicationBuilder()
    .showBanner(false)
    .sources(Parent.class)
    .child(Application.class)
    .run(args);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SpringApplication 启动过程会触发一些事件，你可以针对这些事件通过 &lt;code&gt;SpringApplication.addListeners(…​)&lt;/code&gt; 添加一些监听器:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ApplicationStartedEvent&lt;/li&gt;
  &lt;li&gt;ApplicationEnvironmentPreparedEvent&lt;/li&gt;
  &lt;li&gt;ApplicationPreparedEvent&lt;/li&gt;
  &lt;li&gt;ApplicationFailedEvent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SpringApplication 会注册一个 shutdown hook 以便在应用退出的时候能够保证 &lt;code&gt;ApplicationContext&lt;/code&gt; 优雅地关闭，这样能够保证所有 Spring lifecycle 的回调都会被执行，包括 DisposableBean 接口的实现类以及 @PreDestroy 注解。&lt;/p&gt;

&lt;p&gt;另外，你也可以实现 org.springframework.boot.ExitCodeGenerator 接口来定义你自己的退出时候的逻辑。&lt;/p&gt;

&lt;h1 id=&quot;externalized-configuration&quot;&gt;Externalized Configuration&lt;/h1&gt;

&lt;p&gt;Spring Boot 允许你针对不同的环境配置不同的配置参数，你可以使用 properties文件、YAML 文件、环境变量或者命令行参数来修改应用的配置。你可以在代码中使用 &lt;code&gt;@Value&lt;/code&gt; 注解来获取配置参数的值。&lt;/p&gt;

&lt;p&gt;Spring Boot 使用一个特别的 &lt;code&gt;PropertySource&lt;/code&gt; 来按顺序加载配置，加载顺序如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;命令行参数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;java:comp/env&lt;/code&gt; 中的 JNDI 属性&lt;/li&gt;
  &lt;li&gt;Java 系统环境变量&lt;/li&gt;
  &lt;li&gt;操作系统环境变量&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;RandomValuePropertySource&lt;/code&gt;，随机值，使用 &lt;code&gt;random.*&lt;/code&gt; 来定义&lt;/li&gt;
  &lt;li&gt;jar 包外的 Profile 配置文件，如 application-{profile}.properties 和 YAML 文件&lt;/li&gt;
  &lt;li&gt;jar 包内的 Profile 配置文件，如 application-{profile}.properties 和 YAML 文件&lt;/li&gt;
  &lt;li&gt;jar 包外的 Application 配置，如 application.properties 和 application.yml 文件&lt;/li&gt;
  &lt;li&gt;jar 包内的 Application 配置，如 application.properties 和 application.yml 文件&lt;/li&gt;
  &lt;li&gt;标有 @Configuration 注解的类上的 @PropertySource 注解&lt;/li&gt;
  &lt;li&gt;默认值，使用 &lt;code&gt;SpringApplication.setDefaultProperties&lt;/code&gt; 设置的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于下面的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import org.springframework.stereotype.*
import org.springframework.beans.factory.annotation.*

@Component
public class MyBean {

    @Value(&quot;${name}&quot;)
    private String name;

    // ...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以在 application.properties 中定义一个 name 变量，或者在运行该 jar 时候，指定一个命令行参数（以 &lt;code&gt;--&lt;/code&gt; 标识），例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;java -jar app.jar --name=&quot;Spring&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RandomValuePropertySource 类型变量的示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;my.secret=${random.value}
my.number=${random.int}
my.bignumber=${random.long}
my.number.less.than.ten=${random.int(10)}
my.number.in.range=${random.int[1024,65536]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SpringApplication 会在以下路径查找 &lt;code&gt;application.properties&lt;/code&gt; 并加载该文件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;/config&lt;/code&gt; 目录下&lt;/li&gt;
  &lt;li&gt;当前目录&lt;/li&gt;
  &lt;li&gt;classpath 中 &lt;code&gt;/config&lt;/code&gt; 包下&lt;/li&gt;
  &lt;li&gt;classpath 根路径下&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外，你也可以通过 &lt;code&gt;spring.config.location&lt;/code&gt; 来指定 &lt;code&gt;application.properties&lt;/code&gt; 文件的存放路径，或者通过 &lt;code&gt;spring.config.name&lt;/code&gt; 指定该文件的名称，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -jar myproject.jar --spring.config.name=myproject
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;profiles&quot;&gt;Profiles&lt;/h1&gt;

&lt;p&gt;你可以使用 &lt;code&gt;@Profile&lt;/code&gt; 注解来标注应用使用的环境&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Configuration
@Profile(&quot;production&quot;)
public class ProductionConfiguration {

    // ...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用 &lt;code&gt;spring.profiles.active&lt;/code&gt; 变量来定义应用激活的 profile：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;spring.profiles.active=dev,hsqldb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以通过  SpringApplication 来设置，调用 &lt;code&gt;SpringApplication.setAdditionalProfiles(…​)&lt;/code&gt; 代码即可。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;日志&lt;/h1&gt;

&lt;p&gt;Spring Boot 使用 Commons Logging 作为内部记录日志，你也可以使用 Java Util Logging, Log4J, Log4J2 和 Logback 来记录日志。&lt;/p&gt;

&lt;p&gt;默认情况下，如果你使用了 Starter POMs ，则会使用 Logback 来记录日志。&lt;/p&gt;

&lt;p&gt;默认情况，是输出 INFO 类型的日志，你可以通过设置命令行参数 &lt;code&gt;--debug&lt;/code&gt; 来设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar myapp.jar --debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你的终端支持 ANSI ，则日志支持彩色输出，这个可以通过 &lt;code&gt;spring.output.ansi.enabled&lt;/code&gt; 设置，可配置的值有：&lt;code&gt;ALWAYS&lt;/code&gt;、&lt;code&gt;DETECT&lt;/code&gt;、&lt;code&gt;NEVER&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;可以通过 &lt;code&gt;logging.file&lt;/code&gt; 和 &lt;code&gt;logging.path&lt;/code&gt; 设置日志输出文件名称和路径。&lt;/p&gt;

&lt;p&gt;日志级别使用 &lt;code&gt;logging.level.*=LEVEL&lt;/code&gt; 来定义，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;logging.level.org.springframework.web: DEBUG
logging.level.org.hibernate: ERROR
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spring Boot 通过 &lt;code&gt;logging.config&lt;/code&gt; 来定义日志的配置文件存放路径，对于不同的日志系统，配置文件的名称不同：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Logging&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;System   Customization&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Logback&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;logback.xml or logback.groovy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Log4j&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;log4j.properties or log4j.xml&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Log4j2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;log4j2.xml&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;JDK (Java Util Logging)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;logging.properties&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;section-1&quot;&gt;生产环境的一些特性&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;spring-boot-actuator&lt;/code&gt; 模块提供了一些生产环境的特性，使用 &lt;code&gt;spring-boot-starter-actuator&lt;/code&gt; 这个 Starter POM 可以开启该特性。&lt;/p&gt;

&lt;p&gt;对于 Maven ，添加&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;dependencies&amp;gt;
    &amp;lt;dependency&amp;gt;
        &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
        &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;
    &amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 Gradle，添加&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;dependencies {
    compile(&quot;org.springframework.boot:spring-boot-starter-actuator&quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.spring.io/spring-boot/docs/1.2.2.RELEASE/reference/htmlsingle&quot;&gt;Spring Boot Reference Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/03/13/some-spring-boot-features.html</link>
      <guid>http://blog.javachen.com/2015/03/13/some-spring-boot-features.html</guid>
      <pubDate>2015-03-13T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>如何运行Spring Boot应用</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;介绍&lt;/h1&gt;

&lt;p&gt;Spring Boot 是 Spring 产品中一个新的子项目，致力于简便快捷地搭建基于 Spring 的独立可运行的应用。大多数的 Spring Boot  应用只需要非常少的 Spring 配置。&lt;/p&gt;

&lt;p&gt;你能够使用 Spring Boot  创建 Java 应用并通过 &lt;code&gt;java -jar&lt;/code&gt; 来运行或者创建传统的通过 war 来部署的应用。Spring Boot 也提供了一个命令行工具来运行 spring 脚本。&lt;/p&gt;

&lt;p&gt;Spring Boot 的目标是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;快速开发基于 Spring 的应用&lt;/li&gt;
  &lt;li&gt;开箱即用的微服务&lt;/li&gt;
  &lt;li&gt;提供一些大型项目常用的非功能性特性，例如：嵌入式服务、安全、监控、健康检查、外部配置&lt;/li&gt;
  &lt;li&gt;不用生成代码，没有 xml 配置&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;系统要求&lt;/h1&gt;

&lt;p&gt;截至本文编写之前，Spring Boot 的最新发布版本为 &lt;code&gt;1.2.2.RELEASE&lt;/code&gt;，其需要 Java 7 和 Spring 4.1.3 或以上版本才能运行。当然，你也可以添加一些配置使其能够使用 Java 6。虽然，你可以使用 Java 6 或7 来运行 Spring Boot，但是，在可能的情况下，更建议你使用 Java 8。编译 Spring Boot 需要 Maven 3.2+ 或者 Gradle 1.12+。&lt;/p&gt;

&lt;p&gt;Servlet 容易要求：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Name&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Servlet Version&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Java Version&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tomcat 8&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3.1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Java 7+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tomcat 7&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Java 6+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Jetty 9&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3.1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Java 7+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Jetty 8&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Java 6+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Undertow 1.1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3.1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Java 7+&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;你也可以在兼容 &lt;code&gt;Servlet 3.0+&lt;/code&gt; 的容器中部署 Spring Boot 应用。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;安装&lt;/h1&gt;

&lt;p&gt;首先，需要安装 Java 并确认版本是否满足要求。&lt;/p&gt;

&lt;h2 id=&quot;maven&quot;&gt;Maven安装&lt;/h2&gt;

&lt;p&gt;Spring Boot 可以使用 Maven3.2 以上版本进行编译。使用压缩包解压安装 Maven 或者通过命令安装，例如，在 Mac 上安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install maven
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要创建一个 Spring Boot 的项目，你需要创建一个 Maven 的 POM 文件并继承 &lt;code&gt;spring-boot-starter-parent&lt;/code&gt; 项目，然后添加一些 Spring Boot 子项目的依赖，你也可以使用 &lt;code&gt;spring-boot-maven-plugin&lt;/code&gt; 插件来创建可执行的 jar 包。&lt;/p&gt;

&lt;p&gt;一个典型的 pom.xml 示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&amp;gt;
    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;

    &amp;lt;groupId&amp;gt;com.example&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;myproject&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.0.1-SNAPSHOT&amp;lt;/version&amp;gt;

    &amp;lt;!-- Inherit defaults from Spring Boot --&amp;gt;
    &amp;lt;parent&amp;gt;
        &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
        &amp;lt;artifactId&amp;gt;spring-boot-starter-parent&amp;lt;/artifactId&amp;gt;
        &amp;lt;version&amp;gt;1.2.2.RELEASE&amp;lt;/version&amp;gt;
    &amp;lt;/parent&amp;gt;

    &amp;lt;!-- Add typical dependencies for a web application --&amp;gt;
    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;

    &amp;lt;!-- Package as an executable jar --&amp;gt;
    &amp;lt;build&amp;gt;
        &amp;lt;plugins&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;spring-boot-maven-plugin&amp;lt;/artifactId&amp;gt;
            &amp;lt;/plugin&amp;gt;
        &amp;lt;/plugins&amp;gt;
    &amp;lt;/build&amp;gt;

&amp;lt;/project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你不想继承 spring-boot-starter-parent 父项目，你也可以使用依赖管理来引入对 Spring Boot 的依赖。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;dependencyManagement&amp;gt;
     &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;!-- Import dependency management from Spring Boot --&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-boot-dependencies&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;1.2.2.RELEASE&amp;lt;/version&amp;gt;
            &amp;lt;type&amp;gt;pom&amp;lt;/type&amp;gt;
            &amp;lt;scope&amp;gt;import&amp;lt;/scope&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;
&amp;lt;/dependencyManagement&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以通过下面方式修改 Maven 使用的 Java 版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;properties&amp;gt;
    &amp;lt;java.version&amp;gt;1.8&amp;lt;/java.version&amp;gt;
&amp;lt;/properties&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;gradle-&quot;&gt;Gradle 安装&lt;/h2&gt;

&lt;p&gt;一个典型的 &lt;code&gt;build.gradle&lt;/code&gt; 文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;buildscript {
    repositories {
        jcenter()
        maven { url &quot;http://repo.spring.io/snapshot&quot; }
        maven { url &quot;http://repo.spring.io/milestone&quot; }
    }
    dependencies {
        classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.2.RELEASE&quot;)
    }
}

apply plugin: &#39;java&#39;
apply plugin: &#39;spring-boot&#39;

jar {
    baseName = &#39;myproject&#39;
    version =  &#39;0.0.1-SNAPSHOT&#39;
}

repositories {
    jcenter()
    maven { url &quot;http://repo.spring.io/snapshot&quot; }
    maven { url &quot;http://repo.spring.io/milestone&quot; }
}

dependencies {
    compile(&quot;org.springframework.boot:spring-boot-starter-web&quot;)
    testCompile(&quot;org.springframework.boot:spring-boot-starter-test&quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spring-boot-cli&quot;&gt;安装 Spring Boot CLI&lt;/h2&gt;

&lt;p&gt;你可以手动安装：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://repo.spring.io/release/org/springframework/boot/spring-boot-cli/1.2.2.RELEASE/spring-boot-cli-1.2.2.RELEASE-bin.zip&quot;&gt;spring-boot-cli-1.2.2.RELEASE-bin.zip&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://repo.spring.io/release/org/springframework/boot/spring-boot-cli/1.2.2.RELEASE/spring-boot-cli-1.2.2.RELEASE-bin.tar.gz&quot;&gt;spring-boot-cli-1.2.2.RELEASE-bin.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;或者使用 GVM 安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gvm install springboot
$ spring --version
Spring Boot v1.2.2.RELEASE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你使用 Mac，则可以通过 &lt;a href=&quot;http://brew.sh/&quot;&gt;Homebrew&lt;/a&gt; 或者 &lt;a href=&quot;http://www.macports.org/&quot;&gt;MacPorts&lt;/a&gt; 安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew tap pivotal/tap
$ brew install springboot
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo port install spring-boot-cli
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spring 命令用法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spring
usage: spring [--help] [--version]
       &amp;lt;command&amp;gt; [&amp;lt;args&amp;gt;]

Available commands are:

  run [options] &amp;lt;files&amp;gt; [--] [args]
    Run a spring groovy script

  ... more command help is shown here
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 你可以很快创建一个简单的 web 应用来测试是否安装成功。创建 hello.groovy 文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;@RestController
class ThisWillActuallyRun {

    @RequestMapping(&quot;/&quot;)
    String home() {
        &quot;Hello World!&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，运行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spring run hello.groovy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用浏览器打开 &lt;a href=&quot;http://localhost:8080/&quot;&gt;http://localhost:8080/&lt;/a&gt; 你会看到下面的输出：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;Hello World!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你还可以传递一些命令行的参数，使用 &lt;code&gt;--&lt;/code&gt; 分隔命令行参数，下面命令修改服务启动端口为 9000：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spring run hello.groovy -- --server.port=9000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置 JVM 参数之后，在运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ JAVA_OPTS=-Xmx1024m 
$ spring run hello.groovy
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;spring-boot-cli-&quot;&gt;Spring Boot CLI 使用方法&lt;/h1&gt;

&lt;h1 id=&quot;spring-boot-&quot;&gt;部署 Spring Boot 应用&lt;/h1&gt;

&lt;h2 id=&quot;maven-&quot;&gt;Maven 项目&lt;/h2&gt;

&lt;p&gt;如果你使用 Maven 编译项目，则你可以通过下面命令查看项目依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn dependency:tree
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你的 pom.xml 使用了 spring-boot-starter-parent，则我们可以运行 &lt;code&gt;mvn spring-boot:run&lt;/code&gt; 命令启动应用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn spring-boot:run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，使用浏览器打开 http://localhost:8080/ 你会看到下面的输出：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;Hello World!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你的 pom.xml 中添加了 &lt;code&gt;spring-boot-maven-plugin&lt;/code&gt; 插件，你可以运行 &lt;code&gt;mvn package&lt;/code&gt; 命令在 target 目录生成一个可执行的 jar 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn package

[INFO] Scanning for projects...
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building myproject 0.0.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] .... ..
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ myproject ---
[INFO] Building jar: /Users/developer/example/spring-boot-example/target/myproject-0.0.1-SNAPSHOT.jar
[INFO]
[INFO] --- spring-boot-maven-plugin:1.2.2.RELEASE:repackage (default) @ myproject ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，你可以运行项目命令执行生成的 jar 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -jar target/myproject-0.0.1-SNAPSHOT.jar

  .   ____          _            __ _ _
 /\\ / ___&#39;_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | &#39;_ | &#39;_| | &#39;_ \/ _ | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  &#39;  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::  (v1.2.2.RELEASE)
....... . . .
....... . . . (log output here)
....... . . .
........ Started Example in 2.536 seconds (JVM running for 2.864)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 Maven 运行过程出现内存溢出，则可以添加下面参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export JAVA_OPTS=-Xmx1024m -XX:MaxPermSize=128M -Djava.security.egd=file:/dev/./urandom
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;gradle--1&quot;&gt;Gradle 项目&lt;/h2&gt;

&lt;p&gt;如果你使用 Gradle 编译项目，则可以在项目根路径直接运行下面命令来运行应用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export JAVA_OPTS=-Xmx1024m -XX:MaxPermSize=128M -Djava.security.egd=file:/dev/./urandom

$ ./gradlew bootRun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以先 build 生成一个 jar 文件，然后执行该 jar 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./gradlew build &amp;amp;&amp;amp; java -jar build/libs/myproject-0.0.1-SNAPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以启动远程调试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./gradlew build 

$ java -Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=8000,suspend=n \
       -jar build/libs/myproject-0.0.1-SNAPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.spring.io/spring-boot/docs/1.2.2.RELEASE/reference/htmlsingle&quot;&gt;Spring Boot Reference Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/03/13/how-to-run-spring-boot-application.html</link>
      <guid>http://blog.javachen.com/2015/03/13/how-to-run-spring-boot-application.html</guid>
      <pubDate>2015-03-13T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spring AOP Example Tutorial</title>
      <description>&lt;blockquote&gt;
  &lt;p&gt;这是一篇翻译，原文：&lt;a href=&quot;http://www.journaldev.com/2583/spring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations-xml-configuration&quot;&gt;Spring AOP Example Tutorial – Aspect, Advice, Pointcut, JoinPoint, Annotations, XML Configuration&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Spring 框架发展出了两个核心概念：&lt;a href=&quot;http://www.journaldev.com/2394/dependency-injection-design-pattern-in-java-example-tutorial&quot;&gt;依赖注入&lt;/a&gt; 和面向切面编程（AOP）。我们已经了解了 &lt;a href=&quot;http://www.journaldev.com/2410/spring-dependency-injection-example-with-annotations-and-xml-configuration&quot;&gt;Spring 的依赖注入&lt;/a&gt; 是如何实现的，今天我们来看看面向切面编程的核心概念以及 Spring 框架是如何实现它的。&lt;/p&gt;

&lt;h1 id=&quot;aop-&quot;&gt;AOP 概要&lt;/h1&gt;

&lt;p&gt;大多数的企业应用都会有一些共同的对横切的关注，横切是否适用于不同的对象或者模型。一些共同关注的横切有日志、事务管理以及数据校验等等。在面向对象的编程中，应用模块是有类来实现的，然而面向切面的编程的应用模块是由切面（Aspect）来获取的，他们被配置用于切不同的类。&lt;/p&gt;

&lt;p&gt;AOP 任务将横切任务的直接依赖从类中抽离出来，因为我们不能直接从面向对象编程的模型中获取这些依赖。例如，我们可以有一个单独的类用于记录日志，但是相同功能的类将不得不调用这些方法去获取应用的中的日志。&lt;/p&gt;

&lt;h1 id=&quot;aop--1&quot;&gt;AOP 核心概念&lt;/h1&gt;

&lt;p&gt;在我们深入了解 Spring 框架中 AOP 的实现方式之前，我们需要了解 AOP 中的一些核心概念。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Aspect&lt;/code&gt;：切面，实现企业应用中多个类的共同关注点，例如事务管理。切面可以是使用 Spring XML 配置的一个普通类或者是我们使用 Spring AspectJ 定义的一个标有 @Aspect 注解的类。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Join Point&lt;/code&gt;：连接点，一个连接点是应用程序中的一个特定的点，例如方法执行、异常处理、改变对象变量的值等等。在 Spring AOP 中，一个连接点永远是指一个方法的执行。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Advice&lt;/code&gt;：通知，通知是指在一个特别的连接点上发生的动作。在编程中，他们是当一个连接点匹配到一个切入点时执行的方法。你可以把通知想成 Strust2 中的拦截器或者是 Servlet 中的过滤器。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Pointcut&lt;/code&gt;：切入点，切入点是一些表达式，当匹配到连接点时决定通知是否需要执行。切入点使用不同种类型的表达式来匹配连接点，Spring 框架使用 AspectJ 表达式语法。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Target Object&lt;/code&gt;：通知执行的目标对象。Spring AOP 是使用运行时的代理来实现的，所以该对象永远是一个代理对象。这意味着一个子类在运行期间会被创建，该类的目标方法会被覆盖并且通知会基于他们的配置被引入。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;AOP proxy&lt;/code&gt;：Spring AOP 实现使用 JDK 的动态代理来创建包含有目标类和通知调用的代理类，这些类被称为 AOP 代理类。我们也可以使用 CGLIB 代理来作为 Spring AOP 项目的依赖实现。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Weaving&lt;/code&gt;：织入，将切面和其他用于创建被通知的代理对象的类联系起来的过程。这可以是在运行时完成，也可以是在代码加载过程中或者运行时。Spring AOP 是在运行时完成织入的过程。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;aop--2&quot;&gt;AOP 通知类型&lt;/h1&gt;

&lt;p&gt;基于通知的执行策略，这里有以下几种通知类型：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Before Advice&lt;/code&gt;：在连接点方法执行之前运行。我们可以使用 &lt;code&gt;@Before&lt;/code&gt; 注解来标记一个通知类型为 Before Advice。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;After (finally) Advice&lt;/code&gt;：在连接点方法执行完成之后运行。我们可以使用 &lt;code&gt;@After&lt;/code&gt; 来创建一个 After (finally) Advice。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;After Returning Advice&lt;/code&gt;：在方法返回之后运行，通过 &lt;code&gt;@AfterReturning&lt;/code&gt; 注解创建。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;After Throwing Advice&lt;/code&gt;：在方法抛出异常之后运行，通过 &lt;code&gt;@AfterThrowing&lt;/code&gt; 注解创建。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Around Advice&lt;/code&gt;：这是最重要和最强的通知。这个通知在连接点方法前后运行并且我们可以决定该通知是否运行，通过 &lt;code&gt;@Around&lt;/code&gt; 注解创建。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面提到的知识点可能会使我们困惑，但是当我们看到 Spring AOP 的实现之后，就会豁然开朗了。下面我们来创建一个 Spring AOP 的项目。Spring 支持使用 AspectJ 的注解来创建切面，为了简单，我们将直接使用这些注解。上面提到的所有 AOP 的注解都定义在 org.aspectj.lang.annotation 包中。&lt;/p&gt;

&lt;p&gt;Spring Tool Suite 提供了对 AspectJ 的支持，所以建议你使用它来创建项目。如果你对 STS 不熟悉，可以参考我的 &lt;a href=&quot;http://www.journaldev.com/2433/spring-mvc-tutorial-for-beginners-with-spring-tool-suite&quot;&gt;Spring MVC 教程&lt;/a&gt; 来熟悉如何使用它。&lt;/p&gt;

&lt;p&gt;创建一个简单的 Spring Maven 项目，通过 pom.xml 引入 Spring 的核心库。在项目创建成功之后，我们可以看到下面的目录结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.journaldev.com/wp-content/uploads/2014/03/Spring-AOP-Example-Project.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;spring-aop-aspectj-&quot;&gt;Spring AOP AspectJ 依赖&lt;/h1&gt;

&lt;p&gt;Spring 框架默认提供了对 AOP 的支持，既然我们需要使用 AspectJ 的注解，则需要在 pom.xml 中引入相关的依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&amp;gt;
    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.samples&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;SpringAOPExample&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.0.1-SNAPSHOT&amp;lt;/version&amp;gt;
 
    &amp;lt;properties&amp;gt;
 
        &amp;lt;!-- Generic properties --&amp;gt;
        &amp;lt;java.version&amp;gt;1.6&amp;lt;/java.version&amp;gt;
        &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt;
        &amp;lt;project.reporting.outputEncoding&amp;gt;UTF-8&amp;lt;/project.reporting.outputEncoding&amp;gt;
 
        &amp;lt;!-- Spring --&amp;gt;
        &amp;lt;spring-framework.version&amp;gt;4.0.2.RELEASE&amp;lt;/spring-framework.version&amp;gt;
 
        &amp;lt;!-- Logging --&amp;gt;
        &amp;lt;logback.version&amp;gt;1.0.13&amp;lt;/logback.version&amp;gt;
        &amp;lt;slf4j.version&amp;gt;1.7.5&amp;lt;/slf4j.version&amp;gt;
 
        &amp;lt;!-- Test --&amp;gt;
        &amp;lt;junit.version&amp;gt;4.11&amp;lt;/junit.version&amp;gt;
 
        &amp;lt;!-- AspectJ --&amp;gt;
        &amp;lt;aspectj.version&amp;gt;1.7.4&amp;lt;/aspectj.version&amp;gt;
 
    &amp;lt;/properties&amp;gt;
 
    &amp;lt;dependencies&amp;gt;
        &amp;lt;!-- Spring and Transactions --&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-context&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${spring-framework.version}&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-tx&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${spring-framework.version}&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
 
        &amp;lt;!-- Logging with SLF4J &amp;amp; LogBack --&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.slf4j&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;slf4j-api&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${slf4j.version}&amp;lt;/version&amp;gt;
            &amp;lt;scope&amp;gt;compile&amp;lt;/scope&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;ch.qos.logback&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;logback-classic&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${logback.version}&amp;lt;/version&amp;gt;
            &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;
        &amp;lt;/dependency&amp;gt;
 
        &amp;lt;!-- AspectJ dependencies --&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.aspectj&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;aspectjrt&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${aspectj.version}&amp;lt;/version&amp;gt;
            &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.aspectj&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;aspectjtools&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${aspectj.version}&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;
&amp;lt;/project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要注意的是，我在项目中添加了对 aspectjrt 和 aspectjtools （版本为 1.7.4）的依赖。并且我也把 Spring 的版本更新到了 4.0.2.RELEASE。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;模型类&lt;/h1&gt;

&lt;p&gt;下面我们来创建一个简单的 java bean：&lt;/p&gt;

&lt;p&gt;Employee.java&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.model;
 
import com.journaldev.spring.aspect.Loggable;
 
public class Employee {
 
    private String name;
     
    public String getName() {
        return name;
    }
 
    @Loggable
    public void setName(String nm) {
        this.name=nm;
    }
     
    public void throwException(){
        throw new RuntimeException(&quot;Dummy Exception&quot;);
    }
     
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你有注意到 &lt;code&gt;setName()&lt;/code&gt; 方法上定义了一个 &lt;code&gt;Loggable&lt;/code&gt; 注解吗？它是一个我们项目中创建的自定义注解。我们将在后面介绍它的用法。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;服务类&lt;/h1&gt;

&lt;p&gt;下面，我们来创建一个服务类来处理 Employee 对象：&lt;/p&gt;

&lt;p&gt;EmployeeService.java&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.service;
 
import com.journaldev.spring.model.Employee;
 
public class EmployeeService {
 
    private Employee employee;
     
    public Employee getEmployee(){
        return this.employee;
    }
     
    public void setEmployee(Employee e){
        this.employee=e;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我本来可以使用 Spring 注解来将其配置为一个 Spring 的组件，但是在该项目中我们将会使用 XML 来配置。EmployeeService 是一个非常标准的类，并提供了一个访问 Employee 的点。&lt;/p&gt;

&lt;h1 id=&quot;aop--3&quot;&gt;AOP 配置&lt;/h1&gt;

&lt;p&gt;我项目中的配置 spring.xml 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;
    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd
        http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd&quot;&amp;gt;
 
&amp;lt;!-- Enable AspectJ style of Spring AOP --&amp;gt;
&amp;lt;aop:aspectj-autoproxy /&amp;gt;
 
&amp;lt;!-- Configure Employee Bean and initialize it --&amp;gt;
&amp;lt;bean name=&quot;employee&quot; class=&quot;com.journaldev.spring.model.Employee&quot;&amp;gt;
    &amp;lt;property name=&quot;name&quot; value=&quot;Dummy Name&quot;&amp;gt;&amp;lt;/property&amp;gt;
&amp;lt;/bean&amp;gt;
 
&amp;lt;!-- Configure EmployeeService bean --&amp;gt;
&amp;lt;bean name=&quot;employeeService&quot; class=&quot;com.journaldev.spring.service.EmployeeService&quot;&amp;gt;
    &amp;lt;property name=&quot;employee&quot; ref=&quot;employee&quot;&amp;gt;&amp;lt;/property&amp;gt;
&amp;lt;/bean&amp;gt;
 
&amp;lt;!-- Configure Aspect Beans, without this Aspects advices wont execute --&amp;gt;
&amp;lt;bean name=&quot;employeeAspect&quot; class=&quot;com.journaldev.spring.aspect.EmployeeAspect&quot; /&amp;gt;
&amp;lt;bean name=&quot;employeeAspectPointcut&quot; class=&quot;com.journaldev.spring.aspect.EmployeeAspectPointcut&quot; /&amp;gt;
&amp;lt;bean name=&quot;employeeAspectJoinPoint&quot; class=&quot;com.journaldev.spring.aspect.EmployeeAspectJoinPoint&quot; /&amp;gt;
&amp;lt;bean name=&quot;employeeAfterAspect&quot; class=&quot;com.journaldev.spring.aspect.EmployeeAfterAspect&quot; /&amp;gt;
&amp;lt;bean name=&quot;employeeAroundAspect&quot; class=&quot;com.journaldev.spring.aspect.EmployeeAroundAspect&quot; /&amp;gt;
&amp;lt;bean name=&quot;employeeAnnotationAspect&quot; class=&quot;com.journaldev.spring.aspect.EmployeeAnnotationAspect&quot; /&amp;gt;
 
&amp;lt;/beans&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Spring beans 中使用 AOP，我们需要添加：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;申明 AOP 命名空间，如：&lt;code&gt;xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;添加 &lt;code&gt;aop:aspectj-autoproxy&lt;/code&gt; 节点开启 Spring AspectJ 在运行时自动代理的支持。&lt;/li&gt;
  &lt;li&gt;配置 Aspect 类。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;before-aspect--&quot;&gt;Before Aspect  例子&lt;/h1&gt;

&lt;p&gt;EmployeeAspect.java：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.aspect;
 
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.annotation.Before;
 
@Aspect
public class EmployeeAspect {
 
    @Before(&quot;execution(public String getName())&quot;)
    public void getNameAdvice(){
        System.out.println(&quot;Executing Advice on getName()&quot;);
    }
     
    @Before(&quot;execution(* com.journaldev.spring.service.*.get*())&quot;)
    public void getAllAdvice(){
        System.out.println(&quot;Service method getter called&quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面例子中重要的地方说明如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Aspect 类需要添加 &lt;code&gt;@Aspect&lt;/code&gt; 注解。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;@Before &lt;/code&gt;注解用于创建 Before advice。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;@Before&lt;/code&gt; 注解中的字符串参数是 Pointcut 表达式。&lt;/li&gt;
  &lt;li&gt;getNameAdvice() 通知将会在任何带有 &lt;code&gt;public String getName()&lt;/code&gt;` 方法签名的 Spring Bean 方法执行时执行。这点是非常重要的，如果我们使用 new 操作符来创建一个 Employee bean，该通知并不会执行，其只会在 ApplicationContext 获取该 bean 时执行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;切点方法和重用&lt;/h1&gt;

&lt;p&gt;有时候，我们需要在多个地方上使用相同的切点表达式，我们可以使用一个空方法的 &lt;code&gt;@Pointcut&lt;/code&gt; 注解，然后在通知中将它作为表达式来使用。&lt;/p&gt;

&lt;p&gt;EmployeeAspectPointcut.java&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.aspect;
 
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.annotation.Before;
import org.aspectj.lang.annotation.Pointcut;
 
@Aspect
public class EmployeeAspectPointcut {
 
    @Before(&quot;getNamePointcut()&quot;)
    public void loggingAdvice(){
        System.out.println(&quot;Executing loggingAdvice on getName()&quot;);
    }
     
    @Before(&quot;getNamePointcut()&quot;)
    public void secondAdvice(){
        System.out.println(&quot;Executing secondAdvice on getName()&quot;);
    }
     
    @Pointcut(&quot;execution(public String getName())&quot;)
    public void getNamePointcut(){}
     
    @Before(&quot;allMethodsPointcut()&quot;)
    public void allServiceMethodsAdvice(){
        System.out.println(&quot;Before executing service method&quot;);
    }
     
    //Pointcut to execute on all the methods of classes in a package
    @Pointcut(&quot;within(com.journaldev.spring.service.*)&quot;)
    public void allMethodsPointcut(){}
     
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的例子非常清晰，相对于表达式，我们在使用方法名称作为注解的参数。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;连接点和通知参数&lt;/h1&gt;

&lt;p&gt;我们可以使用 JoinPoint 作为通知方法的参数并且使用他获取方法签名或者目标对象。&lt;/p&gt;

&lt;p&gt;我们可以在连接点中使用 &lt;code&gt;args()&lt;/code&gt; 表达式来匹配任何方法的任何参数。如果我们使用它，则我们需要在通知方法中使用同参数相同的名称。我们也可以在通知参数中使用泛型。&lt;/p&gt;

&lt;p&gt;EmployeeAspectJoinPoint.java&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.aspect;
 
import java.util.Arrays;
 
import org.aspectj.lang.JoinPoint;
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.annotation.Before;
 
@Aspect
public class EmployeeAspectJoinPoint {
 
     
    @Before(&quot;execution(public void com.journaldev.spring.model..set*(*))&quot;)
    public void loggingAdvice(JoinPoint joinPoint){
        System.out.println(&quot;Before running loggingAdvice on method=&quot;+joinPoint.toString());
         
        System.out.println(&quot;Agruments Passed=&quot; + Arrays.toString(joinPoint.getArgs()));
 
    }
     
    //Advice arguments, will be applied to bean methods with single String argument
    @Before(&quot;args(name)&quot;)
    public void logStringArguments(String name){
        System.out.println(&quot;String argument passed=&quot;+name);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;after-advice-&quot;&gt;After Advice 例子&lt;/h1&gt;

&lt;p&gt;EmployeeAfterAspect.java 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.aspect;
 
import org.aspectj.lang.JoinPoint;
import org.aspectj.lang.annotation.After;
import org.aspectj.lang.annotation.AfterReturning;
import org.aspectj.lang.annotation.AfterThrowing;
import org.aspectj.lang.annotation.Aspect;
 
@Aspect
public class EmployeeAfterAspect {
 
    @After(&quot;args(name)&quot;)
    public void logStringArguments(String name){
        System.out.println(&quot;Running After Advice. String argument passed=&quot;+name);
    }
     
    @AfterThrowing(&quot;within(com.journaldev.spring.model.Employee)&quot;)
    public void logExceptions(JoinPoint joinPoint){
        System.out.println(&quot;Exception thrown in Employee Method=&quot;+joinPoint.toString());
    }
     
    @AfterReturning(pointcut=&quot;execution(* getName())&quot;, returning=&quot;returnString&quot;)
    public void getNameReturningAdvice(String returnString){
        System.out.println(&quot;getNameReturningAdvice executed. Returned String=&quot;+returnString);
    }
     
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以在切点表达式中使用 &lt;code&gt;within&lt;/code&gt; 来申明该通知会在一个类的所有方法上执行。&lt;/p&gt;

&lt;h1 id=&quot;around-aspect-&quot;&gt;Around Aspect 例子&lt;/h1&gt;

&lt;p&gt;正如前面提到的，我们可以使用 Around aspect 来定义在方法前后进行执行指定的代码。&lt;/p&gt;

&lt;p&gt;EmployeeAroundAspect.java&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.aspect;
 
import org.aspectj.lang.ProceedingJoinPoint;
import org.aspectj.lang.annotation.Around;
import org.aspectj.lang.annotation.Aspect;
 
@Aspect
public class EmployeeAroundAspect {
 
    @Around(&quot;execution(* com.journaldev.spring.model.Employee.getName())&quot;)
    public Object employeeAroundAdvice(ProceedingJoinPoint proceedingJoinPoint){
        System.out.println(&quot;Before invoking getName() method&quot;);
        Object value = null;
        try {
            value = proceedingJoinPoint.proceed();
        } catch (Throwable e) {
            e.printStackTrace();
        }
        System.out.println(&quot;After invoking getName() method. Return value=&quot;+value);
        return value;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;自定义的注解切点&lt;/h1&gt;

&lt;p&gt;前面提到了 &lt;code&gt;@Loggable&lt;/code&gt;  注解，其定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.aspect;
 
  public @interface Loggable {
 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以创建一个切面来使用该切点，EmployeeAnnotationAspect.java 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.aspect;
 
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.annotation.Before;
 
@Aspect
public class EmployeeAnnotationAspect {
 
    @Before(&quot;@annotation(com.journaldev.spring.aspect.Loggable)&quot;)
    public void myAdvice(){
        System.out.println(&quot;Executing myAdvice!!&quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;myAdvice()&lt;/code&gt; 方法仅仅会在 &lt;code&gt;setName()&lt;/code&gt; 方法执行前执行。&lt;/p&gt;

&lt;h1 id=&quot;spring-aop-xml-configuration&quot;&gt;Spring AOP XML Configuration&lt;/h1&gt;

&lt;p&gt;如果我们使用 Sping 的配置文件来定义切面，则定义方式如下。&lt;/p&gt;

&lt;p&gt;EmployeeXMLConfigAspect.java&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.aspect;
 
import org.aspectj.lang.ProceedingJoinPoint;
 
public class EmployeeXMLConfigAspect {
 
    public Object employeeAroundAdvice(ProceedingJoinPoint proceedingJoinPoint){
        System.out.println(&quot;EmployeeXMLConfigAspect:: Before invoking getName() method&quot;);
        Object value = null;
        try {
            value = proceedingJoinPoint.proceed();
        } catch (Throwable e) {
            e.printStackTrace();
        }
        System.out.println(&quot;EmployeeXMLConfigAspect:: After invoking getName() method. Return value=&quot;+value);
        return value;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 配置文件中定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean name=&quot;employeeXMLConfigAspect&quot; class=&quot;com.journaldev.spring.aspect.EmployeeXMLConfigAspect&quot; /&amp;gt;
 
&amp;lt;!-- Spring AOP XML Configuration --&amp;gt;
&amp;lt;aop:config&amp;gt;
&amp;lt;aop:aspect ref=&quot;employeeXMLConfigAspect&quot; id=&quot;employeeXMLConfigAspectID&quot; order=&quot;1&quot;&amp;gt;
    &amp;lt;aop:pointcut expression=&quot;execution(* com.journaldev.spring.model.Employee.getName())&quot; id=&quot;getNamePointcut&quot;/&amp;gt;
    &amp;lt;aop:around method=&quot;employeeAroundAdvice&quot; pointcut-ref=&quot;getNamePointcut&quot; arg-names=&quot;proceedingJoinPoint&quot;/&amp;gt;
&amp;lt;/aop:asp&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，来看看一个简单的程序来说明切面如何作用在 bean 的方法上。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.journaldev.spring.main;
 
import org.springframework.context.support.ClassPathXmlApplicationContext;
 
import com.journaldev.spring.service.EmployeeService;
 
public class SpringMain {
 
    public static void main(String[] args) {
        ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;);
        EmployeeService employeeService = ctx.getBean(&quot;employeeService&quot;, EmployeeService.class);
         
        System.out.println(employeeService.getEmployee().getName());
         
        employeeService.getEmployee().setName(&quot;Pankaj&quot;);
         
        employeeService.getEmployee().throwException();
         
        ctx.close();
    }
 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们将看到如下输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mar 20, 2014 8:50:09 PM org.springframework.context.support.ClassPathXmlApplicationContext prepareRefresh
INFO: Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@4b9af9a9: startup date [Thu Mar 20 20:50:09 PDT 2014]; root of context hierarchy
Mar 20, 2014 8:50:09 PM org.springframework.beans.factory.xml.XmlBeanDefinitionReader loadBeanDefinitions
INFO: Loading XML bean definitions from class path resource [spring.xml]
Service method getter called
Before executing service method
EmployeeXMLConfigAspect:: Before invoking getName() method
Executing Advice on getName()
Executing loggingAdvice on getName()
Executing secondAdvice on getName()
Before invoking getName() method
After invoking getName() method. Return value=Dummy Name
getNameReturningAdvice executed. Returned String=Dummy Name
EmployeeXMLConfigAspect:: After invoking getName() method. Return value=Dummy Name
Dummy Name
Service method getter called
Before executing service method
String argument passed=Pankaj
Before running loggingAdvice on method=execution(void com.journaldev.spring.model.Employee.setName(String))
Agruments Passed=[Pankaj]
Executing myAdvice!!
Running After Advice. String argument passed=Pankaj
Service method getter called
Before executing service method
Exception thrown in Employee Method=execution(void com.journaldev.spring.model.Employee.throwException())
Exception in thread &quot;main&quot; java.lang.RuntimeException: Dummy Exception
    at com.journaldev.spring.model.Employee.throwException(Employee.java:19)
    at com.journaldev.spring.model.Employee$$FastClassBySpringCGLIB$$da2dc051.invoke(&amp;lt;generated&amp;gt;)
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:711)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
    at org.springframework.aop.aspectj.AspectJAfterThrowingAdvice.invoke(AspectJAfterThrowingAdvice.java:58)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:644)
    at com.journaldev.spring.model.Employee$$EnhancerBySpringCGLIB$$3f881964.throwException(&amp;lt;generated&amp;gt;)
    at com.journaldev.spring.main.SpringMain.main(SpringMain.java:17)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你将会看到通知将会基于切点配置一个个的执行。你应该一个个的配置他们，以免出现混乱。&lt;/p&gt;

&lt;p&gt;上面是 Spring AOP 教程的所有内容，我希望你理解了 Spring AOP 的基本概念并能从例子中学习到更多。你可以从下面链接下载本文中的项目代码。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.journaldev.com/?wpdmact=process&amp;amp;did=MjAuaG90bGluaw==&quot;&gt;Download Spring AOP Project&lt;/a&gt;&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2015/03/11/spring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations-xml-configuration.html</link>
      <guid>http://blog.javachen.com/2015/03/11/spring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations-xml-configuration.html</guid>
      <pubDate>2015-03-11T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>快速了解RESTEasy</title>
      <description>&lt;h1 id=&quot;resteasy&quot;&gt;什么是 RESTEasy&lt;/h1&gt;

&lt;p&gt;RESTEasy 是 JBoss 的一个开源项目，提供各种框架帮助你构建 RESTful Web Services 和 RESTful Java 应用程序。它是 JAX-RS 规范的一个完整实现并通过 JCP 认证。作为一个 JBOSS 的项目，它当然能和 JBOSS 应用服务器很好地集成在一起。 但是，它也能在任何运行 JDK5 或以上版本的 Servlet 容器中运行。RESTEasy 还提供一个 RESTEasy JAX-RS 客户端调用框架，能够很方便与 EJB、Seam、Guice、Spring 和 Spring MVC 集成使用，支持在客户端与服务器端自动实现 GZIP 解压缩。&lt;/p&gt;

&lt;p&gt;官方网站：&lt;a href=&quot;http://resteasy.jboss.org/&quot;&gt;http://resteasy.jboss.org/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;特性&lt;/h1&gt;

&lt;p&gt;直接抄自官网说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fully certified JAX-RS implementation&lt;/li&gt;
  &lt;li&gt;Portable to any app-server/Tomcat that runs on JDK 6 or higher&lt;/li&gt;
  &lt;li&gt;Embeddedable server implementation for junit testing&lt;/li&gt;
  &lt;li&gt;Client framework that leverages JAX-RS annotations so that you can write HTTP clients easily (JAX-RS only defines server bindings)&lt;/li&gt;
  &lt;li&gt;Client “Browser” cache. Supports HTTP 1.1 caching semantics including cache revalidation&lt;/li&gt;
  &lt;li&gt;Server in-memory cache. Local response cache. Automatically handles ETag generation and cache revalidation&lt;/li&gt;
  &lt;li&gt;Rich set of providers for: XML, JSON, YAML, Fastinfoset, Multipart, XOP, Atom, etc.&lt;/li&gt;
  &lt;li&gt;JAXB marshalling into XML, JSON, Jackson, Fastinfoset, and Atom as well as wrappers for maps, arrays, lists, and sets of JAXB Objects.&lt;/li&gt;
  &lt;li&gt;GZIP content-encoding. Automatic GZIP compression/decompression suppport in client and server frameworks&lt;/li&gt;
  &lt;li&gt;Asynchronous HTTP (Comet) abstractions for JBoss Web, Tomcat 6, and Servlet 3.0&lt;/li&gt;
  &lt;li&gt;Asynchronous Job Service.&lt;/li&gt;
  &lt;li&gt;Rich interceptor model.&lt;/li&gt;
  &lt;li&gt;OAuth2 and Distributed SSO with JBoss AS7&lt;/li&gt;
  &lt;li&gt;Digital Signature and encryption support with S/MIME and DOSETA&lt;/li&gt;
  &lt;li&gt;EJB, Seam, Guice, Spring, and Spring MVC integration&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;安装和配置&lt;/h1&gt;

&lt;p&gt;如果你在 Servlet3.0 容器中使用 Resteasy，则需要添加如下依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.jboss.resteasy&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;resteasy-servlet-initializer&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;3.0.9.Final&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则，如果你在 Servlet3.0 之前的容器中使用 Resteasy，则 WEB-INF/web.xml 中需要包括如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;servlet&amp;gt;
    &amp;lt;servlet-name&amp;gt;Resteasy&amp;lt;/servlet-name&amp;gt;
    &amp;lt;servlet-class&amp;gt;
        org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher
    &amp;lt;/servlet-class&amp;gt;
    &amp;lt;init-param&amp;gt;
        &amp;lt;param-name&amp;gt;javax.ws.rs.Application&amp;lt;/param-name&amp;gt;
        &amp;lt;param-value&amp;gt;com.restfully.shop.services.ShoppingApplication&amp;lt;/param-value&amp;gt;
    &amp;lt;/init-param&amp;gt;
&amp;lt;/servlet&amp;gt;

&amp;lt;servlet-mapping&amp;gt;
    &amp;lt;servlet-name&amp;gt;Resteasy&amp;lt;/servlet-name&amp;gt;
    &amp;lt;url-pattern&amp;gt;/*&amp;lt;/url-pattern&amp;gt;
&amp;lt;/servlet-mapping&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，还可以在 &lt;code&gt;&amp;lt;context-param&amp;gt;&lt;/code&gt; 节点配置如下参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;resteasy.servlet.mapping.prefix&lt;/li&gt;
  &lt;li&gt;resteasy.scan&lt;/li&gt;
  &lt;li&gt;resteasy.scan.providers&lt;/li&gt;
  &lt;li&gt;resteasy.scan.resources&lt;/li&gt;
  &lt;li&gt;resteasy.providers&lt;/li&gt;
  &lt;li&gt;resteasy.use.builtin.providers&lt;/li&gt;
  &lt;li&gt;resteasy.resources&lt;/li&gt;
  &lt;li&gt;resteasy.jndi.resources&lt;/li&gt;
  &lt;li&gt;javax.ws.rs.Application&lt;/li&gt;
  &lt;li&gt;resteasy.media.type.mappings&lt;/li&gt;
  &lt;li&gt;resteasy.language.mappings&lt;/li&gt;
  &lt;li&gt;resteasy.document.expand.entity.references&lt;/li&gt;
  &lt;li&gt;resteasy.document.secure.processing.feature&lt;/li&gt;
  &lt;li&gt;resteasy.document.secure.disableDTDs&lt;/li&gt;
  &lt;li&gt;resteasy.wider.request.matching&lt;/li&gt;
  &lt;li&gt;resteasy.use.container.form.params&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上参数在需要使用的时候查阅官方文档的说明即可。&lt;/p&gt;

&lt;p&gt;在 Servlet3.0 之前，你可以将 RESTEasy 配置为 ServletContextListener：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;listener&amp;gt;
  &amp;lt;listener-class&amp;gt;
     org.jboss.resteasy.plugins.server.servlet.ResteasyBootstrap
  &amp;lt;/listener-class&amp;gt;
&amp;lt;/listener&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样，在 Servlet3.0 之前，你可以将 RESTEasy 配置为 Servlet Filter：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;filter&amp;gt;
    &amp;lt;filter-name&amp;gt;Resteasy&amp;lt;/filter-name&amp;gt;
    &amp;lt;filter-class&amp;gt;
        org.jboss.resteasy.plugins.server.servlet.FilterDispatcher
    &amp;lt;/filter-class&amp;gt;
    &amp;lt;init-param&amp;gt;
        &amp;lt;param-name&amp;gt;javax.ws.rs.Application&amp;lt;/param-name&amp;gt;
        &amp;lt;param-value&amp;gt;com.restfully.shop.services.ShoppingApplication&amp;lt;/param-value&amp;gt;
    &amp;lt;/init-param&amp;gt;
&amp;lt;/filter&amp;gt;

&amp;lt;filter-mapping&amp;gt;
    &amp;lt;filter-name&amp;gt;Resteasy&amp;lt;/filter-name&amp;gt;
    &amp;lt;url-pattern&amp;gt;/*&amp;lt;/url-pattern&amp;gt;
&amp;lt;/filter-mapping&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;常见的注解&lt;/h1&gt;

&lt;h2 id=&quot;path-and-get-post&quot;&gt;@Path and @GET, @POST&lt;/h2&gt;

&lt;p&gt;一个示例代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Path(&quot;/library&quot;)
public class Library {

   @GET
   @Path(&quot;/books&quot;)
   public String getBooks() {...}

   @GET
   @Path(&quot;/book/{isbn}&quot;)
   public String getBook(@PathParam(&quot;isbn&quot;) String id) {
      // search my database and get a string representation and return it
   }

   @PUT
   @Path(&quot;/book/{isbn}&quot;)
   public void addBook(@PathParam(&quot;isbn&quot;) String id, @QueryParam(&quot;name&quot;) String name) {...}

   @DELETE
   @Path(&quot;/book/{id}&quot;)
   public void removeBook(@PathParam(&quot;id&quot;) String id {...}
  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;类或方法是存在 @Path 注解或者 HTTP 方法的注解&lt;/li&gt;
  &lt;li&gt;如果方法上没有 HTTP 方法的注解，则称为 JAXRSResourceLocators&lt;/li&gt;
  &lt;li&gt;@Path 注解支持正则表达式映射&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Path(&quot;/resources&quot;)
public class MyResource {

   @GET
   @Path(&quot;{var:.*}/stuff&quot;)
   public String get() {...}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面的 GETs 请求会映射到 get() 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /resources/stuff
GET /resources/foo/stuff
GET /resources/on/and/on/stuff
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表达式的格式是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;{&quot; variable-name [ &quot;:&quot; regular-expression ] &quot;}&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当正则表达式不存在时，类似于：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;([]*)&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如， &lt;code&gt;@Path(&quot;/resources/{var}/stuff&quot;)&lt;/code&gt;` 将会匹配下面请求：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /resources/foo/stuff
GET /resources/bar/stuff
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而不会匹配：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /resources/a/bunch/of/stuff
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;pathparam&quot;&gt;@PathParam&lt;/h2&gt;

&lt;p&gt;@PathParam 是一个参数注解，可以将一个 URL 上的参数映射到方法的参数上，它可以映射到方法参数的类型有基本类型、字符串、或者任何有一个字符串作为构造方法参数的 Java 对象、或者一个有字符串作为参数的静态方法 valueOf 的 Java 对象。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@GET
@Path(&quot;/book/{isbn}&quot;)
public String getBook(@PathParam(&quot;isbn&quot;) ISBN id) {...}


public class ISBN {
  public ISBN(String str) {...}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class ISBN {
 public static ISBN valueOf(String isbn) {...}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Path 注解中可以使用 @PathParam 注解对应的参数，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@GET
@Path(&quot;/aaa{param:b+}/{many:.*}/stuff&quot;)
public String getIt(@PathParam(&quot;param&quot;) String bs, @PathParam(&quot;many&quot;) String many) {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于下面的请求，对应的 param 和 many 变量如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Request&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;param&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;many&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;GET /aaabb/some/stuff&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;bb&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;some&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;GET/aaab/a/lot/of/stuff&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;b&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;a/lot/of&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;另外，@PathParam 注解也可以将 URL 后面的多个参数映射到内置的 javax.ws.rs.core.PathSegment 对象，该对象定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public interface PathSegment {

    /**
     * Get the path segment.
     * &amp;lt;p&amp;gt;
     * @return the path segment
     */
    String getPath();
    /**
     * Get a map of the matrix parameters associated with the path segment
     * @return the map of matrix parameters
     */
    MultivaluedMap&amp;lt;String, String&amp;gt; getMatrixParameters();
    
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 PathSegment 作为参数类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@GET
@Path(&quot;/book/{id}&quot;)
public String getBook(@PathParam(&quot;id&quot;) PathSegment id) {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则下面请求会映射到 getBook 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET http://host.com/library/book;name=EJB 3.0;author=Bill Burke
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;queryparam&quot;&gt;@QueryParam&lt;/h2&gt;

&lt;p&gt;对于下面的请求：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /books?num=5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用 @QueryParam 注解进行映射：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@GET
public String getBooks(@QueryParam(&quot;num&quot;) int num) {
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;headerparam&quot;&gt;@HeaderParam&lt;/h2&gt;

&lt;p&gt;@HeaderParam 注解用于将 HTTP header 中参数映射到方法的调用上，例如从 http header 中获取 From 变量的值映射到 from 参数上：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@GET
public String getBooks(@HeaderParam(&quot;From&quot;) String from) {
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同 PathParam 注解一样，方法的参数类型可以是基本类型、字符串、或者任何有一个字符串作为构造方法参数的 Java 对象、或者一个有字符串作为参数的静态方法 valueOf 的 Java 对象，例如，MediaType 对象有个 valueOf() 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@PUT
public void put(@HeaderParam(&quot;Content-Type&quot;) MediaType contentType, ...)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;matrixparam&quot;&gt;@MatrixParam&lt;/h2&gt;

&lt;p&gt;对于 URL 中的多参数，也可以使用 @MatrixParam 注解，例如对下面的请求，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET http://host.com/library/book;name=EJB 3.0;author=Bill Burke
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用下面代码来处理：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@GET
public String getBook(@MatrixParam(&quot;name&quot;) String name, @MatrixParam(&quot;author&quot;) String author) {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;cookieparam&quot;&gt;@CookieParam&lt;/h2&gt;

&lt;p&gt;获取 Cookie 参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@GET
public String getBooks(@CookieParam(&quot;sessionid&quot;) int id) {
...
}

@GET
publi cString getBooks(@CookieParam(&quot;sessionid&quot;) javax.ws.rs.core.Cookie id) {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同 PathParam 注解一样，方法的参数类型可以是基本类型、字符串、或者任何有一个字符串作为构造方法参数的 Java 对象、或者一个有字符串作为参数的静态方法 valueOf 的 Java 对象。&lt;/p&gt;

&lt;h2 id=&quot;formparam&quot;&gt;@FormParam&lt;/h2&gt;

&lt;p&gt;将表单中的字段映射到方法调用上，例如，对于下面的表单：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;form method=&quot;POST&quot; action=&quot;/resources/service&quot;&amp;gt;
First name: 
&amp;lt;input type=&quot;text&quot; name=&quot;firstname&quot;&amp;gt;
&amp;lt;br&amp;gt;
Last name: 
&amp;lt;input type=&quot;text&quot; name=&quot;lastname&quot;&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 post 方法提交，处理该请求的方法为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Path(&quot;/&quot;)
public class NameRegistry {

    @Path(&quot;/resources/service&quot;)
    @POST
    public void addName(@FormParam(&quot;firstname&quot;) String first, @FormParam(&quot;lastname&quot;) String last) {...}
}    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以添加 &lt;code&gt;application/x-www-form-urlencoded&lt;/code&gt; 来反序列化 URL 中的多参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Path(&quot;/&quot;)
public class NameRegistry {

   @Path(&quot;/resources/service&quot;)
   @POST
   @Consumes(&quot;application/x-www-form-urlencoded&quot;)
   public void addName(@FormParam(&quot;firstname&quot;) String first, MultivaluedMap&amp;lt;String, String&amp;gt; form) {...}
}       
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;form&quot;&gt;@Form&lt;/h2&gt;

&lt;p&gt;@FormParam 只是将表单字段绑定到方法的参数上，而 @Form 可以将表单绑定到一个对象上。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public static class Person{
   @FormParam(&quot;name&quot;)
   private String name;

   @Form(prefix = &quot;invoice&quot;)
   private Address invoice;

   @Form(prefix = &quot;shipping&quot;)
   private Address shipping;
}

public static class Address{

   @FormParam(&quot;street&quot;)
   private String street;
}

@Path(&quot;person&quot;)
public static class MyResource{

   @POST
   @Produces(MediaType.TEXT_PLAIN)
   @Consumes(MediaType.APPLICATION_FORM_URLENCODED)
   public String post(@Form Person p){
      return p.toString();
   }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端可以提交下面的参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name=bill
invoice.street=xxx
shipping.street=yyy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，也可以设置 prefix 参数，映射到 map 和 list：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public static class Person {
    @Form(prefix=&quot;telephoneNumbers&quot;) List&amp;lt;TelephoneNumber&amp;gt; telephoneNumbers;
    @Form(prefix=&quot;address&quot;) Map&amp;lt;String, Address&amp;gt; addresses;
}

public static class TelephoneNumber {
    @FormParam(&quot;countryCode&quot;) private String countryCode;
    @FormParam(&quot;number&quot;) private String number;
}

public static class Address {
    @FormParam(&quot;street&quot;) private String street;
    @FormParam(&quot;houseNumber&quot;) private String houseNumber;
}

@Path(&quot;person&quot;)
public static class MyResource {

    @POST
    @Consumes(MediaType.APPLICATION_FORM_URLENCODED)
    public void post (@Form Person p) {}
}    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，提交下面的参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;request.addFormHeader(&quot;telephoneNumbers[0].countryCode&quot;, &quot;31&quot;);
request.addFormHeader(&quot;telephoneNumbers[0].number&quot;, &quot;0612345678&quot;);
request.addFormHeader(&quot;telephoneNumbers[1].countryCode&quot;, &quot;91&quot;);
request.addFormHeader(&quot;telephoneNumbers[1].number&quot;, &quot;9717738723&quot;);
request.addFormHeader(&quot;address[INVOICE].street&quot;, &quot;Main Street&quot;);
request.addFormHeader(&quot;address[INVOICE].houseNumber&quot;, &quot;2&quot;);
request.addFormHeader(&quot;address[SHIPPING].street&quot;, &quot;Square One&quot;);
request.addFormHeader(&quot;address[SHIPPING].houseNumber&quot;, &quot;13&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;defaultvalue&quot;&gt;@DefaultValue&lt;/h2&gt;

&lt;p&gt;用于设置默认值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@GET
public String getBooks(@QueryParam(&quot;num&quot;) @DefaultValue(&quot;10&quot;) int num) {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;encoded--encoding&quot;&gt;@Encoded 和 @Encoding&lt;/h2&gt;

&lt;p&gt;对 @*Params  注解的参数进行编解码。&lt;/p&gt;

&lt;h2 id=&quot;context&quot;&gt;@Context&lt;/h2&gt;

&lt;p&gt;该注解允许你将以下对象注入到一个实例：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;javax.ws.rs.core.HttpHeaders,&lt;/li&gt;
  &lt;li&gt;javax.ws.rs.core.UriInfo&lt;/li&gt;
  &lt;li&gt;javax.ws.rs.core.Request&lt;/li&gt;
  &lt;li&gt;javax.servlet.HttpServletRequest&lt;/li&gt;
  &lt;li&gt;javax.servlet.HttpServletResponse&lt;/li&gt;
  &lt;li&gt;javax.servlet.ServletConfig&lt;/li&gt;
  &lt;li&gt;javax.servlet.ServletContext&lt;/li&gt;
  &lt;li&gt;javax.ws.rs.core.SecurityContext&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;produces--consumes&quot;&gt;@Produces 和 @Consumes&lt;/h2&gt;

&lt;p&gt;@Consumes 注解定义对应的方法处理的  content-type 请求类型。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Consumes(&quot;text/*&quot;)
@Path(&quot;/library&quot;)
public class Library {

    @POST
    public String stringBook(String book) {...}

    @Consumes(&quot;text/xml&quot;)
    @POST
    public String jaxbBook(Book book) {...}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当客户端发送下面请求时，stringBook() 方法会调用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; POST /library
 content-type: text/plain

 thsi sis anice book
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当客户端发送下面请求时，jaxbBook() 方法会调用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;POST /library
content-type: text/xml

&amp;lt;book name=&quot;EJB 3.0&quot; author=&quot;Bill Burke&quot;/&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;@Produces 用于映射客户端的请求并匹配客户端请求的 Accept header。&lt;/p&gt;

&lt;p&gt;例如，对下面的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Produces(&quot;text/*&quot;)
@Path(&quot;/library&quot;)
public class Library {

    @GET
    @Produces(&quot;application/json&quot;)
    public String getJSON() {...}

    @GET
    public String get() {...}
}    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则，客户端发送下面请求时，getJSON() 会被调用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /library
Accept: application/json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以修改 web.xml 中的配置，对 Accept 和 Accept-Language 做一些映射。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;web-app&amp;gt;
    &amp;lt;display-name&amp;gt;Archetype Created Web Application&amp;lt;/display-name&amp;gt;
    &amp;lt;context-param&amp;gt;
        &amp;lt;param-name&amp;gt;resteasy.media.type.mappings&amp;lt;/param-name&amp;gt;
        &amp;lt;param-value&amp;gt;html : text/html, json : application/json, xml : application/xml&amp;lt;/param-value&amp;gt;
    &amp;lt;/context-param&amp;gt;

   &amp;lt;context-param&amp;gt;
        &amp;lt;param-name&amp;gt;resteasy.language.mappings&amp;lt;/param-name&amp;gt;
        &amp;lt;param-value&amp;gt;en : en-US, es : es, fr : fr&amp;lt;/param-value&amp;gt;
   &amp;lt;/context-param&amp;gt;

    &amp;lt;servlet&amp;gt;
        &amp;lt;servlet-name&amp;gt;Resteasy&amp;lt;/servlet-name&amp;gt;
        &amp;lt;servlet-class&amp;gt;org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher&amp;lt;/servlet-class&amp;gt;
    &amp;lt;/servlet&amp;gt;

    &amp;lt;servlet-mapping&amp;gt;
        &amp;lt;servlet-name&amp;gt;Resteasy&amp;lt;/servlet-name&amp;gt;
        &amp;lt;url-pattern&amp;gt;/*&amp;lt;/url-pattern&amp;gt;
    &amp;lt;/servlet-mapping&amp;gt;
&amp;lt;/web-app&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你调用 /foo/bar.xml.en 的 GET 请求，则等同于发送下面的请求：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /foo/bar
Accept: application/xml
Accept-Language: en-US
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，你也可以设置参数映射，通过参数指定 content-type。修改 web.xml：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;web-app&amp;gt;
    &amp;lt;display-name&amp;gt;Archetype Created Web Application&amp;lt;/display-name&amp;gt;
    &amp;lt;context-param&amp;gt;
        &amp;lt;param-name&amp;gt;resteasy.media.type.param.mapping&amp;lt;/param-name&amp;gt;
        &amp;lt;param-value&amp;gt;someName&amp;lt;/param-value&amp;gt;
    &amp;lt;/context-param&amp;gt;

    &amp;lt;servlet&amp;gt;
        &amp;lt;servlet-name&amp;gt;Resteasy&amp;lt;/servlet-name&amp;gt;
        &amp;lt;servlet-class&amp;gt;org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher&amp;lt;/servlet-class&amp;gt;
    &amp;lt;/servlet&amp;gt;

    &amp;lt;servlet-mapping&amp;gt;
        &amp;lt;servlet-name&amp;gt;Resteasy&amp;lt;/servlet-name&amp;gt;
        &amp;lt;url-pattern&amp;gt;/*&amp;lt;/url-pattern&amp;gt;
    &amp;lt;/servlet-mapping&amp;gt;
&amp;lt;/web-app&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，可以通过调用 http://service.foo.com/resouce?someName=application/xml 来得到一个 application/xml 的返回结果。&lt;/p&gt;

&lt;h2 id=&quot;gzip&quot;&gt;@GZIP&lt;/h2&gt;

&lt;p&gt;配置请求输出内容为 Gzip 压缩，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Path(&quot;/&quot;)
public interface MyProxy {

   @Consumes(&quot;application/xml&quot;)
   @PUT
   public void put(@GZIP Order order);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;jax-rs-resource-locators-and-sub-resources&quot;&gt;JAX-RS Resource Locators and Sub Resources&lt;/h1&gt;

&lt;p&gt;前面提到当方法上只有 @Path 注解没有 HTTP 方法的注解时，则该方法为资源定位器，该方法可以返回一个子资源，然后由资源来定义映射路径和对应的 HTTP 方法。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Path(&quot;/&quot;)
public class ShoppingStore {

   @Path(&quot;/customers/{id}&quot;)
   public Customer getCustomer(@PathParam(&quot;id&quot;) int id) {
      Customer cust = ...; // Find a customer object
      return cust;
   }
}


public class Customer {
    @GET
    public String get() {...}

    @Path(&quot;/address&quot;)
    public String getAddress() {...}

}

public class CorporateCustomer extends Customer {
   
    @Path(&quot;/businessAddress&quot;)
    public String getAddress() {...}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问下面的请求时，会调用 ShoppingStore 类的 getCustomer 方法，然后调用 Customer 类的 get() 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /customer/123
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;类似地，下面的请求会返回 Customer 类的 getAddress() 方法的值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /customer/123/address
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;cors&quot;&gt;CORS&lt;/h1&gt;

&lt;p&gt;在 Application 类中，注册一个单例的提供者类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;CorsFilter filter = new CorsFilter();
filter.getAllowedOrigins().add(&quot;http://localhost&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;content-range-support&quot;&gt;Content-Range Support&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Path(&quot;/&quot;)
public class Resource {
  @GET
  @Path(&quot;file&quot;)
  @Produces(&quot;text/plain&quot;)
  public File getFile()
  {
     return file;
  }
}

Response response = client.target(generateURL(&quot;/file&quot;)).request()
      .header(&quot;Range&quot;, &quot;1-4&quot;).get();
Assert.assertEquals(response.getStatus(), 206);
Assert.assertEquals(4, response.getLength());
System.out.println(&quot;Content-Range: &quot; + response.getHeaderString(&quot;Content-Range&quot;));
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2015/03/10/quick-start-of-resteasy.html</link>
      <guid>http://blog.javachen.com/2015/03/10/quick-start-of-resteasy.html</guid>
      <pubDate>2015-03-10T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Java笔记：异常</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;定义&lt;/h1&gt;

&lt;p&gt;在《java编程思想》中这样定义异常：阻止当前方法或作用域继续执行的问题。异常是Java程序设计中不可分割的一部分，如果不了解如何使用它们，那么我们只能完成很有限的工作。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;分类&lt;/h1&gt;

&lt;p&gt;异常分为3种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Error - 描述了Java运行系统中的内部错误以及资源耗尽的情况。应用程序不应该抛出这种类型的对象。如果这种内部错误出现，除了通知用户错误发生以及尽力安全的退出程序外，在其他方面是无能为力的。&lt;/li&gt;
  &lt;li&gt;编译时异常 Exception - 它指出了合理的应用程序想要捕获的条件。Exception又分为两类：IOException和RuntimeException。由编程导致的错误，会导致RuntimeException异常。而其他错误原因导致的异常（例如因为I/O错误导致曾经运行正确的程序出错），都不会导致RuntimeException异常。&lt;/li&gt;
  &lt;li&gt;运行时 RuntimeException - 表示运行时异常，不强制要求写出显示的捕获代码，但如果没有被捕获到，则线程会被强制中断&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;继承关系：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Throwable
|--Error 
|--Exception 
|--RuntimeException
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 RuntimeException 衍生出来的异常包括下面的问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;错误的类型转换&lt;/li&gt;
  &lt;li&gt;数组越界访问&lt;/li&gt;
  &lt;li&gt;试图访问空指针&lt;/li&gt;
  &lt;li&gt;从 IOException 衍生出来的异常包括：&lt;/li&gt;
  &lt;li&gt;试图从文件尾后面读取数据&lt;/li&gt;
  &lt;li&gt;试图打开一个错误可是的 URL&lt;/li&gt;
  &lt;li&gt;试图用一个字符串来构造一个 Class 对象，而与该字符串对应的类并不存在&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;异常的好处：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、将问题进行封装&lt;/li&gt;
  &lt;li&gt;2、将正常流程代码和问题代码相分离，便于阅读。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;处理异常机制&lt;/h1&gt;

&lt;p&gt;在Java应用程序中，异常处理机制为：抛出异常，捕捉异常。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;抛出异常&lt;/code&gt;：当一个方法出现错误引发异常时，方法创建异常对象并交付运行时系统，异常对象中包含了异常类型和异常出现时的程序状态等异常信息。运行时系统负责寻找处置异常的代码并执行。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;捕获异常&lt;/code&gt;：在方法抛出异常之后，运行时系统将转为寻找合适的异常处理器（exception handler）。潜在的异常处理器是异常发生时依次存留在调用栈中的方法的集合。当异常处理器所能处理的异常类型与方法抛出的异常类型相符时，即为合适的异常处理器。运行时系统从发生异常的方法开始，依次回查调用栈中的方法，直至找到含有合适异常处理器的方法并执行。当运行时系统遍历调用栈而未找到合适的异常处理器，则运行时系统终止。同时，意味着Java程序的终止。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;任何 Java 代码都可以抛出异常，如：自己编写的代码、来自 Java 开发环境包中代码，或者 Java 运行时系统。无论是谁，都可以通过 Java 的 throw 语句抛出异常。&lt;/p&gt;

&lt;p&gt;从方法中抛出的任何异常都必须使用 &lt;code&gt;throws&lt;/code&gt; 子句。&lt;/p&gt;

&lt;p&gt;捕捉异常通过 &lt;code&gt;try-catch&lt;/code&gt; 语句或者 &lt;code&gt;try-catch-finally&lt;/code&gt; 语句实现。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;原则&lt;/h1&gt;

&lt;p&gt;编译时异常Exception,给了几条禁止的原则，他们是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）不要直接忽略异常；&lt;/li&gt;
  &lt;li&gt;2）不要用try-catch包住过多语句；&lt;/li&gt;
  &lt;li&gt;3）不要用异常处理来处理程序的正常控制流；&lt;/li&gt;
  &lt;li&gt;4）不要随便将异常迎函数栈向上传递，能处理尽量处理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;向上传播异常：&lt;/p&gt;

&lt;p&gt;如果不能用上述恢复措施，就检查能不能向上传播，什么情况下可以向上传播呢？有多种说法，一种说法是当本方法恢复不了时，这个说法显然是错误，因为上层也不一定能恢复。另外还有两种说法是：1.当上层逻辑可以恢复程序时；2.当本方法除了打印之外不能做任何处理，而且不确定上层能否处理。这种两种说法都是正确的，但还不够，因为也有的情况，明确知道上层恢复不了也需要上层处理，所以我认为正确的做法是：当你认为本异常应该由上层处理时，才向上传播。&lt;/p&gt;

&lt;p&gt;何时选用编译时异常：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、如果调用者可以恢复此异常情况&lt;/li&gt;
  &lt;li&gt;2、如果调用者不能恢复，但能做出有意义的事，如转译等。如果你不确定调用者能否做出有意义的事，就别使编译时异常，免得被抱怨。&lt;/li&gt;
  &lt;li&gt;3、应尽最大可能使用编译时异常来代替错误码，这条也是编译时异常设计的目的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外，必须注意使用编译时异常的目的是为了恢复执行，所以设计异常类的时候，应提供尽量多的异常数据，以便于上层恢复，比如一个解析错误，可以在设计的异常类写几个变量来存储异常数据：解析出错的句子的内容，解析出错句子的行号，解析出错的字符在行中的位置。这些信息可能帮助调用恢复程序。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;注意事项&lt;/h1&gt;

&lt;p&gt;当使用多个 catch 语句块来捕获异常时，需要将父类的 catch 语句块放到子类型的 catch 块之后，这样才能保证后续的 catch 可能被执行，否则子类型的 catch 将永远无法到达，Java 编译器会报编译错误。&lt;/p&gt;

&lt;p&gt;如果 try 语句块中存在 return 语句，那么首先会执行 finally 语句块中的代码，然后才返回。&lt;/p&gt;

&lt;p&gt;如果 try 语句块中存在 &lt;code&gt;System.exit(0)&lt;/code&gt; 语句，那么久不会执行 finally 语句块的代码了，因为 &lt;code&gt;System.exit(0)&lt;/code&gt;会终止当前运行的 JVM。程序在 JVM 终止前结束执行。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;常见面试题目&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;1、error和exception有什么区别&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;error 表示系统级的错误，是java运行环境内部错误或者硬件问题，不能指望程序来处理这样的问题，除了退出运行外别无选择，它是Java虚拟机抛出的。&lt;/p&gt;

&lt;p&gt;exception 表示程序需要捕捉、需要处理的异常，是由与程序设计的不完善而出现的问题，程序必须处理的问题&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2、运行时异常和一般异常有何不同&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Java 提供了两类主要的异常：runtimeException 和 checkedException&lt;/p&gt;

&lt;p&gt;一般异常（checkedException）主要是指 IO 异常、SQL 异常等。对于这种异常，JVM 要求我们必须对其进行 cathc 处理，所以，面对这种异常，不管我们是否愿意，都是要 写一大堆的 catch 块去处理可能出现的异常。&lt;/p&gt;

&lt;p&gt;运行时异常（runtimeException）我们一般不处理，当出现这类异常的时候程序会由虚拟机接管。比如，我们从来没有去处理过 NullPointerException，而且这个异常还是最 常见的异常之一。&lt;/p&gt;

&lt;p&gt;出现运行时异常的时候，程序会将异常一直向上抛，一直抛到遇到处理代码，如果没有 catch 块进行处理，到了最上层，如果是多线程就有 Thread.run()抛出，如果不是多线程 那么就由 &lt;code&gt;main.run()&lt;/code&gt; 抛出。抛出之后，如果是线程，那么该线程也就终止了，如果是主程序，那么该程序也就终止了。&lt;/p&gt;

&lt;p&gt;其实运行时异常的也是继承自 Exception，也可以用 catch 块对其处理，只是我们一般不处理罢了，也就是说，如果不对运行时异常进行 catch 处理，那么结果不是线程退出就是 主程序终止。&lt;/p&gt;

&lt;p&gt;如果不想终止，那么我们就必须捕获所有可能出现的运行时异常。如果程序中出现了异常数据，但是它不影响下面的程序执行，那么我们就该在catch块里面将异常数据舍弃， 然后记录日志。如果，它影响到了下面的程序运行，那么还是程序退出比较好些。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3、Java 中异常处理机制的原理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jav a通过面向对象的方式对异常进行处理，Java 把异常按照不同的类型进行分类，并提供了良好的接口。在 Java 中，每个异常都是一个对象，它都是 Throwable 或其子类的实例。当一个方法出现异常后就会抛出一个异常对象，该对象中包含有异常信息，调用这个对象的方法可以捕获到这个异常并对异常进行处理。Java 的异常处理是通过5个 关键词来实现的：try、catch、throw、throws、finally。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;try&lt;/code&gt;：用来指定一块预防所有异常的程序&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;catch&lt;/code&gt;：紧跟在try后面，用来捕获异常&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;throw&lt;/code&gt;：用来明确的抛出一个异常&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;throws&lt;/code&gt;：用来标明一个成员函数可能抛出的各种异常&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;finally&lt;/code&gt;：确保一段代码无论发生什么异常都会被执行的一段代码。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;4、你平时在项目中是怎样对异常进行处理的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1）尽量避免出现 runtimeException。例如对于可能出现空指针的代码，带使用对象之前一定要判断一下该对象是否为空，必要的时候对 runtimeException也进行 &lt;code&gt;try catch&lt;/code&gt; 处理。&lt;/p&gt;

&lt;p&gt;2）进行 &lt;code&gt;try catch&lt;/code&gt; 处理的时候要在 catch 代码块中对异常信息进行记录，通过调用异常类的相关方法获取到异常的相关信息。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;5、final、finally、finalize的区别&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;（1）、final 用于声明变量、方法和类的，分别表示变量值不可变，方法不可覆盖，类不可以继承&lt;/p&gt;

&lt;p&gt;（2）、finally 是异常处理中的一个关键字，表示 &lt;code&gt;finally{}&lt;/code&gt; 里面的代码一定要执行&lt;/p&gt;

&lt;p&gt;（3）、finalize 是 Object 类的一个方法，在垃圾回收的时候会调用被回收对象的此方法。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://motang.github.io/tools/2015/02/13/java%E5%BC%82%E5%B8%B8.html&quot;&gt;JAVA异常笔记&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://p.primeton.com/articles/54d09d4fbe20aa40120000ed&quot;&gt;Java异常学习笔记&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/03/04/note-about-java-exception.html</link>
      <guid>http://blog.javachen.com/2015/03/04/note-about-java-exception.html</guid>
      <pubDate>2015-03-04T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>安装和配置Hue</title>
      <description>&lt;p&gt;本文主要记录使用 yum 源安装 Hue 以及配置 Hue 集成 Hdfs、Hive、Impala、Yarn、Kerberos、LDAP、Sentry、Solr 等的过程。&lt;/p&gt;

&lt;p&gt;安装环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：&lt;code&gt;CentOs6.5&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Hadoop：&lt;code&gt;cdh5.2.0&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Hue：&lt;code&gt;3.6.0+cdh5.2.0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hue&quot;&gt;安装 Hue&lt;/h1&gt;

&lt;p&gt;在 Hadoop 集群的一个节点上安装 Hue server，这里我是在我的测试集群中的 &lt;code&gt;cdh1&lt;/code&gt; 节点上安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hue hue-server
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hue-1&quot;&gt;配置 Hue&lt;/h1&gt;

&lt;h2 id=&quot;hue-server&quot;&gt;配置hue server&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[desktop]
    http_host=cdh1
    http_port=8888
    secret_key=qpbdxoewsqlkhztybvfidtvwekftusgdlofbcfghaswuicmqp
    time_zone=Asia/Shanghai
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想配置 SSL，则添加下面设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;ssl_certificate=/path/to/certificate
ssl_private_key=/path/to/key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并使用下面命令生成证书：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Create a key 
$ openssl genrsa 1024 &amp;gt; host.key 
# Create a self-signed certificate 
$ openssl req -new -x509 -nodes -sha1 -key host.key &amp;gt; host.cert
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;db-query&quot;&gt;配置 DB Query&lt;/h2&gt;

&lt;p&gt;DB Query 的相关配置在 hue.ini 中 databases 节点下面，目前共支持 sqlite, mysql, postgresql 和 oracle 四种数据库，默认使用的是 sqlite 数据库，你可以按自己的需要修改为其他的数据库。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[[database]]
    engine=sqlite3
    name=/var/lib/hue/desktop.db
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hadoop-&quot;&gt;配置 Hadoop 参数&lt;/h2&gt;

&lt;h3 id=&quot;hdfs-&quot;&gt;HDFS 集群配置&lt;/h3&gt;

&lt;p&gt;在 hadoop.hdfs_clusters.default 节点下配置以下参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;fs_defaultfs&lt;/code&gt;：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;logical_name&lt;/code&gt;： NameNode 逻辑名称&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;webhdfs_url&lt;/code&gt;：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;security_enabled&lt;/code&gt;：是否开启 Kerberos&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hadoop_conf_dir&lt;/code&gt;： hadoop 配置文件路径&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[hadoop]
  [[hdfs_clusters]]
    [[[default]]]
      # Enter the filesystem uri
      fs_defaultfs=hdfs://mycluster

      # NameNode logical name.
      logical_name=mycluster

      # Use WebHdfs/HttpFs as the communication mechanism.
      # Domain should be the NameNode or HttpFs host.
      # Default port is 14000 for HttpFs.
      ## webhdfs_url=http://localhost:50070/webhdfs/v1
      webhdfs_url=http://cdh1:14000/webhdfs/v1

      # Change this if your HDFS cluster is Kerberos-secured
      security_enabled=true

      hadoop_conf_dir=/etc/hadoop/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;webhdfs--httpfs&quot;&gt;配置 WebHDFS 或者 HttpFS&lt;/h3&gt;

&lt;p&gt;Hue 可以通过下面两种方式访问 Hdfs 中的数据：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;WebHDFS&lt;/code&gt;：提供高速的数据传输，客户端直接和 DataNode 交互&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;HttpFS&lt;/code&gt;：一个代理服务，方便与集群外部的系统集成&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两者都支持 HTTP REST API，但是 Hue 只能配置其中一种方式；对于 HDFS HA部署方式，只能使用 HttpFS。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、对于 WebHDFS 方式，在每个节点上的 hdfs-site.xml 文件添加如下配置并重启服务：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.webhdfs.enabled&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;2、 配置 Hue 为其他用户和组的代理用户。对于 WebHDFS 方式，在 core-site.xml 添加：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;!-- Hue WebHDFS proxy user setting --&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.hue.hosts&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.hue.groups&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 HttpFS 方式，在 /etc/hadoop-httpfs/conf/httpfs-site.xml  中添加下面配置并重启 HttpFS 进程：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;!-- Hue HttpFS proxy user setting --&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;httpfs.proxyuser.hue.hosts&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;httpfs.proxyuser.hue.groups&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 HttpFS 方式，在 core-site.xml 中添加下面配置并重启 hadoop 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;hadoop.proxyuser.httpfs.hosts&amp;lt;/name&amp;gt;  
    &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;hadoop.proxyuser.httpfs.groups&amp;lt;/name&amp;gt;  
    &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;3、修改 /etc/hue/conf/hue.ini 中 hadoop.hdfs_clusters.default.webhdfs_url 属性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于 WebHDFS：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;webhdfs_url=http://cdh1:50070/webhdfs/v1/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 HttpFS：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;webhdfs_url=http://cdh1:14000/webhdfs/v1/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;yarn-&quot;&gt;YARN 集群配置&lt;/h3&gt;

&lt;p&gt;在 hadoop.yarn_clusters.default 节点下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[hadoop]
  [[yarn_clusters]]
    [[[default]]]
        resourcemanager_host=cdh1
        resourcemanager_port=8032
        submit_to=True
        security_enabled=true
        resourcemanager_api_url=http://cdh1:8088
        proxy_api_url=http://cdh1:8088
        history_server_api_url=http://cdh1:19888
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hive&quot;&gt;集成 Hive&lt;/h2&gt;

&lt;p&gt;在 beeswax 节点下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[beeswax]
    hive_server_host=cdh1
    hive_server_port=10000 
    hive_conf_dir=/etc/hive/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里是配置为连接一个 Hive Server2 节点，如有需要可以配置负载均衡，连接一个负载节点。&lt;/p&gt;

&lt;h2 id=&quot;impala&quot;&gt;集成 Impala&lt;/h2&gt;

&lt;p&gt;在 impala 节点下配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[impala]
  # Host of the Impala Server (one of the Impalad)
  server_host=cdh1

  # Port of the Impala Server
  server_port=21050

  # Kerberos principal
  impala_principal=impala/cdh1@JAVACHEN.COM

  # Turn on/off impersonation mechanism when talking to Impala
  impersonation_enabled=True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里是配置为连接一个 Impala Server 节点，如有需要可以配置负载均衡，连接一个负载节点。&lt;/p&gt;

&lt;p&gt;参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/v5-2-x/topics/impala_authorization.html#impersonation_unique_1&quot;&gt;Configuring Per-User Access for Hue&lt;/a&gt; 和 &lt;a href=&quot;http://gethue.com/use-the-impala-app-with-sentry-for-real-security/&quot;&gt;Use the Impala App with Sentry for real security&lt;/a&gt;，在配置 &lt;code&gt;impersonation_enabled&lt;/code&gt; 为 true 的情况下，还需要在 impalad 的启动参数中添加 &lt;code&gt;authorized_proxy_user_config&lt;/code&gt; 参数，修改 /etc/default/impala中的 IMPALA_SERVER_ARGS 添加下面一行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;-authorized_proxy_user_config=hue=*  \
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，如果集群开启了 Kerberos，别忘了配置 &lt;code&gt;impala_principal&lt;/code&gt; 参数。&lt;/p&gt;

&lt;h2 id=&quot;kerberos&quot;&gt;集成 kerberos&lt;/h2&gt;

&lt;p&gt;首先，需要在 kerberos server 节点上生成 hue 用户的凭证，并将其拷贝到 /etc/hue/conf 目录。：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kadmin: addprinc -randkey hue/cdh1@JAVACHEN.COM
$ kadmin: xst -k hue.keytab hue/cdh1@JAVACHEN.COM

$ cp hue.keytab /etc/hue/conf/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，修改 hue.ini 中 kerberos 节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[[kerberos]]
    # Path to Hue&#39;s Kerberos keytab file
    hue_keytab=/etc/hue/conf/hue.keytab

    # Kerberos principal name for Hue
    hue_principal=hue/cdh1@JAVACHEN.COM

    # Path to kinit
    kinit_path=/usr/bin/kinit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，修改 /etc/hadoop/conf/core-site.xml，添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;!--hue kerberos--&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.proxyuser.hue.groups&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.proxyuser.hue.hosts&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hue.kerberos.principal.shortname&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hue&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，重启 hadoop 服务。&lt;/p&gt;

&lt;h2 id=&quot;ldap&quot;&gt;集成 LDAP&lt;/h2&gt;

&lt;p&gt;开启 ldap 验证，使用 ldap 用户登录 hue server，修改 auth 节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[desktop]
    [[auth]]
        backend=desktop.auth.backend.LdapBackend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外修改 ldap 节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[desktop]
    [[ldap]]
        base_dn=&quot;dc=javachen,dc=com&quot;
        ldap_url=ldap://cdh1

        # ldap用户登陆时自动在hue创建用户
        create_users_on_login = true

        # 开启direct bind mechanism
        search_bind_authentication=false

        # ldap登陆用户的模板，username运行时被替换
        ldap_username_pattern=&quot;uid=&amp;lt;username&amp;gt;,ou=people,dc=javachen,dc=com&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：在开启ldap验证前，先普通方法创建一个ldap存在的用户，赋超级用户权限，否则无法管理hue用户。&lt;/p&gt;

&lt;h2 id=&quot;sentry&quot;&gt;集成 Sentry&lt;/h2&gt;

&lt;p&gt;如果 hive 和 impala 中集成了 Sentry，则需要修改 hue.ini 中的 libsentry 节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[libsentry]
  # Hostname or IP of server.
  hostname=cdh1

  # Port the sentry service is running on.
  port=8038

  # Sentry configuration directory, where sentry-site.xml is located.
  sentry_conf_dir=/etc/sentry/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，修改 /etc/sentry/conf/sentry-store-site.xml 确保 hue 用户可以连接 sentry：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;sentry.service.allow.connect&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;impala,hive,solr,hue&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sqoop2&quot;&gt;集成 Sqoop2&lt;/h2&gt;

&lt;p&gt;在 sqoop 节点配置 server_url 参数为 sqoop2 的地址即可。&lt;/p&gt;

&lt;h2 id=&quot;hbase&quot;&gt;集成 HBase&lt;/h2&gt;

&lt;p&gt;在 hbase 节点配置下面参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;truncate_limit&lt;/code&gt;：Hard limit of rows or columns per row fetched before truncating.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase_clusters&lt;/code&gt;：HBase Thrift 服务列表，例如：&lt;code&gt;Cluster1|cdh1:9090,Cluster2|cdh2:9090&lt;/code&gt;，默认为：&lt;code&gt;Cluster|localhost:9090&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;zookeeper&quot;&gt;集成 Zookeeper&lt;/h2&gt;

&lt;p&gt;在 zookeeper 节点配置下面两个参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;host_ports&lt;/code&gt;：zookeeper 节点列表，例如：&lt;code&gt;localhost:2181,localhost:2182,localhost:2183&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;rest_url&lt;/code&gt;：zookeeper 的 REST 接口，默认值为 &lt;a href=&quot;http://localhost:9998&quot;&gt;http://localhost:9998&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;oozie&quot;&gt;集成 Oozie&lt;/h2&gt;

&lt;p&gt;未使用，暂不记录。&lt;/p&gt;

&lt;h1 id=&quot;hue-2&quot;&gt;管理 Hue&lt;/h1&gt;

&lt;p&gt;如果配置了 kerberos，则先获取 hue 凭证：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;kinit -k -t /etc/hue/conf/hue.keytab hue/cdh1@JAVACHEN.COM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动 hue server：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service hue start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止 hue server：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service hue stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hue server 默认使用 8888 作为 web 访问端口，故需要在防火墙上开放该端口。&lt;/p&gt;

&lt;p&gt;你可以在 /var/log/hue 目录查看 hue 的日志，或者通过 &lt;a href=&quot;http://cdh1:8888/logs&quot;&gt;http://cdh1:8888/logs&lt;/a&gt; 查看。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;测试&lt;/h1&gt;

&lt;p&gt;在开启了 LDAP 后，使用 LDAP 中的管理员用户登录 hue，根据提示向导进行设置并将 LDAP 中的用户同步到 Hue Server，然后依次测试每一个功能是否运行正常。&lt;/p&gt;

&lt;p&gt;关于 Hue 的使用向导，请参考 &lt;a href=&quot;http://archive.cloudera.com/cdh5/cdh/5/hue/user-guide/index.html&quot;&gt;http://archive.cloudera.com/cdh5/cdh/5/hue/user-guide/index.html&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Enjoy it !&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2015/02/28/install-and-config-hue.html</link>
      <guid>http://blog.javachen.com/2015/02/28/install-and-config-hue.html</guid>
      <pubDate>2015-02-28T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hadoop Streaming 原理</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;简介&lt;/h1&gt;

&lt;p&gt;Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的优势和能力，来处理大数据。&lt;/p&gt;

&lt;p&gt;一个简单的示例，以 shell 脚本为例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /usr/bin/wc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Streaming 方式是 &lt;code&gt;基于 Unix 系统的标准输入输出&lt;/code&gt; 来进行 MapReduce Job 的运行，它区别与 Pipes 的地方主要是通信协议，Pipes 使用的是 Socket 通信，是对使用 C++ 语言来实现 MapReduce Job 并通过 Socket 通信来与 Hadopp 平台通信，完成 Job 的执行。&lt;/p&gt;

&lt;p&gt;任何支持标准输入输出特性的编程语言都可以使用 Streaming 方式来实现 MapReduce Job，基本原理就是输入从 Unix 系统标准输入，输出使用 Unix 系统的标准输出。&lt;/p&gt;

&lt;p&gt;Hadoop 是使用 Java 语言编写的，所以最直接的方式的就是使用 Java 语言来实现 Mapper 和 Reducer，然后配置 MapReduce Job，提交到集群计算环境来完成计算。但是很多开发者可能对 Java 并不熟悉，而是对一些具有脚本特性的语言，如 C++、Shell、Python、 Ruby、PHP、Perl 有实际开发经验，Hadoop Streaming 为这一类开发者提供了使用 Hadoop 集群来进行处理数据的工具，即工具包 hadoop-streaming.jar。&lt;/p&gt;

&lt;p&gt;在标准的输入输出中，Key 和 Value 是以 Tab 作为分隔符，并且在 Reducer 的标准输入中，Hadoop 框架保证了输入的数据是经过了按 Key 排序的。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;原理&lt;/h1&gt;

&lt;p&gt;Hadoop Streaming 使用了 Unix 的标准输入输出作为 Hadoop 和其他编程语言的开发接口，因此在其他的编程语言所写的程序中，只需要将标准输入作为程序的输入，将标准输出作为程序的输出就可以了。&lt;/p&gt;

&lt;p&gt;mapper 和 reducer 会从标准输入中读取用户数据，一行一行处理后发送给标准输出。Streaming 工具会创建 MapReduce 作业，发送给各个 tasktracker，同时监控整个作业的执行过程。&lt;/p&gt;

&lt;p&gt;如果一个文件（可执行或者脚本）作为 mapper，mapper 初始化时，每一个 mapper 任务会把该文件作为一个单独进程启动，mapper 任务运行时，它把输入切分成行并把每一行提供给可执行文件进程的标准输入。 同时，mapper 收集可执行文件进程标准输出的内容，并把收到的每一行内容转化成 key/value 对，作为 mapper 的输出。 默认情况下，一行中第一个 tab 之前的部分作为 key，之后的（不包括tab）作为 value。如果没有 tab，整行作为 key 值，value 值为 null。&lt;/p&gt;

&lt;p&gt;对于 reducer，类似。&lt;/p&gt;

&lt;p&gt;以上是 Map/Reduce 框架和 streaming mapper/reducer 之间的基本通信协议。&lt;/p&gt;

&lt;p&gt;用户可以定义 &lt;code&gt;stream.non.zero.exit.is.failure&lt;/code&gt; 参数为 true 或者 false 以定义一个以非0状态退出的 streaming 的任务是失败还是成功。默认情况下，以非0状态退出的任务都任务是失败的。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;用法&lt;/h1&gt;

&lt;p&gt;命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar [genericOptions] [streamingOptions]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;streaming-&quot;&gt;streaming 参数&lt;/h2&gt;

&lt;p&gt;以 Hadoop 2.6.0 为例，可选的 streaming 参数如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;是否可选&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-input directoryname or filename&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Required&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapper的输入路径&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-output directoryname&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Required&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;reducer输出路径&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-mapper executable or JavaClassName&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Required&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Mapper可执行程序或 Java 类名&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-reducer executable or JavaClassName&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Required&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Reducer 可执行程序或 Java 类名&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-file filename&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;mapper, reducer 或 combiner 依赖的文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-inputformat JavaClassName&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;key/value 输入格式，默认为 TextInputFormat&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-outputformat JavaClassName&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;key/value 输出格式，默认为  TextOutputformat&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-partitioner JavaClassName&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Class that determines which reduce a key is sent to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-combiner streamingCommand or JavaClassName&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;map 输出结果执行 Combiner 的命令或者类名&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-cmdenv name=value&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;环境变量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-inputreader&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;向后兼容，定义输入的 Reader 类，用于取代输出格式&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-verbose&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;输出日志&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-lazyOutput&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;延时输出&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-numReduceTasks&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义 reduce 数量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-mapdebug&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;map 任务运行失败时候，执行的脚本&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-reducedebug&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;reduce 任务运行失败时候，执行的脚本&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;定义 Java 类作为 mapper 和 reducer：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat \
    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \
    -reducer /usr/bin/wc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 mapper 和 reducer 的可执行文件在集群上不存在，则可以通过  &lt;code&gt;-file&lt;/code&gt; 参数将其提交到集群上去：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper myPythonScript.py \
    -reducer /usr/bin/wc \
    -file myPythonScript.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以将 mapper 和 reducer 的可执行文件用到的文件和配置上传到集群上：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper myPythonScript.py \
    -reducer /usr/bin/wc \
    -file myPythonScript.py \
    -file myDictionary.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以定义其他参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-inputformat JavaClassName
-outputformat JavaClassName
-partitioner JavaClassName
-combiner streamingCommand or JavaClassName
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定义一个环境变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-cmdenv EXAMPLE_DIR=/home/example/dictionaries/   
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;通用参数&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;是否可选&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-conf configuration_file&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义应用的配置文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-D property=value&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义参数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-fs host:port or local&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义 namenode 地址&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-files&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义需要拷贝到 Map/Reduce 集群的文件，多个文件以逗号分隔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-libjars&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义需要引入到 classpath 的 jar 文件，多个文件以逗号分隔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-archives&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Optional&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义需要解压到计算节点的压缩文件，多个文件以逗号分隔&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;定义参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-D mapred.local.dir=/tmp/local
-D mapred.system.dir=/tmp/system
-D mapred.temp.dir=/tmp/temp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定义 reduce 个数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-D mapreduce.job.reduces=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以使用 &lt;code&gt;-D stream.reduce.output.field.separator=SEP&lt;/code&gt; 和 &lt;code&gt;-D stream.num.reduce.output.fields=NUM&lt;/code&gt; 自定义 mapper 输出的分隔符为SEP，并且按 SEP 分隔之后的前 NUM 部分内容作为 key，如果分隔符少于 NUM，则整行作为 key。例如，下面的例子指定分隔符为 &lt;code&gt;....&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar \
    -D stream.map.output.field.separator=. \
    -D stream.num.map.output.key.fields=4 \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /bin/cat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hadoop 提供配置供用户自主设置分隔符：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;-D stream.map.output.field.separator&lt;/code&gt; ：设置 map 输出中 key 和 value 的分隔符 &lt;br /&gt;
&lt;code&gt;-D stream.num.map.output.key.fields&lt;/code&gt; ：设置 map 程序分隔符的位置，该位置之前的部分作为 key，之后的部分作为 value &lt;br /&gt;
&lt;code&gt;-D map.output.key.field.separator&lt;/code&gt; : 设置 map 输出分区时 key 内部的分割符&lt;br /&gt;
&lt;code&gt;-D mapreduce.partition.keypartitioner.options&lt;/code&gt; : 指定分桶时，key 按照分隔符切割后，其中用于分桶 key 所占的列数（配合 &lt;code&gt;-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner&lt;/code&gt; 使用）&lt;br /&gt;
&lt;code&gt;-D stream.reduce.output.field.separator&lt;/code&gt;：设置 reduce 输出中 key 和 value 的分隔符 &lt;br /&gt;
&lt;code&gt;-D stream.num.reduce.output.key.fields&lt;/code&gt;：设置 reduce 程序分隔符的位置&lt;/p&gt;

&lt;p&gt;定义解压文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ls test_jar/
cache.txt  cache2.txt

$ jar cvf cachedir.jar -C test_jar/ .
added manifest
adding: cache.txt(in = 30) (out= 29)(deflated 3%)
adding: cache2.txt(in = 37) (out= 35)(deflated 5%)

$ hdfs dfs -put cachedir.jar samples/cachefile

$ hdfs dfs -cat /user/root/samples/cachefile/input.txt
cachedir.jar/cache.txt
cachedir.jar/cache2.txt

$ cat test_jar/cache.txt
This is just the cache string

$ cat test_jar/cache2.txt
This is just the second cache string

$ hadoop jar hadoop-streaming.jar \
                  -archives &#39;hdfs://hadoop-nn1.example.com/user/root/samples/cachefile/cachedir.jar&#39; \
                  -D mapreduce.job.maps=1 \
                  -D mapreduce.job.reduces=1 \
                  -D mapreduce.job.name=&quot;Experiment&quot; \
                  -input &quot;/user/root/samples/cachefile/input.txt&quot; \
                  -output &quot;/user/root/samples/cachefile/out&quot; \
                  -mapper &quot;xargs cat&quot; \
                  -reducer &quot;cat&quot;

$ hdfs dfs -ls /user/root/samples/cachefile/out
Found 2 items
-rw-r--r--   1 root supergroup        0 2013-11-14 17:00 /user/root/samples/cachefile/out/_SUCCESS
-rw-r--r--   1 root supergroup       69 2013-11-14 17:00 /user/root/samples/cachefile/out/part-00000

$ hdfs dfs -cat /user/root/samples/cachefile/out/part-00000
This is just the cache string
This is just the second cache string
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;复杂的例子&lt;/h2&gt;

&lt;h3 id=&quot;hadoop-partitioner-class&quot;&gt;Hadoop Partitioner Class&lt;/h3&gt;

&lt;p&gt;Hadoop 中有一个类 &lt;a href=&quot;http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/lib/KeyFieldBasedPartitioner.html&quot;&gt;KeyFieldBasedPartitioner&lt;/a&gt;，可以将 map 输出的内容按照分隔后的一定列，而不是整个 key 内容进行分区，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar \
    -D stream.map.output.field.separator=. \
    -D stream.num.map.output.key.fields=4 \
    -D map.output.key.field.separator=. \
    -D mapreduce.partition.keypartitioner.options=-k1,2 \
    -D mapreduce.job.reduces=12 \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /bin/cat \
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关键参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;map.output.key.field.separator=.&lt;/code&gt;：设置 map 输出分区时 key 内部的分割符为 &lt;code&gt;.&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.partition.keypartitioner.options=-k1,2&lt;/code&gt;：设置按前两个字段分区&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.job.reduces=12&lt;/code&gt;：reduce 数为12&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;假设 map 的输出为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;11.12.1.2
11.14.2.3
11.11.4.1
11.12.1.1
11.14.2.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;按照前两个字段进行分区，则会分为三个分区：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;11.11.4.1
-----------
11.12.1.2
11.12.1.1
-----------
11.14.2.3
11.14.2.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在每个分区内对整行内容排序后为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;11.11.4.1
-----------
11.12.1.1
11.12.1.2
-----------
11.14.2.2
11.14.2.3
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;hadoop-comparator-class&quot;&gt;Hadoop Comparator Class&lt;/h3&gt;

&lt;p&gt;Hadoop 中有一个类 &lt;a href=&quot;http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html&quot;&gt;KeyFieldBasedComparator&lt;/a&gt;，提供了 Unix/GNU 中排序的一部分特性。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar \
    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
    -D stream.map.output.field.separator=. \
    -D stream.num.map.output.key.fields=4 \
    -D mapreduce.map.output.key.field.separator=. \
    -D mapreduce.partition.keycomparator.options=-k2,2nr \
    -D mapreduce.job.reduces=1 \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /bin/cat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关键参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.partition.keycomparator.options=-k2,2nr&lt;/code&gt;：指定第二个字段为排序字段，&lt;code&gt;-n&lt;/code&gt; 是指按自然顺序排序，&lt;code&gt;-r&lt;/code&gt; 指倒叙排序。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;假设 map 的输出为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;11.12.1.2
11.14.2.3
11.11.4.1
11.12.1.1
11.14.2.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则 reduce 输出结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;11.14.2.3
11.14.2.2
11.12.1.2
11.12.1.1
11.11.4.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;hadoop-aggregate-package&quot;&gt;Hadoop Aggregate Package&lt;/h3&gt;

&lt;p&gt;Hadoop 中有一个类 &lt;a href=&quot;http://hadoop.apache.org/docs/r2.6.0/org/apache/hadoop/mapred/lib/aggregate/package-summary.html&quot;&gt;Aggregate&lt;/a&gt;，Aggregate 提供了一个特定的 reduce 类和 combiner 类，以及一些对 reduce 输出的聚合函数，例如 sum、min、max 等等。&lt;/p&gt;

&lt;p&gt;为了使用 Aggregate，只需要定义 &lt;code&gt;-reducer aggregate&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper myAggregatorForKeyCount.py \
    -reducer aggregate \
    -file myAggregatorForKeyCount.py \
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;myAggregatorForKeyCount.py  文件大概内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#!/usr/bin/python

import sys;

def generateLongCountToken(id):
    return &quot;LongValueSum:&quot; + id + &quot;\t&quot; + &quot;1&quot;

def main(argv):
    line = sys.stdin.readline();
    try:
        while line:
            line = line[:-1];
            fields = line.split(&quot;\t&quot;);
            print generateLongCountToken(fields[0]);
            line = sys.stdin.readline();
    except &quot;end of file&quot;:
        return None
if __name__ == &quot;__main__&quot;:
     main(sys.argv)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;hadoop-field-selection-class&quot;&gt;Hadoop Field Selection Class&lt;/h3&gt;

&lt;p&gt;Hadoop 中有一个类 &lt;a href=&quot;http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.html&quot;&gt;FieldSelectionMapReduce&lt;/a&gt;，运行你像 unix 中的 cut 命令一样处理文本。&lt;/p&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming.jar \
    -D mapreduce.map.output.key.field.separator=. \
    -D mapreduce.partition.keypartitioner.options=-k1,2 \
    -D mapreduce.fieldsel.data.field.separator=. \
    -D mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0- \
    -D mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5- \
    -D mapreduce.map.output.key.class=org.apache.hadoop.io.Text \
    -D mapreduce.job.reduces=12 \
    -input myInputDirs \
    -output myOutputDir \
    -mapper org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \
    -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关键参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0-&lt;/code&gt;：意思是 map 的输出中 key 部分包括分隔后的第 6、5、1、2、3列，而 value 部分包括分隔后的所有的列&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5-&lt;/code&gt;：意思是 map 的输出中 key 部分包括分隔后的第 0、1、2列，而 value 部分包括分隔后的从第5列开始的所有列&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-5&quot;&gt;测试&lt;/h1&gt;

&lt;p&gt;上面讲了 Hadoop Streaming 的原理和一些用法，现在来运行一些例子做测试。关于如何用 Python 来编写 Hadoop Streaming 程序，可以参考 &lt;a href=&quot;http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/&quot;&gt;Writing an Hadoop MapReduce Program in Python&lt;/a&gt;，中文翻译在 &lt;a href=&quot;http://www.tianjun.ml/essays/19/&quot;&gt;这里&lt;/a&gt;，其他非 Java 的语言，都可以参照这篇文章。&lt;/p&gt;

&lt;p&gt;下面以 word count 为例做测试。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;准备测试数据&lt;/h2&gt;

&lt;p&gt;同 &lt;a href=&quot;http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/&quot;&gt;Writing an Hadoop MapReduce Program in Python&lt;/a&gt;，我们使用古腾堡项目中的三本电子书作为测试：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gutenberg.org/etext/20417&quot;&gt;The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gutenberg.org/etext/5000&quot;&gt;The Notebooks of Leonardo Da Vinci&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gutenberg.org/etext/4300&quot;&gt;Ulysses by James Joyce&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下载这些电子书的 txt格式，并将其上传到 hdfs：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir /tmp/gutenberg/ &amp;amp;&amp;amp; cd /tmp/gutenberg/

$ wget http://www.gutenberg.org/files/20417/20417.txt
$ wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt
$ wget http://www.gutenberg.org/files/4300/4300.txt

$ hadoop fs -copyFromLocal /tmp/gutenberg gutenberg

$ hadoop fs -ls gutenberg
Found 4 items
-rw-r--r--   3 hive hive     674762 2015-02-11 17:34 gutenberg/20417.txt
-rw-r--r--   3 hive hive    1573079 2015-02-11 17:34 gutenberg/4300.txt
-rw-r--r--   3 hive hive    1423803 2015-02-11 17:34 gutenberg/pg5000.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;shell-&quot;&gt;编写 Shell 版程序&lt;/h2&gt;

&lt;p&gt;mapper.sh 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#! /bin/bash

while read LINE; do
  for word in $LINE
  do
    echo &quot;$word 1&quot;
  done
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;reducer.sh 程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#! /bin/bash

count=0
started=0
word=&quot;&quot;
while read LINE;do
  newword=`echo $LINE | cut -d &#39; &#39;  -f 1`
  if [ &quot;$word&quot; != &quot;$newword&quot; ];then
    [ $started -ne 0 ] &amp;amp;&amp;amp; echo -e &quot;$word\t$count&quot;
    word=$newword
    count=1
    started=1
  else
    count=$(( $count + 1 ))
  fi
done
echo -e &quot;$word\t$count&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在本机以脚本方式测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo &quot;foo foo quux labs foo bar quux&quot; | sh mapper.sh  |sort -k1,1| sh reducer.sh
bar 1
foo 3
labs    1
quux    2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以 Hadoop Streaming 方式运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop  jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.reduce.tasks=6 \
    -input gutenberg/* \
    -output gutenberg-output \
    -mapper mapper.sh\
    -reducer reducer.sh\
    -file mapper.sh \
    -file reducer.sh

15/02/11 17:50:59 INFO mapreduce.Job:  map 0% reduce 0%
15/02/11 17:51:18 INFO mapreduce.Job:  map 17% reduce 0%
15/02/11 17:51:52 INFO mapreduce.Job:  map 17% reduce 6%
15/02/11 17:51:53 INFO mapreduce.Job:  map 33% reduce 6%
15/02/11 17:51:55 INFO mapreduce.Job:  map 60% reduce 17%
15/02/11 17:51:56 INFO mapreduce.Job:  map 100% reduce 17%
15/02/11 17:51:59 INFO mapreduce.Job:  map 100% reduce 67%
15/02/11 17:53:11 INFO mapreduce.Job:  map 100% reduce 68%
15/02/11 17:54:49 INFO mapreduce.Job:  map 100% reduce 69%
15/02/11 17:57:12 INFO mapreduce.Job:  map 100% reduce 70%
15/02/11 17:58:45 INFO mapreduce.Job:  map 100% reduce 71%
15/02/11 17:58:55 INFO mapreduce.Job:  map 100% reduce 81%
15/02/11 17:59:05 INFO mapreduce.Job:  map 100% reduce 100%
15/02/11 17:59:08 INFO streaming.StreamJob: Job complete: job_1421752803837_5736
15/02/11 17:59:09 INFO streaming.StreamJob: Output: /user/root/gutenberg-output
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;python-&quot;&gt;编写 Python 版程序&lt;/h2&gt;

&lt;p&gt;mapper.py 程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#!/usr/bin/env python
&quot;&quot;&quot;A more advanced Mapper, using Python iterators and generators.&quot;&quot;&quot;

import sys

def read_input(file):
    for line in file:
        # split the line into words
        yield line.split()

def main(separator=&#39;\t&#39;):
    # input comes from STDIN (standard input)
    data = read_input(sys.stdin)
    for words in data:
        # write the results to STDOUT (standard output);
        # what we output here will be the input for the
        # Reduce step, i.e. the input for reducer.py
        #
        # tab-delimited; the trivial word count is 1
        for word in words:
            print &#39;%s%s%d&#39; % (word, separator, 1)

if __name__ == &quot;__main__&quot;:
    main()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;reducer.py 程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#!/usr/bin/env python
&quot;&quot;&quot;A more advanced Reducer, using Python iterators and generators.&quot;&quot;&quot;

from itertools import groupby
from operator import itemgetter
import sys

def read_mapper_output(file, separator=&#39;\t&#39;):
    for line in file:
        yield line.rstrip().split(separator, 1)

def main(separator=&#39;\t&#39;):
    # input comes from STDIN (standard input)
    data = read_mapper_output(sys.stdin, separator=separator)
    # groupby groups multiple word-count pairs by word,
    # and creates an iterator that returns consecutive keys and their group:
    #   current_word - string containing a word (the key)
    #   group - iterator yielding all [&quot;&amp;amp;lt;current_word&amp;amp;gt;&quot;, &quot;&amp;amp;lt;count&amp;amp;gt;&quot;] items
    for current_word, group in groupby(data, itemgetter(0)):
        try:
            total_count = sum(int(count) for current_word, count in group)
            print &quot;%s%s%d&quot; % (current_word, separator, total_count)
        except ValueError:
            # count was not a number, so silently discard this item
            pass

if __name__ == &quot;__main__&quot;:
    main()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 Java 的一些例子，这个需要单独创建一个 maven 工程，然后做一些测试。&lt;/p&gt;

&lt;h1 id=&quot;section-7&quot;&gt;注意事项&lt;/h1&gt;

&lt;h3 id=&quot;mapper--shell-&quot;&gt;mapper 中不能使用 shell 的别名，但可以使用变量&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hdfs dfs -cat /user/me/samples/student_marks
alice   50
bruce   70
charlie 80
dan     75

$ c2=&#39;cut -f2&#39;; hadoop jar hadoop-streaming-2.6.0.jar \
    -D mapreduce.job.name=&#39;Experiment&#39; \
    -input /user/me/samples/student_marks \
    -output /user/me/samples/student_out \
    -mapper &quot;$c2&quot; -reducer &#39;cat&#39;

$ hdfs dfs -cat /user/me/samples/student_out/part-00000
50
70
75
80
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;mapper--unix-&quot;&gt;mapper 中不能使用 unix 的管道&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;-mapper&lt;/code&gt; 中使用 “cut -f1&lt;/td&gt;
      &lt;td&gt;sed s/foo/bar/g”，会出现 &lt;code&gt;java.io.IOException: Broken pipe&lt;/code&gt; 异常&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;streaming--1&quot;&gt;指定 streaming 临时空间&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-D stream.tmpdir=/export/bigspace/...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-8&quot;&gt;指定多个输入文件&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming-2.6.0.jar \
    -input &#39;/user/foo/dir1&#39; -input &#39;/user/foo/dir2&#39; \
    (rest of the command)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;xml&quot;&gt;处理 XML&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hadoop jar hadoop-streaming-2.6.0.jar \
    -inputreader &quot;StreamXmlRecord,begin=BEGIN_STRING,end=END_STRING&quot; \
    (rest of the command)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;BEGIN_STRING 和 END_STRING 之前的内容会被认为是 map 任务的一条记录。&lt;/p&gt;

&lt;h1 id=&quot;section-9&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html#More_Usage_Examples&quot;&gt;Hadoop Streaming&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dongxicheng.org/mapreduce/hadoop-streaming-programming/&quot;&gt;Hadoop Streaming 编程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/&quot;&gt;Writing an Hadoop MapReduce Program in Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/02/12/hadoop-streaming.html</link>
      <guid>http://blog.javachen.com/2015/02/12/hadoop-streaming.html</guid>
      <pubDate>2015-02-12T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Useful Hadoop Commands</title>
      <description>&lt;h2 id=&quot;hadoop&quot;&gt;hadoop&lt;/h2&gt;

&lt;p&gt;解压 gz 文件到文本文件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -text /hdfs_path/compressed_file.gz | hadoop fs -put - /tmp/uncompressed-file.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解压本地文件 gz 文件并上传到 hdfs&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gunzip -c filename.txt.gz | hadoop fs -put - /tmp/filename.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 awk 处理 csv 文件，参考 &lt;a href=&quot;http://grepalex.com/2013/01/17/awk-with-hadoop-streaming/&quot;&gt;Using awk and friends with Hadoop&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -cat people.txt | awk -F&quot;,&quot; &#39;{ print $1&quot;,&quot;$2&quot;,&quot;$3$4$5 }&#39; | hadoop fs -put - people-coalesed.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 lzo 文件、上传到 hdfs 并添加索引：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ lzop -Uf data.txt
$ hadoop fs -moveFromLocal data.txt.lzo /tmp/
# 1. 单机
$ hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /tmp/data.txt.lzo

# 2. 运行 mr
$ hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /tmp/data.txt.lzo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 people.txt 通过 lzo 压缩了，则可以使用下面命令解压缩、处理数据、压缩文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -cat people.txt.lzo | lzop -dc | awk -F&quot;,&quot; &#39;{ print $1&quot;,&quot;$2&quot;,&quot;$3$4$5 }&#39; | lzop -c | hadoop fs -put - people-coalesed.txt.lzo
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hive&quot;&gt;hive&lt;/h2&gt;

&lt;p&gt;后台运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ nohup hive -f sample.hql &amp;gt; output.out 2&amp;gt;&amp;amp;1 &amp;amp; 

$ nohup hive --database &quot;default&quot; -e &quot;select * from tablename;&quot; &amp;gt; output.out 2&amp;gt;&amp;amp;1 &amp;amp; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;替换分隔符：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hive --database &quot;default&quot; -f query.hql  2&amp;gt; err.txt | sed &#39;s/[\t]/,/g&#39; 1&amp;gt; output.txt 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打印表头：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hive --database &quot;default&quot; -e &quot;SHOW COLUMNS FROM table_name;&quot; | tr &#39;[:lower:]&#39; &#39;[:upper:]&#39; | tr &#39;\n&#39; &#39;,&#39; 1&amp;gt; headers.txt

$ hive --database &quot;default&quot; -e &quot;SET hive.cli.print.header=true; select * from table_name limit 0;&quot; | tr &#39;[:lower:]&#39; &#39;[:upper:]&#39; | sed &#39;s/[\t]/,/g&#39;  1&amp;gt; headers.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看执行时间：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hive -e &quot;select * from tablename;&quot;  2&amp;gt; err.txt  1&amp;gt; out.txt 
$ cat err.txt | grep &quot;Time taken:&quot; | awk &#39;{print $3,$6}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hive 中如何避免用科学计数法表示浮点数？参考 &lt;a href=&quot;http://www.zhihu.com/question/28887115&quot;&gt;http://www.zhihu.com/question/28887115&lt;/a&gt; ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT java_method(&quot;String&quot;, &quot;format&quot;, &quot;%f&quot;, my_column) FROM mytable LIMIT 1
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2015/02/10/useful-commands-in-hadoop.html</link>
      <guid>http://blog.javachen.com/2015/02/10/useful-commands-in-hadoop.html</guid>
      <pubDate>2015-02-10T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Reading List 2015-02</title>
      <description>&lt;p&gt;一直有个想法没有付诸实践，想做个分享知识的网站，类似 &lt;a href=&quot;https://leanote.com/index&quot;&gt;Leanote&lt;/a&gt;、&lt;a href=&quot;http://toutiao.io/&quot;&gt;开发者头条&lt;/a&gt;、&lt;a href=&quot;http://githunt.io/&quot;&gt;GitHunt&lt;/a&gt; 等等的可检索的有思想的一个产品。作为尝试，在想法成型之前，先参考 &lt;a href=&quot;http://www.dbthink.com/archives/910&quot;&gt;http://www.dbthink.com/archives/910&lt;/a&gt;、&lt;a href=&quot;http://yanjunyi.com/blog/category/discovery/&quot;&gt;http://yanjunyi.com/blog/category/discovery/&lt;/a&gt; 的方式整理为知笔记中以前看到、搜集的一些文章、链接、工具等等的。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;前端&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://githunt.io/&quot;&gt;http://githunt.io/&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://ohmycss.com/&quot;&gt;http://ohmycss.com/&lt;/a&gt;：简化版的bootstrap&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://octicons.github.com/&quot;&gt;https://octicons.github.com/&lt;/a&gt; ：css 图标，类似的有http://glyphicons.com/&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/allmarkedup/purl&quot;&gt;https://github.com/allmarkedup/purl&lt;/a&gt;：一个 javascript 的 URL 解析库，&lt;/li&gt;
      &lt;li&gt;select2：javascript 下拉框控件&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://momentjs.com/&quot;&gt;http://momentjs.com/&lt;/a&gt;：一个 javascript 日期时间库&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://adambom.github.com/parallel.js/&quot;&gt;http://adambom.github.com/parallel.js/&lt;/a&gt; Javascript 并行计算库&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://unknownworlds.com/blog/lua-ide-decoda-open-source/&quot;&gt;http://unknownworlds.com/blog/lua-ide-decoda-open-source/&lt;/a&gt; Lua IDE decoda 开源了&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://baixing.github.io/Puerh&quot;&gt;http://baixing.github.io/Puerh&lt;/a&gt; 百姓网 UI 库&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/cloudfuji/kandan&quot;&gt;https://github.com/cloudfuji/kandan&lt;/a&gt; 开源免费的团队聊天工具 Kandan&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://parsleyjs.org/&quot;&gt;http://parsleyjs.org/&lt;/a&gt;非常强大的 jQuery 表单验证库&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://canvasjs.com/&quot;&gt;http://canvasjs.com/&lt;/a&gt; HTML5 绘图工具&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jsonselect.org&quot;&gt;http://jsonselect.org&lt;/a&gt; JSON CSS-like query language&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jsonmate.com/&quot;&gt;http://jsonmate.com/&lt;/a&gt; JSON格式化&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://justgage.com/&quot;&gt;http://justgage.com/&lt;/a&gt; javascript 仪表盘控件&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ez-css.org/ &quot;&gt;http://www.ez-css.org/ &lt;/a&gt;css 布局框架&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blueprintcss.org/&quot;&gt;http://blueprintcss.org/&lt;/a&gt;  css 框架&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://extralogical.net/projects/udon/&quot;&gt;http://extralogical.net/projects/udon/&lt;/a&gt; javascript函数编程&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://fogcreek.github.io/WebPutty/&quot;&gt;http://fogcreek.github.io/WebPutty/&lt;/a&gt; WebPutty is a simple CSS editing and hosting service&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://imakewebthings.com/deck.js/&quot;&gt;http://imakewebthings.com/deck.js/&lt;/a&gt; Modern HTML Presentations&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.fogcreek.com/the-trello-tech-stack/&quot;&gt;http://blog.fogcreek.com/the-trello-tech-stack/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://w2ui.com/web/home&quot;&gt;http://w2ui.com/web/home&lt;/a&gt;  New JavaScript UI Library&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ace.c9.io/#nav=about&quot;&gt;http://ace.c9.io/#nav=about&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.javelinjs.com/&quot;&gt;http://www.javelinjs.com/&lt;/a&gt;  a frontend Javascript library developed at Facebook&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jtyjty99999/mobileTech&quot;&gt;https://github.com/jtyjty99999/mobileTech&lt;/a&gt; 移动 Web App 开发的各种知识和经验的索引&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;后端&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/linkedin/databus&quot;&gt;https://github.com/linkedin/databus&lt;/a&gt; Linkedin 开源的数据库变化监控工具 Databus&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper28.pdf &quot;&gt;http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper28.pdf &lt;/a&gt; Impala: A Modern, Open-Source SQL Engine for Hadoop&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sdk&quot;&gt;微信 SDK&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/node-webot/wechat&quot;&gt;https://github.com/node-webot/wechat&lt;/a&gt; 微信公共平台消息接口服务中间件&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chanjarster/weixin-java-tools&quot;&gt;https://github.com/chanjarster/weixin-java-tools&lt;/a&gt; 微信公众号、企业号Java SDK&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/belerweb/social-sdk&quot;&gt;https://github.com/belerweb/social-sdk&lt;/a&gt; 集成新浪微博开放平台、QQ互联、腾讯微博开发平台、微信公众平台等社交平台的接口的Java库。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/liyiorg/weixin-popular&quot;&gt;https://github.com/liyiorg/weixin-popular&lt;/a&gt; 微信公众平台 Java SDK&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/blahgeek/weixin-mp&quot;&gt;https://github.com/blahgeek/weixin-mp&lt;/a&gt; 微信公众平台，django&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wwj718/werobot_django&quot;&gt;https://github.com/wwj718/werobot_django&lt;/a&gt; 使用werobot作为微信后端,使werobot能存取django中的数据.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wwj718/weixin_robot&quot;&gt;https://github.com/wwj718/weixin_robot&lt;/a&gt; 使用WeRoBot搭建的微信后台&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/foxinmy/weixin4j&quot;&gt;https://github.com/foxinmy/weixin4j&lt;/a&gt; (微信开发工具包)weixin sdk for java&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/huyongliang/WeiXinSDK&quot;&gt;https://github.com/huyongliang/WeiXinSDK&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zengsn/zengsource-weixin&quot;&gt;https://github.com/zengsn/zengsource-weixin&lt;/a&gt; 微信公众平台开发者模式Java SDK&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.7keji.cn/813.html&quot;&gt;http://www.7keji.cn/813.html&lt;/a&gt; 10款微信公共账号开源软件&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;一些调度系统&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/spotify/luigi&quot;&gt;https://github.com/spotify/luigi&lt;/a&gt; Python模块，支持依赖管理、流程管理、可视化和 Hadoop&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/airbnb/chronos&quot;&gt;https://github.com/airbnb/chronos&lt;/a&gt; Mesos 的调度器&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.cascading.org/&quot;&gt;http://docs.cascading.org/&lt;/a&gt; Cascading is the proven application development platform for building data applications on Hadoop&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/thieman/dagobah&quot;&gt;https://github.com/thieman/dagobah&lt;/a&gt; Simple DAG-based job scheduler in Python&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/azkaban/azkaban&quot;&gt;https://github.com/azkaban/azkaban&lt;/a&gt; Azkaban workflow manager&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://demo.gethue.com/&quot;&gt;http://demo.gethue.com/&lt;/a&gt; Cloudera 的 HUE&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/alibaba/zeus&quot;&gt;https://github.com/alibaba/zeus&lt;/a&gt; 阿里宙斯作业平台&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/michael8335/zeus2&quot;&gt;https://github.com/michael8335/zeus2&lt;/a&gt; 在阿里Zeus的基础上开发的支持Hadoop2 的调度平台&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dubbo-&quot;&gt;Dubbo 资料&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.xiaoyaochong.net/wordpress/index.php/2013/04/10/%E5%9F%BA%E4%BA%8Ezookeeper%E7%9A%84dubbo%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/&quot;&gt;基于ZooKeeper的Dubbo注册中心&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.xiaoyaochong.net/wordpress/index.php/2013/04/10/dubbo%e9%9b%86%e7%be%a4%e7%89%b9%e6%80%a7%e5%88%86%e6%9e%90/&quot;&gt;Dubbo集群特性分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.xiaoyaochong.net/wordpress/index.php/2013/04/08/dubbo%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E8%A7%A3%E6%9E%90/&quot;&gt;Dubbo常用功能解析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://my.oschina.net/bieber/blog/287296&quot;&gt;初识分布式服务管理框架-Dubbo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://chenzehe.iteye.com/blog/2160526&quot;&gt;Dubbo框架设计简介&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jm-blog.aliapp.com/?p=3138&quot;&gt;如何更好地学习dubbo源代码&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://javatar.iteye.com/blog/1041832&quot;&gt;Dubbo扩展点重构&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.ganps.net/2014/12/17/dubbox%E5%8D%8F%E8%AE%AE%E8%AF%B4%E6%98%8E/&quot;&gt;Dubbox 样例代码分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://datafans.net/?p=330&quot;&gt;RPC框架系列–Dubbo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/git-xiaozhi/mini-blogawk&quot;&gt;A mini blog with NoSql,Dubbo and Spring&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;总结：&lt;br /&gt;
作为一个程序员，最重要的能力是自我学习、归纳、总结，知识在于总结而不是分享。如何把大量看到的、听到的信息、知识、笔记等转化为自己的经验值，是需要认真考虑的一件事情。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
      <link>http://blog.javachen.com/2015/02/10/reading-list-2015-02.html</link>
      <guid>http://blog.javachen.com/2015/02/10/reading-list-2015-02.html</guid>
      <pubDate>2015-02-10T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>如何在CDH5上运行Spark应用</title>
      <description>&lt;blockquote&gt;
  &lt;p&gt;这篇文章参考 &lt;a href=&quot;http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/&quot;&gt;How-to: Run a Simple Apache Spark App in CDH 5&lt;/a&gt; 编写而成，没有完全参照原文翻译，而是重新进行了整理，例如：spark 版本改为 &lt;code&gt;1.3.0&lt;/code&gt;，添加了 Python 版的程序。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;maven-&quot;&gt;创建 maven 工程&lt;/h1&gt;

&lt;p&gt;使用下面命令创建一个普通的 maven 工程：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn archetype:generate -DgroupId=com.cloudera.sparkwordcount -DartifactId=sparkwordcount -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 sparkwordcount 目录下添加 scala 源文件目录和相应的包目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p sparkwordcount/src/main/scala/com/cloudera/sparkwordcount
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 pom.xml 添加 scala 和 spark 依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;  &amp;lt;dependencies&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;org.scala-lang&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;scala-library&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;2.10.4&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;1.3.0&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;spark-sql_2.10&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;1.3.0&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
  &amp;lt;/dependencies&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加编译 scala 的插件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt; &amp;lt;plugin&amp;gt;
  &amp;lt;groupId&amp;gt;org.scala-tools&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;maven-scala-plugin&amp;lt;/artifactId&amp;gt;
      &amp;lt;executions&amp;gt;
        &amp;lt;execution&amp;gt;
          &amp;lt;goals&amp;gt;
            &amp;lt;goal&amp;gt;compile&amp;lt;/goal&amp;gt;
            &amp;lt;goal&amp;gt;testCompile&amp;lt;/goal&amp;gt;
          &amp;lt;/goals&amp;gt;
        &amp;lt;/execution&amp;gt;
      &amp;lt;/executions&amp;gt;
&amp;lt;/plugin&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加 scala 编译插件需要的仓库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;pluginRepositories&amp;gt;
  &amp;lt;pluginRepository&amp;gt;
    &amp;lt;id&amp;gt;scala-tools.org&amp;lt;/id&amp;gt;
    &amp;lt;name&amp;gt;Scala-tools Maven2 Repository&amp;lt;/name&amp;gt;
    &amp;lt;url&amp;gt;http://scala-tools.org/repo-releases&amp;lt;/url&amp;gt;
  &amp;lt;/pluginRepository&amp;gt;
&amp;lt;/pluginRepositories&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，添加 cdh hadoop 的仓库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;  &amp;lt;repositories&amp;gt;
    &amp;lt;repository&amp;gt;
      &amp;lt;id&amp;gt;scala-tools.org&amp;lt;/id&amp;gt;
      &amp;lt;name&amp;gt;Scala-tools Maven2 Repository&amp;lt;/name&amp;gt;
      &amp;lt;url&amp;gt;http://scala-tools.org/repo-releases&amp;lt;/url&amp;gt;
    &amp;lt;/repository&amp;gt;
    &amp;lt;repository&amp;gt;
      &amp;lt;id&amp;gt;maven-hadoop&amp;lt;/id&amp;gt;
      &amp;lt;name&amp;gt;Hadoop Releases&amp;lt;/name&amp;gt;
      &amp;lt;url&amp;gt;https://repository.cloudera.com/content/repositories/releases/&amp;lt;/url&amp;gt;
    &amp;lt;/repository&amp;gt;
    &amp;lt;repository&amp;gt;
      &amp;lt;id&amp;gt;cloudera-repos&amp;lt;/id&amp;gt;
      &amp;lt;name&amp;gt;Cloudera Repos&amp;lt;/name&amp;gt;
      &amp;lt;url&amp;gt;https://repository.cloudera.com/artifactory/cloudera-repos/&amp;lt;/url&amp;gt;
    &amp;lt;/repository&amp;gt;
  &amp;lt;/repositories&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行下面命令检查工程是否能够成功编译：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mvn package
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;编写示例代码&lt;/h1&gt;

&lt;p&gt;以 &lt;a href=&quot;http://wiki.apache.org/hadoop/WordCount&quot;&gt;WordCount&lt;/a&gt; 为例，该程序需要完成以下逻辑：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;读一个输入文件&lt;/li&gt;
  &lt;li&gt;统计每个单词出现次数&lt;/li&gt;
  &lt;li&gt;过滤少于一定次数的单词&lt;/li&gt;
  &lt;li&gt;对剩下的单词统计每个字母出现次数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 MapReduce 中，上面的逻辑需要两个 MapReduce 任务，而在 Spark 中，只需要一个简单的任务，并且代码量会少 90%。&lt;/p&gt;

&lt;p&gt;编写Scala 程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
 
object SparkWordCount {
  def main(args: Array[String]) {
    val sc = new SparkContext(new SparkConf().setAppName(&quot;Spark Count&quot;))
    val threshold = args(1).toInt
 
    // split each document into words
    val tokenized = sc.textFile(args(0)).flatMap(_.split(&quot; &quot;))
 
    // count the occurrence of each word
    val wordCounts = tokenized.map((_, 1)).reduceByKey(_ + _)
 
    // filter out words with less than threshold occurrences
    val filtered = wordCounts.filter(_._2 &amp;amp;gt;= threshold)
 
    // count characters
    val charCounts = filtered.flatMap(_._1.toCharArray).map((_, 1)).reduceByKey(_ + _)
 
    System.out.println(charCounts.collect().mkString(&quot;, &quot;))
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spark 使用懒执行的策略，意味着只有当&lt;code&gt;动作&lt;/code&gt;执行的时候，&lt;code&gt;转换&lt;/code&gt;才会运行。上面例子中的&lt;code&gt;动作&lt;/code&gt;操作是 &lt;code&gt;collect&lt;/code&gt; 和 &lt;code&gt;saveAsTextFile&lt;/code&gt;，前者是将数据推送给客户端，后者是将数据保存到 HDFS。&lt;/p&gt;

&lt;p&gt;作为对比，Java 版的程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import java.util.ArrayList;
import java.util.Arrays;
import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.*;
import org.apache.spark.SparkConf;
import scala.Tuple2;
 
public class JavaWordCount {
  public static void main(String[] args) {
    JavaSparkContext sc = new JavaSparkContext(new SparkConf().setAppName(&quot;Spark Count&quot;));
    final int threshold = Integer.parseInt(args[1]);
 
    // split each document into words
    JavaRDD tokenized = sc.textFile(args[0]).flatMap(
      new FlatMapFunction() {
        public Iterable call(String s) {
          return Arrays.asList(s.split(&quot; &quot;));
        }
      }
    );
 
    // count the occurrence of each word
    JavaPairRDD counts = tokenized.mapToPair(
      new PairFunction() {
        public Tuple2 call(String s) {
          return new Tuple2(s, 1);
        }
      }
    ).reduceByKey(
      new Function2() {
        public Integer call(Integer i1, Integer i2) {
          return i1 + i2;
        }
      }
    );
 
    // filter out words with less than threshold occurrences
    JavaPairRDD filtered = counts.filter(
      new Function, Boolean&amp;gt;() {
        public Boolean call(Tuple2 tup) {
          return tup._2 &amp;gt;= threshold;
        }
      }
    );
 
    // count characters
    JavaPairRDD charCounts = filtered.flatMap(
      new FlatMapFunction, Character&amp;gt;() {
        public Iterable call(Tuple2 s) {
          ArrayList chars = new ArrayList(s._1.length());
          for (char c : s._1.toCharArray()) {
            chars.add(c);
          }
          return chars;
        }
      }
    ).mapToPair(
      new PairFunction() {
        public Tuple2 call(Character c) {
          return new Tuple2(c, 1);
        }
      }
    ).reduceByKey(
      new Function2() {
        public Integer call(Integer i1, Integer i2) {
          return i1 + i2;
        }
      }
    );
 
    System.out.println(charCounts.collect());
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，Python 版的程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import sys

from pyspark import SparkContext

file=&quot;inputfile.txt&quot;
count=2

if __name__ == &quot;__main__&quot;:
    sc = SparkContext(appName=&quot;PythonWordCount&quot;)
    lines = sc.textFile(file, 1)
    counts = lines.flatMap(lambda x: x.split(&#39; &#39;)) \
                  .map(lambda x: (x, 1))  \
                  .reduceByKey(lambda a, b: a + b)  \
                  .filter(lambda (a, b) : b &amp;gt;= count)  \
                  .flatMap(lambda (a, b): list(a))  \
                  .map(lambda x: (x, 1))  \
                  .reduceByKey(lambda a, b: a + b)

    print &quot;,&quot;.join(str(t) for t in counts.collect())
    sc.stop()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;编译&lt;/h1&gt;

&lt;p&gt;运行下面命令生成 jar：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn package
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行成功之后，会在 target 目录生成 sparkwordcount-0.0.1-SNAPSHOT.jar 文件。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;运行&lt;/h1&gt;

&lt;p&gt;首先，将测试文件 &lt;a href=&quot;https://github.com/sryza/simplesparkapp/blob/master/data/inputfile.txt&quot;&gt;inputfile.txt&lt;/a&gt; 上传到 HDFS 上；&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://github.com/sryza/simplesparkapp/blob/master/data/inputfile.txt
$ hadoop fs -put inputfile.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其次，将 sparkwordcount-0.0.1-SNAPSHOT.jar 上传到集群中的一个节点；然后，使用 spark-submit 脚本运行 Scala 版的程序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit --class com.cloudera.sparkwordcount.SparkWordCount --master local sparkwordcount-0.0.1-SNAPSHOT.jar inputfile.txt 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者，运行 Java 版本的程序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit --class com.cloudera.sparkwordcount.JavaWordCount --master local sparkwordcount-0.0.1-SNAPSHOT.jar inputfile.txt 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 Python 版的程序，运行脚本为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit  --master local PythonWordCount.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果，你的集群部署的是 standalone 模式，则你可以替换 master 参数的值为 &lt;code&gt;spark://&amp;lt;master host&amp;gt;:&amp;lt;master port&amp;gt;&lt;/code&gt;，也可以以 Yarn 的模式运行。&lt;/p&gt;

&lt;p&gt;最后的 Python 版的程序运行输出结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(u&#39;a&#39;, 4),(u&#39;c&#39;, 1),(u&#39;b&#39;, 1),(u&#39;e&#39;, 6),(u&#39;f&#39;, 1),(u&#39;i&#39;, 1),(u&#39;h&#39;, 1),(u&#39;l&#39;, 1),(u&#39;o&#39;, 2),(u&#39;n&#39;, 4),(u&#39;p&#39;, 2),(u&#39;r&#39;, 2),(u&#39;u&#39;, 1),(u&#39;t&#39;, 2),(u&#39;v&#39;, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整的代码在：&lt;a href=&quot;https://github.com/sryza/simplesparkapp&quot;&gt;https://github.com/sryza/simplesparkapp&lt;/a&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2015/02/04/how-to-run-a-simple-apache-spark-app-in-cdh-5.html</link>
      <guid>http://blog.javachen.com/2015/02/04/how-to-run-a-simple-apache-spark-app-in-cdh-5.html</guid>
      <pubDate>2015-02-04T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spark编程指南笔记</title>
      <description>&lt;p&gt;本文是参考Spark官方编程指南（Spark 版本为1.2）整理出来的学习笔记，主要是用于加深对 Spark 的理解，并记录一些知识点。&lt;/p&gt;

&lt;h1 id=&quot;spark&quot;&gt;1. Spark介绍&lt;/h1&gt;

&lt;p&gt;Spark是UC Berkeley AMP lab所开源的类Hadoop MapReduce 框架，都是基于map reduce算法所实现的分布式计算框架，拥有Hadoop MapReduce所具有的优点；不同于MapReduce的是Job中间输出和结果可以保存在内存中，而不需要读写HDFS，因此Spark能更好地适用于machine learning等需要迭代的map reduce算法。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;产生原因&lt;/h2&gt;

&lt;p&gt;1、 MapReduce具有很多局限性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;仅支持Map和Reduce两种操作&lt;/li&gt;
  &lt;li&gt;迭代效率低&lt;/li&gt;
  &lt;li&gt;不适合交互式处理&lt;/li&gt;
  &lt;li&gt;不擅长流式处理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、 现有的计算框架各自为战&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;批处理计算：MapReduce、Hive&lt;/li&gt;
  &lt;li&gt;流式计算：Storm&lt;/li&gt;
  &lt;li&gt;交互式计算：Impala ​&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;设计目标&lt;/h2&gt;

&lt;p&gt;在一个统一的框架下能够进行批处理、流式计算和交互式计算。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;2. 一些概念&lt;/h1&gt;

&lt;p&gt;每一个 Spark 的应用，都是由一个驱动程序构成，它运行用户的 main 函数，在一个集群上执行各种各样的并行操作。&lt;/p&gt;

&lt;p&gt;Spark 提出的最主要抽象概念是&lt;code&gt;弹性分布式数据集&lt;/code&gt;，它是一个有容错机制（划分到集群的各个节点上）并可以被并行操作的元素集合。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;分布在集群中的对象集合&lt;/li&gt;
  &lt;li&gt;存储在磁盘或者内存中&lt;/li&gt;
  &lt;li&gt;通过并行“转换”操作构造&lt;/li&gt;
  &lt;li&gt;失效后自动重构&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前有两种类型的RDD：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;并行集合&lt;/code&gt;：接收一个已经存在的 Scala 集合，然后进行各种并行计算。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;外部数据集&lt;/code&gt;：外部存储系统，例如一个共享的文件系统，HDFS、HBase以及任何支持 Hadoop InputFormat 的数据源。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这两种类型的 RDD 都可以通过相同的方式进行操作。用户可以让 Spark 保留一个 RDD 在内存中，使其能在并行操作中被有效的重复使用，并且，RDD 能自动从节点故障中恢复。&lt;/p&gt;

&lt;p&gt;Spark 的第二个抽象概念是&lt;code&gt;共享变量&lt;/code&gt;，可以在并行操作中使用。在默认情况下，Spark 通过不同节点上的一系列任务来运行一个函数，它将每一个函数中用到的变量的拷贝传递到每一个任务中。有时候，一个变量需要在任务之间，或任务与驱动程序之间被共享。&lt;/p&gt;

&lt;p&gt;Spark 支持两种类型的共享变量：&lt;code&gt;广播变量&lt;/code&gt;，可以在内存的所有的结点上缓存变量；&lt;code&gt;累加器&lt;/code&gt;：只能用于做加法的变量，例如计数或求和。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;3. 如何编程&lt;/h1&gt;

&lt;h2 id=&quot;spark-1&quot;&gt;初始化 Spark&lt;/h2&gt;

&lt;p&gt;在一个Spark程序中要做的第一件事就是创建一个SparkContext对象来告诉Spark如何连接一个集群。为了创建SparkContext，你首先需要创建一个SparkConf对象，这个对象会包含你的应用的一些相关信息。这个通常是通过下面的构造器来实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;new SparkContext(master, appName, [sparkHome], [jars])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;master&lt;/code&gt;：用于指定所连接的 Spark 或者 Mesos 集群的 URL。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;appName&lt;/code&gt; ：应用的名称，将会在集群的 Web 监控 UI 中显示。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sparkHome&lt;/code&gt;：可选，你的集群机器上 Spark 的安装路径（所有机器上路径必须一致）。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;jars&lt;/code&gt;：可选，在本地机器上的 JAR 文件列表，其中包括你应用的代码以及任何的依赖，Spark 将会把他们部署到所有的集群结点上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 python 中初始化，示例代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;//conf = SparkContext(&quot;local&quot;, &quot;Hello Spark&quot;)
conf = SparkConf().setAppName(&quot;Hello Spark&quot;).setMaster(&quot;local&quot;)
sc = SparkContext(conf=conf)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：如果部署到集群，在分布式模式下运行，最后两个参数是必须的，第一个参数可以是以下任一种形式：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Master URL&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;loca&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;默认值，使用一个 Worker 线程本地化运行(完全不并行)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;local[N]&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;使用 N 个 Worker 线程本地化运行，N 为 &lt;code&gt;*&lt;/code&gt; 时，表示使用系统中所有核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;local[N,M]&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;第一个代表的是用到的核个数；第二个参数代表的是容许该作业失败M次&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;spark://HOST:PORT&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;连接到指定的 Spark 单机版集群 master 进程所在的主机和端口，端口默认是7077&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;mesos://HOST:PORT&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;连接到指定的 Mesos 集群。host 参数是Moses master的hostname。端口默认是5050&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;如果你在一个集群上运行 spark-shell，则 master 参数默认为 &lt;code&gt;local&lt;/code&gt;。在实际使用中，当你在集群中运行你的程序，你一般不会把 master 参数写死在代码中，而是通过用 spark-submit 运行程序来获得这个参数。但是，在本地测试以及单元测试时，你仍需要自行传入 local 来运行Spark程序。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;运行代码&lt;/h2&gt;

&lt;p&gt;运行代码有几种方式，一是通过 spark-shell 来运行 scala 代码，一是编写 java 代码并打成包以 spark on yarn 方式运行，还有一种是通过 PySpark 来运行 python 代码。&lt;/p&gt;

&lt;p&gt;在  spark-shell 和 PySpark 命令行中，一个特殊的集成在解释器里的 SparkContext 变量已经建立好了，变量名叫做 sc，创建你自己的 SparkContext 不会起作用。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;4. 弹性分布式数据集&lt;/h1&gt;

&lt;h2 id=&quot;section-6&quot;&gt;4.1 并行集合&lt;/h2&gt;

&lt;p&gt;并行集合是通过调用 SparkContext 的 &lt;code&gt;parallelize&lt;/code&gt; 方法，在一个已经存在的 Scala 集合上创建一个 Seq 对象。&lt;/p&gt;

&lt;p&gt;parallelize 方法还可以接受一个参数 &lt;code&gt;slices&lt;/code&gt;，表示数据集切分的份数。Spark 将会在集群上为每一份数据起一个任务。典型地，你可以在集群的每个 CPU 上分布 2-4个 slices。一般来说，Spark 会尝试根据集群的状况，来自动设定 slices 的数目，当然，你也可以手动设置。&lt;/p&gt;

&lt;p&gt;Scala 示例程序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;scala&amp;gt; val data = Array(1, 2, 3, 4, 5)
data: Array[Int] = Array(1, 2, 3, 4, 5)

scala&amp;gt; var distData = sc.parallelize(data)
distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &amp;lt;console&amp;gt;:14

scala&amp;gt; distData.reduce((a, b) =&amp;gt; a + b)
res4: Int = 15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java 示例程序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;List&amp;lt;Integer&amp;gt; data = Arrays.asList(1, 2, 3, 4, 5);
JavaRDD&amp;lt;Integer&amp;gt; distData = sc.parallelize(data);
Integer sum=distData.reduce((a, b) -&amp;gt; a + b);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python 示例程序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
distData.reduce(lambda a, b: a + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-7&quot;&gt;4.2 外部数据源&lt;/h2&gt;

&lt;p&gt;Spark可以从存储在 HDFS，或者 Hadoop 支持的其它文件系统（包括本地文件，Amazon S3， Hypertable， HBase 等等）上的文件创建分布式数据集。Spark 支持 &lt;code&gt;TextFile&lt;/code&gt;、&lt;code&gt;SequenceFiles&lt;/code&gt; 以及其他任何 &lt;code&gt;Hadoop InputFormat&lt;/code&gt; 格式的输入。&lt;/p&gt;

&lt;p&gt;TextFile 的 RDD 可以通过下面方式创建，该方法接受一个文件的 URI 地址，该地址可以是本地路径，或者 &lt;code&gt;hdfs://&lt;/code&gt;、&lt;code&gt;s3n://&lt;/code&gt; 等 URL 地址。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// scala 语法
val distFile = sc.textFile(&quot;data.txt&quot;)

// java 语法
JavaRDD&amp;lt;String&amp;gt; distFile = sc.textFile(&quot;data.txt&quot;);

// python 语法
distFile = sc.textFile(&quot;data.txt&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一些说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果使用了本地文件路径时，要保证在worker节点上这个文件也能够通过这个路径访问。这点可以通过将这个文件拷贝到所有worker上或者使用网络挂载的共享文件系统来解决。&lt;/li&gt;
  &lt;li&gt;包括textFile在内的所有基于文件的Spark读入方法，都支持将文件夹、压缩文件、包含通配符的路径作为参数。比如，以下代码都是合法的：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;textFile(&quot;/my/directory&quot;)
textFile(&quot;/my/directory/*.txt&quot;)
textFile(&quot;/my/directory/*.gz&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了文本文件之外，Spark 还支持其他格式的输入：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;SparkContext.wholeTextFiles&lt;/code&gt; 方法可以读取一个包含多个小文件的目录，并以 filename，content 键值对的方式返回结果。&lt;/li&gt;
  &lt;li&gt;对于 SequenceFiles，可以使用 SparkContext 的 &lt;code&gt;sequenceFile[K, V]&lt;/code&gt;` 方法创建。像 IntWritable 和 Text 一样，它们必须是 Hadoop 的 Writable 接口的子类。另外，对于几种通用 Writable 类型，Spark 允许你指定原生类型来替代。例如：sequencFile[Int, String] 将会自动读取 IntWritable 和 Texts。&lt;/li&gt;
  &lt;li&gt;对于其他类型的 Hadoop 输入格式，你可以使用 &lt;code&gt;SparkContext.hadoopRDD&lt;/code&gt; 方法，它可以接收任意类型的 JobConf 和输入格式类，键类型和值类型。按照像 Hadoop 作业一样的方法设置输入源就可以了。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;RDD.saveAsObjectFile&lt;/code&gt; 和 &lt;code&gt;SparkContext.objectFile&lt;/code&gt; 提供了以 Java 序列化的简单方式来保存 RDD。虽然这种方式没有 Avro 高效，但也是一种简单的方式来保存任意的 RDD。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rdd-&quot;&gt;4.3 RDD 操作&lt;/h2&gt;

&lt;p&gt;RDD支持两类操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;转化操作，用于从已有的数据集转化产生新的数据集；&lt;/li&gt;
  &lt;li&gt;启动操作，用于在计算结束后向驱动程序返回结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举个例子，map是一个转化操作，可以将数据集中每一个元素传给一个函数，同时将计算结果作为一个新的RDD返回。另一方面，reduce操作是一个启动操作，能够使用某些函数来聚集计算RDD中所有的元素，并且向驱动程序返回最终结果（同时还有一个并行的reduceByKey操作可以返回一个分布数据集）。&lt;/p&gt;

&lt;p&gt;在Spark所有的转化操作都是惰性求值的，就是说它们并不会立刻真的计算出结果。相反，它们仅仅是记录下了转换操作的操作对象（比如：一个文件）。只有当一个启动操作被执行，要向驱动程序返回结果时，转化操作才会真的开始计算。这样的设计使得Spark运行更加高效——比如，我们会发觉由map操作产生的数据集将会在reduce操作中用到，之后仅仅是返回了reduce的最终的结果而不是map产生的庞大数据集。&lt;/p&gt;

&lt;p&gt;在默认情况下，每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。但是，你也可以通过调用persist(或cache)方法来将RDD持久化到内存中，这样Spark就可以在下次使用这个数据集时快速获得。Spark同样提供了对将RDD持久化到硬盘上或在多个节点间复制的支持。&lt;/p&gt;

&lt;p&gt;Scala 示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val lines = sc.textFile(&quot;data.txt&quot;)
val lineLengths = lines.map(s =&amp;gt; s.length)
val totalLength = lineLengths.reduce((a, b) =&amp;gt; a + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Java 示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;JavaRDD&amp;lt;String&amp;gt; lines = sc.textFile(&quot;data.txt&quot;);
JavaRDD&amp;lt;Integer&amp;gt; lineLengths = lines.map(s -&amp;gt; s.length());
int totalLength = lineLengths.reduce((a, b) -&amp;gt; a + b);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python 示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;lines = sc.textFile(&quot;data.txt&quot;)
lineLengths = lines.map(lambda s: len(s))
totalLength = lineLengths.reduce(lambda a, b: a + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一行定义了一个由外部文件产生的基本RDD。这个数据集不是从内存中载入的也不是由其他操作产生的；lines仅仅是一个指向文件的指针。第二行将lineLengths定义为map操作的结果。再强调一次，由于惰性求值的缘故，lineLengths并不会被立即计算得到。最后，我们运行了reduce操作，这是一个启动操作。从这个操作开始，Spark将计算过程划分成许多任务并在多机上运行，每台机器运行自己部分的map操作和reduce操作，最终将自己部分的运算结果返回给驱动程序。&lt;/p&gt;

&lt;p&gt;如果我们希望以后重复使用lineLengths，只需在reduce前加入下面这行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;lineLengths.persist()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这条代码将使得 lineLengths 在第一次计算生成之后保存在内存中。&lt;/p&gt;

&lt;p&gt;除了使用 lambda 表达式，也可以通过函数来运行转换或者动作，使用函数需要注意局部变量的作用域问题。&lt;/p&gt;

&lt;p&gt;例如下面的 Python 代码中的 field 变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class MyClass(object):
    def __init__(self):
        self.field = &quot;Hello&quot;

    def doStuff(self, rdd):
        field = self.field
        return rdd.map(lambda s: field + x)    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果使用 Java 语言，则需要用到匿名内部类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;class GetLength implements Function&amp;lt;String, Integer&amp;gt; {
  public Integer call(String s) { return s.length(); }
}
class Sum implements Function2&amp;lt;Integer, Integer, Integer&amp;gt; {
  public Integer call(Integer a, Integer b) { return a + b; }
}

JavaRDD&amp;lt;String&amp;gt; lines = sc.textFile(&quot;data.txt&quot;);
JavaRDD&amp;lt;Integer&amp;gt; lineLengths = lines.map(new GetLength());
int totalLength = lineLengths.reduce(new Sum());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spark 也支持键值对的操作，这在分组和聚合操作时候用得到。定义一个键值对对象时，需要自定义该对象的 equals() 和 hashCode() 方法。&lt;/p&gt;

&lt;p&gt;在 Scala 中有一个 &lt;a href=&quot;http://www.scala-lang.org/api/2.10.4/index.html#scala.Tuple2&quot;&gt;Tuple2&lt;/a&gt; 对象表示键值对，这是一个内置的对象，通过 &lt;code&gt;(a,b)&lt;/code&gt; 就可以创建一个 Tuple2 对象。在你的程序中，通过导入 &lt;code&gt;org.apache.spark.SparkContext._&lt;/code&gt; 就可以对 Tuple2 进行操作。对键值对的操作方法，可以查看 &lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions&quot;&gt;PairRDDFunctions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下面是一个用 scala 统计单词出现次数的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val lines = sc.textFile(&quot;data.txt&quot;)
val pairs = lines.map(s =&amp;gt; (s, 1))
val counts = pairs.reduceByKey((a, b) =&amp;gt; a + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，你还可以执行 &lt;code&gt;counts.sortByKey()&lt;/code&gt;、&lt;code&gt;counts.collect()&lt;/code&gt; 等操作。&lt;/p&gt;

&lt;p&gt;如果用 Java 统计，则代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;JavaRDD&amp;lt;String&amp;gt; lines = sc.textFile(&quot;data.txt&quot;);
JavaPairRDD&amp;lt;String, Integer&amp;gt; pairs = lines.mapToPair(s -&amp;gt; new Tuple2(s, 1));
JavaPairRDD&amp;lt;String, Integer&amp;gt; counts = pairs.reduceByKey((a, b) -&amp;gt; a + b);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用 Python 统计，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;lines = sc.textFile(&quot;data.txt&quot;)
pairs = lines.map(lambda s: (s, 1))
counts = pairs.reduceByKey(lambda a, b: a + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-8&quot;&gt;测试&lt;/h3&gt;

&lt;p&gt;现在来结合上面的例子实现一个完整的例子。下面，我们来 &lt;a href=&quot;http://segmentfault.com/blog/whuwb/1190000000723037&quot;&gt;分析 Nginx 日志中状态码出现次数&lt;/a&gt;，并且将结果按照状态码从小到大排序。&lt;/p&gt;

&lt;p&gt;先将测试数据上传到 hdfs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -put access.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，编写一个 python 文件，保存为 SimpleApp.py：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pyspark import SparkContext

logFile = &quot;access.log&quot;

sc = SparkContext(&quot;local&quot;, &quot;Simple App&quot;)

rdd = sc.textFile(logFile).cache()

counts = rdd.map(lambda line: line.split()[8]).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).sortByKey(lambda x: x) 

# This is just a demo on how to bring all the sorted data back to a single node.  
# In reality, we wouldn&#39;t want to collect all the data to the driver node.
output = counts.collect()  
for (word, count) in output:  
    print &quot;%s: %i&quot; % (word, count)  

counts.saveAsTextFile(&quot;/data/result&quot;)

sc.stop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，运行下面代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit  --master local[4]   SimpleApp.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行成功之后，你会在终端看到以下输出：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;200: 6827
206: 120
301: 7
304: 10
403: 38
404: 125
416: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并且，在hdfs 上 /user/spark/spark_results/part-00000 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;(u&#39;200&#39;, 6827)
(u&#39;206&#39;, 120)
(u&#39;301&#39;, 7)
(u&#39;304&#39;, 10)
(u&#39;403&#39;, 38)
(u&#39;404&#39;, 125)
(u&#39;416&#39;, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实，这个例子和官方提供的例子很相像，具体请看 &lt;a href=&quot;https://github.com/apache/spark/blob/master/examples/src/main/python/wordcount.py&quot;&gt;wordcount.py&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;如果用 scala 来实现，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;rdd = sc.textFile(&quot;access.log&quot;).cache()
rdd.flatMap(_.split(&#39; &#39;)).map( (_,1)).reduceByKey(_+_).sortByKey(true).saveAsTextFile(&quot;/data/result&quot;)

sc.stop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要对结果按照次数进行排序，则代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;rdd = sc.textFile(&quot;access.log&quot;).cache()
rdd.flatMap(_.split(&#39; &#39;)).map( (_,1)).reduceByKey(_+_).map( x =&amp;gt; (x._2,x._1)).sortByKey(false).map( x =&amp;gt; (x._2,x._1) ).saveAsTextFile(&quot;/data/resultSorted&quot;)

sc.stop()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-9&quot;&gt;常见的转换&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;转换&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;map(func)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;每一个输入元素经过func函数转换后输出一个元素&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;filter(func)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;返回经过 func 函数计算后返回值为 true 的输入元素组成的一个新数据集&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;flatMap(func)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;类似于 map，但是每一个输入元素可以被映射为0或多个输出元素，因此 func 应该返回一个序列&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;mapPartitions(func)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;类似于 map，但独立地在 RDD 的每一个分块上运行，因此在类型为 T 的 RDD 上运行时，func 的函数类型必须是 &lt;code&gt;Iterator[T] ⇒ Iterator[U]&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;mapPartitionsWithSplit(func)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;类似于 mapPartitions, 但 func 带有一个整数参数表示分块的索引值。因此在类型为 T的RDD上运行时，func 的函数类型必须是 &lt;code&gt;(Int, Iterator[T]) ⇒ Iterator[U]&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;sample(withReplacement,fraction, seed)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;根据 fraction 指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed 用于指定随机数生成器种子&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;union(otherDataset)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;返回一个新的数据集，新数据集是由源数据集和参数数据集联合而成&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;distinct([numTasks])) &lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;返回一个包含源数据集中所有不重复元素的新数据集&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;groupByKey([numTasks]) &lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在一个键值对的数据集上调用，返回一个&lt;code&gt;(K，Seq[V])&lt;/code&gt;对的数据集 。注意：默认情况下，只有8个并行任务来做操作，但是你可以传入一个可选的 numTasks 参数来改变它&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;reduceByKey(func, [numTasks]) &lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在一个键值对的数据集上调用时，返回一个键值对的数据集，使用指定的 reduce 函数，将相同 key 的值聚合到一起。类似 groupByKey，reduce 任务个数是可以通过第二个可选参数来配置的&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;sortByKey([ascending], [numTasks]) &lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在一个键值对的数据集上调用，K 必须实现 &lt;code&gt;Ordered&lt;/code&gt; 接口，返回一个按照 Key 进行排序的键值对数据集。升序或降序由 ascending 布尔参数决定&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;join(otherDataset, [numTasks]) &lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在类型为（K,V)和（K,W) 类型的数据集上调用时，返回一个相同key对应的所有元素对在一起的 &lt;code&gt;(K, (V, W))&lt;/code&gt; 数据集&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;cogroup(otherDataset, [numTasks])&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在类型为（K,V)和（K,W) 的数据集上调用，返回一个 &lt;code&gt;(K, Seq[V], Seq[W])&lt;/code&gt; 元组的数据集。这个操作也可以称之为 groupwith&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;cartesian(otherDataset)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;笛卡尔积，在类型为 T 和 U 类型的数据集上调用时，返回一个 (T, U) 对数据集(两两的元素对)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;pipe(command, [envVars])&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;对 RDD 进行管道操作&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;coalesce(numPartitions)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;repartition(numPartitions)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;重新给 RDD 分区&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;repartitionAndSortWithinPartitions(partitioner)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;重新给 RDD 分区，并且每个分区内以记录的 key 排序&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-10&quot;&gt;常见的动作&lt;/h3&gt;

&lt;p&gt;常见的动作列表：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;动作&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;reduce(func)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;通过函数 func 聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的被并行执行。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;collect()&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在驱动程序中，以数组的形式，返回数据集的所有元素。这通常会在使用 filter 或者其它操作并返回一个足够小的数据子集后再使用会比较有用。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;count()&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;返回数据集的元素的个数。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;first()&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;返回数据集的第一个元素，类似于 &lt;code&gt;take(1)&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;take(n)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;返回一个由数据集的前 n 个元素组成的数组。注意，这个操作目前并非并行执行，而是由驱动程序计算所有的元素&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;takeSample(withReplacement,num, seed)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;返回一个数组，在数据集中随机采样 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定的随机数生成器种子&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;takeOrdered(n, [ordering]) &lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;返回自然顺序或者自定义顺序的前 n 个元素&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;saveAsTextFile(path)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;将数据集的元素，以 textfile 的形式，保存到本地文件系统，HDFS或者任何其它 hadoop 支持的文件系统。对于每个元素，Spark 将会调用 &lt;code&gt;toString&lt;/code&gt; 方法，将它转换为文件中的文本行&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;saveAsSequenceFile(path)&lt;/code&gt; (Java and Scala)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;将数据集的元素，以 Hadoop sequencefile 的格式保存到指定的目录下&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;saveAsObjectFile(path)&lt;/code&gt; (Java and Scala)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;将数据集的元素，以 Java 序列化的方式保存到指定的目录下&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;countByKey()&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;对(K,V)类型的 RDD 有效，返回一个 (K，Int) 对的 Map，表示每一个key对应的元素个数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;foreach(func)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在数据集的每一个元素上，运行函数 func 进行更新。这通常用于边缘效果，例如更新一个累加器，或者和外部存储系统进行交互，例如 HBase&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;rdd&quot;&gt;4.4 RDD持久化&lt;/h2&gt;

&lt;p&gt;Spark 的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当我们持久化一个 RDD 是，每一个节点将这个RDD的每一个分片计算并保存到内存中以便在下次对这个数据集（或者这个数据集衍生的数据集）的计算中可以复用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。&lt;/p&gt;

&lt;p&gt;你可以通过调用&lt;code&gt;persist&lt;/code&gt;或&lt;code&gt;cache&lt;/code&gt;方法来标记一个想要持久化的 RDD。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark 的缓存是具有容错性的——如果 RDD 的任意一个分片丢失了，Spark 就会依照这个 RDD 产生的转化过程自动重算一遍。&lt;/p&gt;

&lt;p&gt;另外，每一个持久化的 RDD 都有一个可变的存储级别，这个级别使得用户可以改变 RDD 持久化的储存位置。比如，你可以将数据集持久化到硬盘上，也可以将它以序列化的 Java 对象形式（节省空间）持久化到内存中，还可以将这个数据集在节点之间复制，或者使用 Tachyon 将它储存到堆外。这些存储级别都是通过向 &lt;code&gt;persist()&lt;/code&gt; 传递一个 StorageLevel 对象（Scala, Java, Python）来设置的。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：在Python中，储存的对象永远是通过 Pickle 库序列化过的，所以设不设置序列化级别不会产生影响。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Spark 还会在 shuffle 操作（比如 reduceByKey）中自动储存中间数据，即使用户没有调用 persist。这是为了防止在 shuffle 过程中某个节点出错而导致的全盘重算。不过如果用户打算复用某些结果 RDD，我们仍然建议用户对结果 RDD 手动调用 persist，而不是依赖自动持久化机制。&lt;/p&gt;

&lt;p&gt;使用以下两种方法可以标记要缓存的 RDD：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;lineLengths.persist()  
lineLengths.cache() 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;取消缓存则用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;lineLengths.unpersist()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每一个 RDD 都可以用不同的保存级别进行保存，通过将一个 &lt;code&gt;org.apache.spark.storage.StorageLevel&lt;/code&gt; 对象传递给 &lt;code&gt;persist(self, storageLevel)&lt;/code&gt; 可以控制 RDD 持久化到磁盘、内存或者是跨节点复制等等。&lt;code&gt;cache()&lt;/code&gt; 方法是使用默认存储级别的快捷方法，也就是 &lt;code&gt;StorageLevel.MEMORY_ONLY&lt;/code&gt;。 完整的可选存储级别如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;存储级别&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;意义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;MEMORY_ONLY&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;默认的级别， 将 RDD 作为反序列化的的对象存储在 JVM 中。如果不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;MEMORY_AND_DISK&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;将 RDD 作为反序列化的的对象存储在 JVM 中。如果不能被与内存装下，超出的分区将被保存在硬盘上，并且在需要时被读取&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;MEMORY_ONLY_SER&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;将 RDD 作为序列化的的对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用fast serializer,但在读取时会比较占用CPU&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;MEMORY_AND_DISK_SER&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;与 &lt;code&gt;MEMORY_ONLY_SER&lt;/code&gt; 相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;DISK_ONLY&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;只将 RDD 分区存储在硬盘上&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;MEMORY_ONLY_2&lt;/code&gt;、&lt;code&gt;MEMORY_AND_DISK_2&lt;/code&gt;等&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;OFF_HEAP&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;开发中&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Spark 的不同存储级别，旨在满足内存使用和 CPU 效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果你的 RDD 可以很好的与默认的存储级别契合，就不需要做任何修改了。这已经是 CPU 使用效率最高的选项，它使得 RDD的操作尽可能的快。&lt;/li&gt;
  &lt;li&gt;如果不行，试着使用 &lt;code&gt;MEMORY_ONLY_SER&lt;/code&gt; 并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。&lt;/li&gt;
  &lt;li&gt;尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。&lt;/li&gt;
  &lt;li&gt;如果你想有快速故障恢复能力，使用复制存储级别。例如：用 Spark 来响应web应用的请求。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在 RDD 上持续的运行任务，而不需要等待丢失的分区被重新计算。&lt;/li&gt;
  &lt;li&gt;如果你想要定义你自己的存储级别，比如复制因子为3而不是2，可以使用 &lt;code&gt;StorageLevel&lt;/code&gt; 单例对象的 &lt;code&gt;apply()&lt;/code&gt;方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-11&quot;&gt;5. 共享变量&lt;/h1&gt;

&lt;p&gt;通常情况下，当一个函数传递给一个在远程集群节点上运行的Spark操作（比如map和reduce）时，Spark会对涉及到的变量的所有副本执行这个函数。这些变量会被复制到每个机器上，而且这个过程不会被反馈给驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是，Spark仍然提供了有限的两种共享变量类型用于常见的使用场景：&lt;code&gt;广播变量&lt;/code&gt;和&lt;code&gt;累加器&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-12&quot;&gt;广播变量&lt;/h2&gt;

&lt;p&gt;广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。它们可以被使用，比如，给每一个节点传递一份大输入数据集的拷贝是很低效的。Spark 试图使用高效的广播算法来分布广播变量，以此来降低通信花销。&lt;br /&gt;
可以通过 &lt;code&gt;SparkContext.broadcast(v)&lt;/code&gt; 来从变量 v 创建一个广播变量。这个广播变量是 v 的一个包装，同时它的值可以功过调用 value 方法来获得。以下的代码展示了这一点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;broadcastVar = sc.broadcast([1, 2, 3])
&amp;lt;pyspark.broadcast.Broadcast object at 0x102789f10&amp;gt;

&amp;gt;&amp;gt;&amp;gt; broadcastVar.value
[1, 2, 3]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量v，这样就可以保证v在节点之间只被传递一次。另外，v变量在被广播之后不应该再被修改了，这样可以确保每一个节点上储存的广播变量的一致性（如果这个变量后来又被传输给一个新的节点）。&lt;/p&gt;

&lt;h2 id=&quot;section-13&quot;&gt;累加器&lt;/h2&gt;

&lt;p&gt;累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在MapReduce过程中）或求和运算。Spark原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在Spark的UI中显示出来。这有助于了解计算的累进过程（注意：目前Python中不支持这个特性）。&lt;/p&gt;

&lt;p&gt;可以通过&lt;code&gt;SparkContext.accumulator(v)&lt;/code&gt;来从变量v创建一个累加器。在集群中运行的任务随后可以使用add方法或&lt;code&gt;+=&lt;/code&gt;操作符（在Scala和Python中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的value方法。&lt;/p&gt;

&lt;p&gt;以下的代码展示了向一个累加器中累加数组元素的过程：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;accum = sc.accumulator(0)
Accumulator&amp;lt;id=0, value=0&amp;gt;

&amp;gt;&amp;gt;&amp;gt; sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))
...
10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s

scala&amp;gt; accum.value
10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段代码利用了累加器对 int 类型的内建支持，程序员可以通过继承 &lt;code&gt;AccumulatorParam&lt;/code&gt; 类来创建自己想要的类型支持。AccumulatorParam 的接口提供了两个方法：&lt;code&gt;zero&lt;/code&gt; 用于为你的数据类型提供零值；&lt;code&gt;addInPlace&lt;/code&gt; 用于计算两个值得和。比如，假设我们有一个 &lt;code&gt;Vector&lt;/code&gt;类表示数学中的向量，我们可以这样写：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class VectorAccumulatorParam(AccumulatorParam):
    def zero(self, initialValue):
        return Vector.zeros(initialValue.size)

    def addInPlace(self, v1, v2):
        v1 += v2
        return v1

# Then, create an Accumulator of this type:
vecAccum = sc.accumulator(Vector(...), VectorAccumulatorParam())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;累加器的更新操作只会被运行一次，Spark 提供了保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。&lt;/p&gt;

&lt;p&gt;累加器不会该别 Spark 的惰性求值模型。如果累加器在对RDD的操作中被更新了，它们的值只会在启动操作中作为 RDD 计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。下面的代码段展示了这一点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;accum = sc.accumulator(0)
data.map(lambda x =&amp;gt; acc.add(x); f(x))
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-14&quot;&gt;6. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/programming-guide.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://rdc.taobao.org/?p=2024&quot;&gt;http://rdc.taobao.org/?p=2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/u011391905/article/details/37929731&quot;&gt;http://blog.csdn.net/u011391905/article/details/37929731&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://segmentfault.com/blog/whuwb/1190000000723037&quot;&gt;http://segmentfault.com/blog/whuwb/1190000000723037&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cholerae.com/2015/04/11/-%E7%BF%BB%E8%AF%91-Spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-Python%E7%89%88/&quot;&gt;[翻译]Spark编程指南(Python版)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/02/03/spark-programming-guide.html</link>
      <guid>http://blog.javachen.com/2015/02/03/spark-programming-guide.html</guid>
      <pubDate>2015-02-03T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Require.JS快速入门</title>
      <description>&lt;h2 id=&quot;requirejs-&quot;&gt;Require.JS 介绍&lt;/h2&gt;

&lt;p&gt;Require.JS 是一个基于 AMD 规范的 JavaScript 模块加载框架。实现 JavaScript 文件的异步加载，管理模块之间的依赖性，提升网页的加载速度。&lt;/p&gt;

&lt;p&gt;AMD 是 &lt;code&gt;Asynchronous Module Definition&lt;/code&gt; 的缩写，意思就是 &lt;code&gt;异步模块定义&lt;/code&gt;。它采用异步方式加载模块，模块的加载不影响它后面语句的运行。所有依赖这个模块的语句，都定义在一个回调函数中，等到加载完成之后，这个回调函数才会运行。&lt;/p&gt;

&lt;p&gt;官网地址：&lt;www.requirejs.org&gt;&lt;/www.requirejs.org&gt;&lt;/p&gt;

&lt;p&gt;Require.JS 的诞生主要为了解决两个问题：　　&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）实现 JavaScript 文件的异步加载，避免网页失去响应；&lt;/li&gt;
  &lt;li&gt;2）管理模块之间的依赖性，便于代码的编写和维护。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;requirejs&quot;&gt;加载 Require.JS&lt;/h2&gt;

&lt;p&gt;去&lt;a href=&quot;http://requirejs.org/docs/download.html&quot;&gt;官方网站&lt;/a&gt;下载最新版本，然后创建一个工程，目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;project-directory
├── project.html
└── scripts
    ├── lib
    │   ├── a.js
    │   ├── b.js
    │   ├── backbone.js
    │   └── underscore.js
    ├── main.js
    └── require.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编写 project.html 内容，引入 Require.JS 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        &amp;lt;title&amp;gt;My Sample Project&amp;lt;/title&amp;gt;
        &amp;lt;!-- data-main attribute tells require.js to load
             scripts/main.js after require.js loads. --&amp;gt;
        &amp;lt;script data-main=&quot;scripts/main&quot; src=&quot;scripts/require.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
        &amp;lt;h1&amp;gt;My Sample Project&amp;lt;/h1&amp;gt;
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;加载 scripts/require.js 这个文件，也可能造成网页失去响应。解决办法有两个，一个是把它放在网页底部加载，另一个是写成下面这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;script data-main=&quot;scripts/main&quot; src=&quot;js/require.js&quot; defer async=&quot;true&quot; &amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.mozilla.org/en/docs/Web/HTML/Element/script#Attributes&quot;&gt;async&lt;/a&gt; 属性表明这个文件需要异步加载，避免网页失去响应。IE 不支持这个属性，只支持 &lt;code&gt;defer&lt;/code&gt;，所以把 &lt;code&gt;defer&lt;/code&gt; 也写上。&lt;/p&gt;

&lt;p&gt;加载 Require.JS 以后，下一步就要加载我们自己的代码了，这里我们使用 &lt;a href=&quot;http://requirejs.org/docs/api.html#data-main&quot;&gt;data-main&lt;/a&gt; 属性来指定网页程序的主模块。在上例中，就是 scripts 目录下面的 main.js，这个文件会第一个被 Require.JS 加载。由于 Require.JS 默认的文件后缀名是 js，所以可以把 main.js 简写成 main。&lt;/p&gt;

&lt;p&gt;在浏览器中打开 project.html 文件，查看浏览器是否提示有错误。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;定义模块&lt;/h2&gt;

&lt;p&gt;Require.JS 加载的模块，采用 AMD 规范。也就是说，模块必须按照 AMD 的规定来写。&lt;br /&gt;
具体来说，就是模块必须采用特定的 &lt;code&gt;define()&lt;/code&gt; 函数来定义。如果一个模块不依赖其他模块，那么可以直接定义在 &lt;code&gt;define()&lt;/code&gt; 函数之中。&lt;/p&gt;

&lt;p&gt;定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define(id, dependencies, factory);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;id&lt;/code&gt; : 模块标示符[可省略]&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dependencies&lt;/code&gt; : 所依赖的模块[可省略]&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;factory&lt;/code&gt; : 模块的实现，或者一个 JavaScript 对象。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Require.JS 定义模块的几种方式：&lt;/p&gt;

&lt;p&gt;1、定义一个最简单的模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define({
        color: &quot;black&quot;,
        size: &quot;unisize&quot;
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、传入匿名函数定义一个模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define(function () {
        return {
            color: &quot;black&quot;,
            size: &quot;unisize&quot;
        }
 });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、定义一个模块并依赖于 cart.js&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define([&quot;./cart&quot;], function(cart) {
        return {
            color: &quot;blue&quot;,
            size: &quot;large&quot;,
            addToCart: function() {
                cart.add(this);
            }
        }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、定义一个名称为 hello 且依赖于 cart.js 的模块&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define(&quot;hello&quot;,[&quot;./cart&quot;],function(cart) {
        //do something
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、兼容 commonJS 模块的写法(使用 require 获取依赖模块，使用 exports 导出 API)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define(function(require, exports, module) {
        var base = require(&#39;base&#39;);
        exports.show = function() {
            // todo with module base
        } 
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;举例，在 lib/a.js 中，我们可以定义模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define(function (){
    var add = function (x,y){
        return x+y;
    };
    return {
        add: add
    };
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在 main.js 中使用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require([&#39;lib/a&#39;], function (a){
    alert(a.add(1,1));
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果这个模块还依赖其他模块，那么 &lt;code&gt;define()&lt;/code&gt; 函数的第一个参数，必须是一个数组，指明该模块的依赖性。&lt;/p&gt;

&lt;p&gt;例如，lib/b.js 文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define([&#39;lib/a&#39;], function(a){
    function fun1(){
        alert(2);
    }
    return {
        fun1: fun1,
        fun2: function(){alert(3)}
    };
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定义的模块返回函数个数问题:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;define 的 return 只有一个函数，require 的回调函数可以直接用别名代替该函数名。&lt;/li&gt;
  &lt;li&gt;define 的 return 当有多个函数，require 的回调函数必须用别名调用所有的函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这时候修改 main.js 内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require([&#39;lib/a&#39;,&#39;lib/b&#39;], function(a,b) {
    alert(a.add(0,1))

    b.fun1();
    b.fun2();
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在浏览器中打开 project.html 文件，查看输出内容。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;全局配置&lt;/h2&gt;

&lt;p&gt;如果你想改变 RequireJS 的默认配置来使用自己的配置，你可以使用 &lt;code&gt;require.config&lt;/code&gt; 函数。&lt;code&gt;require.config()&lt;/code&gt; 方法一般写在主模块(main.js)最开始。参数为一个对象，通过对象的键值对加载进行配置:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;baseUrl&lt;/code&gt;：用于加载模块的根路径。如果 baseUrl 没有指定，那么就会使用 &lt;code&gt;data-main&lt;/code&gt; 中指定的路径。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;paths&lt;/code&gt;：用于映射不存在根路径下面的模块路径。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;map&lt;/code&gt; : 对于给定的相同的模块名，加载不同的模块，而不是加载相同的模块&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;packages&lt;/code&gt; : 配置从 CommonJS 包来加载模块&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;waitSeconds&lt;/code&gt;：是指定最多花多长等待时间来加载一个 JavaScript 文件，用户不指定的情况下默认为 7 秒。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;shims&lt;/code&gt;：配置在脚本/模块外面并没有使用 RequireJS 的函数依赖并且初始化函数。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;deps&lt;/code&gt;：加载依赖关系。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其他可配置的选项还包括 locale、context、callback 等，有兴趣的读者可以在 RequireJS 的官方网站查阅&lt;a href=&quot;http://www.requirejs.org/docs/api.html#config&quot;&gt;相关文档&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;main.js 中修改如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require.config({
    //By default load any module IDs from scripts/main
    baseUrl: &#39;scripts&#39;,
    paths: {
        &quot;jquery&quot; : [&quot;http://libs.baidu.com/jquery/2.0.3/jquery&quot;, &quot;scripts/jquery&quot;],
        &quot;a&quot;:&quot;lib/a&quot;
    }
});

//通过别名来引用模块
require([&quot;jquery&quot;,&quot;a&quot;],function($){
    $(function(){
        alert(&quot;load finished&quot;);
    })
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个例子中，通过 baseUrl 把根路径设置为了 scripts，通过 paths 的配置会使我们的模块名字更精炼，paths 还有一个重要的功能，就是可以配置多个路径，如果远程加载没有成功，可以加载本地的库。&lt;/p&gt;

&lt;p&gt;上面例子中，先从远程加载 jquery 模块，如果加载失败，则从本地 scripts/jquery 加载；a 模块名称为 lib/a 路径的简称，加载 a 模块时，实际上是加载的 script/lib/a.js 文件。&lt;/p&gt;

&lt;p&gt;通过 require加载的模块一般都需要符合 AMD 规范即使用 define 来申明模块，但是部分时候需要加载非 AMD 规范的 JavaScript，这时候就需要用到另一个功能：&lt;code&gt;shim&lt;/code&gt;。比如我要是用 underscore 类库，但是他并没有实现 AMD 规范，那我们可以这样配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require.config({
    shim: {
        &quot;underscore&quot; : {
            exports : &quot;_&quot;;
        }
    }
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样配置后，我们就可以在其他模块中引用 underscore 模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require([&quot;underscore&quot;], function(_){
    _.each([1,2,3], alert);
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在新版本的 jquery 中，继承了 AMD 规范，所以可以直接使用 &lt;code&gt;require[&quot;jquery&quot;]&lt;/code&gt;，但是我们经常用到的 jquery 插件基本都不符合AMD规范。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require.config({
    shim: {
        &quot;underscore&quot; : {
            exports : &quot;_&quot;;
        },
        &quot;jquery.form&quot; : {
            deps : [&quot;jquery&quot;]
        }
    }
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以简写为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require.config({
    shim: {
        &quot;underscore&quot; : {
            exports : &quot;_&quot;;
        },
        &quot;jquery.form&quot; : [&quot;jquery&quot;]
    }
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样配置之后我们就可以使用加载插件后的 jquery 了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require.config([&quot;jquery&quot;, &quot;jquery.form&quot;], function($){
    $(function(){
        $(&quot;#form&quot;).ajaxSubmit({...});
    })
})
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;requirejs--1&quot;&gt;Require.JS 插件&lt;/h2&gt;

&lt;p&gt;Require.JS 还提供一系列插件，实现一些特定的功能。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;domready&lt;/code&gt; 插件，可以让回调函数在页面 DOM 结构加载完成后再运行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require([&#39;domready!&#39;], function (doc){
    // called once the DOM is ready
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tex t和 image 插件，则是允许 Require.JS 加载文本和图片文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;define([
    &#39;text!review.txt&#39;,
    &#39;image!cat.jpg&#39;
    ],
    function(review,cat){
        console.log(review);
        document.body.appendChild(cat);
    }
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;类似的插件还有 json 和 mdown，用于加载 json 文件和 markdown 文件。&lt;/p&gt;

&lt;p&gt;插件清单：&lt;a href=&quot;https://github.com/jrburke/requirejs/wiki/Plugins&quot;&gt;https://github.com/jrburke/requirejs/wiki/Plugins&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;其他问题&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;1、循环依赖&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在一些情况中，我们可能需要模块 moduleA 和 moduleA 中的函数需要依赖一些应用。这就是循环依赖。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// js/app/moduleA.js
define( [ &quot;require&quot;, &quot;app/app&quot;],
    function( require, app ) {
        return {
            foo: function( title ) {
                var app = require( &quot;app/app&quot; );
                return app.something();
            }
        }
    }
);
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;2、得到模块的地址&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果你需要得到模块的地址，你可以这么做……&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var path = require.toUrl(&quot;./style.css&quot;);  
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;3、JSONP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以这样处理 JSONP：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require( [ 
    &quot;http://someapi.com/foo?callback=define&quot;
], function (data) {
    console.log(data);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;代码合并于压缩&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;请参考 &lt;a href=&quot;http://blog.jobbole.com/39205/&quot;&gt;优化 RequireJS 项目（合并与压缩）&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://yuankeqiang.lofter.com/post/8de51_b83cf3&quot;&gt;使用require.js进行JavaScript模块化编程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/02/02/quick-start-of-requirejs.html</link>
      <guid>http://blog.javachen.com/2015/02/02/quick-start-of-requirejs.html</guid>
      <pubDate>2015-02-02T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>用Yeoman构建AngularJS项目</title>
      <description>&lt;p&gt;这篇文章不是一篇翻译也不是一篇原创文章，类似于一篇学习笔记，主要是记录一些关键的过程，方便查阅加深理解和记忆。&lt;/p&gt;

&lt;h1 id=&quot;yeoman-&quot;&gt;Yeoman 介绍&lt;/h1&gt;

&lt;p&gt;Yeoman 是 Google 的团队和外部贡献者团队合作开发的，他的目标是通过 Grunt（一个用于开发任务自动化的命令行工具）和 Bower（一个HTML、CSS、Javascript 和图片等前端资源的包管理器）的包装为开发者创建一个易用的工作流。&lt;/p&gt;

&lt;p&gt;Yeoman 的目的不仅是要为新项目建立工作流，同时还是为了解决前端开发所面临的诸多严重问题，例如零散的依赖关系。&lt;/p&gt;

&lt;p&gt;Yeoman 主要有三部分组成：&lt;code&gt;yo&lt;/code&gt;（脚手架工具）、&lt;code&gt;grunt&lt;/code&gt;（构建工具）、&lt;code&gt;bower&lt;/code&gt;（包管理器）。这三个工具是分别独立开发的，但是需要配合使用，来实现我们高效的工作流模式。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yeoman/yo&quot;&gt;Yo&lt;/a&gt; 搭建新应用的脚手架，编写你的 Grunt 配置并且安装你有可能在构建中需要的相关的 Grunt 任务。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gruntjs.com/&quot;&gt;Grunt&lt;/a&gt; 被用来构建，预览以及测试你的项目，感谢来自那些由 Yeoman 团队和 runt-contrib 所管理的任务的帮助。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bower.io/&quot;&gt;Bower&lt;/a&gt; 被用来进行依赖管理，所以你不再需要手动的下载和管理你的脚本了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面这幅图很形象的表明了他们三者之间的协作关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yeoman.io/assets/img/workflow.c3cc.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yeoman 特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;快速创建骨架应用程序&lt;/code&gt;。使用可自定义的模板（例如：HTML5、Boilerplate、Twitter Bootstrap 等）、AMD（通过 RequireJS）以及其他工具轻松地创建新项目的骨架。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;自动编译 CoffeeScrip 和 Compass&lt;/code&gt;。在做出变更的时候，Yeoman 的 LiveReload 监视进程会自动编译源文件，并刷新浏览器，而不需要你手动执行。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;自动完善你的脚本&lt;/code&gt;。所有脚本都会自动针对 JSHint 运行，从而确保它们遵循语言的最佳实践。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;内建的预览服务器&lt;/code&gt;。你不需要启动自己的 HTTP 服务器。内建的服务器用一条命令就可以启动。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;非常棒的图像优化&lt;/code&gt;。使用 OptPNG 和 JPEGTran 对所有图像做了优化。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;生成 AppCache 清单&lt;/code&gt;。Yeoman 会为你生成应用程序缓存的清单，你只需要构建项目就好。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;“杀手级”的构建过程&lt;/code&gt;。你所做的工作不仅被精简到最少，让你更加专注，为你节省大量工作。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;集成的包管理&lt;/code&gt;。Yeoman 让你可以通过命令行轻松地查找新的包，安装并保持更新，而不需要你打开浏览器。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;对 ES6 模块语法的支持&lt;/code&gt;。你可以使用最新的 ECMAScript 6 模块语法来编写模块。这还是一种实验性的特性，它会被转换成 ES5，从而你可以在所有流行的浏览器中使用编写的代码。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;PhantomJS 单元测试&lt;/code&gt;。你可以通过 PhantomJS 轻松地运行单元测试。当你创建新的应用程序的时候，它还会为你自动创建测试内容的骨架。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;安装&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;安装前提&lt;/h2&gt;

&lt;p&gt;一份完整的&lt;a href=&quot;https://github.com/yeoman/yeoman/wiki/Getting-Started&quot;&gt;新手上路&lt;/a&gt;指南在这里可以找到，但是对于那些希望快速上手操练的家伙，请确定你已经安装了 &lt;a href=&quot;http://nodejs.org/&quot;&gt;Node.js&lt;/a&gt;, &lt;a href=&quot;http://git-scm.org/&quot;&gt;Git&lt;/a&gt;。&lt;a href=&quot;http://ruby-lang.org/&quot;&gt;Ruby&lt;/a&gt; 和 &lt;a href=&quot;http://compass-style.org/install&quot;&gt;Compass&lt;/a&gt; 是可选的(如果你想要使用Compass)。&lt;/p&gt;

&lt;p&gt;Node.js 版本要求为 v0.10.x+，npm 版本要求为 v2.1.0+，运行下面命令 检查版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ node --version &amp;amp;&amp;amp; npm --version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以检查 Git 版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git --version
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;yeoman--1&quot;&gt;安装 Yeoman 工具集&lt;/h2&gt;

&lt;p&gt;确保 Node 安装之后，安装 Yeoman 工具集：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npm install --global yo bower grunt-cli
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行下面命令检查是否安装成功：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yo --version &amp;amp;&amp;amp; bower --version &amp;amp;&amp;amp; grunt --version

1.4.5
1.3.12
grunt-cli v0.1.13
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;angularjs-&quot;&gt;安装 AngularJS 的生成器&lt;/h2&gt;

&lt;p&gt;Yeoman 生成器使用 &lt;a href=&quot;https://www.npmjs.org/&quot;&gt;npm&lt;/a&gt; 命令,现在可用的生成器数量已经超过了 &lt;a href=&quot;http://yeoman.io/generators/&quot;&gt;1000+个生成器&lt;/a&gt;，这其中很多都是由开源社区编写的。&lt;/p&gt;

&lt;p&gt;你可以安装 web 应用的生成器&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npm install -g generator-webapp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以安装 &lt;a href=&quot;https://www.npmjs.org/package/generator-angular&quot;&gt;generator-angular&lt;/a&gt; 生成器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npm install --global generator-angular
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;创建项目&lt;/h1&gt;

&lt;p&gt;创建一个目录用于作为工程目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir mytodo &amp;amp;&amp;amp; cd mytodo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以查看所有的生成器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yo
? &#39;Allo june! What would you like to do?
  Run a generator
  Angular
  Karma
  Webapp
❯ Mocha
  ──────────────
  Update your generators
(Move up and down to reveal more choices)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行 Angular 生成器，会提示你是否使用 Sass 和引入 Bootstrap，以及加载哪些 Angular 模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yo angular

     _-----_
    |       |    .--------------------------.
    |--(o)--|    |    Welcome to Yeoman,    |
   `---------´   |   ladies and gentlemen!  |
    ( _´U`_ )    &#39;--------------------------&#39;
    /___A___\
     |  ~  |
   __&#39;.___.&#39;__
 ´   `  |° ´ Y `

Out of the box I include Bootstrap and some AngularJS recommended modules.

? Would you like to use Sass (with Compass)? No
? Would you like to include Bootstrap? Yes
? Which modules would you like to include? angular-animate.js, angular-cookies.js, angular-resource.js, angular-route.js, angular-sanitize.js, angular-touch.js

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;选择需要下载的模块，然后回车。过一段时间之后，生成的目录结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mytodo
├── Gruntfile.js
├── app
│   ├── 404.html
│   ├── favicon.ico
│   ├── images
│   │   └── yeoman.png
│   ├── index.html
│   ├── robots.txt
│   ├── scripts
│   │   ├── app.js
│   │   └── controllers
│   ├── styles
│   │   └── main.css
│   └── views
│       ├── about.html
│       └── main.html
├── bower.json
├── bower_components
├── package.json
└── test
    ├── karma.conf.js
    └── spec
        └── controllers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例中的所有 js 代码都使用了严格模式，有关严格模式的内容可以参考 &lt;a href=&quot;http://www.waylau.com/javascript-use-strict-mode/&quot;&gt;http://www.waylau.com/javascript-use-strict-mode/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;运行下面命令启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grunt serve
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问 &lt;localhost:9000&gt;，你会看到：&lt;/localhost:9000&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/2015/yeoman-web.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;angularjs--1&quot;&gt;创建 AngularJS 应用&lt;/h1&gt;

&lt;h2 id=&quot;todo-&quot;&gt;创建新模板展现 Todo 列表&lt;/h2&gt;

&lt;p&gt;打开 scripts/controllers/main.js ，代码修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&#39;use strict&#39;;

angular.module(&#39;webApp&#39;).controller(&#39;MainCtrl&#39;, function ($scope) {
    $scope.todos = [&#39;Item 1&#39;, &#39;Item 2&#39;, &#39;Item 3&#39;];
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 views/main.html，将 todos 中的项目以 input 标签形式输出：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;div class=&quot;container&quot;&amp;gt;
  &amp;lt;h2&amp;gt;My todos&amp;lt;/h2&amp;gt;
  &amp;lt;p class=&quot;form-group&quot; ng-repeat=&quot;todo in todos&quot;&amp;gt;
    &amp;lt;input type=&quot;text&quot; ng-model=&quot;todo&quot; class=&quot;form-control&quot;&amp;gt;
  &amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;页面会显示如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yeoman.io/assets/img/codelab/image_15.cb9d.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;todo--1&quot;&gt;添加一个 todo 项&lt;/h2&gt;

&lt;p&gt;首先，添加一个输入框和添加按钮，将 views/main.html 修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;div class=&quot;container&quot;&amp;gt;
  &amp;lt;h2&amp;gt;My todos&amp;lt;/h2&amp;gt;

  &amp;lt;!-- Todos input --&amp;gt;
  &amp;lt;form role=&quot;form&quot; ng-submit=&quot;addTodo()&quot;&amp;gt;
    &amp;lt;div class=&quot;row&quot;&amp;gt;
      &amp;lt;div class=&quot;input-group&quot;&amp;gt;
        &amp;lt;input type=&quot;text&quot; ng-model=&quot;todo&quot; placeholder=&quot;What needs to be done?&quot; class=&quot;form-control&quot;&amp;gt;
        &amp;lt;span class=&quot;input-group-btn&quot;&amp;gt;
          &amp;lt;input type=&quot;submit&quot; class=&quot;btn btn-primary&quot; value=&quot;Add&quot;&amp;gt;
        &amp;lt;/span&amp;gt;
      &amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/form&amp;gt;
  &amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;

  &amp;lt;!-- Todos list --&amp;gt;
  &amp;lt;p class=&quot;form-group&quot; ng-repeat=&quot;todo in todos&quot;&amp;gt;
    &amp;lt;input type=&quot;text&quot; ng-model=&quot;todo&quot; class=&quot;form-control&quot;&amp;gt;
  &amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候，页面内容如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yeoman.io/assets/img/codelab/image_16.c919.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;修改 main.js 添加 addTodo() 事件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&#39;use strict&#39;;

angular.module(&#39;webApp&#39;)
  .controller(&#39;MainCtrl&#39;, function ($scope) {
    $scope.todos = [&#39;Item 1&#39;, &#39;Item 2&#39;, &#39;Item 3&#39;];
    $scope.addTodo = function () {
      $scope.todos.push($scope.todo);
      $scope.todo = &#39;&#39;;
    };
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，就完成了添加 todo 项的功能。&lt;/p&gt;

&lt;h2 id=&quot;todo--2&quot;&gt;移除一个 todo 项目&lt;/h2&gt;

&lt;p&gt;先在列表中每一个 todo 项目的边上加上一个移除按钮，修改 views/main.html 中 &lt;code&gt;Todos list&lt;/code&gt; 注释部分的代码为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;!-- Todos list --&amp;gt;
&amp;lt;p class=&quot;input-group&quot; ng-repeat=&quot;todo in todos&quot;&amp;gt;
  &amp;lt;input type=&quot;text&quot; ng-model=&quot;todo&quot; class=&quot;form-control&quot;&amp;gt;
  &amp;lt;span class=&quot;input-group-btn&quot;&amp;gt;
    &amp;lt;button class=&quot;btn btn-danger&quot; ng-click=&quot;removeTodo($index)&quot; aria-label=&quot;Remove&quot;&amp;gt;X&amp;lt;/button&amp;gt;
  &amp;lt;/span&amp;gt;
&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 main.js 添加 &lt;code&gt;removeTodo($index)&lt;/code&gt; 事件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&#39;use strict&#39;;

angular.module(&#39;webApp&#39;)
  .controller(&#39;MainCtrl&#39;, function ($scope) {
    $scope.todos = [&#39;Item 1&#39;, &#39;Item 2&#39;, &#39;Item 3&#39;];
    $scope.addTodo = function () {
      $scope.todos.push($scope.todo);
      $scope.todo = &#39;&#39;;
    };

    $scope.removeTodo = function (index) {
      $scope.todos.splice(index, 1);
    };
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，删除按钮能够响应删除事件了。虽然我们可以添加和移除 Todo 事项，但是这些记录都不能永久地保存。一旦页面被刷新了，更改的记录都会不见了，又恢复到 main.js 中设置的todo 数组的值。&lt;/p&gt;

&lt;p&gt;另外，上面添加 Todo 项时，如果重复添加相同的记录，则后台会报错，这是因为脚本中没有做校验。&lt;/p&gt;

&lt;h2 id=&quot;todo--3&quot;&gt;对 Todo 事项进行排序&lt;/h2&gt;

&lt;p&gt;接下来，我们安装并引入 &lt;a href=&quot;https://github.com/angular-ui/ui-sortable&quot;&gt;AngularUI Sortable&lt;/a&gt; 模块，使得 Todo 事项可以排序。这里，我们需要使用 bower 安装 angular-ui-sortable 和 jquery-ui：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower install --save angular-ui-sortable jquery-ui
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加参数 &lt;code&gt;--save&lt;/code&gt; 可以更新 bower.json 文件中关于 angular-ui-sortable 和 jquery-ui 的依赖，这样你就不用手动去 bower.json 中更新依赖了。&lt;/p&gt;

&lt;p&gt;再次启动 grunt server：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grunt serve
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了使用Sortable模块，我们需要在 scripts/app.js 中更新Angular 模块，将 Sortable 可以加载到我们的应用中，更改前代码， 将 ui.sortable 添加进数组中,如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;angular.module(&#39;webApp&#39;, [
    &#39;ngAnimate&#39;,
    &#39;ngCookies&#39;,
    &#39;ngResource&#39;,
    &#39;ngRoute&#39;,
    &#39;ngSanitize&#39;,
    &#39;ngTouch&#39;,
    &#39;ui.sortable&#39;
  ])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，在 views/main.html 中，我们需要将 &lt;code&gt;ui-sortable&lt;/code&gt; 指令作为一个 div 将 &lt;code&gt;ng-repeat&lt;/code&gt; 层包起来。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;!-- Todos list --&amp;gt;
&amp;lt;div ui-sortable ng-model=&quot;todos&quot;&amp;gt;
  &amp;lt;p class=&quot;input-group&quot; ng-repeat=&quot;todo in todos&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加一些内联的 CSS，将鼠标显示为 “可移动” 样式来告诉用户这些 todo 项是可以移动的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;p class=&quot;input-group&quot; ng-repeat=&quot;todo in todos&quot; style=&quot;padding:5px 10px; cursor: move;&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;!-- Todos list --&amp;gt;
&amp;lt;div ui-sortable ng-model=&quot;todos&quot;&amp;gt;
  &amp;lt;p class=&quot;input-group&quot; ng-repeat=&quot;todo in todos&quot; style=&quot;padding:5px 10px; cursor: move;&quot;&amp;gt;
    &amp;lt;input type=&quot;text&quot; ng-model=&quot;todo&quot; class=&quot;form-control&quot;&amp;gt;
    &amp;lt;span class=&quot;input-group-btn&quot;&amp;gt;
      &amp;lt;button class=&quot;btn btn-danger&quot; ng-click=&quot;removeTodo($index)&quot; aria-label=&quot;Remove&quot;&amp;gt;X&amp;lt;/button&amp;gt;
    &amp;lt;/span&amp;gt;
  &amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务浏览器，我们就可以对 Todo 事项进行拖拽排序了。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;持久化存储&lt;/h2&gt;

&lt;p&gt;之前项目的数据，当浏览器刷新后就不会保存了。我们可以安装 Angular 模块 &lt;a href=&quot;http://gregpike.net/demos/angular-local-storage/demo.html&quot;&gt;angular-local-storage&lt;/a&gt;，快速实现本地存储。&lt;/p&gt;

&lt;p&gt;下载依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower install --save angular-local-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑 scripts/app.js 添加 LocalStorageModule的 适配器:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;angular
  .module(&#39;webApp&#39;, [
    &#39;ngAnimate&#39;,
    &#39;ngCookies&#39;,
    &#39;ngResource&#39;,
    &#39;ngRoute&#39;,
    &#39;ngSanitize&#39;,
    &#39;ngTouch&#39;,
    &#39;ui.sortable&#39;,
    &#39;LocalStorageModule&#39;
])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时也要配置 localStorageServiceProvider，用 ls 作为 localStorage名称前缀：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;.config([&#39;localStorageServiceProvider&#39;, function(localStorageServiceProvider){
  localStorageServiceProvider.setPrefix(&#39;ls&#39;);
}])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整的 scripts/app.js 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&#39;use strict&#39;;

angular
  .module(&#39;webApp&#39;, [
    &#39;ngAnimate&#39;,
    &#39;ngCookies&#39;,
    &#39;ngResource&#39;,
    &#39;ngRoute&#39;,
    &#39;ngSanitize&#39;,
    &#39;ngTouch&#39;,
    &#39;ui.sortable&#39;,
    &#39;LocalStorageModule&#39;
  ]).config([&#39;localStorageServiceProvider&#39;, function(localStorageServiceProvider){
    localStorageServiceProvider.setPrefix(&#39;ls&#39;);
  }]).config(function ($routeProvider) {
    $routeProvider
      .when(&#39;/&#39;, {
        templateUrl: &#39;views/main.html&#39;,
        controller: &#39;MainCtrl&#39;
      })
      .when(&#39;/about&#39;, {
        templateUrl: &#39;views/about.html&#39;,
        controller: &#39;AboutCtrl&#39;
      })
      .otherwise({
        redirectTo: &#39;/&#39;
      });
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，需要修改 scripts/controllers/main.js ，改为从本地存储访问数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&#39;use strict&#39;;

angular.module(&#39;webApp&#39;).controller(&#39;MainCtrl&#39;, function ($scope, localStorageService) {

    // 初始化时为空
    var todosInStore = localStorageService.get(&#39;todos&#39;);

    $scope.todos = todosInStore &amp;amp;&amp;amp; todosInStore.split(&#39;\n&#39;) || [];

    // 监听变化
    $scope.$watch(&#39;todos&#39;, function () {
      localStorageService.add(&#39;todos&#39;, $scope.todos.join(&#39;\n&#39;));
    }, true);

    $scope.addTodo = function () {
      $scope.todos.push($scope.todo);
      $scope.todo = &#39;&#39;;
    };

    $scope.removeTodo = function (index) {
      $scope.todos.splice(index, 1);
    };

  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在浏览器中查看应用，你会发现 todo 列表中没有任何东西。因为这个应用从本地存储中读取了 todo 数组，而本地存储中还没有任何 todo 项。&lt;/p&gt;

&lt;p&gt;在添加一些项目到列表后，我们再次刷新我们的浏览器的时候，这些项目都还在。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/2015/yeoman-web2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;测试&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://karma-runner.github.io/&quot;&gt;Karma&lt;/a&gt; 是一个 JS 测试框架。Angular 生成器本身已经包括了两个测试框架：&lt;a href=&quot;https://code.angularjs.org/1.2.16/docs/guide/e2e-testing&quot;&gt;ngScenario&lt;/a&gt; 和 &lt;a href=&quot;http://jasmine.github.io/&quot;&gt;Jasmine&lt;/a&gt;。当之前我们运行 &lt;code&gt;yo angular&lt;/code&gt; 的时候，在 mytodo 文件夹下会生成了一个 test 目录，还有一个 karma.conf.js 文件，它会被放入在 Node 模块中以使用 Karma。我们将会编辑一个 Jasmine 脚本来完成我们的测试。现在先来看看要怎么进行测试。&lt;/p&gt;

&lt;p&gt;先安装依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npm install -g phantomjs
$ npm install grunt-karma --save-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;karma-&quot;&gt;更新 Karma 配置&lt;/h2&gt;

&lt;p&gt;首先，修改 karma.conf.js，添加&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;  &#39;bower_components/jquery/dist/jquery.js&#39;,
  &#39;bower_components/jquery-ui/ui/jquery-ui.js&#39;,
  &#39;bower_components/angular-ui-sortable/sortable.js&#39;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终的样子是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;files: [
  &#39;bower_components/angular/angular.js&#39;,
  &#39;bower_components/angular-mocks/angular-mocks.js&#39;,
  &#39;bower_components/angular-animate/angular-animate.js&#39;,
  &#39;bower_components/angular-cookies/angular-cookies.js&#39;,
  &#39;bower_components/angular-resource/angular-resource.js&#39;,
  &#39;bower_components/angular-route/angular-route.js&#39;,
  &#39;bower_components/angular-sanitize/angular-sanitize.js&#39;,
  &#39;bower_components/angular-touch/angular-touch.js&#39;,
  &#39;bower_components/jquery/dist/jquery.js&#39;,
  &#39;bower_components/jquery-ui/ui/jquery-ui.js&#39;,
  &#39;bower_components/angular-ui-sortable/sortable.js&#39;,
  &#39;app/scripts/**/*.js&#39;,
  &#39;test/mock/**/*.js&#39;,
  &#39;test/spec/**/*.js&#39;
],
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;运行测试&lt;/h2&gt;

&lt;p&gt;现在回到命令行结束 grunt server 的进程（使用 &lt;code&gt;Ctrl+c&lt;/code&gt;）。在 Gruntfile.js 中已经有了用于运行测试的 grunt 任务，可以直接像下面这样运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grunt test
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;添加更多测试&lt;/h2&gt;

&lt;p&gt;修改 test/spec/controllers/main.js 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&#39;use strict&#39;;

describe(&#39;Controller: MainCtrl&#39;, function () {

  // load the controller&#39;s module
  beforeEach(module(&#39;webApp&#39;));

  var MainCtrl,
    scope;

  // Initialize the controller and a mock scope
  beforeEach(inject(function ($controller, $rootScope) {
    scope = $rootScope.$new();
    MainCtrl = $controller(&#39;MainCtrl&#39;, {
      $scope: scope
    });
  }));

  it(&#39;should attach a list of awesomeThings to the scope&#39;, function () {
    expect(scope.awesomeThings.length).toBe(3);
  });

  it(&#39;should add items to the list&#39;, function () {
    scope.todo = &#39;Test 1&#39;;
    scope.addTodo();
    expect(scope.todos.length).toBe(4);
  });

  it(&#39;should add then remove an item from the list&#39;, function () {
    scope.todo = &#39;Test 1&#39;;
    scope.addTodo();
    scope.removeTodo(0);
    expect(scope.todos.length).toBe(2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多有关单元测试的内容，请参考 &lt;a href=&quot;http://andyshora.com/unit-testing-best-practices-angularjs.html&quot;&gt;Unit Testing Best Practices in AngularJS&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-7&quot;&gt;发布应用&lt;/h1&gt;

&lt;p&gt;为了将应用发布为产品版本，还需要做很多工作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;校验我们的代码&lt;/li&gt;
  &lt;li&gt;运行我们的测试&lt;/li&gt;
  &lt;li&gt;合并和缩小脚本和样式来减少网络请求&lt;/li&gt;
  &lt;li&gt;优化任何使用到的图像&lt;/li&gt;
  &lt;li&gt;对所有输出进行编译处理，使程序瘦身&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实现上述目标只需一句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grunt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个命令将会完成 Grunt 的任务以及根据 Gruntfile.js 文件进行配置，创建一个可以运行的应用版本。只需等上一分钟，你就能得到一个完整的编译版本，和一份编译过程耗时的报告。&lt;/p&gt;

&lt;p&gt;编译完成后的文件，放在了 dist 目录下，是一个可以拿去服务器上的部署的真正的产品。&lt;/p&gt;

&lt;p&gt;你也可以运行下面命令自动编译项目，并且启动 web 服务器&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grunt serve:dist
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-8&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;Anugular 生成器也支持创建新的视图、指令和控制器。例如：可以通过运行 &lt;code&gt;yo angular:route routeName&lt;/code&gt; 搭建一个新的控制器，同时在 app.js 中的路由也会被更新。&lt;/p&gt;

&lt;p&gt;了解更多有关于 Angular 生成器的 Yeoman 命令，请查看 &lt;a href=&quot;https://github.com/yeoman/generator-angular#readme&quot;&gt;generator readme&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当然，Yeoman 还可以做更多的事情，它还支持其他框架的脚手架。&lt;/p&gt;

&lt;p&gt;除了 Yeoman 之外，还有几个框架可以生产 Angular 项目，请参考 &lt;a href=&quot;http://www.sitepoint.com/5-angular-js-seeds-bootstrap-apps-2/&quot;&gt;5 Angular JS Seeds &amp;amp; Bootstrap Apps&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-9&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://yeoman.io/codelab.html&quot;&gt;LET’S SCAFFOLD A WEB APP WITH YEOMAN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.waylau.com/build-angularjs-app-with-yeoman-in-windows/&quot;&gt;在Windows环境下用Yeoman构建AngularJS项目&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/02/02/build-angularjs-app-with-yeoman.html</link>
      <guid>http://blog.javachen.com/2015/02/02/build-angularjs-app-with-yeoman.html</guid>
      <pubDate>2015-02-02T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Django中SQL查询</title>
      <description>&lt;p&gt;当 Django 中模型提供的查询 API 不能满足要求时，你可能需要使用原始的 sql 查询，这时候就需要用到  &lt;code&gt;Manager.raw()&lt;/code&gt; 方法。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Manager&lt;/code&gt; 类提供下面的一个方法，可以用于执行 sql：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Manager.raw(raw_query, params=None, translations=None)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用方法为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; for p in Person.objects.raw(&#39;SELECT * FROM myapp_person&#39;):
...     print(p)
John Smith
Jane Jones
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;raw()&lt;/code&gt; 可以通过名称自动将 sql 语句中的字段映射到模型，所以你不必考虑 sql 语句中的字段顺序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Person.objects.raw(&#39;SELECT id, first_name, last_name, birth_date FROM myapp_person&#39;)
...
&amp;gt;&amp;gt;&amp;gt; Person.objects.raw(&#39;SELECT last_name, birth_date, first_name, id FROM myapp_person&#39;)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;字段映射&lt;/h2&gt;

&lt;p&gt;如果一个表中有和 person 相同的数据，只是字段名称不一致，你可以使用 sql 语句中 的 &lt;code&gt;as&lt;/code&gt; 关键字给字段取别名：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Person.objects.raw(&#39;&#39;&#39;SELECT first AS first_name,
...                              last AS last_name,
...                              bd AS birth_date,
...                              pk AS id,
...                       FROM some_other_table&#39;&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，你还可以添加 &lt;code&gt;translations&lt;/code&gt; 参数，手动设置映射关系：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; name_map = {&#39;first&#39;: &#39;first_name&#39;, &#39;last&#39;: &#39;last_name&#39;, &#39;bd&#39;: &#39;birth_date&#39;, &#39;pk&#39;: &#39;id&#39;}
&amp;gt;&amp;gt;&amp;gt; Person.objects.raw(&#39;SELECT * FROM some_other_table&#39;, translations=name_map)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;延迟获取字段&lt;/h2&gt;

&lt;p&gt;下面的查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; people = Person.objects.raw(&#39;SELECT id, first_name FROM myapp_person&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sql 语句中只包含两个字段，如果你从 people 中获取不在 sql 语句中的字段的值的时候，&lt;strong&gt;其会从数据库再查询一次&lt;/strong&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; for p in Person.objects.raw(&#39;SELECT id, first_name FROM myapp_person&#39;):
...     print(p.first_name, # This will be retrieved by the original query
...           p.last_name) # This will be retrieved on demand
...
John Smith
Jane Jones
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;但是，需要记住的是 sql 语句中一定要包括主键，因为只有主键才能唯一标识数据库中的一条记录。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-2&quot;&gt;传递参数&lt;/h2&gt;

&lt;p&gt;如果你想给 sql 语句传递参数，你可以添加 &lt;code&gt;raw()&lt;/code&gt; 的 &lt;code&gt;params&lt;/code&gt; 参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; lname = &#39;Doe&#39;
&amp;gt;&amp;gt;&amp;gt; Person.objects.raw(&#39;SELECT * FROM myapp_person WHERE last_name = %s&#39;, [lname])
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;记住：&lt;/strong&gt;上面的 sql 语句一定不要写成下面的方式，因为这样可能会有 sql 注入的问题！&lt;/p&gt;

  &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&#39;SELECT * FROM myapp_person WHERE last_name = %s&#39; % lname
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;sql&quot;&gt;运行自定义的 sql&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Manager.raw()&lt;/code&gt; 只能执行 sql 查询，确不能运行  &lt;code&gt;UPDATE&lt;/code&gt;、&lt;code&gt;INSERT&lt;/code&gt; 或 &lt;code&gt;DELETE&lt;/code&gt; 查询，在这种情况下，你需要使用 &lt;code&gt;django.db.connection&lt;/code&gt; 类。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; from django.db import connection

def my_custom_sql(self):
    cursor = connection.cursor()

    cursor.execute(&quot;UPDATE bar SET foo = 1 WHERE baz = %s&quot;, [self.baz])

    cursor.execute(&quot;SELECT foo FROM bar WHERE baz = %s&quot;, [self.baz])
    row = cursor.fetchone()

    return row
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Django 1.6 之后，修改数据之后会自动提交事务，而不用手动调用 &lt;code&gt;transaction.commit_unless_managed()&lt;/code&gt; 了。&lt;/p&gt;

&lt;p&gt;如果 sql 中你想使用 &lt;code&gt;%&lt;/code&gt; 字符 ，则你需要写两次：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;cursor.execute(&quot;SELECT foo FROM bar WHERE baz = &#39;30%&#39;&quot;)
cursor.execute(&quot;SELECT foo FROM bar WHERE baz = &#39;30%%&#39; AND id = %s&quot;, [self.id])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有多个数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import connections
cursor = connections[&#39;my_db_alias&#39;].cursor()
# Your code here...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，Python 的 DB API 返回的结果只包括数据不包括每个字段的名称，你可以牺牲一点性能代价，将查询结果转换为字典：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def dictfetchall(cursor):
    &quot;Returns all rows from a cursor as a dict&quot;
    desc = cursor.description
    return [
        dict(zip([col[0] for col in desc], row))
        for row in cursor.fetchall()
    ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Django 1.7 中，可以使用 &lt;code&gt;cursor&lt;/code&gt; 作为一个上下文管理器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;with connection.cursor() as c:
    c.execute(...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其等价于：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;c = connection.cursor()
try:
    c.execute(...)
finally:
    c.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;本文参考：&lt;a href=&quot;https://docs.djangoproject.com/en/1.7/topics/db/sql/&quot;&gt;Performing raw SQL queries&lt;/a&gt;。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2015/01/30/raw-sql-query-in-django.html</link>
      <guid>http://blog.javachen.com/2015/01/30/raw-sql-query-in-django.html</guid>
      <pubDate>2015-01-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>安装和部署Presto</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 安装环境&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs6.5&lt;/li&gt;
  &lt;li&gt;Hadoop 集群：CDH5.3&lt;/li&gt;
  &lt;li&gt;JDK 版本：jdk1.8.0_31&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了测试简单，我是将 Presto 的 coordinator 和 worker 都部署在 &lt;code&gt;cdh1&lt;/code&gt; 节点上，并且该节点上部署了 hive-metastore 服务。下面的安装和部署过程参考自 &lt;a href=&quot;http://prestodb.io/docs/current/installation.html&quot;&gt;http://prestodb.io/docs/current/installation.html&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;presto&quot;&gt;2. 安装 Presto&lt;/h1&gt;

&lt;p&gt;下载 Presto 的压缩包，目前最新版本为 &lt;a href=&quot;https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.90/presto-server-0.90.tar.gz&quot;&gt;presto-server-0.90&lt;/a&gt;，然后解压为 presto-server-0.90 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.90/presto-server-0.90.tar.gz
tar zxvf presto-server-0.90.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解压后的目录结构为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[$ presto-server-0.90]# tree -L 2
.
├── bin
│   ├── launcher
│   ├── launcher.properties
│   ├── launcher.py
│   └── procname
├── lib
├── NOTICE
├── plugin
│   ├── cassandra
│   ├── example-http
│   ├── hive-cdh4
│   ├── hive-cdh5
│   ├── hive-hadoop1
│   ├── hive-hadoop2
│   ├── kafka
│   ├── ml
│   ├── mysql
│   ├── postgresql
│   ├── raptor
│   └── tpch
└── README.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 plugin 目录可以看到所有 Presto 支持的插件有哪些，这里我主要使用 hive-cdh5 插件，也成为连接器。&lt;/p&gt;

&lt;h1 id=&quot;presto-1&quot;&gt;3. 配置 Presto&lt;/h1&gt;

&lt;p&gt;在 presto-server-0.90 目录创建 etc 目录，并创建以下文件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;node.properties&lt;/code&gt;：每个节点的环境配置&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;jvm.config&lt;/code&gt;：jvm 参数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;config.properties&lt;/code&gt;：配置 Presto Server 参数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log.properties&lt;/code&gt;：配置日志等级&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Catalog Properties&lt;/code&gt;：Catalog 的配置&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;etc/node.properties&lt;/code&gt; 示例配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;node.environment=production
node.id=ffffffff-ffff-ffff-ffff-ffffffffffff
node.data-dir=/var/presto/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;node.environment&lt;/code&gt;：环境名称。一个集群节点中的所有节点的名称应该保持一致。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;node.id&lt;/code&gt;：节点唯一标识的名称。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;node.data-dir&lt;/code&gt;：数据和日志存放路径。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;etc/jvm.config&lt;/code&gt; 示例配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;-server
-Xmx16G
-XX:+UseConcMarkSweepGC
-XX:+ExplicitGCInvokesConcurrent
-XX:+CMSClassUnloadingEnabled
-XX:+AggressiveOpts
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
-XX:ReservedCodeCacheSize=150M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;etc/config.properties&lt;/code&gt; 包含 Presto Server 相关的配置，每一个 Presto Server 可以通时作为 coordinator 和 worker 使用。你可以将他们配置在一个极点上，但是，在一个大的集群上建议分开配置以提高性能。&lt;/p&gt;

&lt;p&gt;coordinator 的最小配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;coordinator=true
node-scheduler.include-coordinator=false
http-server.http.port=8080
task.max-memory=1GB
discovery-server.enabled=true
discovery.uri=http://cdh1:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;worker 的最小配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;coordinator=false
http-server.http.port=8080
task.max-memory=1GB
discovery.uri=http://cdh1:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可选的，作为测试，你可以在一个节点上同时配置两者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;coordinator=true
node-scheduler.include-coordinator=true
http-server.http.port=8080
task.max-memory=1GB
discovery-server.enabled=true
discovery.uri=http://cdh1:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;coordinator&lt;/code&gt;：Presto 实例是否以 coordinator 对外提供服务&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;node-scheduler.include-coordinator&lt;/code&gt;：是否允许在 coordinator 上进行调度任务&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;http-server.http.port&lt;/code&gt;：HTTP 服务的端口&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;task.max-memory=1GB&lt;/code&gt;：每一个任务（对应一个节点上的一个查询计划）所能使用的最大内存&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;discovery-server.enabled&lt;/code&gt;：是否使用 Discovery service 发现集群中的每一个节点。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;discovery.uri&lt;/code&gt;：Discovery server 的 url&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;etc/log.properties&lt;/code&gt; 可以设置某一个 java 包的日志等级：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;com.facebook.presto=INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 Catalog 的配置，首先需要创建 etc/catalog 目录，然后根据你想使用的连接器来创建对应的配置文件，比如，你想使用 jmx 连接器，则创建 jmx.properties：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;connector.name=jmx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你想使用 hive 的连接器，则创建 hive.properties：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;connector.name=hive-cdh5
hive.metastore.uri=thrift://cdh1:9083  #修改为 hive-metastore 服务所在的主机名称，这里我是安装在 cdh1节点
hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多关于连接器的说明，请参考 &lt;a href=&quot;http://prestodb.io/docs/current/connector.html&quot;&gt;Connectors &lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;presto-2&quot;&gt;4. 运行 Presto&lt;/h1&gt;

&lt;p&gt;你可以使用下面命令后台启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/launcher start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以前台启动，观察输出日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/launcher run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，你也可以通过下面命令停止：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/launcher stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多命令，你可以通过 &lt;code&gt;--help&lt;/code&gt; 参数来查看。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[root@cdh1 presto-server-0.90]# bin/launcher --help
Usage: launcher [options] command

Commands: run, start, stop, restart, kill, status

Options:
  -h, --help            show this help message and exit
  -v, --verbose         Run verbosely
  --launcher-config=FILE
                        Defaults to INSTALL_PATH/bin/launcher.properties
  --node-config=FILE    Defaults to INSTALL_PATH/etc/node.properties
  --jvm-config=FILE     Defaults to INSTALL_PATH/etc/jvm.config
  --config=FILE         Defaults to INSTALL_PATH/etc/config.properties
  --log-levels-file=FILE
                        Defaults to INSTALL_PATH/etc/log.properties
  --data-dir=DIR        Defaults to INSTALL_PATH
  --pid-file=FILE       Defaults to DATA_DIR/var/run/launcher.pid
  --launcher-log-file=FILE
                        Defaults to DATA_DIR/var/log/launcher.log (only in daemon mode)
  --server-log-file=FILE
                        Defaults to DATA_DIR/var/log/server.log (only in daemon mode)
  -D NAME=VALUE         Set a Java system property
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动之后，你可以观察 /var/presto/data/ 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@cdh1 /var/presto/data/]# tree
.
├── etc -&amp;gt; /opt/presto-server-0.90/etc
├── plugin -&amp;gt; /opt/presto-server-0.90/plugin
└── var
    ├── log
    │   ├── http-request.log
    │   ├── launcher.log
    │   └── server.log
    └── run
        └── launcher.pid

5 directories, 4 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 /var/presto/data/var/log 目录可以查看日志：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;launcher.log&lt;/code&gt;：启动日志&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;server.log&lt;/code&gt;：Presto Server 输出日志&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;http-request.log&lt;/code&gt;：HTTP 请求日志&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;presto-cli&quot;&gt;5. 测试 Presto CLI&lt;/h1&gt;

&lt;p&gt;下载 &lt;a href=&quot;https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.90/presto-cli-0.90-executable.jar&quot;&gt;presto-cli-0.90-executable.jar&lt;/a&gt; 并将其重命名为 presto-cli（你也可以重命名为 presto），然后添加执行权限。&lt;/p&gt;

&lt;p&gt;运行下面命令进行测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[root@cdh1 bin]# ./presto-cli --server localhost:8080 --catalog hive --schema default
presto:default&amp;gt; show tables;
 Table
-------
(0 rows)

Query 20150126_062137_00012_qgwvy, FINISHED, 1 node
Splits: 2 total, 2 done (100.00%)
0:00 [0 rows, 0B] [0 rows/s, 0B/s]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 执行 show tables 命令之前，你可以查看 &lt;a href=&quot;http://cdh1:8080/&quot;&gt;http://cdh1:8080/&lt;/a&gt; 页面：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/presto/presto-web-01.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;然后在执行该命令之后再观察页面变化。单击第一行记录，会跳转到明细页面：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/presto/presto-web-02.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以运行 &lt;code&gt;--help&lt;/code&gt; 命令查看更多参数，例如你可以在命令行直接运行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./presto-cli --server localhost:8080 --catalog hive --schema default --execute &quot;show tables;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，Presto 的查询结果是使用 &lt;code&gt;less&lt;/code&gt; 程序分页输出的，你可以通过修改环境变量 &lt;code&gt;PRESTO_PAGER&lt;/code&gt; 的值将其改为其他命令，如 &lt;code&gt;more&lt;/code&gt;，或者将其置为空以禁止分页输出。&lt;/p&gt;

&lt;h1 id=&quot;jdbc&quot;&gt;6. 测试 jdbc&lt;/h1&gt;

&lt;p&gt;使用 jdbc 连接 Presto，需要下载 jdbc 驱动 &lt;a href=&quot;https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.90/presto-jdbc-0.90.jar&quot;&gt;presto-jdbc-0.90&lt;/a&gt; 并将其加到你的应用程序的 classpath 中。&lt;/p&gt;

&lt;p&gt;支持以下几种 JDBC URL 格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jdbc:presto://host:port
jdbc:presto://host:port/catalog
jdbc:presto://host:port/catalog/schema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;连接 hive 数据库中 sales 库，示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jdbc:presto://cdh1:8080/hive/sales
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;7. 总结&lt;/h1&gt;

&lt;p&gt;本文主要记录 Presto 的安装部署过程，并使用 hive-cdh5 连接器进行简单测试。下一步，需要基于一些生产数据做一些功能测试以及和 impala 做一些对比测试。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2015/01/26/install-and-deploy-presto.html</link>
      <guid>http://blog.javachen.com/2015/01/26/install-and-deploy-presto.html</guid>
      <pubDate>2015-01-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Presto介绍</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 简介&lt;/h1&gt;

&lt;p&gt;Presto 是一个运行在集群之上的分布式系统。一个完全的安装报考一个 coordinator  进程和多个 workers 进程。查询通过一个客户端例如 Presto CLI 提交到 coordinator 进程。这个 coordinator 进程解析、分析并且生成查询的执行计划，然后将执行过程分发到 workers 进程。&lt;/p&gt;

&lt;p&gt;下面是一个架构图（图来自 &lt;a href=&quot;http://www.dw4e.com/?p=141&quot;&gt;http://www.dw4e.com/?p=141&lt;/a&gt;，此图将官网的架构图稍微修改了一下，增加了 Discovery 的服务，这样可能看起来会更清楚一些）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.dw4e.com/wp-content/uploads/2013/11/presto.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Presto 查询引擎是一个 Master-Slave 的架构，由一个 Coordinato r节点，一个 Discovery Server 节点，多个 Worker 节点组成，Discovery Server 通常内嵌于 Coordinator 节点中。Coordinator 负责解析 SQL 语句，生成执行计划，分发执行任务给 Worker 节点执行。Worker 节点负责实际执行查询任务。Worker 节点启动后向 Discovery Server 服务注册，Coordinator 从 Discovery Server 获得可以正常工作的 Worker 节点。如果配置了 Hive Connector，需要配置一个 Hive MetaStore 服务为 Presto 提供 Hive 元信息，Worker 节点与 HDFS 交互读取数据。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 要求&lt;/h1&gt;

&lt;p&gt;Presto 有以下几个基本要求：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linux 或者 Mac OS X 系统&lt;/li&gt;
  &lt;li&gt;Java 8，64位&lt;/li&gt;
  &lt;li&gt;Python 2.4++&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2.1 连接器&lt;/h2&gt;

&lt;p&gt;Presto 支持可插拔的连接器用于提供数据查询。不同连接器的要求不一样。&lt;/p&gt;

&lt;h3 id=&quot;hadoophive&quot;&gt;HADOOP/HIVE&lt;/h3&gt;

&lt;p&gt;Presto 支持读以下版本的 hive 数据：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Hadoop 1.x，使用 &lt;code&gt;hive-hadoop1&lt;/code&gt; 连接器&lt;/li&gt;
  &lt;li&gt;Apache Hadoop 2.x，使用 &lt;code&gt;hive-hadoop2&lt;/code&gt; 连接器&lt;/li&gt;
  &lt;li&gt;Cloudera CDH 4，&lt;code&gt;使用 hive-cdh4&lt;/code&gt; 连接器&lt;/li&gt;
  &lt;li&gt;Cloudera CDH 5，&lt;code&gt;使用 hive-cdh5&lt;/code&gt; 连接器&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支持以下格式：Text、SequenceFile、RCFile、ORC。&lt;/p&gt;

&lt;p&gt;另外，还需要一个远程的 Hive metastore 服务。本地的或者嵌入式模式是不支持的。Presto 不使用 MapReduce 并且只需要 HDFS。&lt;/p&gt;

&lt;h3 id=&quot;cassandra&quot;&gt;CASSANDRA&lt;/h3&gt;

&lt;p&gt;Cassandra 2.x 是需要的。这个连接器是完全独立于 Hive 连接器的并且仅仅需要一个安装好的 Cassandra 集群。&lt;/p&gt;

&lt;h3 id=&quot;tpc-h&quot;&gt;TPC-H&lt;/h3&gt;

&lt;p&gt;TPC-H 连接器动态地生成数据用于实验和测试 Presto。这个连接器没有额外的要求。&lt;/p&gt;

&lt;p&gt;当然，Presto 还支持一些其他的连接器，包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;JMX&lt;/li&gt;
  &lt;li&gt;Kafka&lt;/li&gt;
  &lt;li&gt;MySQL&lt;/li&gt;
  &lt;li&gt;PostgreSQL&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;3. 使用场景&lt;/h1&gt;

&lt;h2 id=&quot;what-presto-is-not&quot;&gt;3.1 What Presto Is Not&lt;/h2&gt;

&lt;p&gt;Presto 支持 SQL 并提供了一个标准数据库的语法特性，但其不是一个通常意义上的关系数据库，他不是关系数据库，如 MySQL、PostgreSQL 或者 Oracle 的替代品。Presto 不是设计用来解决在线事物处理（OLTP）。&lt;/p&gt;

&lt;h2 id=&quot;what-presto-is&quot;&gt;3.2 What Presto Is&lt;/h2&gt;

&lt;p&gt;Presto 是一个工具，被用来通过分布式查询来有效的查询大量的数据。Presto 是一个可选的工具，可以用来查询 HDFS，通过使用 MapReduce 的作业的流水线，例如 hive，pig，但是又不限于查询 HDFS 数据，它还能查询其他的不同数据源的数据，包括关系数据库以及其他的数据源，比如 cassandra。&lt;/p&gt;

&lt;p&gt;Presto 被设计为处理数据仓库和分析：分析数据，聚合大量的数据并产生报表，这些场景通常被定义为 OLAP。&lt;/p&gt;

&lt;h2 id=&quot;who-uses-presto&quot;&gt;3.3 Who uses Presto?&lt;/h2&gt;

&lt;p&gt;国外：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Facebook，Presto 的开发者&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;国内：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;腾讯，待考证&lt;/li&gt;
  &lt;li&gt;美团，&lt;a href=&quot;http://tech.meituan.com/presto.html&quot;&gt;Presto实现原理和美团的使用实践&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;窝窝团，&lt;a href=&quot;http://www.cnblogs.com/zhengyun_ustc/p/55solution7.html&quot;&gt;#数据技术选型#即席查询Shib+Presto，集群任务调度HUE+Oozie&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-4&quot;&gt;4. 资料&lt;/h1&gt;

&lt;p&gt;以下是一些资料，希望对你了解 Presto 有所帮助：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Presto官方文档：&lt;a href=&quot;http://prestodb.io/&quot;&gt;http://prestodb.io/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/tagomoris/shib&quot;&gt;Shib&lt;/a&gt;：Shib is a web-client written in Node.js designed to query Presto and Hive.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Facebook Presto团队介绍Presto的文章： &lt;a href=&quot;https://www.facebook.com/notes/facebook-engineering/presto-interacting-with-petabytes-of-data-at-facebook/10151786197628920&quot;&gt;https://www.facebook.com/notes/facebook-engineering/presto-interacting-with-petabytes-of-data-at-facebook/10151786197628920&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SlideShare两个分享Presto 的PPT： &lt;a href=&quot;http://www.slideshare.net/zhusx/presto-overview?from_search=1&quot;&gt;http://www.slideshare.net/zhusx/presto-overview?from_search=1&lt;/a&gt; 和 &lt;a href=&quot;http://www.slideshare.net/frsyuki/hadoop-source-code-reading-15-in-japan-presto&quot;&gt;http://www.slideshare.net/frsyuki/hadoop-source-code-reading-15-in-japan-presto&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.dw4e.com/?p=141&quot;&gt;Presto的单节点和多节点配置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wangmeng.us/notes/Impala/&quot;&gt;Impala Presto wiki&lt;/a&gt; 主要介绍了 Presto 的架构、原理和工作流程，以及和 impala 的对比。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/tonghu008/p/3547795.html&quot;&gt;记录Presto数据查询引擎的配置过程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2015/01/23/presto-overview.html</link>
      <guid>http://blog.javachen.com/2015/01/23/presto-overview.html</guid>
      <pubDate>2015-01-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>CDH 5.2中Impala认证集成LDAP和Kerberos</title>
      <description>&lt;p&gt;这是一篇翻译的文章，原文为 &lt;a href=&quot;http://blog.cloudera.com/blog/2014/10/new-in-cdh-5-2-impala-authentication-with-ldap-and-kerberos/&quot;&gt;New in CDH 5.2: Impala Authentication with LDAP and Kerberos&lt;/a&gt;。由于翻译水平有限，难免会一些翻译不准确的地方，欢迎指正！&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Impala 认证现在可以通过 LDAP 和 Kerberos 联合使用来解决。下文来解释为什么和怎样解决。&lt;/p&gt;

&lt;p&gt;Impala，是基于 Apache Hadoop 的一个开源的分析数据库，使用 Kerberos 和 LDAP 来支持认证-作为一种角色来证明你是否是你所说的你是谁。Kerberos 在1.0版本中就已经被支持了，而 LDAP 是最近才被支持，在 CDH 5.2 中，你能够同时使用两者。&lt;/p&gt;

&lt;p&gt;同时使用 LDAP 和 Kerberos 提供了显著的价值；Kerberos 保留了核心的认证协议并且总是用在 Impala 进程彼此之间以及和 Hadoop 集群连接的时候。然后，Kerberos 需要更多的维护支持。LDAP 是在整个企业中无处不在，并且通常由通过 ODBC 和 JDBC 驱动程序连接到 Impala 的客户端应用程序来使用。两者的组合使用是有一定道理的。&lt;/p&gt;

&lt;p&gt;下表表明了各种组合和它们的使用场景：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Impala 认证&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;使用场景&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;No Authentication&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;非安全的集群。我们确定既然你在阅读这篇文章，那你不会对这种场景感兴趣&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Kerberos Only&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hadoop 集群开启 Kerberos 认证，并且连接到 Impala 的每个用户或者客户端都有一个 Kerberos principal。（详细说明见下文。）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;LDAP Only&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hadoop 集群 没有开启 Kerberos 认证，但是你想认证连接 Impala 的外包的客户端，如你使用 Active Directory 或者其他的 LDAP 服务。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Kerberos and LDAP&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hadoop 集群开启 Kerberos 认证。在 Impala 外部的用户没有 Kerberos principal，但是有一个 LDAP 的凭证。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;http://blog.cloudera.com/wp-content/uploads/2014/10/impala-ldap-f1.png&quot; alt=&quot;impala-ldap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这篇文章中，我将解释为何以及如何使用 LDAP 和 Kerberos 的组合来建立 Impala 认证。&lt;/p&gt;

&lt;h1 id=&quot;kerberos&quot;&gt;1. Kerberos&lt;/h1&gt;

&lt;p&gt;Kerberos 仍然是 Apache Hadoop 的主要认证机制。下面是 Kerberos 的一些术语将会帮助你理解这篇文章讨论的内容。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;principal&lt;/code&gt; 是 Kerberos 主体，就要一个用户或者一个守护进程。对于我们来说，一个 principal 对于守护进程来说是 &lt;code&gt;name/hostname@realm&lt;/code&gt;，或者对于用户来说仅仅是 &lt;code&gt;name@realm&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;name&lt;/code&gt; 字段可能是一个进程，例如 &lt;code&gt;impala&lt;/code&gt;，或者是一个用户名，例如 &lt;code&gt;m&lt;/code&gt;yoder`。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hostname&lt;/code&gt; 可能是一个机器的全名称，或者是一个 Hadoop 定义的 _HOST 字符串，通常会机器全名称自动替换。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;realm&lt;/code&gt; 类似于（但不必要和其一样）一个 DNS 域名。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kerberos 主体能够证明他们是谁，要么通过提供一个密码（如果主体是人），或者通过提供“密钥表”的文件。 Impala 守护进程需要一个密钥表文件，该文件必须得到很好的保护：任何可以读取密钥表文件的人可以冒充 Impala 守护进程。&lt;/p&gt;

&lt;p&gt;Basic support for Kerberos in impala for this process is straightforward: Supply the following arguments, and the daemons will use the given principal and the keys in the keytab file to take on the identity of the principal for all communication.&lt;/p&gt;

&lt;p&gt;在 Impala 中对 Kerberos 基本的支持很简单：提供以下参数，守护程序将使用给定的主体和 key，在密钥表文件中验证所有通信的主体的身份。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;--principal=impala/hostname@realm
--keytab_file=/full/path/to/keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有另一外一种情况是 Impala 守护进程（impalad）运行在一个负载均衡器下。&lt;/p&gt;

&lt;p&gt;当客户端通过负载平衡器（一个代理）运行查询时，客户端期望 impalad 有一个和负载平衡器的名称的主体。所以当 Impalad 对外部查询提供服务时，需要使用一个和代理相同名称的主体，但是当做后台程序之间的通讯时需要一个主体匹配实际的主机名称。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;--principal=impala/proxy-hostname@realm
--be_principal=impala/actual-hostname@realm
--keytab_file=/full/path/to/keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个参数 &lt;code&gt;--principal&lt;/code&gt; 定义 Impalad 服务外部查询时使用哪一个主体，&lt;code&gt;--be_principal&lt;/code&gt; 参数定义 Impalad 进程之间通信时使用哪一个主体。两个主体的 key 必须存在于相同的 keytab 文件中。&lt;/p&gt;

&lt;h2 id=&quot;kerberos-1&quot;&gt;调试 Kerberos&lt;/h2&gt;

&lt;p&gt;Kerberos是一个优雅的协议，但是当出现错误的时候实际的实现不是非常有帮助的。下面主要有两件事情需要检查，在出现认证失败的时候：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Time&lt;/code&gt;。Kerberos 是依赖于同步时钟，因此在所有使用 Kerberos 的机器上安装和使用 NTP（网络时间协议）是一个最佳实践。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;DNS&lt;/code&gt;。请确保您的主机名是全名称的并且正向（名称 - &amp;gt; IP）和反向（IP-&amp;gt;名称）DNS查找是否正确。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，也可以设置两个环境变量以输出 Kerberos 调试信息。输出可能是一小部分的内容，但通常是帮助你解决问题。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;KRB5_TRACE=/full/path/to/trace/output.log&lt;/code&gt;：该环境变量指定调试日志输出路径。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;JAVA_TOOL_OPTIONS=-Dsun.security.krb5.debug=true&lt;/code&gt;：该环境变量会传给 Impala 守护进程，并传递给内部的 java 组件。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kerberos-flags&quot;&gt;Kerberos Flags&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_kerberos.html&quot;&gt;Cloudera documentation for Kerberos and Impala&lt;/a&gt; 给出了详细的说明，这里只做简要介绍：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://blog.cloudera.com/wp-content/uploads/2014/10/impala-auth-tab2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ldap&quot;&gt;2. LDAP&lt;/h1&gt;

&lt;p&gt;Kerberos 是伟大的，但它确实需要最终用户有一个有效的 Kerberos 证书，这在许多环境中是不实际的，因为每个与 Impala 和Hadoop集群交互的用户都必须配置 Kerberos 主体。对于使用 Active Directory 来管理用户帐户的组织，可能需要在 MIT Kerberos 领域频繁的创建每个用户对应的用户帐户。取而代之的时许多企业环境使用 LDAP 协议，在客户使用自己的用户名和密码进行身份验证自己。&lt;/p&gt;

&lt;p&gt;当配置为使用LDAP，把 impalad 看作为一个 LDAP 代理：客户端（ Impala shell，ODBC，JDBC，hue 等等）发送其用户名和密码到 impalad ，然后 impalad 将用户名和密码发送给 LDAP 服务器并尝试登录。在 LDAP 术语中，impalad 发出 LDAP “绑定” 操作。如果 LDAP 服务器为该次登陆尝试返回成功，则 impalad 接受连接。&lt;/p&gt;

&lt;p&gt;LDAP只用于验证外部客户，如Impala shell，ODBC，JDBC，和 hue。所有其他后端认证由 Kerberos 的处理。&lt;/p&gt;

&lt;h2 id=&quot;ldap-&quot;&gt;LDAP 配置&lt;/h2&gt;

&lt;p&gt;LDAP 是复杂的（而且强大的），因为它非常灵活；有许多方式来配置 LDAP 实体和验证这些实体。在一般情况下，每个人都在 LDAP 具有专有名称或DN，可被认为是 LDAP 中的用户名或主体。&lt;/p&gt;

&lt;p&gt;让我们来看看对于两个不同的LDAP服务器用户是如何设置的。第一个用户名为“Test1 Person”，并存在 Windows2008 Active Directory 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Test1 Person, Users, ad.sec.cloudera.com
dn: CN=Test1 Person,CN=Users,DC=ad,DC=sec,DC=cloudera,DC=com
cn: Test1 Person
sAMAccountName: test1
userPrincipalName: test1@ad.sec.cloudera.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二个用户是我：用户 myoder 的项，存在于 OpenLDAP 服务器中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# myoder, People, cloudera.com
dn: uid=myoder,ou=People,dc=cloudera,dc=com
cn: Michael Yoder
uid: myoder
homeDirectory: /home/myoder
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了简单，上面中的其他的许多项都被删除了。下面来看看这两个账户的相同点和不同点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;DN&lt;/code&gt;：在注释后面的第一行就是 DN。这是一个 LDAP 账户最主要的标识串。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CN&lt;/code&gt;：通用名称。对 AD 来说，他和 DN 是一样的；对于 OpenLDAP 来说他是一个人的名称，他不是 DN 中的 uid。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sAMAccountName&lt;/code&gt;：只存在 AD 中的条目，是一个用户名称的过时的形式。尽管其过时了，但是被广泛使用中。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;userPrincipalName&lt;/code&gt;：只存在 AD 中的条目，通过转换，应该映射到用户的邮箱名称。其通常像这样的格式：&lt;code&gt;sAMAccountName@fully.qualified.domain.com&lt;/code&gt;。这是 Active Directory 更现代的名称并且被广泛使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里还有一些额外有趣的关于 AD 的实现细节。通常，LDAP 认证是基于 DN。对于 AD，下面&lt;a href=&quot;http://msdn.microsoft.com/en-us/library/cc223499.aspx&quot;&gt;几项会被轮流尝试&lt;/a&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;首先，使用 DN&lt;/li&gt;
  &lt;li&gt;userPrincipalName&lt;/li&gt;
  &lt;li&gt;sAMAccountName + “@” + the DNS domain name&lt;/li&gt;
  &lt;li&gt;Netbios domain name + “&quot; + the sAMAccountName&lt;/li&gt;
  &lt;li&gt;其他的一些机制，请看上面的链接里的内容。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ldap--impalad&quot;&gt;LDAP 和 Impalad&lt;/h2&gt;

&lt;p&gt;考虑所有这些的不同点，幸运的是 Impala 进程提供了几个机制去标识 LDAP 的配置。首先，使用简单的配置：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;--enable_ldap_auth&lt;/code&gt; 必须设置为 true&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--ldap_uri=ldap://ldapserver.your.company.com&lt;/code&gt; 必须定义&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;仅仅设置这些，传递给 impalad 的用户名（通过 impala shell、jdbc、odbc 等等）直接传递到 LDAP 服务器。这个过程对于 AD 来说没什么问题，如果用户名称是全名称，就像 &lt;code&gt;test1@ad.sec.cloudera.com&lt;/code&gt; - 其使用 &lt;code&gt;userPrincipal&lt;/code&gt; 或者 &lt;code&gt;sAMAccountName&lt;/code&gt; 加上 DNS 域名都会匹配上。&lt;/p&gt;

&lt;p&gt;也可以设置 impalad 启动参数以便 这域名（在当前情况下是 &lt;code&gt;ad.sec.cloudera.com&lt;/code&gt; ）可以自动添加到 用户名上，这需要通过设置参数 &lt;code&gt;--ldap_domain=ad.sec.cloudera.com&lt;/code&gt; 添加到 impalad 的启动参数中。现在，当一个客户端用户访问时，例如 test1 用户，其会将域名名称追加到用户名称之后以便追加后的结果 &lt;code&gt;test1@ad.sec.cloudera.com&lt;/code&gt; 传递给 AD 。这种方式对于你的用户来说是很方便的。&lt;/p&gt;

&lt;p&gt;目前，对于 AD 来说都运行正常。但是对于其他的 LDAP 服务器，例如 OpenLDAP 呢？OpenLDAP 没有 sAMAccountName 或者 userPrincipalName 中的任何一个，取而代之的是我们不得不直接使用 DN 进行认证。用户将不会去关心他们的 LDAP DN！&lt;/p&gt;

&lt;p&gt;幸运地是，impalade 对于这种场景也提供了一些参数。&lt;code&gt;--ldap_baseDN=X&lt;/code&gt; 参数用户将用户名称转换为 LDAP DN，以便这个 DN 结果看上去像 &lt;code&gt;uid=username,X&lt;/code&gt;。例如，如果设置 &lt;code&gt;--ldap_baseDN=ou=People,dc=cloudera,dc=com &lt;/code&gt;，传递的用户名是 myoder，传到到 LDAP 的查询将是 &lt;code&gt;uid=myoder,ou=People,dc=cloudera,dc=com&lt;/code&gt;，这将会和 myoder 的 DN 想匹配。&lt;/p&gt;

&lt;p&gt;为了获得最大的灵活性，也可以通过 &lt;code&gt;--ldap_bind_pattern&lt;/code&gt; 参数将指定用户名任意映射到一个DN。这个想法是，该参数中必须有一个名称为 #UID 的占位符，并且 &lt;code&gt;#UID&lt;/code&gt; 会被用户名替代。例如你可以通过指定 &lt;code&gt;--ldap_bind_pattern=uid=#UID,ou=People,dc=cloudera,dc=com&lt;/code&gt; 来定义 &lt;code&gt;--ldap_baseDN&lt;/code&gt;。当 myoder 这个用户进来时，其将会替换 &lt;code&gt;#UID&lt;/code&gt;，并且我们将得到和上面一直的字符串。这个参数应该在需要对 DN 有更多控制的时候才使用。&lt;/p&gt;

&lt;h2 id=&quot;ldap--tls&quot;&gt;LDAP 和 TLS&lt;/h2&gt;

&lt;p&gt;当使用 LDAP 时，传递个 LDAP 服务器的用户名和密码都是明文的。这意味着没有任何的保护，任何人都可以看到传输中的密码。为了阻止这一点，你必须使用 TLS（Transport Layer Security，更为人熟知的是 SSL） 来保护连接。 这里有两种不同的连接需要保护：客户端和 impalad 进程之间以及 impalad 进程和 LDAP 服务器之间。&lt;/p&gt;

&lt;h3 id=&quot;impalad-&quot;&gt;客户端和 impalad 之间&lt;/h3&gt;

&lt;p&gt;TLS 连接的认证需要使用证书来完成，所以 impalad 进程（作为 TLS server）需要他自己的证书。Impalad 呈现该证书给客户端以证明他的确是 impalad 进程。为了提供该证书，impalad 进程需要设置下面两个参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;--ssl_server_certificate=/full/path/to/impalad-cert.pem 
--ssl_private_key=/full/path/to/impalad-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在客户端必须使用 TLS 和 impalad 通信。在 impala-shell 中，你必须设置 &lt;code&gt;--ssl&lt;/code&gt; 和 &lt;code&gt;--ca_cert=/full/path/to/ca-certificate.pem&lt;/code&gt; 这两个参数。ca_cert 参数必须指定为上面 &lt;code&gt;ssl_server_certificate&lt;/code&gt; 参数定义的值。对于 ODBC 连接来说，请参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Connectors/PDF/Cloudera-ODBC-Driver-for-Impala-Install-Guide.pdf&quot;&gt; the Cloudera ODBC driver for Impala&lt;/a&gt;。其提供了一个全面的描述。关于设置证书、认证和 TLS 的必要性。&lt;/p&gt;

&lt;p&gt;坦白地说，在 impala 客户端和 impalad 进程之间使用 TLS 是一个很好的想法，不管是否使用 LDAP。然而，你的查询，以及这些查询的结果都是通过明文传输的。&lt;/p&gt;

&lt;h3 id=&quot;impalad--ldap-&quot;&gt;Impalad 和 LDAP 服务器之间&lt;/h3&gt;

&lt;p&gt;这里有两种方法开启和 LDAP 服务器之间的 TLS ：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 impalad 启动参数中添加 &lt;code&gt;--ldap_tls&lt;/code&gt;。该连接会使用 LDAP 的端口，但是在第一次连接之后，会发送一个 &lt;code&gt;STRATETLS&lt;/code&gt; 的请求，该请求会使用 TLS 升级该连接为一个安全的连接并且使用相同的端口。&lt;/li&gt;
  &lt;li&gt;URL 使用 &lt;code&gt;ldaps://&lt;/code&gt; 开头。这将会使用一个不同于  &lt;code&gt;ldap://&lt;/code&gt; 的端口。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后，到 LDAP 服务器的连接需要他自己的认证；在这种情况下，你知道 impalad 是在和正确的 ldap 服务器通话并且你不会将你的密码发送给一个无赖的人为工具的请求。你需要添加 &lt;code&gt;--ldap_ca_certificate&lt;/code&gt; 参数到 impalad 定义 LDAP 服务器的证书存放的位置。&lt;/p&gt;

&lt;h3 id=&quot;ldap-flags&quot;&gt;LDAP Flags&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_ldap.html&quot;&gt;Cloudera documentation for LDAP and Impala&lt;/a&gt; 一文包括这部分的信息，并且建议你阅读 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_ssl.html&quot;&gt;TLS between the Impala client and the Impala daemon&lt;/a&gt; 这篇文章。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://blog.cloudera.com/wp-content/uploads/2014/10/impala-auth-tab3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;3. 一起使用&lt;/h1&gt;

&lt;p&gt;如果我们想同事使用 Kerberos 和 LDAP 认证，则需要如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;impalad --enable_ldap_auth \
    --ldap_uri=ldap://ldapserver.your.company.com \
    --ldap_tls \
    --ldap_ca_certificate=/full/path/to/certs/ldap-ca-cert.pem \
    --ssl_server_certificate=/full/path/to/certs/impala-cert.pem \
    --ssl_private_key=/full/path/to/certs/impala-key.pem \
    --principal=impala/_HOST@EXAMPLE.COM \
    --keytab_file=/full/path/to/keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当使用 Kerberos 认证时，使用  impala shell 连接：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;impala-shell.sh --ssl \
    --ca_cert=/full/path/to/cert/impala-ca-cert.pem \
    -k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当使用 LDAP 认证时：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;impala-shell.sh --ssl \
   --ca_cert=/full/path/to/cert/impala-ca-cert.pem \
   -l -u myoder@cloudera.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原文作者为 Michael Yoder，Cloudera 软件工程师。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2015/01/23/new-in-cdh-5-2-impala-authentication-with-ldap-and-kerberos.html</link>
      <guid>http://blog.javachen.com/2015/01/23/new-in-cdh-5-2-impala-authentication-with-ldap-and-kerberos.html</guid>
      <pubDate>2015-01-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Maven的一些技巧</title>
      <description>&lt;p&gt;本文主要收集一些 Maven 的使用技巧，包括 Maven 常见命令、创建多模块项目、上传本地 jar 到插件以及常用的插件等等，本篇文章会保持不停的更新。&lt;/p&gt;

&lt;p&gt;命令行创建 maven 项目：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn archetype:generate -DgroupId=com.javachen.spark -DartifactId=spark-examples -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Maven安装本地jar到本地仓库，举例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn install:install-file -DgroupId=com.gemstone.gemfire -DartifactId=gfsh -Dversion=6.6 -Dpackaging=jar -Dfile=/backup/gfsh-6.6.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决m2e插件maven-dependency-plugin问题：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;build&amp;gt;
    &amp;lt;pluginManagement&amp;gt;
        &amp;lt;plugins&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;org.eclipse.m2e&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;lifecycle-mapping&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;1.0.0&amp;lt;/version&amp;gt;
                &amp;lt;configuration&amp;gt;
                    &amp;lt;lifecycleMappingMetadata&amp;gt;
                        &amp;lt;pluginExecutions&amp;gt;
                            &amp;lt;pluginExecution&amp;gt;
                                &amp;lt;pluginExecutionFilter&amp;gt;
                                    &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
                                    &amp;lt;artifactId&amp;gt;maven-dependency-plugin&amp;lt;/artifactId&amp;gt;
                                    &amp;lt;versionRange&amp;gt;[2.0,)&amp;lt;/versionRange&amp;gt;
                                    &amp;lt;goals&amp;gt;
                                        &amp;lt;goal&amp;gt;copy-dependencies&amp;lt;/goal&amp;gt;
                                    &amp;lt;/goals&amp;gt;
                                &amp;lt;/pluginExecutionFilter&amp;gt;
                                &amp;lt;action&amp;gt;
                                    &amp;lt;ignore /&amp;gt;
                                &amp;lt;/action&amp;gt;
                            &amp;lt;/pluginExecution&amp;gt;
                        &amp;lt;/pluginExecutions&amp;gt;
                    &amp;lt;/lifecycleMappingMetadata&amp;gt;
                &amp;lt;/configuration&amp;gt;
            &amp;lt;/plugin&amp;gt;
        &amp;lt;/plugins&amp;gt;
    &amp;lt;/pluginManagement&amp;gt;

    &amp;lt;plugins&amp;gt;
        &amp;lt;plugin&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;maven-dependency-plugin&amp;lt;/artifactId&amp;gt;
            &amp;lt;executions&amp;gt;
                &amp;lt;execution&amp;gt;
                    &amp;lt;id&amp;gt;copy-dependencies&amp;lt;/id&amp;gt;
                    &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
                    &amp;lt;goals&amp;gt;
                        &amp;lt;goal&amp;gt;copy-dependencies&amp;lt;/goal&amp;gt;
                    &amp;lt;/goals&amp;gt;
                    &amp;lt;configuration&amp;gt;
                        &amp;lt;outputDirectory&amp;gt;${project.build.directory}/lib&amp;lt;/outputDirectory&amp;gt;
                        &amp;lt;excludeTransitive&amp;gt;false&amp;lt;/excludeTransitive&amp;gt;
                        &amp;lt;stripVersion&amp;gt;true&amp;lt;/stripVersion&amp;gt;
                    &amp;lt;/configuration&amp;gt;
                &amp;lt;/execution&amp;gt;
            &amp;lt;/executions&amp;gt;
        &amp;lt;/plugin&amp;gt;
    &amp;lt;/plugins&amp;gt;
&amp;lt;/build&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2015/01/20/maven-skills.html</link>
      <guid>http://blog.javachen.com/2015/01/20/maven-skills.html</guid>
      <pubDate>2015-01-20T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Django中的ORM</title>
      <description>&lt;p&gt;通过《&lt;a href=&quot;/2014/01/11/how-to-create-a-django-site.html&quot;&gt;如何创建一个Django网站&lt;/a&gt;》大概清楚了如何创建一个简单的 Django 网站，并了解了Django 中&lt;a href=&quot;/2014/10/30/django-template.html&quot;&gt;模板&lt;/a&gt;和&lt;a href=&quot;/2015/01/14/django-model.html&quot;&gt;模型&lt;/a&gt;使用方法。本篇文章主要在此基础上，了解 Django 中 ORM 相关的用法。&lt;/p&gt;

&lt;p&gt;一个 blog 的应用中 mysite/blog/models.py 有以下实体：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models

class Blog(models.Model):
    name = models.CharField(max_length=100)
    tagline = models.TextField()

    def __str__(self):              # __unicode__ on Python 2
        return self.name

class Author(models.Model):
    name = models.CharField(max_length=50)
    email = models.EmailField()

    def __str__(self):              # __unicode__ on Python 2
        return self.name

class Entry(models.Model):
    blog = models.ForeignKey(Blog)
    headline = models.CharField(max_length=255)
    body_text = models.TextField()
    pub_date = models.DateField()
    mod_date = models.DateField()
    authors = models.ManyToManyField(Author)
    n_comments = models.IntegerField()
    n_pingbacks = models.IntegerField()
    rating = models.IntegerField()

    def __str__(self):              # __unicode__ on Python 2
        return self.headline
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;创建对象&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from blog.models import Blog
&amp;gt;&amp;gt;&amp;gt; b = Blog(name=&#39;Beatles Blog&#39;, tagline=&#39;All the latest Beatles news.&#39;)
&amp;gt;&amp;gt;&amp;gt; b.save()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以修改实体，然后保存：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; b5.name = &#39;New name&#39;
&amp;gt;&amp;gt;&amp;gt; b5.save()    
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;保存外键信息&lt;/h2&gt;

&lt;p&gt;下面例子更新 entry 实例的 blog 属性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from blog.models import Entry
&amp;gt;&amp;gt;&amp;gt; entry = Entry.objects.get(pk=1)
&amp;gt;&amp;gt;&amp;gt; cheese_blog = Blog.objects.get(name=&quot;Cheddar Talk&quot;)
&amp;gt;&amp;gt;&amp;gt; entry.blog = cheese_blog
&amp;gt;&amp;gt;&amp;gt; entry.save()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是更新一个 ManyToManyField 字段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from blog.models import Author
&amp;gt;&amp;gt;&amp;gt; joe = Author.objects.create(name=&quot;Joe&quot;)
&amp;gt;&amp;gt;&amp;gt; entry.authors.add(joe)
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; john = Author.objects.create(name=&quot;John&quot;)
&amp;gt;&amp;gt;&amp;gt; paul = Author.objects.create(name=&quot;Paul&quot;)
&amp;gt;&amp;gt;&amp;gt; george = Author.objects.create(name=&quot;George&quot;)
&amp;gt;&amp;gt;&amp;gt; ringo = Author.objects.create(name=&quot;Ringo&quot;)
&amp;gt;&amp;gt;&amp;gt; entry.authors.add(john, paul, george, ringo)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;查询对象&lt;/h2&gt;

&lt;p&gt;Django 中查询数据库需要 Manager 和 QuerySet 两个对象。从数据库里检索对象，可以通过模型的 Manage 来建立 QuerySet,一个 QuerySet 表现为一个数据库中对象的结合，他可以有0个一个或多个过滤条件，在 SQL里 QuerySet 相当于 select 语句用 where 或 limit 过滤。你通过模型的 Manage 来获取 QuerySet。&lt;/p&gt;

&lt;h3 id=&quot;manager&quot;&gt;Manager&lt;/h3&gt;

&lt;p&gt;Manager 对象附在模型类里，如果没有特指定，每个模型类都会有一个 objects 属性，它构成了这个模型在数据库所有基本查询。&lt;/p&gt;

&lt;p&gt;Manager 的几个常用方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;all&lt;/code&gt;：返回一个包含模式里所有数据库记录的 QuerySet&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;filter&lt;/code&gt;：返回一个包含符合指定条件的模型记录的 QuerySet&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;exclude&lt;/code&gt;：和 filter 相反，查找不符合条件的那些记录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;get&lt;/code&gt;：获取单个符合条件的记录（没有找到或者又超过一个结果都会抛出异常）&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;order_by&lt;/code&gt;：改变 QuerySet 默认的排序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;你可以通过模型的 Manager 对象获取 QuerySet 对象：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Blog.objects
&amp;lt;django.db.models.manager.Manager object at ...&amp;gt;
&amp;gt;&amp;gt;&amp;gt; b = Blog(name=&#39;Foo&#39;, tagline=&#39;Bar&#39;)
&amp;gt;&amp;gt;&amp;gt; b.objects
Traceback:
    ...
AttributeError: &quot;Manager isn&#39;t accessible via Blog instances.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取所有的 blog 内容:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; all_entries = Entry.objects.all()

#正向排序
Entry.objects.all().order_by(&quot;headline&quot;)
#反向排序
Entry.objects.all().order_by(&quot;-headline&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取 headline 为 Python 开头的 blog :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Entry.objects.filter(headline__startswith=&quot;Python&quot;)

#支持链式操作
Entry.objects.filter(headline__startswith=&quot;Python&quot;).exclude(pub_date__gte=datetime.now()).filter(pub_date__gte=datetime(2014, 1, 1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;queryset-&quot;&gt;QuerySet 类&lt;/h3&gt;

&lt;p&gt;QuerySet 接受动态的关键字参数，然后转换成合适的 SQL 语句在数据库上执行。&lt;/p&gt;

&lt;p&gt;QuerySet 的几个常用方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;distinct&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;values&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;values_list&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;select_related&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;filter&lt;/code&gt;：返回一个包含符合指定条件的模型记录的 QuerySet&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;extra&lt;/code&gt;：增加结果集以外的字段&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-3&quot;&gt;延时查询&lt;/h4&gt;

&lt;p&gt;每次你完成一个 QuerySet，你获得一个全新的结果集，不包括前面的。每次完成的结果集是可以贮存，使用或复用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; q1 = Entry.objects.filter(headline__startswith=&quot;What&quot;)
&amp;gt;&amp;gt;&amp;gt; q2 = q1.exclude(pub_date__gte=datetime.date.today())
&amp;gt;&amp;gt;&amp;gt; q3 = q1.filter(pub_date__gte=datetime.date.today())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;三个 QuerySets 是分开的，第一个是 headline 以 “What” 单词开头的结果集，第二个是第一个的子集，即 pub_date 不大于现在的，第三个是第一个的子集 ，pub_date 大于现在的。&lt;/p&gt;

&lt;p&gt;QuerySets 是延迟的，创建 QuerySets 不会触及到数据库操作，你可以多个过滤合并到一起，直到求值的时候 django才会开始查询。如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; q = Entry.objects.filter(headline__startswith=&quot;What&quot;)
&amp;gt;&amp;gt;&amp;gt; q = q.filter(pub_date__lte=datetime.date.today())
&amp;gt;&amp;gt;&amp;gt; q = q.exclude(body_text__icontains=&quot;food&quot;)
&amp;gt;&amp;gt;&amp;gt; print(q)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;虽然看起来执行了三个过滤条件，实际上最后执行 &lt;code&gt;print q&lt;/code&gt; 的时候，django 才开始查询执行 SQL 到数据库。&lt;/p&gt;

&lt;p&gt;可以使用 python 的数组限制语法限定 QuerySet，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Entry.objects.all()[:5]
&amp;gt;&amp;gt;&amp;gt; Entry.objects.all()[5:10]

&amp;gt;&amp;gt;&amp;gt; Entry.objects.all().order_by(&quot;headline&quot;)[:4]
&amp;gt;&amp;gt;&amp;gt; Entry.objects.all().order_by(&quot;headline&quot;)[4:8]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般的，限制 QuerySet 返回新的 QuerySet，不会立即求值查询，除非你使用了 “step” 参数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Entry.objects.all()[:10:2]
&amp;gt;&amp;gt;&amp;gt; Entry.objects.order_by(&#39;headline&#39;)[0]
&amp;gt;&amp;gt;&amp;gt; Entry.objects.order_by(&#39;headline&#39;)[0:1].get()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section-4&quot;&gt;字段过滤&lt;/h4&gt;

&lt;p&gt;字段查找是指定 SQL 语句的 WHERE 条件从句，通过 QuerySet 的方法 &lt;code&gt;filter()&lt;/code&gt;, &lt;code&gt;exclude()&lt;/code&gt; 和 &lt;code&gt;get()&lt;/code&gt; 指定查询关键字。&lt;/p&gt;

&lt;p&gt;格式为：&lt;code&gt;field__lookuptype=value&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;lookuptype 有以下几种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;gt&lt;/code&gt; ： 大于&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;gte&lt;/code&gt; : 大于等于&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;in&lt;/code&gt; : 包含&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;lt&lt;/code&gt; : 小于&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;lte&lt;/code&gt; : 小于等于&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;exact&lt;/code&gt;：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;iexact&lt;/code&gt;：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;contains&lt;/code&gt;：包含查询，区分大小写&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;icontains&lt;/code&gt;：不区分大小写&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;startswith&lt;/code&gt;：匹配开头&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;endswith&lt;/code&gt;：匹配结尾&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;istartswith&lt;/code&gt;：匹配开头，不区分大小写&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;iendswith&lt;/code&gt;：匹配结尾，不区分大小写&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(pub_date__lte=&#39;2006-01-01&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等价于:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT * FROM blog_entry WHERE pub_date &amp;lt;= &#39;2006-01-01&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当实体中存在  ForeignKey 时，其外键字段名称为模型名称加上 ‘_id’：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(blog_id=4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是一些举例：&lt;/p&gt;

&lt;p&gt;a、exact&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Entry.objects.get(headline__exact=&quot;Man bites dog&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相当于：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT ... WHERE headline = &#39;Man bites dog&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果查询没有提供双下划线，那么会默认 &lt;code&gt;__exact&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Entry.objects.get(id__exact=14) # Explicit form
Entry.objects.get(id=14) # __exact is implied

#主键查询
Entry.objects.get(pk=14) # pk implies id__exact
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b、iexact——忽略大小写&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Blog.objects.get(name__iexact=&quot;beatles blog&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将要匹配 blog 名称为 “Beatles Blog”, “beatles blog”, 甚至是 “BeAtlES blOG”。&lt;/p&gt;

&lt;p&gt;c、contains——包含查询，区分大小写&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Entry.objects.get(headline__contains=&#39;Lennon&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转化为 SQL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT ... WHERE headline LIKE &#39;%Lennon%&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有百分号，则会进行转义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Entry.objects.filter(headline__contains=&#39;%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转义为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT ... WHERE headline LIKE &#39;%\%%&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;d、in 查询&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Get blogs  with id 1, 4 and 7
Entry.objects.filter(pk__in=[1,4,7])
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section-5&quot;&gt;跨关系查询&lt;/h4&gt;

&lt;p&gt;跨关系查询是针对有主外键依赖关系的对象而言的，例如上面的 Author 和 Entry 对象是多对多的映射，可以通过 Entry 对象来过滤 Author的 name：&lt;/p&gt;

&lt;p&gt;获取所有 blog 名称为 Beatles Blog 的 Entry 列表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(blog__name=&#39;Beatles Blog&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以反向查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Blog.objects.filter(entry__headline__contains=&#39;Lennon&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果跨越多层关系查询，中间模型没有值，django会作为空对待不会发生异常。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Blog.objects.filter(entry__authors__name=&#39;Lennon&#39;)
Blog.objects.filter(entry__authors__name__isnull=True)
Blog.objects.filter(entry__authors__isnull=False,
        entry__authors__name__isnull=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也支持多条件跨关系查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Blog.objects.filter(entry__headline__contains=&#39;Lennon&#39;,
        entry__pub_date__year=2008)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Blog.objects.filter(entry__headline__contains=&#39;Lennon&#39;).filter(
        entry__pub_date__year=2008)        
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;extra--sql&quot;&gt;使用 Extra 调整 SQL&lt;/h4&gt;

&lt;p&gt;用extra可以修复QuerySet生成的原始SQL的各个部分，它接受四个关键字参数。如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;select&lt;/code&gt;：修改select语句&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;where&lt;/code&gt;：提供额外的where子句&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tables&lt;/code&gt;：提供额外的表&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;params&lt;/code&gt;：安全的替换动态参数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;增加结果集以外的字段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;queryset.extra(select={&#39;成年&#39;:&#39;age&amp;gt;18&#39;}) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;提供额外的 where 条件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;queryset.extra(where=[&quot;first like &#39;%小明%&#39; &quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;提供额外的表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;queryset.extra(tables=[&#39;myapp_person&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安全的替换动态参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;##&#39;%s&#39; is not replaced with normal string 
matches = Author.objects.all().extra(where=[&quot;first = &#39;%s&#39; &quot;], params= [unknown-input ( ) ])
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;f-&quot;&gt;F 关键字参数&lt;/h4&gt;

&lt;p&gt;前面给的例子里，我们建立了过滤，比照模型字段值和一个固定的值，但是如果我们想比较同一个模型里的一个字段和另一个字段的值，django 提供 &lt;code&gt;F()&lt;/code&gt;——专门取对象中某列值的操作。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from django.db.models import F
&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(n_comments__gt=F(&#39;n_pingbacks&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，还支持加减乘除和模计算：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(n_comments__gt=F(&#39;n_pingbacks&#39;) * 2)
&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(rating__lt=F(&#39;n_comments&#39;) + F(&#39;n_pingbacks&#39;))
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(authors__name=F(&#39;blog__name&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于日期类型字段，可以使用 timedelta 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from datetime import timedelta
&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(mod_date__gt=F(&#39;pub_date&#39;) + timedelta(days=3))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还支持位操作 &lt;code&gt;.bitand()&lt;/code&gt; 和 &lt;code&gt;.bitor()&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; F(&#39;somefield&#39;).bitand(16)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;section-6&quot;&gt;主键查找&lt;/h4&gt;

&lt;p&gt;Django 支持使用 pk 代替主键：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Blog.objects.get(id__exact=14) # Explicit form
&amp;gt;&amp;gt;&amp;gt; Blog.objects.get(id=14) # __exact is implied
&amp;gt;&amp;gt;&amp;gt; Blog.objects.get(pk=14) # pk implies id__exact
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pk 还可以用于其他的查找类型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Get blogs entries with id 1, 4 and 7
&amp;gt;&amp;gt;&amp;gt; Blog.objects.filter(pk__in=[1,4,7])

# Get all blog entries with id &amp;gt; 14
&amp;gt;&amp;gt;&amp;gt; Blog.objects.filter(pk__gt=14)

&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(blog__id__exact=3) # Explicit form
&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(blog__id=3)        # __exact is implied
&amp;gt;&amp;gt;&amp;gt; Entry.objects.filter(blog__pk=3)        # __pk implies __id__exact
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;q-&quot;&gt;Q 关键字参数&lt;/h4&gt;

&lt;p&gt;QuerySet 可以通过一个叫 Q 的关键字参数封装类进一步参数化，允许使用更复杂的逻辑查询。其结果 Q对 象可以作为 filter 或 exclude 方法的关键字参数。&lt;/p&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db.models import Q
Q(question__startswith=&#39;What&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;支持 &amp;amp; 和&lt;/td&gt;
      &lt;td&gt;操作符：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Q(question__startswith=&#39;Who&#39;) | Q(question__startswith=&#39;What&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的查询翻译成 sql 语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;WHERE question LIKE &#39;Who%&#39; OR question LIKE &#39;What%&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;取反操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Q(question__startswith=&#39;Who&#39;) | ~Q(pub_date__year=2005)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以用在 &lt;code&gt;filter()&lt;/code&gt;、&lt;code&gt;exclude()&lt;/code&gt;、&lt;code&gt;get()&lt;/code&gt; 中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Poll.objects.get(
    Q(question__startswith=&#39;Who&#39;),
    Q(pub_date=date(2005, 5, 2)) | Q(pub_date=date(2005, 5, 6))
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;翻译成 sql 语句为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT * from polls WHERE question LIKE &#39;Who%&#39;
    AND (pub_date = &#39;2005-05-02&#39; OR pub_date = &#39;2005-05-06&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-7&quot;&gt;删除对象&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt;entry = Entry.objects.get(pk=1)
&amp;gt;&amp;gt;&amp;gt;entry.delete()
&amp;gt;&amp;gt;&amp;gt;Blog.objects.all().delete()

&amp;gt;&amp;gt;&amp;gt;Entry.objects.filter(pub_date__year=2005).delete()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-8&quot;&gt;关系对象&lt;/h2&gt;

&lt;p&gt;当对象之间存在映射关系或者关联时，该如何查询呢？&lt;/p&gt;

&lt;p&gt;当你在模型里定义一个关系时，模型实例会有一个方便的 API 来访问关系对象。以下分几种映射关系分别描述。&lt;/p&gt;

&lt;h3 id=&quot;one-to-many&quot;&gt;One-to-many关系&lt;/h3&gt;

&lt;p&gt;如果一个对象有ForeignKey，这个模型实例访问关系对象通过简单的属性:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; e = Entry.objects.get(id=2)
&amp;gt;&amp;gt;&amp;gt; e.blog # Returns the related Blog object.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以凭借外键属性获取和赋值，修改外键值知道执行 &lt;code&gt;save()&lt;/code&gt; 方法才会保存到数据库:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; e = Entry.objects.get(id=2)
&amp;gt;&amp;gt;&amp;gt; e.blog = some_blog
&amp;gt;&amp;gt;&amp;gt; e.save()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果关联的对象可以为空，则可以将关联对象职位 None，删除关联：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; e = Entry.objects.get(id=2)
&amp;gt;&amp;gt;&amp;gt; e.blog = None
&amp;gt;&amp;gt;&amp;gt; e.save() # &quot;UPDATE blog_entry SET blog_id = NULL ...;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;子查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; e = Entry.objects.get(id=2)
&amp;gt;&amp;gt;&amp;gt; print(e.blog)  # Hits the database to retrieve the associated Blog.
&amp;gt;&amp;gt;&amp;gt; print(e.blog)  # Doesn&#39;t hit the database; uses cached version.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用 select_related() 方法，该方法会提前将关联对象查询出来：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; e = Entry.objects.select_related().get(id=2)
&amp;gt;&amp;gt;&amp;gt; print(e.blog)  # Doesn&#39;t hit the database; uses cached version.
&amp;gt;&amp;gt;&amp;gt; print(e.blog)  # Doesn&#39;t hit the database; uses cached version.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以通过 &lt;code&gt;模型_set&lt;/code&gt; 来访问关系对象的另一边，在 Blog 对象并没有维护 Entry 列表，但是你可以通过下面方式从 Blog 对象访问 Entry 列表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; b = Blog.objects.get(id=1)
&amp;gt;&amp;gt;&amp;gt; b.entry_set.all() # Returns all Entry objects related to Blog.

# b.entry_set is a Manager that returns QuerySets.
&amp;gt;&amp;gt;&amp;gt; b.entry_set.filter(headline__contains=&#39;Lennon&#39;)
&amp;gt;&amp;gt;&amp;gt; b.entry_set.count()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;模型_set&lt;/code&gt; 可以通过 &lt;code&gt;related_name&lt;/code&gt; 属性来修改，例如将 Entry 模型中的定义修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; blog = ForeignKey(Blog, related_name=&#39;entries&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的查询就会变成：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; b = Blog.objects.get(id=1)
&amp;gt;&amp;gt;&amp;gt; b.entries.all() # Returns all Entry objects related to Blog.

# b.entries is a Manager that returns QuerySets.
&amp;gt;&amp;gt;&amp;gt; b.entries.filter(headline__contains=&#39;Lennon&#39;)
&amp;gt;&amp;gt;&amp;gt; b.entries.count()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;many-to-many&quot;&gt;Many-to-many关系&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;e = Entry.objects.get(id=3)
e.authors.all() # Returns all Author objects for this Entry.
e.authors.count()
e.authors.filter(name__contains=&#39;John&#39;)

a = Author.objects.get(id=5)
a.entry_set.all() # Returns all Entry objects for this Author.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;one-to-one&quot;&gt;One-to-one关系&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class EntryDetail(models.Model):
    entry = models.OneToOneField(Entry)
    details = models.TextField()

ed = EntryDetail.objects.get(id=2)
ed.entry # Returns the related Entry object.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当反向查询时：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;e = Entry.objects.get(id=2)
e.entrydetail # returns the related EntryDetail object
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候如果没有关联对象，则会抛出 &lt;code&gt;DoesNotExist&lt;/code&gt; 异常。&lt;/p&gt;

&lt;p&gt;并且还可以修改：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;e.entrydetail = ed
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-9&quot;&gt;参考资料&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.djangoproject.com/en/1.7/topics/db/queries/&quot;&gt;Making queries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://my.oschina.net/u/877170/blog/288334&quot;&gt;Eclipse的django开发学习笔记（2）–模型（M）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.pythontip.com/blog/post/6358/&quot;&gt;Django：模型的使用&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/linjiqin/archive/2014/07/01/3817954.html&quot;&gt;django orm总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/01/15/django-orm.html</link>
      <guid>http://blog.javachen.com/2015/01/15/django-orm.html</guid>
      <pubDate>2015-01-15T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Django中的模型</title>
      <description>&lt;p&gt;Django 中的模型主要用于定义数据的来源信息，其包括一些必要的字段和一些对存储的数据的操作。通常，一个模型对应着数据库中的一个表。&lt;/p&gt;

&lt;p&gt;简单的概念：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Django 中每一个 Model 都继承自 &lt;code&gt;django.db.models.Model&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;在 Model 当中每一个属性 attribute 都代表一个数据库字段。&lt;/li&gt;
  &lt;li&gt;通过 Django Model API 可以执行数据库的增删改查, 而不需要写一些数据库的查询语句。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 模型&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1.1 一个示例&lt;/h2&gt;

&lt;p&gt;下面在 myapp 应用种定义了一个 Person 模型，包括两个字段：first_name 和 last_name。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models

class Person(models.Model):
    first_name = models.CharField(max_length=30)
    last_name = models.CharField(max_length=30)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;first_name 和 last_name 是模型的字段，每一个字段对应着一个类的属性，并且每一个属性对应数据库表中的一个列。&lt;/p&gt;

&lt;p&gt;上面的 Person 模型对应数据库中的表如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;CREATE TABLE myapp_person (
    &quot;id&quot; serial NOT NULL PRIMARY KEY,
    &quot;first_name&quot; varchar(30) NOT NULL,
    &quot;last_name&quot; varchar(30) NOT NULL
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;表名 &lt;code&gt;myapp_person&lt;/code&gt; 是由模型的元数据自动生成的，格式为 &lt;code&gt;应用_模型&lt;/code&gt;，你可以设置元数据覆盖该值。&lt;/li&gt;
  &lt;li&gt;id 字段在模型中是自动添加的，同样该字段名称也可以通过元数据覆盖。&lt;/li&gt;
  &lt;li&gt;上面的 sql 语法是 PostgreSQL 中的语法，你可以通过设置数据库类型，生成不同数据库对应的 sql。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;1.2 使用模型&lt;/h2&gt;

&lt;p&gt;使用模型之前，你需要先创建应用，然后将其加入 &lt;code&gt;INSTALLED_APPS&lt;/code&gt;，然后编写该应用中的 models.py 文件，最后运行 &lt;code&gt;manage.py makemigrations&lt;/code&gt; 和 &lt;code&gt;manage.py migrate&lt;/code&gt; 在数据库中创建该实体对应的表。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;INSTALLED_APPS = (
    #...
    &#39;myapp&#39;,
    #...
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;manage.py 参数列表&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;syncdb&lt;/code&gt;：创建所有应用所需的数据表&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sql &lt;/code&gt;：显示CREATETABLE调用&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sqlall&lt;/code&gt; 如同上面的sql一样，从sql文件中初始化数据载入语句&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sqlindexs&lt;/code&gt;：显示对主键创建索引的调用&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sqlclear&lt;/code&gt;：显示DROP TABLE的调用&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sqlcustom&lt;/code&gt;：显示指定.sql文件里的自定义SQL语句&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;loaddata&lt;/code&gt;：载入初始数据&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dumpdata&lt;/code&gt;：把原有的数据库里的数据输出伟JSON，XML等格式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;sql、sqlall、sql、sqlindexs、sqlclear、sqlcustom 不更新数据库，只打印SQL语句以作检验之用。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;1.3 模型属性&lt;/h2&gt;

&lt;p&gt;每个模型有一个默认的属性  &lt;code&gt;Manager&lt;/code&gt;，他是模型访问数据库的接口。如果没有自定义的 Manager，则其默认名称为 &lt;code&gt;objects&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;1.4 模型字段&lt;/h2&gt;

&lt;p&gt;字段名称不能和 clean、save 或者 delete 冲突。一个示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models

class Musician(models.Model):
    first_name = models.CharField(max_length=50)
    last_name = models.CharField(max_length=50)
    instrument = models.CharField(max_length=100)

class Album(models.Model):
    artist = models.ForeignKey(Musician)
    name = models.CharField(max_length=100)
    release_date = models.DateField()
    num_stars = models.IntegerField()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;模型中的每一个字段都必须为  &lt;code&gt;Field&lt;/code&gt; 类的一个实例，Django 使用该类型来决定数据库中对应的列的类型，并且每一个字段都有一些可选的参数。&lt;/p&gt;

&lt;p&gt;模型的字段可能的类型及参数如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;字段名&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;意义&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;AutoField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个能够根据可用ID自增的 IntegerField&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;BooleanField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个真/假字段&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CharField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(max_length)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;适用于中小长度的字符串。对于长段的文字，请使用 TextField&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CommaSeparatedIntegerField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(max_length)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个用逗号分隔开的整数字段&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DateField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;([auto_now], [auto_now_add])&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;日期字段&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DateTimeField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;时间日期字段,接受跟 DateField 一样的额外选项&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;EmailField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个能检查值是否是有效的电子邮件地址的 CharField&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FileField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(upload_to)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个文件上传字段&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FilePathField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(path,[match],[recursive])&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个拥有若干可选项的字段，选项被限定为文件系统中某个目录下的文件名&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FloatField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(max_digits,decimal_places)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个浮点数，对应 Python 中的 float 实例&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ImageField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(upload_to, [height_field] ,[width_field])&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;像 FileField 一样，只不过要验证上传的对象是一个有效的图片。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;IntegerField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个整数。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;IPAddressField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个IP地址，以字符串格式表示（例如： “24.124.1.30” ）。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;NullBooleanField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;就像一个 BooleanField ，但它支持 None /Null 。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PhoneNumberField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;它是一个 CharField ，并且会检查值是否是一个合法的美式电话格式&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PositiveIntegerField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;和 IntegerField 类似，但必须是正值。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PositiveSmallIntegerField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;与 PositiveIntegerField 类似，但只允许小于一定值的值,最大值取决于数据库&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SlugField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;嵌条 就是一段内容的简短标签，这段内容只能包含字母、数字、下划线或连字符。通常用于 URL 中&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SmallIntegerField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;和 IntegerField 类似，但是只允许在一个数据库相关的范围内的数值（通常是-32,768到&lt;/td&gt;
      &lt;td&gt;+32,767）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;TextField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个不限长度的文字字段&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;TimeField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;时分秒的时间显示。它接受的可指定参数与 DateField 和 DateTimeField 相同。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;URLField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;用来存储 URL 的字段。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;USStateField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;美国州名称缩写，两个字母。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;XMLField&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(schema_path)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;它就是一个 TextField ，只不过要检查值是匹配指定schema的合法XML。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通用字段参数列表如下（所有的字段类型都可以使用下面的参数，所有的都是可选的。）：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数名&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;意义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;null&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;如果设置为 True 的话，Django将在数据库中存储空值为 NULL 。默认False 。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;blank&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;如果是 True ，该字段允许留空，默认为 False 。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;choices&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个包含双元素元组的可迭代的对象，用于给字段提供选项。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;db_column&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;当前字段在数据库中对应的列的名字。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;db_index&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;如果为 True ，Django会在创建表时对这一列创建数据库索引。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;default&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;字段的默认值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;editable&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;如果为 False ，这个字段在管理界面或表单里将不能编辑。默认为 True 。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;help_text&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;在管理界面表单对象里显示在字段下面的额外帮助文本。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;primary_key&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;如果为 True ，这个字段就会成为模型的主键。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;radio_admin&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;如果 radio_admin 设置为 True 的话，Django 就会使用单选按钮界面。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;unique&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;如果是 True ，这个字段的值在整个表中必须是唯一的。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;unique_for_date&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;把它的值设成一个 DataField 或者 DateTimeField 的字段的名称，可以确保字段在这个日期内不会出现重复值。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;unique_for_month&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;和 unique_for_date 类似，只是要求字段在指定字段的月份内唯一。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;unique_for_year&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;和 unique_for_date 及 unique_for_month 类似，只是时间范围变成了一年。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;verbose_name&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;除 ForeignKey 、 ManyToManyField 和 OneToOneField 之外的字段都接受一个详细名称作为第一个位置参数。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;举例1，一个 choices 类型的例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models

class Person(models.Model):
    SHIRT_SIZES = (
        (&#39;S&#39;, &#39;Small&#39;),
        (&#39;M&#39;, &#39;Medium&#39;),
        (&#39;L&#39;, &#39;Large&#39;),
    )
    name = models.CharField(max_length=60)
    shirt_size = models.CharField(max_length=1, choices=SHIRT_SIZES)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以通过 &lt;code&gt;get_FOO_display&lt;/code&gt; 来访问 choices 字段显示的名称：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; p = Person(name=&quot;Fred Flintstone&quot;, shirt_size=&quot;L&quot;)
&amp;gt;&amp;gt;&amp;gt; p.save()
&amp;gt;&amp;gt;&amp;gt; p.shirt_size
u&#39;L&#39;
&amp;gt;&amp;gt;&amp;gt; p.get_shirt_size_display()
u&#39;Large&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;举例2，自定义主键：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models

class Fruit(models.Model):
    name = models.CharField(max_length=100, primary_key=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 primary_key 可以指定某一个字段为主键。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; fruit = Fruit.objects.create(name=&#39;Apple&#39;)
&amp;gt;&amp;gt;&amp;gt; fruit.name = &#39;Pear&#39;
&amp;gt;&amp;gt;&amp;gt; fruit.save()
&amp;gt;&amp;gt;&amp;gt; Fruit.objects.values_list(&#39;name&#39;, flat=True)
[&#39;Apple&#39;, &#39;Pear&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;说明：&lt;br /&gt;
values_list 函数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;meta-&quot;&gt;1.5 Meta 元数据类&lt;/h2&gt;

&lt;p&gt;模型里定义的变量 fields 和关系 relationships 提供了数据库的布局以及稍后查询模型时要用的变量名–经常你还需要添加__unicode__ 和 get_absolute_url 方法或是重写 内置的 save 和 delete方法。&lt;/p&gt;

&lt;p&gt;然而，模型的定义还有第三个方面–告知Django关于这个模型的各种元数据信息的嵌套类 Meta，Meta 类处理的是模型的各种元数据的使用和显示：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;比如在一个对象对多个对象是，它的名字应该怎么显示&lt;/li&gt;
  &lt;li&gt;查询数据表示默认的排序顺序是什么&lt;/li&gt;
  &lt;li&gt;数据表的名字是什么&lt;/li&gt;
  &lt;li&gt;多变量唯一性 （这种限制没有办法在每个单独的变量声明上定义）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Meta类有以下属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;abstract&lt;/code&gt;：定义当前的模型类是不是一个抽象类。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;app_label&lt;/code&gt;：这个选项只在一种情况下使用，就是你的模型类不在默认的应用程序包下的 models.py 文件中，这时候你需要指定你这个模型类是那个应用程序的&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;db_table&lt;/code&gt;：指定自定义数据库表名&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;db_tablespace&lt;/code&gt;：指定这个模型对应的数据库表放在哪个数据库表空间&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;get_latest_by&lt;/code&gt;：由于 Django 的管理方法中有个 &lt;code&gt;lastest()&lt;/code&gt;方法，就是得到最近一行记录。如果你的数据模型中有 DateField 或 DateTimeField 类型的字段，你可以通过这个选项来指定 &lt;code&gt;lastest()&lt;/code&gt; 是按照哪个字段进行选取的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;managed&lt;/code&gt;：由于 Django 会自动根据模型类生成映射的数据库表，如果你不希望 Django 这么做，可以把 managed 的值设置为 False。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;order_with_respect_to&lt;/code&gt;：这个选项一般用于多对多的关系中，它指向一个关联对象。就是说关联对象找到这个对象后它是经过排序的。指定这个属性后你会得到一个 &lt;code&gt;get_XXX_order()&lt;/code&gt; 和 &lt;code&gt;set_XXX_order()&lt;/code&gt; 的方法,通过它们你可以设置或者返回排序的对象。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ordering&lt;/code&gt;：定义排序字段&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;permissions&lt;/code&gt;：为了在 Django Admin 管理模块下使用的，如果你设置了这个属性可以让指定的方法权限描述更清晰可读&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;proxy&lt;/code&gt;：为了实现代理模型使用的&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;unique_together&lt;/code&gt;：定义多个字段保证数据的唯一性&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;verbose_name&lt;/code&gt;：给你的模型类起一个更可读的名字&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;verbose_name_plural&lt;/code&gt;：这个选项是指定模型的复数形式是什么&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;1.6 模型方法&lt;/h2&gt;

&lt;p&gt;Manager 提供的是表级别的方法，模型中还可以定义字段级别的方法。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models

class Person(models.Model):
    first_name = models.CharField(max_length=50)
    last_name = models.CharField(max_length=50)
    birth_date = models.DateField()

    def baby_boomer_status(self):
        &quot;Returns the person&#39;s baby-boomer status.&quot;
        import datetime
        if self.birth_date &amp;lt; datetime.date(1945, 8, 1):
            return &quot;Pre-boomer&quot;
        elif self.birth_date &amp;lt; datetime.date(1965, 1, 1):
            return &quot;Baby boomer&quot;
        else:
            return &quot;Post-boomer&quot;

    def _get_full_name(self):
        &quot;Returns the person&#39;s full name.&quot;
        return &#39;%s %s&#39; % (self.first_name, self.last_name)
    full_name = property(_get_full_name)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后一个方法是 &lt;code&gt;property&lt;/code&gt; 的一个示例。&lt;/p&gt;

&lt;p&gt;每一个模型有一些 Django 自动添加的方法，你也可以在模型的定义中覆盖这些方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;__str__()&lt;/code&gt; (Python 3)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;__unicode__()&lt;/code&gt; (Python 2)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;get_absolute_url()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;你也可以覆盖模型中和数据库相关的方法，通常是  &lt;code&gt;save()&lt;/code&gt; 和 &lt;code&gt;delete()&lt;/code&gt; 两个方法。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models

class Blog(models.Model):
    name = models.CharField(max_length=100)
    tagline = models.TextField()

    def save(self, *args, **kwargs):
        do_something()
        super(Blog, self).save(*args, **kwargs) # Call the &quot;real&quot; save() method.
        do_something_else()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sql&quot;&gt;1.7 执行自定义的 sql&lt;/h2&gt;

&lt;p&gt;这部分内容请参考：&lt;a href=&quot;/2015-01-30-raw-sql-query-in-django.html&quot;&gt;Django中SQL查询&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;2. 模型之间的关系&lt;/h1&gt;

&lt;p&gt;Django 提供了三种模型之间的关联关系： &lt;code&gt;many-to-one&lt;/code&gt;、&lt;code&gt;many-to-many&lt;/code&gt; 和 &lt;code&gt;one-to-one&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;2.1 多对一&lt;/h2&gt;

&lt;p&gt;定义多对一的关系，需要使用 &lt;code&gt;django.db.models.ForeignKey&lt;/code&gt; 来引用被关联的模型。&lt;/p&gt;

&lt;p&gt;例如，一本书有多个作者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Author(models.Model):
    name = models.CharField(max_length=100)

class Book(models.Model):
    title = models.CharField(max_length=100)
    author = models.ForeignKey(Author)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Django 的外键表现很直观，其主要参数就是它要引用的模型类；但是注意要把被引用的模型放在前面。不过，如果不想留意顺序，也可以用字符串代替。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Book(models.Model):
  title = models.CharField(max_length=100)
  author = models.ForeignKey(&quot;Author&quot;)
  #if Author class is defined in another file myapp/models.py
  #author = models.ForeignKey(&quot;myapp.Author&quot;)

class Author(models.Model):
   name = models.CharField(max_length=100)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果要引用自己为外键，可以设置 &lt;code&gt;models.ForeignKey(&quot;self&quot;)&lt;/code&gt; ，这在定义层次结构等类似场景很常用，比如 Employee 类可以具有类似 supervisor 或是 hired_by 这样的属性。&lt;/p&gt;

&lt;p&gt;外键 ForeignKey 只定义了关系的一端，但是另一端可以根据关系追溯回来，因为这是一种多对一的关系，多个子对象可以引用同一个父对象，而父对象可以访问到一组子对象。看下面的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#取一本书“Moby Dick”
book = Book.objects.get(title=&quot;Moby Dick&quot;)

#取作者名字
author = Book.author

#获取这个作者所有的书
books = author.book_set.all()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里从 Author 到 Book 的反向关系式通过 &lt;code&gt;Author.book_set&lt;/code&gt; 属性来表示的（这是一个manager对象），是由 ORM 自动添加的，可以通过在 ForeignKey 里指定 &lt;code&gt;related_name&lt;/code&gt; 参数来改变它的名字。比如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Book(models.Model):
  author = models.ForeignKey(&quot;Author&quot;, related_name = &quot;books&quot;)

#获取这个作者所有的书
books = author.books.all()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对简单的对象层次来说， &lt;code&gt;related_name&lt;/code&gt; 不是必需的，但是更复杂的关系里，比如当有多个 ForeignKey 的时候就一定要指定了。&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;2.2 多对多&lt;/h2&gt;

&lt;p&gt;上面的例子假设的是一本书只有一个作者，一个作者有多本书，所以是多对一的关系；但是如果一本书也有多个作者呢？这就是多对多的关系；由于SQL没有定义这种关系，必须通过外键用它能理解的方式实现多对多&lt;/p&gt;

&lt;p&gt;这里 Django 提供了第二种关系对象映射变量 &lt;code&gt;ManyToManyField&lt;/code&gt;，语法上来讲， 这和 ForeignKey 是一模一样的，你在关系的一端定义，把要关联的类传递进来，ORM 会自动为另一端生成使用这个关系必要的方法和属性。&lt;/p&gt;

&lt;p&gt;不过由于 ManyToManyField 的特性，在哪一端定义它通常都没有关系，因为这个关系是对称的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Author(models.Model):
  name = models.CharField(max_length=100)

class Book(models.Model):
  title = models.CharField(max_length=100)
  authors = models.ManyToManyField(Author)

#获取一本书
book = Book.objects.get(title=&quot;Python Web Dev Django&quot;)

#获取该书所有的作者
authors = Book.author_set.all()

#获取第三个作者出版过的所有的书
books = authors[2].book_set.all()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ManyToManyField 的秘密在于它在背后创建了一张新的表来满足这类关系的查询的需要，而这张表用的则是 SQL 外键，其中每一行都代表了两个对象的一个关系，同时包含了两端的外键&lt;/p&gt;

&lt;p&gt;这张查询表在 Django ORM 中一般是隐藏的，不可以单独查询，只能通过关系的某一端查询；不过可以在 ManyToManyField 上指定一个特殊的选项 through 来指向一个显式的中间模型类，更方便你的手动管理关系的两端&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Author(models.Model):
  name = models.CharField(max_length=100)

class Book(models.Model):
  title = models.CharField(max_length=100)
  authors = models.ManyToManyField(Author, through = &quot;Authoring&quot;)

class Authoring(models.Model):
  collaboration_type = models.CharField(max_length=100)
  book = model.ForeignKey(Book)
  author = model.ForeignKey(Author)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查询 Author 和 Book 的方法和之前完全一样，另外还能构造对 authoring 的查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;chan_essay_compilations = Book.objects.filter(
    author__name__endswith = &#39;Chun&#39;
    authoring__collaboration_type = &#39;essays&#39;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-9&quot;&gt;2.3 一对一&lt;/h2&gt;

&lt;p&gt;定义一个一对一的关系，需要使用 &lt;code&gt;OneToOneField&lt;/code&gt; 类。&lt;/p&gt;

&lt;p&gt;例如，Restaurant 和 Place 为一对一：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models

class Place(models.Model):
    name = models.CharField(max_length=50)
    address = models.CharField(max_length=80)

    def __str__(self):              # __unicode__ on Python 2
        return &quot;%s the place&quot; % self.name

class Restaurant(models.Model):
    place = models.OneToOneField(Place, primary_key=True)
    serves_hot_dogs = models.BooleanField(default=False)
    serves_pizza = models.BooleanField(default=False)

    def __str__(self):              # __unicode__ on Python 2
        return &quot;%s the restaurant&quot; % self.place.name

class Waiter(models.Model):
    restaurant = models.ForeignKey(Restaurant)
    name = models.CharField(max_length=50)

    def __str__(self):              # __unicode__ on Python 2
        return &quot;%s the waiter at %s&quot; % (self.name, self.restaurant)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个 Restaurant 都会有一个 Place，实际上你也可以使用继承的方式来定义。&lt;/p&gt;

&lt;p&gt;下面是一些操作的例子。&lt;/p&gt;

&lt;p&gt;创建 Place：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; p1 = Place(name=&#39;Demon Dogs&#39;, address=&#39;944 W. Fullerton&#39;)
&amp;gt;&amp;gt;&amp;gt; p1.save()
&amp;gt;&amp;gt;&amp;gt; p2 = Place(name=&#39;Ace Hardware&#39;, address=&#39;1013 N. Ashland&#39;)
&amp;gt;&amp;gt;&amp;gt; p2.save()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 Restaurant 并关联到 Place：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; r = Restaurant(place=p1, serves_hot_dogs=True, serves_pizza=False)
&amp;gt;&amp;gt;&amp;gt; r.save()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，可以这样访问：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; r.place
&amp;lt;Place: Demon Dogs the place&amp;gt;
&amp;gt;&amp;gt;&amp;gt; p1.restaurant
&amp;lt;Restaurant: Demon Dogs the restaurant&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候，p2 没有与之关联的 Restaurant，如果通过 p2 访问 Restaurant 就会提示异常：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from django.core.exceptions import ObjectDoesNotExist
&amp;gt;&amp;gt;&amp;gt; try:
&amp;gt;&amp;gt;&amp;gt;     p2.restaurant
&amp;gt;&amp;gt;&amp;gt; except ObjectDoesNotExist:
&amp;gt;&amp;gt;&amp;gt;     print(&quot;There is no restaurant here.&quot;)
There is no restaurant here.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以通过下面的方式来避免出现异常：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; hasattr(p2, &#39;restaurant&#39;)
False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面可以做一些赋值操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; r.place = p2
&amp;gt;&amp;gt;&amp;gt; r.save()
&amp;gt;&amp;gt;&amp;gt; p2.restaurant
&amp;lt;Restaurant: Ace Hardware the restaurant&amp;gt;
&amp;gt;&amp;gt;&amp;gt; r.place
&amp;lt;Place: Ace Hardware the place&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以反向赋值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; p1.restaurant = r
&amp;gt;&amp;gt;&amp;gt; p1.restaurant
&amp;lt;Restaurant: Demon Dogs the restaurant&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查询所有的 Restaurant 和 Place：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Restaurant.objects.all()
[&amp;lt;Restaurant: Demon Dogs the restaurant&amp;gt;, &amp;lt;Restaurant: Ace Hardware the restaurant&amp;gt;]
&amp;gt;&amp;gt;&amp;gt; Place.objects.order_by(&#39;name&#39;)
[&amp;lt;Place: Ace Hardware the place&amp;gt;, &amp;lt;Place: Demon Dogs the place&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，也可以使用跨关系查找：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Restaurant.objects.get(place=p1)
&amp;lt;Restaurant: Demon Dogs the restaurant&amp;gt;
&amp;gt;&amp;gt;&amp;gt; Restaurant.objects.get(place__pk=1)
&amp;lt;Restaurant: Demon Dogs the restaurant&amp;gt;
&amp;gt;&amp;gt;&amp;gt; Restaurant.objects.filter(place__name__startswith=&quot;Demon&quot;)
[&amp;lt;Restaurant: Demon Dogs the restaurant&amp;gt;]
&amp;gt;&amp;gt;&amp;gt; Restaurant.objects.exclude(place__address__contains=&quot;Ashland&quot;)
[&amp;lt;Restaurant: Demon Dogs the restaurant&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;反向查找：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Place.objects.get(pk=1)
&amp;lt;Place: Demon Dogs the place&amp;gt;
&amp;gt;&amp;gt;&amp;gt; Place.objects.get(restaurant__place=p1)
&amp;lt;Place: Demon Dogs the place&amp;gt;
&amp;gt;&amp;gt;&amp;gt; Place.objects.get(restaurant=r)
&amp;lt;Place: Demon Dogs the place&amp;gt;
&amp;gt;&amp;gt;&amp;gt; Place.objects.get(restaurant__place__name__startswith=&quot;Demon&quot;)
&amp;lt;Place: Demon Dogs the place&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个 Waiter：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; w = r.waiter_set.create(name=&#39;Joe&#39;)
&amp;gt;&amp;gt;&amp;gt; w.save()
&amp;gt;&amp;gt;&amp;gt; w
&amp;lt;Waiter: Joe the waiter at Demon Dogs the restaurant&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; Waiter.objects.filter(restaurant__place=p1)
[&amp;lt;Waiter: Joe the waiter at Demon Dogs the restaurant&amp;gt;]
&amp;gt;&amp;gt;&amp;gt; Waiter.objects.filter(restaurant__place__name__startswith=&quot;Demon&quot;)
[&amp;lt;Waiter: Joe the waiter at Demon Dogs the restaurant&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-10&quot;&gt;3. 模型继承&lt;/h1&gt;

&lt;p&gt;Django目前支持几种不同的继承方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用单个表。整个继承树共用一张表。使用唯一的表，包含所有基类和子类的字段。&lt;/li&gt;
  &lt;li&gt;每个具体类一张表，这种方式下，每张表都包含具体类和继承树上所有父类的字段。因为多个表中有重复字段，从整个继承树上来说，字段是冗余的。&lt;/li&gt;
  &lt;li&gt;每个类一张表，继承关系通过表的JOIN操作来表示。这种方式下，每个表只包含类中定义的字段，不存在字段冗余，但是要同时操作子类和所有父类所对应的表。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;方式一：每个类一张表&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models
 
class Person(models.Model):
  name = models.CharField(max_length=20)
  sex = models.BooleanField(default=True)
 
class teacher(Person):
  subject = models.CharField(max_length=20)
 
class student(Person):
  course = models.CharField(max_length=20)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行 &lt;code&gt;python manage.py sqlall&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;BEGIN;
CREATE TABLE &quot;blog_person&quot; (
    &quot;id&quot; integer NOT NULL PRIMARY KEY,
    &quot;name&quot; varchar(20) NOT NULL,
    &quot;sex&quot; bool NOT NULL
)
;
CREATE TABLE &quot;blog_teacher&quot; (
    &quot;person_ptr_id&quot; integer NOT NULL PRIMARY KEY REFERENCES &quot;blog_person&quot; (&quot;id&quot;),
    &quot;subject&quot; varchar(20) NOT NULL
)
;
CREATE TABLE &quot;blog_student&quot; (
    &quot;person_ptr_id&quot; integer NOT NULL PRIMARY KEY REFERENCES &quot;blog_person&quot; (&quot;id&quot;),
    &quot;course&quot; varchar(20) NOT NULL
)
;
 
COMMIT;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;方式二：每个具体类一张表，父类不需要创建表&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models
 
class Person(models.Model):
  name = models.CharField(max_length=20)
  sex = models.BooleanField(default=True)
 
  class Meta:
    abstract = True
 
class teacher(Person):
  subject = models.CharField(max_length=20)
 
class student(Person):
  course = models.CharField(max_length=20)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行 &lt;code&gt;python manage.py sqlall&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;BEGIN;
CREATE TABLE &quot;blog_teacher&quot; (
    &quot;id&quot; integer NOT NULL PRIMARY KEY,
    &quot;name&quot; varchar(20) NOT NULL,
    &quot;sex&quot; bool NOT NULL,
    &quot;subject&quot; varchar(20) NOT NULL
)
;
CREATE TABLE &quot;blog_student&quot; (
    &quot;id&quot; integer NOT NULL PRIMARY KEY,
    &quot;name&quot; varchar(20) NOT NULL,
    &quot;sex&quot; bool NOT NULL,
    &quot;course&quot; varchar(20) NOT NULL
)
;
 
COMMIT;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过 Meta 嵌套类自定义每个子类的表名：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models
 
class Person(models.Model):
  name = models.CharField(max_length=20)
  sex = models.BooleanField(default=True)
 
  class Meta:
    abstract = True
 
class teacher(Person):
  subject = models.CharField(max_length=20)
 
  class Meta:
    db_table = &quot;Teacher&quot;
 
class student(Person):
  course = models.CharField(max_length=20)
 
  class Meta:
    db_table = &quot;Student&quot;
 ```   

方式三：代理模型，为子类增加方法，但不能增加属性

 ``` python  
from django.db import models
 
class Person(User):
  class Meta:
    proxy = True
 
  def some_function(self):
    pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样的方式不会改变数据存储结构，但可以纵向的扩展子类Person的方法，并且基础User父类的所有属性和方法。&lt;/p&gt;

&lt;h1 id=&quot;section-11&quot;&gt;4. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.c77cc.cn/article-64.html&quot;&gt;Django 数据模型的字段列表整理&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.tuicool.com/articles/vU7vIz&quot;&gt;跟我一起Django - 04 定义和使用模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://iluoxuan.iteye.com/blog/1703061&quot;&gt;django的模型总结&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.coocla.org/414.html&quot;&gt;django ORM数据模型的定义&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/01/14/django-model.html</link>
      <guid>http://blog.javachen.com/2015/01/14/django-model.html</guid>
      <pubDate>2015-01-14T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>AngularJS PhoneCat代码分析</title>
      <description>&lt;p&gt;AngularJS 官方网站提供了一个用于学习的示例项目：PhoneCat。这是一个Web应用，用户可以浏览一些Android手机，了解它们的详细信息，并进行搜索和排序操作。&lt;/p&gt;

&lt;p&gt;本文主要分析 AngularJS 官方网站提供的一个用于学习的示例项目 PhoneCat 的构建、测试过程以及代码的运行原理。希望能够对 PhoneCat 项目有一个更加深入全面的认识。这其中包括以下内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;该项目如何运行起来的&lt;/li&gt;
  &lt;li&gt;该项目如何进行前端单元测试&lt;/li&gt;
  &lt;li&gt;AngularJS 相关代码分析&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;以下内容如有理解不正确，欢迎指正！&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境搭建&lt;/h1&gt;

&lt;p&gt;对于 PhoneCat 项目的开发环境和测试环境的搭建，官方网站上提供了详细的指导：&lt;a href=&quot;http://docs.angularjs.org/tutorial&quot;&gt;http://docs.angularjs.org/tutorial&lt;/a&gt;，你可以找到一些中文的翻译。&lt;/p&gt;

&lt;p&gt;PhoneCat 项目的源代码托管在 GitHub 上，可以通过下面命令下载源代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone --depth=20 https://github.com/angular/angular-phonecat.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;--depth=20&lt;/code&gt; 选项的意思为：仅下载最近20次的代码提交版本；这么做可以减少下载的文件大小，加快下载。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;PhoneCat 是一个 Web 应用程序，因此最好在 Web 服务器中运行，以期获得最佳结果。官方推荐安装 &lt;a href=&quot;http://nodejs.org/download/&quot;&gt;Node.js&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PhoneCat 项目的运行与测试依赖一些别的工具，可以在安装 &lt;a href=&quot;http://www.oschina.net/p/nodejs&quot;&gt;Node.js&lt;/a&gt; 后通过 npm 命令来安装这些依赖包。以下命令需在 angular-phonecat 项目路径下运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npm install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行该命令后，会在 angular-phonecat 项目路径下安装以下依赖包：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Bower&lt;/code&gt; 包管理器&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Http-Server&lt;/code&gt; 轻量级Web服务器&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Karma&lt;/code&gt;  用于运行单元测试&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Protractor&lt;/code&gt; 用于运行端到端测试&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;几乎所有的 AngularJS 学习教程，都会写到用这个命令来启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ node scripts/web-server.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但实际上 PhoneCat 项目已经放弃使用 web-server 了，git 上取下来的的项目里没有 scripts/web-server.js 文件了。&lt;/p&gt;

&lt;p&gt;我们可以用下面的方式来启动工程：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npm start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过 &lt;a href=&quot;http://localhost:8000/app/index.html&quot;&gt;http://localhost:8000/app/index.html&lt;/a&gt; 访问。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 依赖包介绍&lt;/h1&gt;

&lt;p&gt;在克隆项目之后，目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;➜  angular-phonecat git:(master) ✗ tree -L 2
.
├── LICENSE
├── README.md
├── app
│   ├── bower_components
│   ├── css
│   ├── img
│   ├── index.html
│   ├── js
│   ├── partials
│   └── phones
├── bower.json
├── package.json
├── scripts
│   ├── private
│   └── update-repo.sh
└── test
    ├── e2e
    ├── karma.conf.js
    ├── protractor-conf.js
    └── unit

20 directories, 8 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个目录下存在一个文件 package.json，该文件是做什么用的呢？&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在 NodeJS 项目中，用 package.json 文件来声明项目中使用的模块，这样在新的环境部署时，只要在 package.json 文件所在的目录执行 &lt;code&gt;npm install&lt;/code&gt; 命令即可安装所需要的模块。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于 package.json 中可配置的选项请参考 &lt;a href=&quot;http://blog.csdn.net/woxueliuyun/article/details/39294375&quot;&gt;package.json字段全解&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;从该文件可以看出 PhoneCat 的依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;  &quot;devDependencies&quot;: {
    &quot;karma&quot;: &quot;^0.12.16&quot;,
    &quot;karma-chrome-launcher&quot;: &quot;^0.1.4&quot;,
    &quot;karma-jasmine&quot;: &quot;^0.1.5&quot;,
    &quot;protractor&quot;: &quot;~1.0.0&quot;,
    &quot;http-server&quot;: &quot;^0.6.1&quot;,
    &quot;tmp&quot;: &quot;0.0.23&quot;,
    &quot;bower&quot;: &quot;^1.3.1&quot;,
    &quot;shelljs&quot;: &quot;^0.2.6&quot;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以及一些脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&quot;scripts&quot;: {
    &quot;postinstall&quot;: &quot;bower install&quot;,

    &quot;prestart&quot;: &quot;npm install&quot;,
    &quot;start&quot;: &quot;http-server -a 0.0.0.0 -p 8000&quot;,

    &quot;pretest&quot;: &quot;npm install&quot;,
    &quot;test&quot;: &quot;node node_modules/karma/bin/karma start test/karma.conf.js&quot;,
    &quot;test-single-run&quot;: &quot;node node_modules/karma/bin/karma start test/karma.conf.js  --single-run&quot;,

    &quot;preupdate-webdriver&quot;: &quot;npm install&quot;,
    &quot;update-webdriver&quot;: &quot;webdriver-manager update&quot;,

    &quot;preprotractor&quot;: &quot;npm run update-webdriver&quot;,
    &quot;protractor&quot;: &quot;protractor test/protractor-conf.js&quot;,

    &quot;update-index-async&quot;: &quot;node -e \&quot;require(&#39;shelljs/global&#39;); sed(&#39;-i&#39;, /\\/\\/@@NG_LOADER_START@@[\\s\\S]*\\/\\/@@NG_LOADER_END@@/, &#39;//@@NG_LOADER_START@@\\n&#39; + cat(&#39;bower_components/angular-loader/angular-loader.min.js&#39;) + &#39;\\n//@@NG_LOADER_END@@&#39;, &#39;app/index-async.html&#39;);\&quot;&quot;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上可以看出运行 &lt;code&gt;npm start&lt;/code&gt; 之前会运行 &lt;code&gt;npm install&lt;/code&gt;，然后运行 &lt;code&gt;http-server -a 0.0.0.0 -p 8000&lt;/code&gt; 启动一个 web 服务器，最后是运行 &lt;code&gt;bower install&lt;/code&gt; 安装 bower 管理的包。&lt;/p&gt;

&lt;p&gt;bower 管理的包由 bower.json 文件定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;{
  &quot;name&quot;: &quot;angular-phonecat&quot;,
  &quot;description&quot;: &quot;A starter project for AngularJS&quot;,
  &quot;version&quot;: &quot;0.0.0&quot;,
  &quot;homepage&quot;: &quot;https://github.com/angular/angular-phonecat&quot;,
  &quot;license&quot;: &quot;MIT&quot;,
  &quot;private&quot;: true,
  &quot;dependencies&quot;: {
    &quot;angular&quot;: &quot;1.3.x&quot;,
    &quot;angular-mocks&quot;: &quot;1.3.x&quot;,
    &quot;jquery&quot;: &quot;~2.1.1&quot;,
    &quot;bootstrap&quot;: &quot;~3.1.1&quot;,
    &quot;angular-route&quot;: &quot;1.3.x&quot;,
    &quot;angular-resource&quot;: &quot;1.3.x&quot;,
    &quot;angular-animate&quot;: &quot;1.3.x&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，package.json 文件中还定义了一些测试相关的命令。&lt;/p&gt;

&lt;h2 id=&quot;bower&quot;&gt;bower&lt;/h2&gt;

&lt;p&gt;关于 &lt;a href=&quot;http://bower.io/&quot;&gt;bower&lt;/a&gt; 的介绍，参考博客内文章：&lt;a href=&quot;/2014/05/10/bower-intro.html&quot;&gt;bower介绍&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在本项目中，bower 下载的包保存在 angular-phonecat/app/bower_components 目录下，依赖如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── bower_components
│   ├── angular
│   ├── angular-animate
│   ├── angular-mocks
│   ├── angular-resource
│   ├── angular-route
│   ├── bootstrap
│   └── jquery
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;karma&quot;&gt;karma&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://karma-runner.github.io/0.12/index.html&quot;&gt;Karma&lt;/a&gt; 是一个 Javascript 测试运行工具，可以帮助你关闭反馈循环。Karma 可以在特定的文件被修改时运行测试，它也可以在不同的浏览器上并行测试。不同的设备可以指向 Karma 服务器来覆盖实际场景。&lt;/p&gt;

&lt;p&gt;关于 Karma 的使用，本文不做介绍。&lt;/p&gt;

&lt;h2 id=&quot;http-server&quot;&gt;http-server&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/nodeapps/http-server&quot;&gt;http-server&lt;/a&gt; 是一个简单的零配置命令行 HTTP 服务器，基于 &lt;a href=&quot;http://www.oschina.net/p/nodejs&quot;&gt;Node.js&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在命令行中使用方式是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ node http-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在package.json 中定义方式是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt; &quot;scripts&quot;: {
     &quot;start&quot;: &quot;http-server -a 0.0.0.0 -p 8000&quot;,
 }
 ```

 支持的参数：

```bash
 -p 端口号 (默认 8080)

-a IP 地址 (默认 0.0.0.0)

-d 显示目录列表 (默认 &#39;True&#39;)

-i 显示 autoIndex (默认 &#39;True&#39;)

-e or --ext 如果没有提供默认的文件扩展名(默认 &#39;html&#39;)

-s or --silent 禁止日志信息输出

--cors 启用 CORS 

-o 在开始服务后打开浏览器

-h or --help 打印列表并退出

-c 为 cache-control max-age header 设置Cache time(秒) ，禁用 caching, 则值设为 -1 .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;protractor&quot;&gt;Protractor&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.protractortest.org/&quot;&gt;Protractor&lt;/a&gt; 是一个端对端的测试运行工具，模拟用户交互，帮助你验证你的 Angular 应用的运行状况。&lt;/p&gt;

&lt;p&gt;Protractor 使用 &lt;a href=&quot;http://jasmine.github.io/&quot;&gt;Jasmine&lt;/a&gt; 测试框架来定义测试。Protractor 为不同的页面交互提供一套健壮的 API。&lt;/p&gt;

&lt;p&gt;当然，也有其他的端对端工具，不过 Protractor 有着自己的优势，它知道怎么和 AngularJS 的代码一起运行，特别是面临 $digest 循环的时候。&lt;/p&gt;

&lt;p&gt;关于 Protractor 的使用，本文不做介绍。&lt;/p&gt;

&lt;h2 id=&quot;shelljs&quot;&gt;ShellJS&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://shelljs.org/&quot;&gt;ShellJS&lt;/a&gt; 是 &lt;a href=&quot;http://www.oschina.net/p/nodejs&quot;&gt;Node.js&lt;/a&gt; 扩展，用于实现 Unix shell 命令执行，支持 Windows。&lt;/p&gt;

&lt;p&gt;一个示例代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;require(&#39;shelljs/global&#39;);

if (!which(&#39;git&#39;)) {
  echo(&#39;Sorry, this script requires git&#39;);
  exit(1);
}

// Copy files to release dir
mkdir(&#39;-p&#39;, &#39;out/Release&#39;);
cp(&#39;-R&#39;, &#39;stuff/*&#39;, &#39;out/Release&#39;);

// Replace macros in each .js file
cd(&#39;lib&#39;);
ls(&#39;*.js&#39;).forEach(function(file) {
  sed(&#39;-i&#39;, &#39;BUILD_VERSION&#39;, &#39;v0.1.2&#39;, file);
  sed(&#39;-i&#39;, /.*REMOVE_THIS_LINE.*\n/, &#39;&#39;, file);
  sed(&#39;-i&#39;, /.*REPLACE_LINE_WITH_MACRO.*\n/, cat(&#39;macro.js&#39;), file);
});
cd(&#39;..&#39;);

// Run external tool synchronously
if (exec(&#39;git commit -am &quot;Auto-commit&quot;&#39;).code !== 0) {
  echo(&#39;Error: Git commit failed&#39;);
  exit(1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 PhoneCat 中，主要是用在下面：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&quot;update-index-async&quot;: &quot;node -e \&quot;require(&#39;shelljs/global&#39;); sed(&#39;-i&#39;, /\\/\\/@@NG_LOADER_START@@[\\s\\S]*\\/\\/@@NG_LOADER_END@@/, &#39;//@@NG_LOADER_START@@\\n&#39; + cat(&#39;bower_components/angular-loader/angular-loader.min.js&#39;) + &#39;\\n//@@NG_LOADER_END@@&#39;, &#39;app/index-async.html&#39;);\&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 测试&lt;/h1&gt;

&lt;h2 id=&quot;section-3&quot;&gt;运行单元测试&lt;/h2&gt;

&lt;p&gt;PhoneCat 项目中的单元测试是使用 Karma 来完成的，所有的单元测试用例都存放在 test/unit 目录下。可以通过执行以下命令来运行单元测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npm test
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;值得一提的是，在运行单元测试前，计算机上必须安装 Google Chrome 浏览器，&lt;strong&gt;因为这里用到了 karma-chrome-launcher&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-4&quot;&gt;运行端到端测试&lt;/h2&gt;

&lt;p&gt;PhoneCat 项目使用端到端测试来保证 Web 应用的可操作性，而这个端到端测试是通过使用 Protractor 来实现的，所有的端到端测试用例都存放在test/e2e 目录下。可以通过执行以下步骤来运行端到端测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;//更新webdriver，此命令只需运行一次
$ npm run update-webdriver
//运行PhoneCat
$ npm start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开另一个命令行窗口，在其中运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npm run protractor
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-5&quot;&gt;4. 代码分析&lt;/h1&gt;

&lt;p&gt;在介绍了 PhoneCat 的运行和测试环境后，来看看 PhoneCat 的页面和 js 是怎么组织起来的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;首先，从 index.html 内容可以看到 PhoneCat 页面使用 bootstrap 框架，并且引入了 jquery 以及 angular 的相关依赖，包括一些附加模块：&lt;code&gt;路由&lt;/code&gt;、&lt;code&gt;动画&lt;/code&gt;、&lt;code&gt;资源&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;angular 应用范围由 &lt;code&gt;ng-app&lt;/code&gt; 定义在 html 节点上，即作用于整个页面，其名称为 &lt;code&gt;phonecatApp&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;通过 &lt;code&gt;ng-view&lt;/code&gt; 指定加载子视图的位置，这里主要包括 &lt;code&gt;partials/phone-list.html&lt;/code&gt; 和 &lt;code&gt;partials/phone-detail.html&lt;/code&gt; 两个视图。&lt;/li&gt;
  &lt;li&gt;app.js 是应用的入口，并且依赖 animations.js、controllers.js、filters.js、services.js 等文件。从这里可以看出，一个 angular 应用的 js 大概包括哪几个部分的内容。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;app.js 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
//JavaScript语法支持严格模式:如果在语法检测时发现语法问题，则整个代码块失效，并导致一个语法异常；如果在运行期出现了违反严格模式的代码，则抛出执行异常。
&#39;use strict&#39;;

/* App Module */
//定义一个模块，模块名称和页面 ng-app 中内容一致
var phonecatApp = angular.module(&#39;phonecatApp&#39;, [
  &#39;ngRoute&#39;,
  &#39;phonecatAnimations&#39;,
  &#39;phonecatControllers&#39;,
  &#39;phonecatFilters&#39;,
  &#39;phonecatServices&#39;
]);

//定义路由
phonecatApp.config([&#39;$routeProvider&#39;,
  function($routeProvider) {
    $routeProvider.
      when(&#39;/phones&#39;, {
        templateUrl: &#39;partials/phone-list.html&#39;,
        controller: &#39;PhoneListCtrl&#39;
      }).
      when(&#39;/phones/:phoneId&#39;, {
        templateUrl: &#39;partials/phone-detail.html&#39;,
        controller: &#39;PhoneDetailCtrl&#39;
      }).
      otherwise({
        redirectTo: &#39;/phones&#39;
      });
  }]);
  ```

phonecatApp 模块依赖其他几个模块：ngRoute、phonecatAnimations、phonecatControllers、phonecatFilters、phonecatServices。

ngRoute 是内置的路由模块，定义路由规则：

- 当访问 `/phones`，由 `PhoneListCtrl` 控制器处理，并且由 `partials/phone-list.html` 模板渲染显示内容。
- 当访问 `/phones/:phoneId`，由 `PhoneDetailCtrl` 控制器处理，并且由 `partials/phone-detail.html` 模板渲染显示内容。
- 如果不满足上面条件，则重定向到 `/phones`

phonecatAnimations 模块是定义动画效果，没有真个模块不影响程序的主要功能的运行，故不分析这部分代码。

phonecatControllers 模块定义在 controllers.js 文件中：

```javascript
&#39;use strict&#39;;

/* Controllers */
var phonecatControllers = angular.module(&#39;phonecatControllers&#39;, []);

// 定义 PhoneListCtrl，并注入 Phone 对象
phonecatControllers.controller(&#39;PhoneListCtrl&#39;, [&#39;$scope&#39;, &#39;Phone&#39;,
  function($scope, Phone) {
    $scope.phones = Phone.query();
    $scope.orderProp = &#39;age&#39;;
  }]);

// 定义 PhoneDetailCtrl，并注入 Phone 对象和 $routeParams，$routeParams 封装了路由参数。
phonecatControllers.controller(&#39;PhoneDetailCtrl&#39;, [&#39;$scope&#39;, &#39;$routeParams&#39;, &#39;Phone&#39;,
  function($scope, $routeParams, Phone) {
    $scope.phone = Phone.get({phoneId: $routeParams.phoneId}, function(phone) {
      //回调方法
      $scope.mainImageUrl = phone.images[0];
    });

    $scope.setImage = function(imageUrl) {
      $scope.mainImageUrl = imageUrl;
    }
  }]);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;phonecatFilters 模块定义在 filter.js 文件中，主要是自定义了一个过滤器 &lt;code&gt;checkmark&lt;/code&gt;：根据输入是否有内容判断返回 &lt;code&gt;✓&lt;/code&gt; 还是 &lt;code&gt;✘&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;phonecatServices 模块定义在 services.js 文件中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&#39;use strict&#39;;

/* Services */
var phonecatServices = angular.module(&#39;phonecatServices&#39;, [&#39;ngResource&#39;]);

// 定义 Phone 服务，并提供了一个 query 方法，还包括一个内置的 get 方法。调用 get 方法实际上就是调用 query 方法，并且可以传递一个参数 phoneId
phonecatServices.factory(&#39;Phone&#39;, [&#39;$resource&#39;,
  function($resource){
    return $resource(&#39;phones/:phoneId.json&#39;, {}, {
      query: {method:&#39;GET&#39;, params:{phoneId:&#39;phones&#39;}, isArray:true}
    });
  }]);
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-6&quot;&gt;5. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.lifelaf.com/blog/?p=1206&quot;&gt;AngularJS初探：搭建PhoneCat项目的开发与测试环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/ElvinLong/p/3939938.html&quot;&gt;Angular 实例项目 angular-phonecat 的一些问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/01/09/angular-phonecat-examples.html</link>
      <guid>http://blog.javachen.com/2015/01/09/angular-phonecat-examples.html</guid>
      <pubDate>2015-01-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>AngularJS基本知识点</title>
      <description>&lt;p&gt;AngularJS 是一个 MV* 框架，最适于开发客户端的单页面应用。它不是个功能库，而是用来开发动态网页的框架。它专注于扩展 HTML 的功能，提供动态数据绑定（data binding），而且它能跟其它框架（如 JQuery 等）合作融洽。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 一个简单示例&lt;/h1&gt;

&lt;p&gt;通过下面的示例代码，可以运行一个简单的 AngularJS 应用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;body&amp;gt;

&amp;lt;div ng-app=&quot;&quot;&amp;gt;
  &amp;lt;p&amp;gt;在输入框中尝试输入：&amp;lt;/p&amp;gt;
  &amp;lt;p&amp;gt;姓名：&amp;lt;input type=&quot;text&quot; ng-model=&quot;name&quot;&amp;gt;&amp;lt;/p&amp;gt;
  &amp;lt;p ng-bind=&quot;name&quot;&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;

&amp;lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.3.8/angular.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过浏览器访问该页面，在输入框中输入的内容会立即显示在输入框下面。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当网页加载完毕，AngularJS 自动运行。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-app&lt;/code&gt; 指令告诉 AngularJS ，div 元素是 AngularJS 应用程序的”所有者”，相当于是个作用域的概率。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-model&lt;/code&gt; 指令把输入域的值绑定到应用程序变量 name。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-bind&lt;/code&gt; 指令把应用程序变量 name 绑定到某个段落的 innerHTML。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;angularjs-&quot;&gt;2. AngularJS 指令&lt;/h1&gt;

&lt;p&gt;AngularJS 指令是扩展的 HTML 属性，带有前缀 &lt;code&gt;ng-&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;HTML5 允许扩展的（自制的）属性，以 &lt;code&gt;data-&lt;/code&gt; 开头。AngularJS 属性以 &lt;code&gt;ng-&lt;/code&gt;开头，但是您可以使用 &lt;code&gt;data-ng-&lt;/code&gt; 来让网页对 HTML5 有效。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;常见的指令&lt;/h2&gt;

&lt;h3 id=&quot;ng-app-&quot;&gt;ng-app 指令&lt;/h3&gt;

&lt;p&gt;ng-app 指令初始化一个 AngularJS 应用程序。&lt;/p&gt;

&lt;h3 id=&quot;ng-init-&quot;&gt;ng-init 指令&lt;/h3&gt;

&lt;p&gt;ng-init 指令初始化应用程序数据，这个不常使用。通常情况下，不使用 ng-init。您将使用一个控制器或模块来代替它。&lt;/p&gt;

&lt;h3 id=&quot;ng-model-&quot;&gt;ng-model 指令&lt;/h3&gt;

&lt;p&gt;ng-model 指令把元素值（比如输入域的值）绑定到应用程序。&lt;/p&gt;

&lt;h3 id=&quot;ng-bind-&quot;&gt;ng-bind 指令&lt;/h3&gt;

&lt;p&gt;ng-bind 指令把应用程序数据绑定到 HTML 视图。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;div ng-app=&quot;&quot;&amp;gt;
  &amp;lt;p&amp;gt;在输入框中尝试输入：&amp;lt;/p&amp;gt;
  &amp;lt;p&amp;gt;姓名：&amp;lt;input type=&quot;text&quot; ng-model=&quot;name&quot;&amp;gt;&amp;lt;/p&amp;gt;
  &amp;lt;p ng-bind=&quot;name&quot;&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;ng-repeat-&quot;&gt;ng-repeat 指令&lt;/h3&gt;

&lt;p&gt;ng-repeat 指令会重复一个 HTML 元素：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;div ng-app=&quot;&quot; ng-init=&quot;names=[
{name:&#39;Jani&#39;,country:&#39;Norway&#39;},
{name:&#39;Hege&#39;,country:&#39;Sweden&#39;},
{name:&#39;Kai&#39;,country:&#39;Denmark&#39;}]&quot;&amp;gt;

&amp;lt;p&amp;gt;循环对象：&amp;lt;/p&amp;gt;
  &amp;lt;ul&amp;gt;
    &amp;lt;li ng-repeat=&quot;x in names&quot;&amp;gt;
      
    &amp;lt;/li&amp;gt;
  &amp;lt;/ul&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除此之外，它还提供了几个变量可供使用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;$index&lt;/code&gt; 当前索引&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;$first&lt;/code&gt; 是否为头元素&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;$middle&lt;/code&gt; 是否为非头非尾元素&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;$last&lt;/code&gt; 是否为尾元素&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;样式相关的指令&lt;/h2&gt;

&lt;h3 id=&quot;ng-class&quot;&gt;ng-class&lt;/h3&gt;

&lt;p&gt;ng-class用来给元素绑定类名，其表达式的返回值可以是以下三种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;类名字符串，可以用空格分割多个类名，如 ‘class1 class2’；&lt;/li&gt;
  &lt;li&gt;类名数组，数组中的每一项都会层叠起来生效；&lt;/li&gt;
  &lt;li&gt;一个名值对应的map，其键值为类名，值为boolean类型，当值为true时，该类会被加在元素上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;与 ng-class 相近的，ng 还提供了ng-class-odd、ng-class-even 两个指令，用来配合 ng-repeat 分别在奇数列和偶数列使用对应的类。这个用来在表格中实现隔行换色再方便不过了。&lt;/p&gt;

&lt;h3 id=&quot;ng-style&quot;&gt;ng-style&lt;/h3&gt;

&lt;p&gt;ng-style 用来绑定元素的 css 样式，其表达式的返回值为一个 js 对象，键为 css 样式名，值为该样式对应的合法取值。用法比较简单：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;$scope.style = {color:&#39;red&#39;};　

&amp;lt;div ng-style=&quot;{color:&#39;red&#39;}&quot;&amp;gt;ng-style测试&amp;lt;/div&amp;gt;
&amp;lt;div ng-style=&quot;style&quot;&amp;gt;ng-style测试&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;ng-showng-hideng-switch&quot;&gt;ng-show、ng-hide、ng-switch&lt;/h3&gt;

&lt;p&gt;对于比较常用的元素显隐控制，ng 也做了封装，ng-show 和 ng-hide 的值为 boolean 类型的表达式，当值为 true 时，对应的 show 或 hide 生效。框架会用 &lt;code&gt;display:block&lt;/code&gt; 和 &lt;code&gt;display:none&lt;/code&gt; 来控制元素的显隐。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;div ng-app=&quot;&quot;&amp;gt;
    &amp;lt;div ng-show=&quot;true&quot;&amp;gt;1&amp;lt;/div&amp;gt;
    &amp;lt;div ng-show=&quot;false&quot;&amp;gt;2&amp;lt;/div&amp;gt;
    &amp;lt;div ng-hide=&quot;true&quot;&amp;gt;3&amp;lt;/div&amp;gt;
    &amp;lt;div ng-hide=&quot;false&quot;&amp;gt;4&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;后一个 ng-switch 是根据一个值来决定哪个节点显示，其它节点移除：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;div ng-init=&quot;a=2&quot;&amp;gt;
    &amp;lt;ul ng-switch on=&quot;a&quot;&amp;gt;
          &amp;lt;li ng-switch-when=&quot;1&quot;&amp;gt;1&amp;lt;/li&amp;gt;
          &amp;lt;li ng-switch-when=&quot;2&quot;&amp;gt;2&amp;lt;/li&amp;gt;
          &amp;lt;li ng-switch-default&amp;gt;other&amp;lt;/li&amp;gt;
    &amp;lt;/ul&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;事件指令&lt;/h2&gt;

&lt;p&gt;事件相关的指令有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ng-change&lt;/li&gt;
  &lt;li&gt;ng-click&lt;/li&gt;
  &lt;li&gt;ng-dblclick&lt;/li&gt;
  &lt;li&gt;ng-mousedown&lt;/li&gt;
  &lt;li&gt;ng-mouseenter&lt;/li&gt;
  &lt;li&gt;ng-mouseleave&lt;/li&gt;
  &lt;li&gt;ng-mousemove&lt;/li&gt;
  &lt;li&gt;ng-mouseover&lt;/li&gt;
  &lt;li&gt;ng-mouseup&lt;/li&gt;
  &lt;li&gt;ng-submit&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于事件对象本身，在函数调用时可以直接使用 $event 进行传递：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;p ng-click=&quot;click($event)&quot;&amp;gt;点击&amp;lt;/p&amp;gt;
&amp;lt;p ng-click=&quot;click($event.target)&quot;&amp;gt;点击&amp;lt;/p&amp;gt;

$scope.click = function($event){
         alert($event.target);
         //……………………
}　　
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;表单指令&lt;/h2&gt;

&lt;p&gt;表单控件类的模板指令，最大的作用是它预定义了需要绑定的数据的格式。这样，就可以对于既定的数据进行既定的处理。&lt;/p&gt;

&lt;h3 id=&quot;form&quot;&gt;form&lt;/h3&gt;

&lt;p&gt;ng 对 form 这个标签作了包装，对应的指令是 ng-form。&lt;/p&gt;

&lt;p&gt;从 ng 的角度来说， form 标签，是一个模板指令，也创建了一个 &lt;code&gt;FormController&lt;/code&gt; 的实例。这个实例就提供了相应的属性和方法。同时，它里面的控件也是一个 &lt;code&gt;NgModelController&lt;/code&gt; 实例。&lt;/p&gt;

&lt;p&gt;很重要的一点， form 的相关方法要生效，必须为 form 标签指定 name 和 ng-controller ，并且每个控件都要绑定一个变量。 form 和控件的名字，即是 $scope 中的相关实例的引用变量名。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;&amp;gt;
  &amp;lt;input type=&quot;text&quot; name=&quot;a&quot; required ng-model=&quot;a&quot;  /&amp;gt;
  &amp;lt;span ng-click=&quot;see()&quot;&amp;gt;&amp;lt;/span&amp;gt;
&amp;lt;/form&amp;gt;

var TestCtrl = function($scope){
  $scope.see = function(){
    console.log($scope.test_form);
    console.log($scope.test_form.a);
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除去对象的方法与属性， form 这个标签本身有一些动态类可以使用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;ng-valid&lt;/code&gt; 当表单验证通过时的设置&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-invalid&lt;/code&gt; 当表单验证失败时的设置&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-pristine&lt;/code&gt; 表单的未被动之前拥有&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-dirty&lt;/code&gt; 表单被动过之后拥有&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;form 对象的属性有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;$pristine&lt;/code&gt; 表单是否未被动过&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;$dirty&lt;/code&gt; 表单是否被动过&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;$valid&lt;/code&gt; 表单是否验证通过&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;$invalid&lt;/code&gt; 表单是否验证失败&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;$error&lt;/code&gt; 表单的验证错误&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中的 $error 对象包含有所有字段的验证信息，及对相关字段的 NgModelController 实例的引用。它的结构是一个对象， key 是失败信息， required ， minlength 之类的， value 是对应的字段实例列表。&lt;/p&gt;

&lt;h3 id=&quot;input&quot;&gt;input&lt;/h3&gt;

&lt;p&gt;input 控件的相关可用属性为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;name&lt;/code&gt; 名字&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-model&lt;/code&gt; 绑定的数据&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;required&lt;/code&gt; 是否必填&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-required&lt;/code&gt; 是否必填&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-minlength&lt;/code&gt; 最小长度&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-maxlength&lt;/code&gt; 最大长度&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-pattern&lt;/code&gt; 匹配模式&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-change&lt;/code&gt; 值变化时的回调&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;&amp;gt;
  &amp;lt;input type=&quot;text&quot; name=&quot;a&quot; ng-model=&quot;a&quot; required ng-pattern=&quot;/abc/&quot; /&amp;gt;
  &amp;lt;span ng-click=&quot;see()&quot;&amp;gt;&amp;lt;/span&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;input 控件，它还有一些扩展，这些扩展有些有自己的属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;input type=&quot;number&quot; &lt;/code&gt;多了 number 错误类型，多了 max ， min 属性。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;input type=&quot;url&quot; &lt;/code&gt;多了 url 错误类型。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;input type=&quot;email&quot; &lt;/code&gt;多了 email 错误类型。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;checkbox&quot;&gt;checkbox&lt;/h3&gt;

&lt;p&gt;是 input 的扩展，不过，它没有验证相关的东西，只有选中与不选中两个值：ng-true-value、ng-false-value:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;&amp;gt;
  &amp;lt;input type=&quot;checkbox&quot; name=&quot;a&quot; ng-model=&quot;a&quot; ng-true-value=&quot;AA&quot; ng-false-value=&quot;BB&quot; /&amp;gt;
  &amp;lt;span&amp;gt;&amp;lt;/span&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;radio&quot;&gt;radio&lt;/h3&gt;

&lt;p&gt;也是 input 的扩展。和 checkbox 一样，但它只有一个值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;&amp;gt;
  &amp;lt;input type=&quot;radio&quot; name=&quot;a&quot; ng-model=&quot;a&quot; value=&quot;AA&quot; /&amp;gt;
  &amp;lt;input type=&quot;radio&quot; name=&quot;a&quot; ng-model=&quot;a&quot; value=&quot;BB&quot; /&amp;gt;
  &amp;lt;span&amp;gt;&amp;lt;/span&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;textarea&quot;&gt;textarea&lt;/h3&gt;

&lt;p&gt;同 input&lt;/p&gt;

&lt;h3 id=&quot;select&quot;&gt;select&lt;/h3&gt;

&lt;p&gt;它里面的一个叫做 &lt;code&gt;ng-options&lt;/code&gt; 的属性用于数据呈现。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot; ng-init=&quot;o=[0,1,2,3]; a=o[1];&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;x for x in o&quot;&amp;gt;
      &amp;lt;option value=&quot;&quot;&amp;gt;可以加这个空值&amp;lt;/option&amp;gt;
    &amp;lt;/select&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 $scope 中， select 绑定的变量，其值和普通的 value 无关，可以是一个对象：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;
    ng-init=&quot;o=[{name: &#39;AA&#39;}, {name: &#39;BB&#39;}]; a=o[1];&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;x.name for x in o&quot; /&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示与值分别指定， &lt;code&gt;x.v as x.name for x in o&lt;/code&gt; ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;
    ng-init=&quot;o=[{name: &#39;AA&#39;, v: &#39;00&#39;}, {name: &#39;BB&#39;, v: &#39;11&#39;}]; a=o[1].v;&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;x.v as x.name for x in o&quot; /&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;加入分组的， &lt;code&gt;x.name group by x.g for x in o&lt;/code&gt; ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;
    ng-init=&quot;o=[{name: &#39;AA&#39;, g: &#39;00&#39;}, {name: &#39;BB&#39;, g: &#39;11&#39;}, {name: &#39;CC&#39;, g: &#39;00&#39;}]; a=o[1];&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;x.name group by x.g for x in o&quot; /&amp;gt;
&amp;lt;/form&amp;gt;
  ```

分组了还分别指定显示与值的， x.v as x.name group by x.g for x in o ：

```html
&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot; ng-init=&quot;o=[{name: &#39;AA&#39;, g: &#39;00&#39;, v: &#39;=&#39;}, {name: &#39;BB&#39;, g: &#39;11&#39;, v: &#39;+&#39;}, {name: &#39;CC&#39;, g: &#39;00&#39;, v: &#39;!&#39;}]; a=o[1].v;&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;x.v as x.name group by x.g for x in o&quot; /&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果参数是对象的话，基本也是一样的，只是把遍历的对象改成 (key, value) ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot; ng-init=&quot;o={a: 0, b: 1}; a=o.a;&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;k for (k, v) in o&quot; /&amp;gt;
&amp;lt;/form&amp;gt;

&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;
    ng-init=&quot;o={a: {name: &#39;AA&#39;, v: &#39;00&#39;}, b: {name: &#39;BB&#39;, v: &#39;11&#39;}}; a=o.a.v;&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;v.v as v.name for (k, v) in o&quot; /&amp;gt;
&amp;lt;/form&amp;gt;

&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;
    ng-init=&quot;o={a: {name: &#39;AA&#39;, v: &#39;00&#39;, g: &#39;==&#39;}, b: {name: &#39;BB&#39;, v: &#39;11&#39;, g: &#39;==&#39;}}; a=o.a;&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;v.name group by v.g for (k, v) in o&quot; /&amp;gt;
&amp;lt;/form&amp;gt;

&amp;lt;form name=&quot;test_form&quot; ng-controller=&quot;TestCtrl&quot;
    ng-init=&quot;o={a: {name: &#39;AA&#39;, v: &#39;00&#39;, g: &#39;==&#39;}, b: {name: &#39;BB&#39;, v: &#39;11&#39;, g: &#39;==&#39;}}; a=o.a.v;&quot;&amp;gt;
    &amp;lt;select ng-model=&quot;a&quot; ng-options=&quot;v.v as v.name group by v.g for (k, v) in o&quot; /&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有一些表单控件功能相关的指令：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;ng-src&lt;/code&gt;  src 属性&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-href&lt;/code&gt;  href 属性&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-checked&lt;/code&gt; 控制 radio 和 checkbox 的选中状态&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-selected&lt;/code&gt; 控制下拉框的选中状态&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-disabled&lt;/code&gt; 控制失效状态&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-multiple&lt;/code&gt; 控制多选&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ng-readonly&lt;/code&gt; 控制只读状态&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上指令的取值均为 boolean 类型，当值为 true 时相关状态生效，道理比较简单就不多做解释。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt; 上面的这些只是单向绑定，即只是从数据到模板，不能反作用于数据。要双向绑定，还是要使用 ng-model 。&lt;/p&gt;

&lt;h1 id=&quot;angularjs--1&quot;&gt;3. AngularJS 过滤器&lt;/h1&gt;

&lt;p&gt;过滤器（filter）正如其名，作用就是接收一个输入，通过某个规则进行处理，然后返回处理后的结果。主要用在数据的格式化上，例如获取一个数组中的子集，对数组中的元素进行排序等。过滤器通常是伴随标记来使用的，将你 model 中的数据格式化为需要的格式。表单的控制功能主要涉及到数据验证以及表单控件的增强。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;内置过滤器&lt;/h2&gt;

&lt;p&gt;ng 内置了一些过滤器，它们是：&lt;br /&gt;
currency(货币)、date(日期)、filter(子串匹配)、json(格式化json对象)、limitTo(限制个数)、lowercase(小写)、uppercase(大写)、number(数字)、orderBy(排序)。&lt;/p&gt;

&lt;p&gt;过滤器使用示例：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// 使用currency可以将数字格式化为货币，默认是美元符号，你可以自己传入所需的符号&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;currency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;￥&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// 参数用来指定所要的格式，y M d h m s E 分别表示 年 月 日 时 分 秒 星期，你可以自由组合它们。也可以使用不同的个数来限制格式化的位数。另外参数也可以使用特定的描述性字符串，例如“shortTime”将会把时间格式为12:05 pm这样的。&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;yyyy-MM-dd hh:mm:ss EEEE&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;}{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;a&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// filter 过滤器从数组中选择一个子集：&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;a&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//匹配属性值中含有a的&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;//匹配属性值中含有4的&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;i&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//参数是对象，匹配name属性中含有i的&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;//参数是函数　&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// json过滤器可以把一个js对象格式化为json字符串&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jsonTest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// 列表截取 limitTo ，支持正负数&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;limitTo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;// number过滤器可以为一个数字加上千位分割，像这样，123,456,789。同时接收一个参数，可以指定float类型保留几位小数：&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// 大小写 lowercase ， uppercase ：&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;abc&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;uppercase&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Abc&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;lowercase&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;orderBy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;age&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//按age属性值进行排序，若是-age，则倒序&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;orderBy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;orderFunc&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;//按照函数的返回值进行排序&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;childrenArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;orderBy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;//如果age相同，按照name进行排序&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;section-6&quot;&gt;过滤器使用方式&lt;/h2&gt;

&lt;p&gt;在模块中定义过滤器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var app = angular.module(&#39;Demo&#39;, [], angular.noop);
  app.filter(&#39;map&#39;, function(){
    var filter = function(input){
      return input + &#39;...&#39;;
    };
    return filter;
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在模板中使用：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;示例数据: {{ a | map }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1 id=&quot;angularjs-ajax&quot;&gt;4. AngularJS Ajax&lt;/h1&gt;

&lt;p&gt;$http 服务是 AngularJS 的核心服务之一，它帮助我们通过 XMLHttpRequest 对象或 JSONP 与远程 HTTP 服务进行交流。&lt;/p&gt;

&lt;p&gt;$http 服务是这样一个函数：它接受一个设置对象，其中指定了如何创建 HTTP 请求；它将返回一个 promise 对象，其中提供两个方法： success 方法和 error方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;$http.get({url:&quot;/xxx.action&quot;}).success(function(data){
    alert(data);
}).error(function(){
    alert(&quot;error&quot;);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$http 接受的配置项有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;method&lt;/code&gt; 方法&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;url&lt;/code&gt; 路径&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;params&lt;/code&gt; GET请求的参数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;data&lt;/code&gt; post请求的参数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;headers&lt;/code&gt; 头&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;transformRequest&lt;/code&gt; 请求预处理函数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;transformResponse&lt;/code&gt; 响应预处理函数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cache&lt;/code&gt; 缓存&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;timeout&lt;/code&gt; 超时毫秒，超时的请求会被取消&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;withCredentials&lt;/code&gt; 跨域安全策略的一个东西&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中的 transformRequest 和 transformResponse 及 headers 已经有定义的，如果自定义则会覆盖默认定义&lt;/p&gt;

&lt;p&gt;对于几个标准的 HTTP 方法，有对应的 shortcut ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;$http.delete(url, config)
$http.get(url, config)
$http.head(url, config)
$http.jsonp(url, config)
$http.post(url, data, config)
$http.put(url, data, config)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意其中的 JSONP 方法，在实现上会在页面中添加一个 script 标签，然后放出一个 GET 请求。你自己定义的，匿名回调函数，会被 ng 自已给一个全局变量。在定义请求，作为 GET 参数，你可以使用 &lt;code&gt;JSON_CALLBACK&lt;/code&gt; 这个字符串来暂时代替回调函数名，之后 ng 会为你替换成真正的函数名：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var p = $http({
    method: &#39;JSONP&#39;,
    url: &#39;/json&#39;,
    params: {callback: &#39;JSON_CALLBACK&#39;}
});
p.success(function(response, status, headers, config){
    console.log(response);
    $scope.name = response.name;
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$http 有两个属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;defaults&lt;/code&gt; 请求的全局配置&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pendingRequests&lt;/code&gt; 当前的请求队列状态&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;$http.defaults.transformRequest = function(data){
    console.log(&#39;here&#39;); return data;}

console.log($http.pendingRequests);
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;angularjs--2&quot;&gt;5. AngularJS 控制器&lt;/h1&gt;

&lt;p&gt;AngularJS 应用程序被控制器控制，控制器是 JavaScript 对象，由标准的 JavaScript 对象的构造函数 创建。&lt;/p&gt;

&lt;p&gt;ng-controller 指令定义了应用程序控制器，给所在的 DOM 元素创建了一个新的 $scope 对象，并将这个 $scope 对象包含进外层 DOM 元素的 $scope 对象里。&lt;/p&gt;

&lt;p&gt;示例如下：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ng-app=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ng-controller=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;personController&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;

名： &lt;span class=&quot;nt&quot;&gt;&amp;lt;input&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;text&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ng-model=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;person.firstName&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;br&amp;gt;&lt;/span&gt;
姓： &lt;span class=&quot;nt&quot;&gt;&amp;lt;input&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;text&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ng-model=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;person.lastName&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;br&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;br&amp;gt;&lt;/span&gt;
姓名： {{fullName()}}

&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;personController&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;person&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;firstName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;John&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;lastName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Doe&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
     &lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fullName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
         &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
         &lt;span class=&quot;nx&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;person&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; 
         &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;firstName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lastName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
     &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;$scope 是一个把 view（一个DOM元素）连结到 controller 上的对象。在我们的 MVC 结构里，这个 $scope 将成为 model，它提供一个绑定到DOM元素（以及其子元素）上的excecution context。&lt;/p&gt;

&lt;p&gt;尽管听起来有点复杂，但 $scope 实际上就是一个JavaScript 对象，controller 和 view 都可以访问它，所以我们可以利用它在两者间传递信息。在这个 $scope 对象里，我们既存储数据，又存储将要运行在view上的函数。&lt;/p&gt;

&lt;p&gt;每一个 Angular 应用都会有一个 $rootScope。这个 $rootScope 是最顶级的 scope，它对应着含有 ng-app 指令属性的那个 DOM 元素&lt;/p&gt;

&lt;p&gt;所有scope都遵循原型继承（prototypal inheritance），这意味着它们都能访问父scope们。&lt;/p&gt;

&lt;h1 id=&quot;angularjs--3&quot;&gt;6. AngularJS 模块&lt;/h1&gt;

&lt;p&gt;AngularJS 本身的一个默认模块叫做 ng ，它提供了 $http ， $q 等等服务。&lt;/p&gt;

&lt;p&gt;服务只是模块提供的多种机制中的一种，其它的还有命令（ directive ），过滤器（ filter ），及其它配置信息。&lt;/p&gt;

&lt;p&gt;然后在额外的 js 文件中有一个附加的模块叫做 ngResource ， 它提供了一个 $resource 服务。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;定义模块&lt;/h2&gt;

&lt;p&gt;定义模块的方法是使用 angular.module 。调用时声明了对其它模块的依赖，并定义了“初始化”函数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;  var my_module = angular.module(&#39;MyModule&#39;, [], function(){
      console.log(&#39;here&#39;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段代码定义了一个叫做 MyModule 的模块， my_module 这个引用可以在接下来做其它的一些事，比如定义服务。&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;定义服务&lt;/h2&gt;

&lt;p&gt;ng的服务是这样定义的：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Angular services are singletons objects or functions that carry out specific tasks common to web apps.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;它是一个单例对象或函数，对外提供特定的功能。&lt;/li&gt;
  &lt;li&gt;首先是一个单例，即无论这个服务被注入到任何地方，对象始终只有一个实例。&lt;/li&gt;
  &lt;li&gt;其次这与我们自己定义一个function然后在其他地方调用不同，因为服务被定义在一个模块中，所以其使用范围是可以被我们管理的。ng的避免全局变量污染意识非常强。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ng提供了很多内置的服务，可以到API中查看 &lt;a href=&quot;http://docs.angularjs.org/api/&quot;&gt;http://docs.angularjs.org/api/&lt;/a&gt;。如同指令一样，系统内置的服务以$开头，我们也可以自己定义一个服务。&lt;/p&gt;

&lt;p&gt;在这里呢，就要先介绍一下叫 provider 的东西。简单来说， provider 是被 &lt;code&gt;注入控制器&lt;/code&gt; 使用的一个对象，注入机制通过调用一个 provider 的 $get() 方法，把得到的东西作为参数进行相关调用（比如把得到的服务作为一个 Controller 的参数）。&lt;/p&gt;

&lt;p&gt;在这里 &lt;code&gt;服务&lt;/code&gt; 的概念就比较不明确，对使用而言，服务仅指 $get() 方法返回的东西，但是在整体机制上，服务又要指提供了 $get() 方法的整个对象。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;  //这是一个provider
  var pp = function(){
    this.$get = function(){
      return {&#39;haha&#39;: &#39;123&#39;};
    }
  }
  
  //我在模块的初始化过程当中, 定义了一个叫 PP 的服务
  var app = angular.module(&#39;Demo&#39;, [], function($provide){
    $provide.provider(&#39;PP&#39;, pp);
  });
  
  //PP服务实际上就是 pp 这个 provider 的 $get() 方法返回的东西
  app.controller(&#39;TestCtrl&#39;,
    function($scope, PP){
      console.log(PP);
    }
  );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ng 还有相关的 shortcut，&lt;strong&gt;第一个是 factory 方法&lt;/strong&gt;，由 $provide 提供， module 的 factory 是一个引用，作用一样。这个方法直接把一个函数当成是一个对象的 $get() 方法，这样你就不用显式地定义一个 provider 了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var app = angular.module(&#39;Demo&#39;, [], function($provide){
    $provide.factory(&#39;PP&#39;, function(){
        return {&#39;hello&#39;: &#39;123&#39;};
    });
});
app.controller(&#39;TestCtrl&#39;, function($scope, PP){ console.log(PP) });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 module 中使用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var app = angular.module(&#39;Demo&#39;, [], function(){ });
app.factory(&#39;PP&#39;, function(){return {&#39;abc&#39;: &#39;123&#39;}});
app.controller(&#39;TestCtrl&#39;, function($scope, PP){ console.log(PP) });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;第二个是 service 方法&lt;/strong&gt;，也是由 $provide 提供， module 中有对它的同名引用。 service 和 factory 的区别在于，前者是要求提供一个“构造方法”，后者是要求提供 $get() 方法。意思就是，前者一定是得到一个 object ，后者可以是一个数字或字符串。它们的关系大概是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var app = angular.module(&#39;Demo&#39;, [], function(){ });
app.service = function(name, constructor){
    app.factory(name, function(){
      return (new constructor());
    });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;service 方法的使用就很简单了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var app = angular.module(&#39;Demo&#39;, [], function(){ });
app.service(&#39;PP&#39;, function(){
    this.abc = &#39;123&#39;;
});
app.controller(&#39;TestCtrl&#39;, function($scope, PP){ console.log(PP) });
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-9&quot;&gt;引入模块并使用服务&lt;/h2&gt;

&lt;p&gt;结合上面的 &lt;code&gt;定义模块&lt;/code&gt; 和 &lt;code&gt;定义服务&lt;/code&gt;，我们可以方便地组织自己的额外代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// 定义服务
angular.module(&#39;MyModule&#39;, [], function($provide){
    $provide.factory(&#39;S1&#39;, function(){
      return &#39;I am S1&#39;;
    });
    $provide.factory(&#39;S2&#39;, function(){
      return {see: function(){return &#39;I am S2&#39;}}
    });
});

// 调用服务
var app = angular.module(&#39;Demo&#39;, [&#39;MyModule&#39;], angular.noop);
app.controller(&#39;TestCtrl&#39;, function($scope, S1, S2){
    console.log(S1)
    console.log(S2.see())
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;ngresource&quot;&gt;7. 附加模块 ngResource&lt;/h1&gt;

&lt;p&gt;ngResource 这个是 ng 官方提供的一个附加模块。&lt;code&gt;附加&lt;/code&gt; 的意思就是，如果你打算用它，那么你需要引入一人单独的 js 文件，然后在声明“根模块”时注明依赖的 ngResource 模块，接着就可以使用它提供的 $resource 服务了。&lt;/p&gt;

&lt;p&gt;$resource 服务主要是包装了 AJAX  的调用，使用 $resource 需要先定义“资源”，也就是先定义一些 HTTP 请求。&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h1 id=&quot;angularjs--4&quot;&gt;8. AngularJS 路由&lt;/h1&gt;

&lt;p&gt;ng的路由(ngRoute)是一个单独的模块，包含以下内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;服务 &lt;code&gt;$routeProvider&lt;/code&gt; 用来定义一个路由表，即地址栏与视图模板的映射&lt;/li&gt;
  &lt;li&gt;服务 &lt;code&gt;$routeParams&lt;/code&gt; 保存了地址栏中的参数，例如 &lt;code&gt;{id : 1, name : &#39;tom&#39;}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;服务 &lt;code&gt;$route&lt;/code&gt; 完成路由匹配，并且提供路由相关的属性访问及事件，如访问当前路由对应的 controller&lt;/li&gt;
  &lt;li&gt;指令 &lt;code&gt;ngView&lt;/code&gt; 用来在主视图中指定加载子视图的区域&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-10&quot;&gt;定义路由&lt;/h2&gt;

&lt;p&gt;第一步：引入文件和依赖&lt;/p&gt;

&lt;p&gt;ngRoute 也是一个附件模块，故需要在页面上引入 &lt;code&gt;angular-route.min.js&lt;/code&gt; 并在模块中注入对 ngRoute 的依赖，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var app = angular.module(&#39;MyApp&#39;, [&#39;ngRoute&#39;]);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二步：定义路由表&lt;/p&gt;

&lt;p&gt;$routeProvider 提供了定义路由表的服务，它有两个核心方法：&lt;code&gt;when(path,route)&lt;/code&gt; 和 &lt;code&gt;otherwise(params)&lt;/code&gt;，先看一下核心中的核心 when(path,route)方法。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;when(path,route)&lt;/code&gt; 方法接收两个参数:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;path 是一个 string 类型，表示该条路由规则所匹配的路径，它将与地址栏的内容($location.path)值进行匹配。如果需要匹配参数，可以在 path 中使用冒号加名称的方式，如：path为 &lt;code&gt;/show/:name&lt;/code&gt;，如果地址栏是 &lt;code&gt;/show/tom&lt;/code&gt; ，那么参数 name 和所对应的值 tom 便会被保存在 $routeParams 中，像这样：&lt;code&gt;{name : tom}&lt;/code&gt;。我们也可以用 &lt;code&gt;*&lt;/code&gt; 进行模糊匹配，如：&lt;code&gt;/show*/:name&lt;/code&gt; 将匹配&lt;code&gt;/showInfo/tom&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;route 参数是一个 object，用来指定当 path 匹配后所需的一系列配置项，包括以下内容：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;controller&lt;/code&gt; function 或 string 类型。在当前模板上执行的 controller 函数，生成新的 scope；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;controllerAs&lt;/code&gt; string 类型，为 controller 指定别名；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;template&lt;/code&gt; string 或 function 类型，视图所用的模板，这部分内容将被 ngView 引用；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;templateUrl&lt;/code&gt; string 或 function 类型，当视图模板为单独的 html 文件或是使用了 &lt;code&gt;&amp;lt;script type=&quot;text/ng-template&quot;&amp;gt;&lt;/code&gt;定义模板时使用；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;resolve&lt;/code&gt; 指定当前 controller 所依赖的其他模块；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;redirectTo&lt;/code&gt; 重定向的地址。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第三步：在主视图模板中指定加载子视图的位置&lt;/p&gt;

&lt;p&gt;只需在模板中简单的使用此 ngView 指令:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;div ng-view&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-11&quot;&gt;9. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.w3cschool.cc/angularjs/angularjs-tutorial.html&quot;&gt;AngularJS 教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/yy374864125/article/details/41349417&quot;&gt;angularjs学习总结 详细教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zouyesheng.com/angular.html#toc34&quot;&gt;AngularJS学习笔记&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一些资料：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mgechev/angularjs-style-guide/blob/master/README-zh-cn.md&quot;&gt;AngularJS最佳实践和风格指南&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/peiransun/angularjs-cn&quot;&gt;AngularJS中译本&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zensh/AngularjsTutorial_cn&quot;&gt;AngularJS入门教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/xufei/Make-Your-Own-AngularJS/blob/master/01.md&quot;&gt;构建自己的AngularJS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.waylau.com/build-angularjs-app-with-yeoman-in-windows/&quot;&gt;在Windows环境下用Yeoman构建AngularJS项目&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/01/08/basic-concepts-of-angularjs.html</link>
      <guid>http://blog.javachen.com/2015/01/08/basic-concepts-of-angularjs.html</guid>
      <pubDate>2015-01-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Gradle构建多模块项目</title>
      <description>&lt;p&gt;废话不多说，直接进入主题。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 创建项目&lt;/h1&gt;

&lt;p&gt;首先创建项目，名称为 test：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir test &amp;amp;&amp;amp; cd test
gradle init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候的项目结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  test  tree
.
├── build.gradle
├── gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
└── settings.gradle

2 directories, 6 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，创建多个模块，这里以 core 和 web 模块为例，先创建两个目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p core/src/main/java
mkdir -p core/src/test/java
mkdir -p web/src/main/java
mkdir -p web/src/test/java
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候的项目结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  test  tree
.
├── build.gradle
├── core
│   └── src
│       ├── main
│       │   └── java
│       └── test
│           └── java
├── gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
├── settings.gradle
└── web
    └── src
        ├── main
        │   └── java
        └── test
            └── java

14 directories, 6 files
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 修改配置&lt;/h1&gt;

&lt;p&gt;接下来修改根目录下的 settings.gradle 文件，引入子模块：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;include &#39;core&#39;,&#39;web&#39;  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改根目录下的 build.gradle：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;
// 所有子项目的通用配置
subprojects { 
    apply plugin: &#39;java&#39;
    apply plugin: &#39;eclipse&#39;
    apply plugin: &#39;idea&#39;

    version = &#39;1.0&#39;  

    // JVM 版本号要求
    sourceCompatibility = 1.7
    targetCompatibility = 1.7  

    // java编译的时候缺省状态下会因为中文字符而失败
    [compileJava,compileTestJava,javadoc]*.options*.encoding = &#39;UTF-8&#39;
    
    //定义版本号
    ext {  
        springVersion = &#39;3.2.11.RELEASE&#39;  
        hibernateVersion=&#39;4.3.1.Final&#39;  
    } 

    repositories {
        mavenCentral()
    }

    jar {
        manifest {
            attributes(&quot;Implementation-Title&quot;: &quot;Gradle&quot;)
        }
    }

    configurations {
        // 所有需要忽略的包定义在此
        all*.exclude group: &#39;commons-httpclient&#39;
        all*.exclude group: &#39;commons-logging&#39;
        all*.exclude group: &#39;commons-beanutils&#39;, module: &#39;commons-beanutils&#39;
    }

    dependencies {
        // 通用依赖
        compile(  
                &quot;org.springframework:spring-context:$springVersion&quot;,  
                &quot;org.springframework:spring-orm:$springVersion&quot;,  
                &quot;org.springframework:spring-tx:$springVersion&quot;,  
                &quot;org.springframework.data:spring-data-jpa:1.5.2.RELEASE&quot;,  
                &quot;org.hibernate:hibernate-entitymanager:$hibernateVersion&quot;,  
                &quot;c3p0:c3p0:0.9.1.2&quot;,  
                &quot;mysql:mysql-connector-java:5.1.26&quot;,  
                &quot;org.slf4j:slf4j-nop:1.7.6&quot;,  
                &quot;commons-fileupload:commons-fileupload:1.3.1&quot;,  
                &quot;com.fasterxml.jackson.core:jackson-databind:2.3.1&quot;  
        )

        // 依赖maven中不存在的jar
        ext.jarTree = fileTree(dir: &#39;libs&#39;, include: &#39;**/*.jar&#39;)
        ext.rootProjectLibs = new File(rootProject.rootDir, &#39;libs&#39;).getAbsolutePath()
        ext.jarTree += fileTree(dir: rootProjectLibs, include: &#39;**/*.jar&#39;)

        compile jarTree

        // 测试依赖
        testCompile(  
                &quot;org.springframework:spring-test:$springVersion&quot;,  
                &quot;junit:junit:4.11&quot;  
        ) 
    }

    // 显示当前项目下所有用于 compile 的 jar.
    task listJars(description: &#39;Display all compile jars.&#39;) &amp;lt;&amp;lt; {
        configurations.compile.each { File file -&amp;gt; println file.name }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来可以修改 core/build.gradle 来定义 core 模块的依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;// jar包的名字
archivesBaseName = &#39;core&#39;

// 还可以定义其他配置，这里直接继承父模块中的配置
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;web 模块需要依赖 core 模块，故定义 web/build.gradle 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin:&quot;war&quot;  

dependencies{  
    // 依赖 core 模块
    compile project(&quot;:core&quot;)  
    compile(  
            &quot;org.springframework:spring-webmvc:$springVersion&quot;,  
            &quot;org.apache.taglibs:taglibs-standard-impl:1.2.1&quot;  
    )  
    //系统提供的依赖
    providedCompile(  
            &quot;javax.servlet:javax.servlet-api:3.1.0&quot;,  
            &quot;javax.servlet.jsp:jsp-api:2.2.1-b03&quot;,  
            &quot;javax.servlet.jsp.jstl:javax.servlet.jsp.jstl-api:1.2.1&quot;  
    )  
}  

task jarWithoutResources(type: Jar) {  
    baseName project.name  
    from(&quot;$buildDir/classes/main&quot;)  
}  

war{  
    dependsOn jarWithoutResources  
    from(&quot;$projectDir/src/main/resources&quot;) {  
        include &quot;*.properties&quot;  
        into(&quot;WEB-INF/classes&quot;)  
    }  
    classpath=classpath - sourceSets.main.output  
    classpath fileTree(dir:libsDir, include:&quot;${project.name}-${version}.jar&quot;)  
}  
task(&#39;jarPath&#39;)&amp;lt;&amp;lt;{  
    configurations.runtime.resolve().each {  
        print it.toString()+&quot;;&quot;  
    }  
    println();  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 编译项目&lt;/h1&gt;

&lt;p&gt;查看所有 jar：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gradle listJars
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看各个模块的依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gradle :core:dependencies
$ gradle :web:dependencies
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译所有模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gradle build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对比一下，这时候的目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  test  tree
.
├── build.gradle
├── core
│   ├── build
│   │   ├── libs
│   │   │   └── core-1.0.jar
│   │   └── tmp
│   │       └── jar
│   │           └── MANIFEST.MF
│   ├── build.gradle
│   └── src
│       ├── main
│       │   └── java
│       └── test
│           └── java
├── gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
├── settings.gradle
└── web
    ├── build
    │   ├── libs
    │   │   ├── web-1.0.jar
    │   │   └── web-1.0.war
    │   └── tmp
    │       ├── jarWithoutResources
    │       │   └── MANIFEST.MF
    │       └── war
    │           └── MANIFEST.MF
    ├── build.gradle
    └── src
        ├── main
        │   └── java
        └── test
            └── java

23 directories, 14 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，core和web模块都是gradle项目了，你也可以单独编译某一个模块，例如，编译core模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd core
$ rm -rf build
$ gradle build
$ tree
.
├── build
│   ├── libs
│   │   └── core-1.0.jar
│   └── tmp
│       └── jar
│           └── MANIFEST.MF
├── build.gradle
└── src
    ├── main
    │   └── java
    └── test
        └── java

9 directories, 3 files
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4. 一些小技巧&lt;/h1&gt;

&lt;h2 id=&quot;gradle-dependencies&quot;&gt;1. 善用 gradle dependencies&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;gradle dependencies &amp;gt; depend.log
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;java-&quot;&gt;2. java 编译时候报编码错误&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;[compileJava,compileTestJava,javadoc]*.options*.encoding = &#39;UTF-8&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;gradle-&quot;&gt;3. 忽略掉 .gradle 目录&lt;/h2&gt;

&lt;p&gt;修改 .gitignore 忽略该目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*.sw?
.#*
*#
*~
.classpath
.project
.settings
bin
build
target
dependency-reduced-pom.xml
*.sublime-*
/scratch
.gradle
README.html
.idea
*.iml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;maven--jar-&quot;&gt;4. Maven 库中没有的 jar 该怎么管理&lt;/h2&gt;

&lt;p&gt;在顶级目录增加一个 libs 文件夹，这个文件夹里面的 jar 是对所有项目都起作用的。&lt;/p&gt;

&lt;p&gt;如果是某个项目自用的，则可以在该项目的 source 下面创建个 libs，具体实现是在顶级目录下的 build.gradle 中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;ext.jarTree = fileTree(dir: &#39;libs&#39;, include: &#39;**/*.jar&#39;)
ext.rootProjectLibs = new File(rootProject.rootDir, &#39;libs&#39;).getAbsolutePath()
ext.jarTree += fileTree(dir: rootProjectLibs, include: &#39;**/*.jar&#39;)

compile jarTree
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;jar-&quot;&gt;5. jar 包定义外移&lt;/h2&gt;

&lt;p&gt;暂时还没有这样的需求，详细说明请参考 &lt;a href=&quot;https://github.com/someok/gradle-multi-project-example/blob/master/readme.md#jar-%E5%8C%85%E5%AE%9A%E4%B9%89%E5%A4%96%E7%A7%BB&quot;&gt;jar 包定义外移&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;build-&quot;&gt;6. 如何指定 build 输出目录和版本号&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;buildDir = &quot;target&quot;
version = &#39;1.0&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;gradle--1&quot;&gt;7. 在执行 Gradle 命令时如何指定参数&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;gradle task -P profile=development
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;gradle--idea-javadoc&quot;&gt;8. Gradle 和 idea 集成时如何不自动下载依赖源码和javadoc&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;idea {
    module {
        downloadJavadoc = false
        downloadSources = false
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/xiejx618/article/details/38469865&quot;&gt;gradle多模块开发&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/someok/gradle-multi-project-example/blob/master/readme.md&quot;&gt;Gradle 多项目管理示例&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/xguo/p/3175377.html&quot;&gt;构建工具之 - Gradle一般使用常见问答&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2015/01/07/build-multi-module-project-with-gradle.html</link>
      <guid>http://blog.javachen.com/2015/01/07/build-multi-module-project-with-gradle.html</guid>
      <pubDate>2015-01-07T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Spring Boot和Gradle创建AngularJS项目</title>
      <description>&lt;p&gt;&lt;a href=&quot;http://projects.spring.io/spring-boot&quot;&gt;Spring Boot&lt;/a&gt; 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。&lt;/p&gt;

&lt;p&gt;本文主要是记录使用 Spring Boot 和 Gradle 创建项目的过程，其中会包括 Spring Boot 的安装及使用方法，希望通过这篇文章能够快速搭建一个项目。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 开发环境&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统: mac&lt;/li&gt;
  &lt;li&gt;JDK：1.7.0_60&lt;/li&gt;
  &lt;li&gt;Gradle：2.2.1&lt;/li&gt;
  &lt;li&gt;IDE：Idea&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 创建项目&lt;/h1&gt;

&lt;p&gt;你可以通过 &lt;a href=&quot;http://start.spring.io/&quot;&gt;Spring Initializr&lt;/a&gt; 来创建一个空的项目，也可以手动创建，这里我使用的是手动创建 gradle 项目。&lt;/p&gt;

&lt;p&gt;参考 &lt;a href=&quot;/2014/09/15/build-project-with-gradle.html&quot;&gt;使用Gradle构建项目&lt;/a&gt; 创建一个 ng-spring-boot 项目，执行的命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir ng-spring-boot &amp;amp;&amp;amp; cd ng-spring-boot
$ gradle init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ng-spring-boot 目录结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  ng-spring-boot  tree
.
├── build.gradle
├── gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
└── settings.gradle

2 directories, 6 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后修改 build.gradle 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;buildscript {
    ext {
        springBootVersion = &#39;1.2.2.RELEASE&#39;
    }
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}&quot;)
    }
}

apply plugin: &#39;java&#39;
apply plugin: &#39;eclipse&#39;
apply plugin: &#39;idea&#39;
apply plugin: &#39;spring-boot&#39;

jar {
    baseName = &#39;ng-spring-boot&#39;
    version = &#39;0.0.1-SNAPSHOT&#39;
}
sourceCompatibility = 1.7
targetCompatibility = 1.7

repositories {
    mavenCentral()
    maven { url &quot;https://repo.spring.io/libs-release&quot; }
}

dependencies {
    compile(&quot;org.springframework.boot:spring-boot-starter-data-jpa&quot;)
    compile(&quot;org.springframework.boot:spring-boot-starter-web&quot;)
    compile(&quot;org.springframework.boot:spring-boot-starter-actuator&quot;)
    runtime(&quot;org.hsqldb:hsqldb&quot;)
    testCompile(&quot;org.springframework.boot:spring-boot-starter-test&quot;)
}

eclipse {
    classpath {
         containers.remove(&#39;org.eclipse.jdt.launching.JRE_CONTAINER&#39;)
         containers &#39;org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/JavaSE-1.7&#39;
    }
}

task wrapper(type: Wrapper) {
    gradleVersion = &#39;2.3&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 spring-boot-gradle-plugin 插件可以提供一些创建可执行 jar 和从源码运行项目的任务，它还提供了 &lt;code&gt;ResolutionStrategy&lt;/code&gt; 以方便依赖中不用写版本号。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 创建一个可执行的类&lt;/h1&gt;

&lt;p&gt;首先，新建一个符合 Maven 规范的目录结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p src/main/java/com/javachen
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个 Sping boot 启动类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.javachen;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;

@Configuration
@EnableAutoConfiguration
@ComponentScan
public class Application {

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;main 方法使用了 SpringApplication 工具类。这将告诉Spring去读取 Application 的元信息，并在Spring的应用上下文作为一个组件被管理。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;@Configuration&lt;/code&gt; 注解告诉 spring 该类定义了 application context 的 bean 的一些配置。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;@ComponentScan&lt;/code&gt; 注解告诉 Spring 遍历带有 &lt;code&gt;@Component&lt;/code&gt; 注解的类。这将保证 Spring 能找到并注册 GreetingController，因为它被 &lt;code&gt;@RestController&lt;/code&gt; 标记，这也是 &lt;code&gt;@Component&lt;/code&gt; 的一种。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;@EnableAutoConfiguration&lt;/code&gt; 注解会基于你的类加载路径的内容切换合理的默认行为。比如，因为应用要依赖内嵌版本的 tomcat，所以一个tomcat服务器会被启动并代替你进行合理的配置。再比如，因为应用要依赖 Spring 的 MVC 框架,一个 Spring MVC 的 DispatcherServlet 将被配置并注册，并且不再需要 web.xml 文件。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;你还可以添加 &lt;code&gt;@EnableWebMvc&lt;/code&gt; 注解配置 Spring Mvc。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面三个注解还可以用 @SpringBootApplication 代替：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.javachen.examples.springboot;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication // same as @Configuration @EnableAutoConfiguration @ComponentScan
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以修改该类的 main 方法，获取 ApplicationContext：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.javachen;

import java.util.Arrays;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.context.ApplicationContext;
import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;

@SpringBootApplication 
public class Application {

    public static void main(String[] args) {
        ApplicationContext ctx = SpringApplication.run(Application.class, args);

        System.out.println(&quot;Let&#39;s inspect the beans provided by Spring Boot:&quot;);

        String[] beanNames = ctx.getBeanDefinitionNames();
        Arrays.sort(beanNames);
        for (String beanName : beanNames) {
            System.out.println(beanName);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4. 创建一个实体类&lt;/h1&gt;

&lt;p&gt;创建一个实体类 src/main/java/com/javachen/model/Item.java：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.javachen.model;

import javax.persistence.*;


@Entity
public class Item {
  @Id
  @GeneratedValue(strategy=GenerationType.IDENTITY)
  private Integer id;
  @Column
  private boolean checked;
  @Column
  private String description;

  public Integer getId() {
    return id;
  }

  public void setId(Integer id) {
    this.id = id;
  }

  public boolean isChecked() {
    return checked;
  }

  public void setChecked(boolean checked) {
    this.checked = checked;
  }

  public String getDescription() {
    return description;
  }

  public void setDescription(String description) {
    this.description = description;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5. 创建控制类&lt;/h1&gt;

&lt;p&gt;创建一个 Restfull 的控制类，该类主要提供增删改查的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.javachen.controller;

import com.javachen.model.Item;
import com.javachen.repository.ItemRepository;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.dao.EmptyResultDataAccessException;
import org.springframework.http.HttpStatus;
import org.springframework.web.bind.annotation.*;

import javax.persistence.EntityNotFoundException;
import java.util.List;

@RestController
@RequestMapping(&quot;/items&quot;)
public class ItemController {
  @Autowired
  private ItemRepository repo;

  @RequestMapping(method = RequestMethod.GET)
  public List&amp;lt;Item&amp;gt; findItems() {
    return repo.findAll();
  }

  @RequestMapping(method = RequestMethod.POST)
  public Item addItem(@RequestBody Item item) {
    item.setId(null);
    return repo.saveAndFlush(item);
  }

  @RequestMapping(value = &quot;/{id}&quot;, method = RequestMethod.PUT)
  public Item updateItem(@RequestBody Item updatedItem, @PathVariable Integer id) {
    Item item = repo.getOne(id);
    item.setChecked(updatedItem.isChecked());
    item.setDescription(updatedItem.getDescription());
    return repo.saveAndFlush(item);
  }

  @RequestMapping(value = &quot;/{id}&quot;, method = RequestMethod.DELETE)
  @ResponseStatus(value = HttpStatus.NO_CONTENT)
  public void deleteItem(@PathVariable Integer id) {
    repo.delete(id);
  }

  @ResponseStatus(HttpStatus.BAD_REQUEST)
  @ExceptionHandler(value = { EmptyResultDataAccessException.class, EntityNotFoundException.class })
  public void handleNotFound() { }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Greeting 对象会被转换成 JSON 字符串，这得益于 Spring 的 HTTP 消息转换支持，你不必人工处理。由于 Jackson2 在 classpath 里，Spring的 &lt;code&gt;MappingJackson2HttpMessageConverter&lt;/code&gt; 会自动完成这一工作。&lt;/p&gt;

&lt;p&gt;这段代码使用 Spring4 新的注解：&lt;code&gt;@RestController&lt;/code&gt;，表明该类的每个方法返回对象而不是视图。它实际就是 &lt;code&gt;@Controller&lt;/code&gt; 和 &lt;code&gt;@ResponseBody&lt;/code&gt; 混合使用的简写方法。&lt;/p&gt;

&lt;h1 id=&quot;jpa-&quot;&gt;6. 创建 JPA 仓库&lt;/h1&gt;

&lt;p&gt;使用 JAP 来持久化数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.javachen.repository;

import com.javachen.model.Item;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;

import java.util.List;

public interface ItemRepository extends JpaRepository&amp;lt;Item, Integer&amp;gt; {

  @Query(&quot;SELECT i FROM Item i WHERE i.checked=true&quot;)
  List&amp;lt;Item&amp;gt; findChecked();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spring Boot 可以自动配置嵌入式的数据库，包括  H2、HSQL 和 Derby，你不需要配置数据库链接的 url，只需要添加相关的依赖即可。另外，你还需要依赖 spring-jdbc，在本例中，我们是引入了对 spring-boot-starter-data-jpa 的依赖。如果你想使用其他类型的数据库，则需要配置 &lt;code&gt;spring.datasource.*&lt;/code&gt; 属性，一个示例是在 application.properties 中配置如下属性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;spring.datasource.url=jdbc:mysql://localhost/test
spring.datasource.username=dbuser
spring.datasource.password=dbpass
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 src/main/resources/application.properties 文件，修改 JPA 相关配置，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;spring.jpa.hibernate.ddl-auto=create-drop
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;/p&gt;

  &lt;p&gt;SpringApplication 会在以下路径查找 application.properties 并加载该文件：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;/config 目录下&lt;/li&gt;
    &lt;li&gt;当前目录&lt;/li&gt;
    &lt;li&gt;classpath 中 /config 包下&lt;/li&gt;
    &lt;li&gt;classpath 根路径下&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section-5&quot;&gt;7. 运行项目&lt;/h1&gt;

&lt;p&gt;可以在项目根路径直接运行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export JAVA_OPTS=-Xmx1024m -XX:MaxPermSize=128M -Djava.security.egd=file:/dev/./urandom

$ ./gradlew bootRun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以先 build 生成一个 jar 文件，然后执行该 jar 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./gradlew build &amp;amp;&amp;amp; java -jar build/libs/ng-spring-boot-0.0.1-SNAPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动过程中你会看到如下内容，这部分内容是在 Application 类中打印出来的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Let&#39;s inspect the beans provided by Spring Boot:
application
beanNameHandlerMapping
defaultServletHandlerMapping
dispatcherServlet
embeddedServletContainerCustomizerBeanPostProcessor
handlerExceptionResolver
helloController
httpRequestHandlerAdapter
messageSource
mvcContentNegotiationManager
mvcConversionService
mvcValidator
org.springframework.boot.autoconfigure.MessageSourceAutoConfiguration
org.springframework.boot.autoconfigure.PropertyPlaceholderAutoConfiguration
org.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration
org.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration$DispatcherServletConfiguration
org.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration$EmbeddedTomcat
org.springframework.boot.autoconfigure.web.ServerPropertiesAutoConfiguration
org.springframework.boot.context.embedded.properties.ServerProperties
org.springframework.context.annotation.ConfigurationClassPostProcessor.enhancedConfigurationProcessor
org.springframework.context.annotation.ConfigurationClassPostProcessor.importAwareProcessor
org.springframework.context.annotation.internalAutowiredAnnotationProcessor
org.springframework.context.annotation.internalCommonAnnotationProcessor
org.springframework.context.annotation.internalConfigurationAnnotationProcessor
org.springframework.context.annotation.internalRequiredAnnotationProcessor
org.springframework.web.servlet.config.annotation.DelegatingWebMvcConfiguration
propertySourcesBinder
propertySourcesPlaceholderConfigurer
requestMappingHandlerAdapter
requestMappingHandlerMapping
resourceHandlerMapping
simpleControllerHandlerAdapter
tomcatEmbeddedServletContainerFactory
viewControllerHandlerMapping
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以启动远程调试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./gradlew build 

$ java -Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=8000,suspend=n \
       -jar build/libs/spring-boot-examples-0.0.1-SNAPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，打开浏览器访问 &lt;a href=&quot;http://localhost:8080/items&quot;&gt;http://localhost:8080/items&lt;/a&gt;，你会看到页面输出一个空的数组。然后，你可以使用浏览器的 Restfull 插件来添加、删除、修改数据。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;8. 添加前端库文件&lt;/h1&gt;

&lt;p&gt;这里主要使用 Bower 来管理前端依赖，包括 angular 和 bootstrap。&lt;/p&gt;

&lt;p&gt;配置 Bower ，需要在项目根目录下创建 .bowerrc 和 bower.json 两个文件。&lt;/p&gt;

&lt;p&gt;.bowerrc 文件制定下载的依赖存放路径：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
  &quot;directory&quot;: &quot;src/main/resources/static/bower_components&quot;,
  &quot;json&quot;: &quot;bower.json&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bower.json 文件定义依赖关系：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
  &quot;name&quot;: &quot;ng-spring-boot&quot;,
  &quot;dependencies&quot;: {
    &quot;angular&quot;: &quot;~1.3.0&quot;,
    &quot;angular-resource&quot;: &quot;~1.3.0&quot;,
    &quot;bootstrap-css-only&quot;: &quot;~3.2.0&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你没有安装 Bower，则运行下面命令进行安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install -g bower
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装之后下载依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bower install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行成功之后，查看 src/main/resources/static/bower_components 目录结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;src/main/resources/static/bower_components
├── angular
├── angular-resource
└── bootstrap-css-only
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-7&quot;&gt;9. 创建前端页面&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;/p&gt;

  &lt;p&gt;前端页面和 js 存放到 src/main/resources/static/ 目录下，是因为 Spring Boot 会自动在 /static 或者 /public 或者 /resources 或者 /META-INF/resources 加载静态页面。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;indexhtml&quot;&gt;创建 index.html&lt;/h2&gt;

&lt;p&gt;创建 src/main/resources/static 目录存放静态页面 index.html：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&quot;en&quot;&amp;gt;
  &amp;lt;head&amp;gt;
    &amp;lt;link rel=&quot;stylesheet&quot; href=&quot;./bower_components/bootstrap-css-only/css/bootstrap.min.css&quot; /&amp;gt;
  &amp;lt;/head&amp;gt;
  &amp;lt;body ng-app=&quot;myApp&quot;&amp;gt;
    &amp;lt;div class=&quot;container&quot; ng-controller=&quot;AppController&quot;&amp;gt;
      &amp;lt;div class=&quot;page-header&quot;&amp;gt;
        &amp;lt;h1&amp;gt;A checklist&amp;lt;/h1&amp;gt;
      &amp;lt;/div&amp;gt;
      &amp;lt;div class=&quot;alert alert-info&quot; role=&quot;alert&quot; ng-hide=&quot;items &amp;amp;amp;&amp;amp;amp; items.length &amp;gt; 0&quot;&amp;gt;
        There are no items yet.
      &amp;lt;/div&amp;gt;
      &amp;lt;form class=&quot;form-horizontal&quot; role=&quot;form&quot; ng-submit=&quot;addItem(newItem)&quot;&amp;gt;
        &amp;lt;div class=&quot;form-group&quot; ng-repeat=&quot;item in items&quot;&amp;gt;
          &amp;lt;div class=&quot;checkbox col-xs-9&quot;&amp;gt;
            &amp;lt;label&amp;gt;
              &amp;lt;input type=&quot;checkbox&quot; ng-model=&quot;item.checked&quot; ng-change=&quot;updateItem(item)&quot;/&amp;gt; 
            &amp;lt;/label&amp;gt;
          &amp;lt;/div&amp;gt;
          &amp;lt;div class=&quot;col-xs-3&quot;&amp;gt;
            &amp;lt;button class=&quot;pull-right btn btn-danger&quot; type=&quot;button&quot; title=&quot;Delete&quot;
              ng-click=&quot;deleteItem(item)&quot;&amp;gt;
              &amp;lt;span class=&quot;glyphicon glyphicon-trash&quot;&amp;gt;&amp;lt;/span&amp;gt;
            &amp;lt;/button&amp;gt;
          &amp;lt;/div&amp;gt;
        &amp;lt;/div&amp;gt;
        &amp;lt;hr /&amp;gt;
        &amp;lt;div class=&quot;input-group&quot;&amp;gt;
          &amp;lt;input type=&quot;text&quot; class=&quot;form-control&quot; ng-model=&quot;newItem&quot; placeholder=&quot;Enter the description...&quot; /&amp;gt;
          &amp;lt;span class=&quot;input-group-btn&quot;&amp;gt;
            &amp;lt;button class=&quot;btn btn-default&quot; type=&quot;submit&quot; ng-disabled=&quot;!newItem&quot; title=&quot;Add&quot;&amp;gt;
              &amp;lt;span class=&quot;glyphicon glyphicon-plus&quot;&amp;gt;&amp;lt;/span&amp;gt;
            &amp;lt;/button&amp;gt;
          &amp;lt;/span&amp;gt;
        &amp;lt;/div&amp;gt;
      &amp;lt;/form&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;script type=&quot;text/javascript&quot; src=&quot;./bower_components/angular/angular.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script type=&quot;text/javascript&quot; src=&quot;./bower_components/angular-resource/angular-resource.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script type=&quot;text/javascript&quot; src=&quot;./bower_components/lodash/dist/lodash.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script type=&quot;text/javascript&quot; src=&quot;./app/app.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script type=&quot;text/javascript&quot; src=&quot;./app/controllers.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script type=&quot;text/javascript&quot; src=&quot;./app/services.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;angularjs&quot;&gt;初始化 AngularJS&lt;/h2&gt;

&lt;p&gt;这里使用闭包的方式来初始化 AngularJS，代码见 src/main/resources/static/app/app.js ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;(function(angular) {
  angular.module(&quot;myApp.controllers&quot;, []);
  angular.module(&quot;myApp.services&quot;, []);
  angular.module(&quot;myApp&quot;, [&quot;ngResource&quot;, &quot;myApp.controllers&quot;, &quot;myApp.services&quot;]);
}(angular));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;resource-factory&quot;&gt;创建 resource factory&lt;/h2&gt;

&lt;p&gt;代码见 src/main/resources/static/app/services.js ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;(function(angular) {
  var ItemFactory = function($resource) {
    return $resource(&#39;/items/:id&#39;, {
      id: &#39;@id&#39;
    }, {
      update: {
        method: &quot;PUT&quot;
      },
      remove: {
        method: &quot;DELETE&quot;
      }
    });
  };
  
  ItemFactory.$inject = [&#39;$resource&#39;];
  angular.module(&quot;myApp.services&quot;).factory(&quot;Item&quot;, ItemFactory);
}(angular));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-8&quot;&gt;创建控制器&lt;/h2&gt;

&lt;p&gt;代码见 src/main/resources/static/app/controllers.js ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;(function(angular) {
  var AppController = function($scope, Item) {
    Item.query(function(response) {
      $scope.items = response ? response : [];
    });
    
    $scope.addItem = function(description) {
      new Item({
        description: description,
        checked: false
      }).$save(function(item) {
        $scope.items.push(item);
      });
      $scope.newItem = &quot;&quot;;
    };
    
    $scope.updateItem = function(item) {
      item.$update();
    };
    
    $scope.deleteItem = function(item) {
      item.$remove(function() {
        $scope.items.splice($scope.items.indexOf(item), 1);
      });
    };
  };
  
  AppController.$inject = [&#39;$scope&#39;, &#39;Item&#39;];
  angular.module(&quot;myApp.controllers&quot;).controller(&quot;AppController&quot;, AppController);
}(angular));
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-9&quot;&gt;10. 测试前端页面&lt;/h1&gt;

&lt;p&gt;再一次打开浏览器，访问 &lt;a href=&quot;http://localhost:8080/&quot;&gt;http://localhost:8080/&lt;/a&gt; 进行测试。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/2015/checklist-first-run.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-10&quot;&gt;11. 总结&lt;/h1&gt;

&lt;p&gt;本文主要是记录快速使用 Spring Boot 和 Gradle 创建 AngularJS 项目的过程。，希望能对你有所帮助。&lt;/p&gt;

&lt;p&gt;文中相关的源码在 &lt;a href=&quot;https://github.com/javachen/spring-examples/tree/master/ng-spring-boot&quot;&gt;ng-spring-boot&lt;/a&gt;，你可以下载该项目，然后编译、运行代码。&lt;/p&gt;

&lt;p&gt;该项目也可以使用 maven 编译、运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn clean package

$ java -jar target/ng-spring-boot-0.0.1-SNAPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者直接运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn spring-boot:run
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-11&quot;&gt;11. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://g00glen00b.be/prototyping-spring-boot-angularjs/&quot;&gt;Rapid prototyping with Spring Boot and AngularJS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spring.io/guides/gs/spring-boot/&quot;&gt;Building an Application with Spring Boot&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spring.io/guides/gs/rest-service/&quot;&gt;Building a RESTful Web Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spring.io/guides/gs/rest-service-cors&quot;&gt;Enabling Cross Origin Requests for a RESTful Web Service&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2015/01/06/build-app-with-spring-boot-and-gradle.html</link>
      <guid>http://blog.javachen.com/2015/01/06/build-app-with-spring-boot-and-gradle.html</guid>
      <pubDate>2015-01-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Python中的多线程</title>
      <description>&lt;h3 id=&quot;section&quot;&gt;线程模块&lt;/h3&gt;

&lt;p&gt;Python 通过两个标准库 thread 和 threading 提供对线程的支持。Python 的 thread 模块是比较底层的模块，Python 的 threading 模块是对 thread 做了一些包装的，可以更加方便的被使用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;thread 模块提供的其他方法：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;start_new_thread(function,args,kwargs=None)&lt;/code&gt;：生一个新线程，在新线程中用指定参数和可选的 kwargs 调用 function 函数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;allocate_lock()&lt;/code&gt;：分配一个 &lt;strong&gt;LockType&lt;/strong&gt; 类型的锁对象（注意，此时还没有获得锁）&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;interrupt_main()&lt;/code&gt;：在其他线程中终止主线程&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;get_ident()&lt;/code&gt;：获得一个代表当前线程的魔法数字，常用于从一个字典中获得线程相关的数据。这个数字本身没有任何含义，并且当线程结束后会被新线程复用&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;exit()&lt;/code&gt;：退出线程&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;LockType 类型锁对象的函数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;acquire(wait=None)&lt;/code&gt;：尝试获取锁对象&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;locked()&lt;/code&gt;：如果获得了锁对象返回 True，否则返回 False&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;release()&lt;/code&gt;：释放锁&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;thread 还提供了一个 ThreadLocal 类用于管理线程相关的数据，名为 &lt;code&gt;thread._local&lt;/code&gt;，threading 中引用了这个类。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;由于 thread 提供的线程功能不多，无法在主线程结束后继续运行，不提供条件变量等等原因，一般不使用 thread 模块。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;threading 模块提供的其他方法：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;threading.currentThread()&lt;/code&gt;：返回当前的线程变量。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;threading.enumerate()&lt;/code&gt;：返回一个包含正在运行的线程的 list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;threading.activeCount()&lt;/code&gt;：返回正在运行的线程数量，与 &lt;code&gt;len(threading.enumerate())&lt;/code&gt; 有相同的结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了使用方法外，线程模块同样提供了 Thread 类来处理线程，构造方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Thread(group=None, target=None, name=None, args=(), kwargs={})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;参数说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;group&lt;/code&gt;：线程组，目前还没有实现，库引用中提示必须是 None；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;target&lt;/code&gt;：要执行的方法；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;name&lt;/code&gt;：线程名；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;args/kwargs&lt;/code&gt;：要传入方法的参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thread 类提供了以下实例方法:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;run()&lt;/code&gt;：用以表示线程活动的方法。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;start()&lt;/code&gt;：启动线程活动。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;join([timeout])&lt;/code&gt;：阻塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的 timeout（可选参数）。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;isAlive()&lt;/code&gt;：返回线程是否活动的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;getName()&lt;/code&gt;：返回线程名。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;setName()&lt;/code&gt;：设置线程名。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;threading 模块提供的类：Thread, Lock, Rlock, Condition, [Bounded]Semaphore, Event, Timer, local。&lt;/p&gt;

&lt;h3 id=&quot;thread-&quot;&gt;使用 thread 模块创建线程&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;函数式：&lt;/strong&gt; 调用 thread 模块中的 &lt;code&gt;start_new_thread()&lt;/code&gt; 函数来产生新线程。语法如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;thread.start_new_thread ( function, args[, kwargs] )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;参数说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;function&lt;/code&gt; - 线程函数。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;args&lt;/code&gt; - 传递给线程函数的参数，他必须是个 tuple 类型。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;kwargs&lt;/code&gt; - 可选参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例子1：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -* - coding: UTF-8 -* -

import thread
import time

# 为线程定义一个函数
def print_time( threadName, delay):
    count = 0
    while count &amp;lt; 5:
      time.sleep(delay)
      count += 1
      print &quot;%s: %s&quot; % ( threadName, time.ctime(time.time()) )


# 创建两个线程
try:
   thread.start_new_thread( print_time, (&quot;Thread-1&quot;, 1, ) )
   thread.start_new_thread( print_time, (&quot;Thread-2&quot;, 2, ) )
except:
   print &quot;Error: unable to start thread&quot;

while 1:
   pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Thread-1: Tue Dec 23 14:54:51 2014
Thread-2: Tue Dec 23 14:54:52 2014
Thread-1: Tue Dec 23 14:54:52 2014
Thread-1: Tue Dec 23 14:54:53 2014
Thread-2: Tue Dec 23 14:54:54 2014
Thread-1: Tue Dec 23 14:54:54 2014
Thread-1: Tue Dec 23 14:54:55 2014
Thread-2: Tue Dec 23 14:54:56 2014
Thread-2: Tue Dec 23 14:54:58 2014
Thread-2: Tue Dec 23 14:55:00 2014
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主线程没有结束，是因为有个 while 循环一直在执行 pass 语句，导致程序一直没有退出。如果想要主线程主动结束，可以在线程函数中调用 &lt;code&gt;thread.exit()&lt;/code&gt;，他抛出SystemExit exception，达到退出线程的目的。&lt;/p&gt;

&lt;h3 id=&quot;threading-&quot;&gt;使用 Threading 模块创建线程&lt;/h3&gt;

&lt;p&gt;使用 Threading 模块创建线程，直接从 &lt;code&gt;threading.Thread&lt;/code&gt; 继承，然后重写 &lt;code&gt;__init__&lt;/code&gt; 方法和&lt;code&gt;run&lt;/code&gt; 方法。&lt;/p&gt;

&lt;p&gt;例子2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -* - coding: UTF-8 -* -
#!/usr/bin/python

from threading import Thread
import time

#继承父类threading.Thread
class myThread(Thread):

    def __init__(self, name, delay):
        Thread.__init__(self)
        self.name = name
        self.delay = delay

    def run(self):                   #把要执行的代码写到run函数里面 线程在创建后会直接运行run函数
        print &quot;Starting &quot; + self.name
        print_time(self.name, self.delay, 5)
        print &quot;Exiting &quot; + self.name

def print_time(threadName, delay, counter):
    while counter:
        time.sleep(delay)
        print &quot;%s: %s&quot; % (threadName, time.ctime(time.time()))
        counter -= 1

# 创建新线程
thread1 = myThread(&quot;Thread-1&quot;, 1)
thread2 = myThread(&quot;Thread-2&quot;, 2)

# 开启线程
thread1.start()
thread2.start()

print &quot;Exiting Main Thread&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Starting Thread-1
Starting Thread-2
Starting Thread-3
Thread-3 processing One
Thread-2 processing Two
Thread-3 processing Three
Thread-1 processing Four
Thread-2 processing Five
Exiting Thread-3
Exiting Thread-1
Exiting Thread-2
Exiting Main Thread
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-1&quot;&gt;线程同步&lt;/h3&gt;

&lt;p&gt;如果多个线程共同对某个数据修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步。&lt;/p&gt;

&lt;p&gt;使用 Thread 对象的 &lt;code&gt;Lock&lt;/code&gt; 和 &lt;code&gt;Rlock&lt;/code&gt; 可以实现简单的线程同步，这两个对象都有 acquire 方法和 release 方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到 acquire 和 release 方法之间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lock&lt;/strong&gt;（指令锁）是可用的最低级的同步指令。Lock 处于锁定状态时，不被特定的线程拥有。Lock 包含两种状态——锁定和非锁定，以及两个基本的方法。&lt;/p&gt;

&lt;p&gt;可以认为 Lock 有一个锁定池，当线程请求锁定时，将线程至于池中，直到获得锁定后出池。池中的线程处于状态图中的同步阻塞状态。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;构造方法：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Lock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;实例方法：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;acquire([timeout])&lt;/code&gt;：使线程进入同步阻塞状态，尝试获得锁定。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;release()&lt;/code&gt;：释放锁。使用前线程必须已获得锁定，否则将抛出异常。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RLock&lt;/strong&gt;（可重入锁）是一个可以被同一个线程请求多次的同步指令。RLock 使用了&lt;strong&gt;拥有的线程&lt;/strong&gt;和&lt;strong&gt;递归等级&lt;/strong&gt;的概念，处于锁定状态时，RLock 被某个线程拥有。拥有RLock的线程可以再次调用acquire()，释放锁时需要调用 release() 相同次数。&lt;/p&gt;

&lt;p&gt;可以认为 RLock 包含一个锁定池和一个初始值为0的计数器，每次成功调用 &lt;code&gt;acquire()/release()&lt;/code&gt;，计数器将 +1/-1，为0时锁处于未锁定状态。&lt;/p&gt;

&lt;p&gt;例子3：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -* - coding: UTF-8 -* -

#!/usr/bin/python

from threading import Thread,Lock
import time

threadLock = Lock()

class myThread (Thread):
    def __init__(self, name, delay):
        Thread.__init__(self)
        self.name = name
        self.delay = delay

    def run(self):
        print &quot;Starting &quot; + self.name
       # 获得锁，成功获得锁定后返回True
       # 可选的timeout参数不填时将一直阻塞直到获得锁定
       # 否则超时后将返回False
        threadLock.acquire()
        print_time(self.name, self.delay, 3)
        # 释放锁
        threadLock.release()

def print_time(threadName, delay, counter):
    while counter:
        time.sleep(delay)
        print &quot;%s: %s&quot; % (threadName, time.ctime(time.time()))
        counter -= 1

# 创建新线程
thread1 = myThread( &quot;Thread-1&quot;, 1)
thread2 = myThread(&quot;Thread-2&quot;, 2)

# 开启新线程
thread1.start()
thread2.start()

# 等待所有线程完成
thread1.join()
thread2.join()

print &quot;Exiting Main Thread&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Starting Thread-1
Starting Thread-2
Thread-1: Tue Dec 23 16:06:32 2014
Thread-1: Tue Dec 23 16:06:33 2014
Thread-1: Tue Dec 23 16:06:34 2014
Thread-2: Tue Dec 23 16:06:36 2014
Thread-2: Tue Dec 23 16:06:38 2014
Thread-2: Tue Dec 23 16:06:40 2014
Exiting Main Thread
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子3和例子2的区别在于，例子上中 &lt;code&gt;print_time&lt;/code&gt; 方法前后添加了 threadLock 的两个方法，并且在主线程调用了两个线程的 join 方法，&lt;br /&gt;
使得主线程阻塞直到两个子线程运行完成。待子线程运行完成之后，最后才会打印 &lt;code&gt;Exiting Main Thread&lt;/code&gt; ，即表示主线程运行完成。&lt;/p&gt;

&lt;p&gt;除了使用 Lock 类获取锁之外，我们还可以使用 Condition 类，condition 的 acquire() 和 release() 方法内部调用了 lock 的 acquire() 和 release()，所以我们可以用 condiction 实例取代 lock 实例，但 lock 的行为不会改变。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;线程优先级队列&lt;/h3&gt;

&lt;p&gt;Python 的 Queue 模块中提供了同步的、线程安全的队列类，包括 FIFO（先入先出)队列 Queue，LIFO（后入先出）队列 LifoQueue，和优先级队列 PriorityQueue。这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列来实现线程间的同步。&lt;/p&gt;

&lt;p&gt;Queue模块中的常用方法:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Queue.qsize()&lt;/code&gt;：返回队列的大小&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.empty()&lt;/code&gt;：如果队列为空，返回 True，反之 False&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.full()&lt;/code&gt;：如果队列满了，返回 True，反之 False&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.full&lt;/code&gt;：与 maxsize 大小对应&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.get([block[, timeout]])&lt;/code&gt;：获取队列，timeout 等待时间&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.get_nowait()&lt;/code&gt;：相当 &lt;code&gt;Queue.get(False)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.put(item)&lt;/code&gt;：写入队列，timeout 等待时间&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.put_nowait(item)&lt;/code&gt;：相当 &lt;code&gt;Queue.put(item, False)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.task_done()&lt;/code&gt;：在完成一项工作之后，&lt;code&gt;Queue.task_done()&lt;/code&gt; 函数向任务已经完成的队列发送一个信号&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Queue.join()&lt;/code&gt;：实际上意味着等到队列为空，再执行别的操作&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例4：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -* - coding: UTF-8 -* -

#!/usr/bin/python

from threading import Thread,Lock
import Queue
import time

threadList = [&quot;Thread-1&quot;, &quot;Thread-2&quot;, &quot;Thread-3&quot;]
nameList = [&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;]
workQueue = Queue.Queue(10)
queueLock = Lock()
threads = []
exitFlag = 0

class myThread (Thread):
    def __init__(self, name, q):
        Thread.__init__(self)
        self.name = name
        self.q = q

    def run(self):
        print &quot;Starting &quot; + self.name
        process_data(self.name, self.q)
        print &quot;Exiting &quot; + self.name

def process_data(threadName, q):
    while not exitFlag:
        queueLock.acquire()
        if not workQueue.empty():
            data = q.get()
            queueLock.release()
            print &quot;%s processing %s&quot; % (threadName, data)
        else:
            queueLock.release()
        time.sleep(1)

# 创建新线程
for tName in threadList:
    thread = myThread(tName, workQueue)
    thread.start()
    threads.append(thread)

# 填充队列
queueLock.acquire()
for word in nameList:
    workQueue.put(word)
queueLock.release()

# 等待队列清空
while not workQueue.empty():
    pass

# 通知线程是时候退出
exitFlag = 1

# 等待所有线程完成
for t in threads:
    t.join()
print &quot;Exiting Main Thread&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子4中创建了3个线程读取队列的数据，当队列为空时候，三个线程停止运行，另外主线程会一直阻塞直到三个子线程运行完毕，最后再打印 “Exiting Main Thread”。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;生产者和消费者模型&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;生产者的工作是产生一块数据，放到 buffer 中，如此循环。与此同时，消费者在消耗这些数据（例如从 buffer 中把它们移除），每次一块。&lt;br /&gt;
这个为描述了两个共享固定大小缓冲队列的进程，即生产者和消费者。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;示例5：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from threading import Thread, Condition
import time
import random

queue = []
MAX_NUM = 10
condition = Condition()

class ProducerThread(Thread):
    def run(self):
        nums = range(5)
        global queue
        while True:
            condition.acquire()
            if len(queue) == MAX_NUM:
                print &quot;Queue full, producer is waiting&quot;
                condition.wait()
                print &quot;Space in queue, Consumer notified the producer&quot;
            num = random.choice(nums)
            queue.append(num)
            print &quot;Produced&quot;, num
            condition.notify()
            condition.release()
            time.sleep(random.random())

class ConsumerThread(Thread):
    def run(self):
        global queue
        while True:
            condition.acquire()
            if not queue:
                print &quot;Nothing in queue, consumer is waiting&quot;
                condition.wait()
                print &quot;Producer added something to queue and notified the consumer&quot;
            num = queue.pop(0)
            print &quot;Consumed&quot;, num
            condition.notify()
            condition.release()
            time.sleep(random.random())

ProducerThread().start()
ConsumerThread().start()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面例子中使用了 &lt;strong&gt;Condition&lt;/strong&gt;，Condition（条件变量）通常与一个锁关联。需要在多个 Contidion 中共享一个锁时，可以传递一个 Lock/RLock 实例给构造方法，否则它将自己生成一个RLock实例。&lt;/p&gt;

&lt;p&gt;可以认为，除了 Lock 带有的锁定池外，Condition 还包含一个等待池，池中的线程处于状态图中的等待阻塞状态，直到另一个线程调用 notify()/notifyAll() 通知；得到通知后线程进入锁定池等待锁定。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;构造方法：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;Condition([lock/rlock])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;实例方法：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;acquire([timeout])/release()&lt;/code&gt;：调用关联的锁的相应方法。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;wait([timeout])&lt;/code&gt;：调用这个方法将使线程进入 Condition 的等待池等待通知，并释放锁。使用前线程必须已获得锁定，否则将抛出异常。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;notify()&lt;/code&gt;：调用这个方法将从等待池挑选一个线程并通知，收到通知的线程将自动调用 acquire() 尝试获得锁定（进入锁定池）；其他线程仍然在等待池中。调用这个方法不会释放锁定。使用前线程必须已获得锁定，否则将抛出异常。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;notifyAll()&lt;/code&gt;：调用这个方法将通知等待池中所有的线程，这些线程都将进入锁定池尝试获得锁定。调用这个方法不会释放锁定。使用前线程必须已获得锁定，否则将抛出异常。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例子5中生产者和消费者共享一个 list 集合，其实也可以换成 queue。&lt;/p&gt;

&lt;p&gt;例子6：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from threading import Thread
import time
import random
from Queue import Queue

queue = Queue(3)

class ProducerThread(Thread):
    def run(self):
        nums = range(5)
        global queue
        while True:
            num = random.choice(nums)
            queue.put(num)
            print &quot;Produced&quot;, num
            time.sleep(random.random())

class ConsumerThread(Thread):
    def run(self):
        global queue
        while True:
            num = queue.get()
            queue.task_done()
            print &quot;Consumed&quot;, num
            time.sleep(random.random())

ProducerThread().start()
ConsumerThread().start()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;解释：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在原来使用 list 的位置，改为使用 Queue 实例（下称队列）。&lt;/li&gt;
  &lt;li&gt;这个队列有一个 condition ，它有自己的 lock。如果你使用 Queue，你不需要为 condition 和 lock 而烦恼。&lt;/li&gt;
  &lt;li&gt;生产者调用队列的 put 方法来插入数据。&lt;/li&gt;
  &lt;li&gt;put() 在插入数据前有一个获取 lock 的逻辑。&lt;/li&gt;
  &lt;li&gt;同时，put() 也会检查队列是否已满。如果已满，它会在内部调用 wait()，生产者开始等待。&lt;/li&gt;
  &lt;li&gt;消费者使用get方法。&lt;/li&gt;
  &lt;li&gt;get() 从队列中移出数据前会获取 lock。&lt;/li&gt;
  &lt;li&gt;get() 会检查队列是否为空，如果为空，消费者进入等待状态。&lt;/li&gt;
  &lt;li&gt;get() 和 put() 都有适当的 notify()。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;本文主要介绍了 Python 中创建多线程的两种方法，并简单说了 threading 模块中的 Lock、RLock、Condition 三个类以及 Queue 类的使用方法，另外，还通过代码实现了生产者和消费者模型。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;参考文章&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.w3cschool.cc/python/python-multithreading.html&quot;&gt;Python多线程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/52412/&quot;&gt;Python中的生产者消费者问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/12/23/thread-in-python.html</link>
      <guid>http://blog.javachen.com/2014/12/23/thread-in-python.html</guid>
      <pubDate>2014-12-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Django创建Blog</title>
      <description>&lt;p&gt;本文参考 &lt;a href=&quot;http://www.yaconiello.com/blog/part-1-creating-blog-system-using-django-markdown/&quot;&gt;Part 1: Creating a blog system using django + markdown&lt;/a&gt; 使用 django、bootstrap3 创建一个支持 markdown 语法的简单 blog，这是一个很好的学习 django 的例子，基本上涉及了 django 的方方面面，本文中相关的源码见 &lt;a href=&quot;https://github.com/javachen/django_blog&quot;&gt;github&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;相对于原文做了一些修改：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于 django1.7，修复了不兼容的配置&lt;/li&gt;
  &lt;li&gt;前端使用 bootstrap3，并修改了页面的一些内容&lt;/li&gt;
  &lt;li&gt;【TODO】代码中关键的地方添加注释，帮助理解&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装依赖&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install markdown pygments django-pagedown
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;我使用的 django 版本为 &lt;code&gt;1.7&lt;/code&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 创建项目&lt;/h1&gt;

&lt;p&gt;创建项目 django_blog 和 blog app：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;django-admin.py startproject django_blog
python manage.py startapp blog
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;settingspy&quot;&gt;修改 settings.py&lt;/h3&gt;

&lt;p&gt;设置 SITE_ID 并修改时区等：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;SITE_ID = 1

LANGUAGE_CODE = &#39;zh_CN&#39;

TIME_ZONE = &#39;Asia/Shanghai&#39;

USE_TZ = True 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加 app:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;INSTALLED_APPS = (
    &#39;django.contrib.admin&#39;,
    &#39;django.contrib.sites&#39;,
    &#39;django.contrib.auth&#39;,
    &#39;django.contrib.contenttypes&#39;,
    &#39;django.contrib.sessions&#39;,
    &#39;django.contrib.messages&#39;,
    &#39;django.contrib.staticfiles&#39;,
    &#39;pagedown&#39;, # App for adding markdown preview to the django admin
    &#39;blog&#39;, # Blog Module
    &#39;django.contrib.comments&#39;, # comments for the blog

)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置静态文件目录和模板目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Additional locations of static files
STATICFILES_DIRS = (
    os.path.join(BASE_DIR,&#39;static&#39;),
    # Put strings here, like &quot;/home/html/static&quot; or &quot;C:/www/django/static&quot;.
    # Always use forward slashes, even on Windows.
    # Don&#39;t forget to use absolute paths, not relative paths.
)

# List of finder classes that know how to find static files in
# various locations.
STATICFILES_FINDERS = (
    &#39;django.contrib.staticfiles.finders.FileSystemFinder&#39;,
    &#39;django.contrib.staticfiles.finders.AppDirectoriesFinder&#39;,
#    &#39;django.contrib.staticfiles.finders.DefaultStorageFinder&#39;,
)

TEMPLATE_DIRS = (
    os.path.join(BASE_DIR,&#39;templates&#39;),
)
# List of callables that know how to import templates from various sources.
TEMPLATE_LOADERS = (
    &#39;django.template.loaders.filesystem.Loader&#39;,
    &#39;django.template.loaders.app_directories.Loader&#39;,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 创建模型&lt;/h1&gt;

&lt;p&gt;在 blog/models.py 中创建模型 Category 和 Article：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.db import models
from django.utils.translation import ugettext as _
from markdown import markdown

class Category(models.Model) :
    &quot;&quot;&quot;Category Model&quot;&quot;&quot;
    title = models.CharField(
        verbose_name = _(u&#39;Title&#39;),
        help_text = _(u&#39; &#39;),
        max_length = 255
    )
    slug = models.SlugField(
        verbose_name = _(u&#39;Slug&#39;),
        help_text = _(u&#39;Uri identifier.&#39;),
        max_length = 255,
        unique = True
    )

    class Meta:
        app_label = _(u&#39;blog&#39;)
        verbose_name = _(u&quot;Category&quot;)
        verbose_name_plural = _(u&quot;Categories&quot;)
        ordering = [&#39;title&#39;,]

    def __unicode__(self):
        return &quot;%s&quot; % (self.title,)

class Article(models.Model) :
    &quot;&quot;&quot;Article Model&quot;&quot;&quot;
    title = models.CharField(
        verbose_name = _(u&#39;Title&#39;),
        help_text = _(u&#39; &#39;),
        max_length = 255
    )
    slug = models.SlugField(
        verbose_name = _(u&#39;Slug&#39;),
        help_text = _(u&#39;Uri identifier.&#39;),
        max_length = 255,
        unique = True
    )
    content_markdown = models.TextField(
        verbose_name = _(u&#39;Content (Markdown)&#39;),
        help_text = _(u&#39; &#39;),
    )
    content_markup = models.TextField(
        verbose_name = _(u&#39;Content (Markup)&#39;),
        help_text = _(u&#39; &#39;),
    )
    categories = models.ManyToManyField(
        Category,
        verbose_name = _(u&#39;Categories&#39;),
        help_text = _(u&#39; &#39;),
        null = True,
        blank = True
    )
    date_publish = models.DateField(
        verbose_name = _(u&#39;Publish Date&#39;),
        help_text = _(u&#39; &#39;)
    )

    class Meta:
        app_label = _(u&#39;blog&#39;)
        verbose_name = _(u&quot;Article&quot;)
        verbose_name_plural = _(u&quot;Articles&quot;)
        ordering = [&#39;-date_publish&#39;]

    def save(self):
        self.content_markup = markdown(self.content_markdown, [&#39;codehilite&#39;])
        super(Article, self).save()

    def __unicode__(self):
        return &quot;%s&quot; % (self.title,)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将模型加入到 django admin，修改 blog/admin.py:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.contrib import admin
from django import forms
from pagedown.widgets import AdminPagedownWidget
from models import Category, Article

class CategoryAdmin(admin.ModelAdmin):
    prepopulated_fields = {&#39;slug&#39;: (&#39;title&#39;,)}
    list_display = (&#39;title&#39;, )
    search_fields = (&#39;title&#39;, )
    fieldsets = (
        (
            None,
            {
                &#39;fields&#39;: (&#39;title&#39;, &#39;slug&#39;,)
            }
        ),
    )

class ArticleForm(forms.ModelForm):
    class Meta:
        model = Article
        widgets = {
            &#39;content_markdown&#39; : AdminPagedownWidget(),
        }
        exclude = [&#39;content_markup&#39;,]

class ArticleAdmin(admin.ModelAdmin):
    form = ArticleForm
    prepopulated_fields = {&#39;slug&#39;: (&#39;title&#39;,)}
    list_display = (&#39;title&#39;, &#39;date_publish&#39;)
    search_fields = (&#39;title&#39;, &#39;content_markdown&#39;,)
    list_filter = (&#39;categories&#39;,)
    fieldsets = (
        (
            None,
            {
                &#39;fields&#39;: (&#39;title&#39;, &#39;slug&#39;, &#39;content_markdown&#39;, &#39;categories&#39;, &#39;date_publish&#39;,)
            }
        ),
    )

admin.site.register(Category, CategoryAdmin)
admin.site.register(Article, ArticleAdmin)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;url-&quot;&gt;4. 创建 url 路由&lt;/h1&gt;

&lt;p&gt;修改 django_blog/urls.py：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.conf.urls import patterns, include, url
from django.contrib import admin
from blog.views import index

urlpatterns = patterns(&#39;&#39;,
    url(r&#39;^admin/&#39;, include(admin.site.urls)),
    url(r&#39;^$&#39;, index, name=&#39;home&#39;),

    url(&#39;^blog/archive/(?P&amp;lt;year&amp;gt;[\d]+)/(?P&amp;lt;month&amp;gt;[\d]+)/$&#39;, &#39;blog.views.date_archive&#39;, name=&quot;blog_date_archive&quot;),
    url(&#39;^blog/archive/(?P&amp;lt;slug&amp;gt;[-\w]+)/$&#39;, &#39;blog.views.category_archive&#39;, name=&quot;blog_category_archive&quot;),
    url(&#39;^blog/(?P&amp;lt;slug&amp;gt;[-\w]+)/$&#39;, &#39;blog.views.single&#39;, name=&quot;blog_article_single&quot;),
    url(&#39;^blog/$&#39;, &#39;blog.views.index&#39;, name=&quot;blog_article_index&quot;),
    url(r&#39;^comments/&#39;, include(&#39;django.contrib.comments.urls&#39;)),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;5. 创建视图&lt;/h1&gt;

&lt;p&gt;创建 blog/views.py：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.core.paginator import Paginator, EmptyPage, PageNotAnInteger
from django.shortcuts import render, get_object_or_404
from models import Category, Article
from django.shortcuts import render_to_response
import calendar, datetime

# Create your views here.

def index(request) :
    &quot;&quot;&quot;The news index&quot;&quot;&quot;
    archive_dates = Article.objects.dates(&#39;date_publish&#39;,&#39;month&#39;, order=&#39;DESC&#39;)
    categories = Category.objects.all()

    page = request.GET.get(&#39;page&#39;)
    article_queryset = Article.objects.all()
    paginator = Paginator(article_queryset, 5)

    try:
        articles = paginator.page(page)
    except PageNotAnInteger:
        # If page is not an integer, deliver first page.
        articles = paginator.page(1)
    except EmptyPage:
        # If page is out of range (e.g. 9999), deliver last page of results.
        articles = paginator.page(paginator.num_pages)

    return render(
        request,
        &quot;blog/article/index.html&quot;,
        {
            &quot;articles&quot; : articles,
            &quot;archive_dates&quot; : archive_dates,
            &quot;categories&quot; : categories
        }
    )

def single(request, slug) :
    &quot;&quot;&quot;A single article&quot;&quot;&quot;
    article = get_object_or_404(Article, slug=slug)
    archive_dates = Article.objects.dates(&#39;date_publish&#39;,&#39;month&#39;, order=&#39;DESC&#39;)
    categories = Category.objects.all()
    return render(
        request,
        &quot;blog/article/single.html&quot;,
        {
            &quot;article&quot; : article,
            &quot;archive_dates&quot; : archive_dates,
            &quot;categories&quot; : categories
        }
    )

def date_archive(request, year, month) :
    &quot;&quot;&quot;The blog date archive&quot;&quot;&quot;
    # this archive pages dates
    year = int(year)
    month = int(month)
    month_range = calendar.monthrange(year, month)
    start = datetime.datetime(year=year, month=month,day=1)#.replace(tzinfo=utc)
    end = datetime.datetime(year=year, month=month, day=month_range[1])#.replace(tzinfo=utc)
    archive_dates = Article.objects.dates(&#39;date_publish&#39;,&#39;month&#39;, order=&#39;DESC&#39;)
    categories = Category.objects.all()

    # Pagination
    page = request.GET.get(&#39;page&#39;)
    article_queryset = Article.objects.filter(date_publish__range=(start.date(), end.date()))
    paginator = Paginator(article_queryset, 5)

    try:
        articles = paginator.page(page)
    except PageNotAnInteger:
        # If page is not an integer, deliver first page.
        articles = paginator.page(1)
    except EmptyPage:
        # If page is out of range (e.g. 9999), deliver last page of results.
        articles = paginator.page(paginator.num_pages)

    return render(
        request,
        &quot;blog/article/date_archive.html&quot;,
        {
            &quot;start&quot; : start,
            &quot;end&quot; : end,
            &quot;articles&quot; : articles,
            &quot;archive_dates&quot; : archive_dates,
            &quot;categories&quot; : categories
        }
    )

def category_archive(request, slug):
    archive_dates = Article.objects.dates(&#39;date_publish&#39;,&#39;month&#39;, order=&#39;DESC&#39;)
    categories = Category.objects.all()
    category = get_object_or_404(Category, slug=slug)

    # Pagination
    page = request.GET.get(&#39;page&#39;)
    article_queryset = Article.objects.filter(categories=category)
    paginator = Paginator(article_queryset, 5)

    try:
        articles = paginator.page(page)
    except PageNotAnInteger:
        # If page is not an integer, deliver first page.
        articles = paginator.page(1)
    except EmptyPage:
        # If page is out of range (e.g. 9999), deliver last page of results.
        articles = paginator.page(paginator.num_pages)
    return render(
        request,
        &quot;blog/article/category_archive.html&quot;,
        {
            &quot;articles&quot; : articles,
            &quot;archive_dates&quot; : archive_dates,
            &quot;categories&quot; : categories,
            &quot;category&quot; : category
        }
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;6. 创建模板&lt;/h1&gt;

&lt;p&gt;创建 templates/blog/base.html：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;lang=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;en&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;head&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;meta&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;charset=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;utf-8&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;

        &lt;span class=&quot;nt&quot;&gt;&amp;lt;title&amp;gt;&lt;/span&gt;{% block title %}{% endblock %}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;

        &lt;span class=&quot;nt&quot;&gt;&amp;lt;meta&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;viewport&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;content=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- CSS --&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;link&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;http://cdn.bootcss.com/bootstrap/3.3.1/css/bootstrap.min.css&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;rel=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;stylesheet&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;link&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;http://cdn.bootcss.com/bootstrap/3.3.1/css/bootstrap-theme.min.css&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;rel=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;stylesheet&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;link&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{{ STATIC_URL }}css/style.css&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;rel=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;stylesheet&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;link&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{{ STATIC_URL }}css/pygments.css&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;rel=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;stylesheet&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- HTML5 shim, for IE6-8 support of HTML5 elements --&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;&amp;lt;!--[if lt IE 9]&amp;gt;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;        &amp;lt;script src=&amp;quot;http://html5shim.googlecode.com/svn/trunk/html5.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;        &amp;lt;![endif]--&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;

    &lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- Part 1: Wrap all page content here --&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;wrap&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- Begin page content --&amp;gt;&lt;/span&gt;
            &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;container&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;masthead&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
                    &lt;span class=&quot;nt&quot;&gt;&amp;lt;ul&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;nav nav-pills pull-right&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
                        &lt;span class=&quot;nt&quot;&gt;&amp;lt;li&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{%&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get_full_path&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;%}&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;active&amp;quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{%&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;endif&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;%}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Home&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
                        &lt;span class=&quot;nt&quot;&gt;&amp;lt;li&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{%&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;quot;/&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;blog&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/&amp;quot;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get_full_path&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;%}&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;active&amp;quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{%&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;endif&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;%}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% url &amp;#39;blog_article_index&amp;#39; %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Blog&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
                        &lt;span class=&quot;nt&quot;&gt;&amp;lt;li&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{%&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;quot;/&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;about&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/&amp;quot;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get_full_path&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;%}&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;active&amp;quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{%&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;endif&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;%}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% url &amp;#39;blog_article_index&amp;#39; %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;About&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
                    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/ul&amp;gt;&lt;/span&gt;
                    &lt;span class=&quot;nt&quot;&gt;&amp;lt;h3&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;text-muted&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;My Blog&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h3&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;page-header&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
                    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;lead&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Simple is beauty!&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
                {% block breadcrumb %}{% endblock %}
                {% block content %}{% endblock%}
            &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

            &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;push&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

        &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

        &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;footer&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
            &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;container&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;muted credit&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;©2014&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;style=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;float:right;&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/legal/&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Legal&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;&lt;/span&gt;

            &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- Le javascript&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;        ================================================== --&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- Placed at the end of the document so the pages load faster --&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;http://cdn.bootcss.com/jquery/1.11.1/jquery.min.js&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;http://cdn.bootcss.com/bootstrap/3.3.1/js/bootstrap.min.js&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;其他 html 文件见&lt;a href=&quot;https://github.com/javachen/django_blog&quot;&gt;源代码&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;7. 运行项目&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python manage.py makemigrations
python manage.py migrate
python manage.py syncdb
python manage.py runserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开浏览器访问后台页面 &lt;a href=&quot;http://localhost:8000/admin&quot;&gt;http://localhost:8000/admin&lt;/a&gt; 添加分类和文章，然后再访问前台页面 &lt;a href=&quot;http://localhost:8000/&quot;&gt;http://localhost:8000/&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;8. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yaconiello.com/blog/part-1-creating-blog-system-using-django-markdown/&quot;&gt;Part 1: Creating a blog system using django + markdown&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yaconiello.com/blog/part-2-creating-blog-system-using-django-markdown/&quot;&gt;Part 2: Creating a blog system using django + markdown&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yaconiello.com/blog/part-3-creating-blog-system-using-django-markdown/&quot;&gt;Part 3: Creating a blog system using django + markdown&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/12/19/create-blog-using-django.html</link>
      <guid>http://blog.javachen.com/2014/12/19/create-blog-using-django.html</guid>
      <pubDate>2014-12-19T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>JPA的使用</title>
      <description>&lt;p&gt;JPA，Java 持久化规范，是从EJB2.x以前的实体 Bean 分离出来的，EJB3 以后不再有实体 bean，而是将实体 bean 放到 JPA 中实现。&lt;/p&gt;

&lt;p&gt;JPA 是 sun 提出的一个对象持久化规范，各 JavaEE 应用服务器自主选择具体实现，JPA 的设计者是 Hibernate 框架的作者，因此Hibernate作为Jboss服务器中JPA的默认实现，Oracle的Weblogic使用EclipseLink(以前叫TopLink)作为默认的JPA实现，IBM的Websphere和Sun的Glassfish默认使用OpenJPA(Apache的一个开源项目)作为其默认的JPA实现。&lt;/p&gt;

&lt;p&gt;JPA 的底层实现是一些流行的开源 ORM 框架，因此JPA其实也就是java实体对象和关系型数据库建立起映射关系，通过面向对象编程的思想操作关系型数据库的规范。&lt;/p&gt;

&lt;h1 id=&quot;jpa-&quot;&gt;1. JPA 历史&lt;/h1&gt;

&lt;p&gt;早期版本的EJB，定义持久层结合使用 &lt;code&gt;javax.ejb.EntityBean&lt;/code&gt; 接口作为业务逻辑层。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;同时引入 EJB3.0 的持久层分离，并指定为JPA1.0（Java持久性API）。这个API规范随着 JAVA EE5 对2006年5月11日使用JSR220规范发布。&lt;/li&gt;
  &lt;li&gt;JPA2.0 的JAVA EE 6规范发布于2009年12月10日并成 Java Community Process JSR317 的一部分。&lt;/li&gt;
  &lt;li&gt;JPA2.1 使用 JSR338 的 JAVA EE7的规范发布于2013年4月22日。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;jpa--1&quot;&gt;2. JPA 架构&lt;/h1&gt;

&lt;p&gt;下图显示了JPA核心类和JPA接口。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/jpa/JPA-01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;类或接口&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;EntityManagerFactory&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;这是一个 EntityManager 的工厂类。它创建并管理多个 EntityManager 实例。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;EntityManager&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;这是一个接口，它管理的持久化操作的对象。它的工作原理类似工厂的查询实例。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Entity&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;实体是持久性对象是存储在数据库中的记录。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;EntityTransaction&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;它与 EntityManager 是一对一的关系。对于每一个 EntityManager ，操作是由 EntityTransaction 类维护。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Persistence&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;这个类包含静态方法来获取 EntityManagerFactory 实例。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Query&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;该接口由每个 JPA 供应商，能够获得符合标准的关系对象。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在上述体系结构中，类和接口之间的关系属于javax.persistence包。下图显示了它们之间的关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/jpa/JPA-02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;EntityManagerFactory 和 EntityManager 的关系是1对多。这是一个工厂类 EntityManager 实例。&lt;/li&gt;
  &lt;li&gt;EntityManager 和 EntityTransaction 之间的关系是1对1。对于每个 EntityManager 操作，只有一个 EntityTransaction 实例。&lt;/li&gt;
  &lt;li&gt;EntityManager 和 Query 之间的关系是1对多。查询数众多可以使用一个 EntityManager 实例执行。&lt;/li&gt;
  &lt;li&gt;EntityManager 实体之间的关系是1对多。一个 EntityManager 实例可以管理多个实体。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;jpa--2&quot;&gt;搭建 JPA 开发环境&lt;/h1&gt;

&lt;h2 id=&quot;maven-&quot;&gt;创建 maven 工程&lt;/h2&gt;
&lt;p&gt;创建一个空的 maven 工程，然后编写 pom.xml 文件，添加下面配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;properties&amp;gt;
	&amp;lt;spring.version&amp;gt;4.1.2.RELEASE&amp;lt;/spring.version&amp;gt;
	&amp;lt;hibernate.version&amp;gt;4.1.9.Final&amp;lt;/hibernate.version&amp;gt;
	&amp;lt;hibernate-jpa.version&amp;gt;2.0-cr-1&amp;lt;/hibernate-jpa.version&amp;gt;
&amp;lt;/properties&amp;gt;

&amp;lt;dependencies&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-core&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-beans&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-context&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-aop&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-context-support&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-tx&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-orm&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-jdbc&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;

	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.hibernate.java-persistence&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;jpa-api&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${hibernate-jpa.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;
	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.hibernate&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;hibernate-entitymanager&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${hibernate.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;

	&amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;com.h2database&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;h2&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;1.3.156&amp;lt;/version&amp;gt;
  &amp;lt;/dependency&amp;gt;

	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;spring-test&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
	&amp;lt;/dependency&amp;gt;

	&amp;lt;dependency&amp;gt;
		&amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;
		&amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;
		&amp;lt;version&amp;gt;4.11&amp;lt;/version&amp;gt;
		&amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
	&amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;创建实体&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.javachen.spring4.jpa.entity;

import javax.persistence.*;
import javax.validation.constraints.Max;
import javax.validation.constraints.Min;
import javax.validation.constraints.NotNull;
import javax.validation.constraints.Size;
import java.io.Serializable;

@Entity
@NamedQuery(query = &quot;Select e from Person e where e.id = :id&quot;,
        name = &quot;find person by id&quot;)
@Table(name = &quot;T_PERSON&quot;)
public class Person implements Serializable {
    private static final long serialVersionUID = 1L;

    @Id
    @Column(name = &quot;PERSON_ID&quot;)
    @GeneratedValue
    private Integer id;

    @Column(name = &quot;PERSON_NAME&quot;)
    @Size(min = 1, max = 30)
    @NotNull
    private String name;

    @Column(name = &quot;AGE&quot;)
    @Min(1)
    @Max(200)
    @NotNull
    private Integer age;

    @Column(name = &quot;salary&quot;)
    private Double salary;

		//省略 set、get 方法

    @Override
    public String toString() {
        return &quot;Person [id=&quot; + id + &quot;, name=&quot; + name + &quot;, age=&quot; + age + &quot;,salary=&quot;+salary+&quot;]&quot;;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;persistencexml&quot;&gt;persistence.xml&lt;/h2&gt;

&lt;p&gt;在这个文件中，我们将注册数据库，并指定实体类。另外，在上述所示的包的层次结构，根据JPA的内容包含在 persistence.xml 。&lt;/p&gt;

&lt;p&gt;在 &lt;code&gt;src/main/resources/META-INF/&lt;/code&gt; 目录创建一个文件，名称为 persistence.xml，内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&amp;gt;
&amp;lt;persistence xmlns=&quot;http://java.sun.com/xml/ns/persistence&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; version=&quot;2.0&quot;
    xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd&quot; &amp;gt;
    &amp;lt;persistence-unit name=&quot;db1-unit&quot;
                      transaction-type=&quot;RESOURCE_LOCAL&quot;&amp;gt;
        &amp;lt;class&amp;gt;com.javachen.spring4.jpa.entity.Person&amp;lt;/class&amp;gt;
        &amp;lt;properties&amp;gt;
            &amp;lt;property name=&quot;hibernate.connection.driver_class&quot; value=&quot;net.sf.log4jdbc.DriverSpy&quot;/&amp;gt;
            &amp;lt;property name=&quot;hibernate.connection.url&quot; value=&quot;jdbc:log4jdbc:h2:mem:example&quot;/&amp;gt;
            &amp;lt;property name=&quot;hibernate.connection.username&quot; value=&quot;sa&quot;/&amp;gt;
            &amp;lt;property name=&quot;hibernate.connection.password&quot; value=&quot;&quot;/&amp;gt;
            &amp;lt;property name=&quot;hibernate.dialect&quot; value=&quot;org.hibernate.dialect.H2Dialect&quot;/&amp;gt;

            &amp;lt;property name=&quot;hibernate.jdbc.batch_size&quot; value=&quot;30&quot; /&amp;gt;
            &amp;lt;property name=&quot;hibernate.use_sql_comments&quot; value=&quot;true&quot; /&amp;gt;
            &amp;lt;property name=&quot;hibernate.hbm2ddl.auto&quot; value=&quot;create&quot; /&amp;gt;
            &amp;lt;property name=&quot;hibernate.show_sql&quot; value=&quot;true&quot; /&amp;gt;
            &amp;lt;property name=&quot;hibernate.ejb.naming_strategy&quot;
                      value=&quot;org.hibernate.cfg.ImprovedNamingStrategy&quot; /&amp;gt;
            &amp;lt;property name=&quot;hibernate.connection.charSet&quot;
                      value=&quot;UTF-8&quot; /&amp;gt;
            &amp;lt;property name=&quot;hibernate.current_session_context_class&quot; value=&quot;thread&quot;/&amp;gt;
        &amp;lt;/properties&amp;gt;
    &amp;lt;/persistence-unit&amp;gt;
&amp;lt;/persistence&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有多个持久化单元，则可以配置多个 persistence-unit 节点。&lt;/p&gt;

&lt;p&gt;以下是 persistence.xml 所有配置项的一个示例说明：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;  

&amp;lt;persistence version=&quot;1.0&quot;  
xmlns:persistence=&quot;http://java.sun.com/xml/ns/persistence&quot;  
xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  
xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/persistence persistence_1_0.xsd &quot;&amp;gt;  

&amp;lt;!--
     Name属性用于定义持久化单元的名字 (name必选,空值也合法);
     transaction-type 指定事务类型(可选：JTA、RESOURCE_LOCAL)
--&amp;gt;  
&amp;lt;persistence-unit name=&quot;unitName&quot; transaction-type=&quot;JTA&quot;&amp;gt;  

   &amp;lt;!-- 描述信息.(可选) --&amp;gt;  
   &amp;lt;description&amp;gt; &amp;lt;/description&amp;gt;  

   &amp;lt;!-- javax.persistence.PersistenceProvider接口的一个实现类(可选) --&amp;gt;  
   &amp;lt;provider&amp;gt;&amp;lt;/provider&amp;gt;  

   &amp;lt;!-- Jta-data-source和 non-jta-data-source用于分别指定持久化提供商使用的JTA和/或non-JTA数据源的全局JNDI名称(可选) --&amp;gt;  
   &amp;lt;jta-data-source&amp;gt;java:/MySqlDS&amp;lt;/jta-data-source&amp;gt;  
   &amp;lt;non-jta-data-source&amp;gt;&amp;lt;/non-jta-data-source&amp;gt;  

   &amp;lt;!-- 声明orm.xml所在位置.(可选) --&amp;gt;  
   &amp;lt;mapping-file&amp;gt;product.xml&amp;lt;/mapping-file&amp;gt;  

   &amp;lt;!-- 以包含persistence.xml的jar文件为基准的相对路径,添加额外的jar文件.(可选) --&amp;gt;  
   &amp;lt;jar-file&amp;gt;../lib/model.jar&amp;lt;/jar-file&amp;gt;  

   &amp;lt;!-- 显式列出实体类,在Java SE 环境中应该显式列出.(可选) --&amp;gt;  
   &amp;lt;class&amp;gt;com.domain.User&amp;lt;/class&amp;gt;  

   &amp;lt;!-- 声明是否扫描jar文件中标注了@Enity类加入到上下文.若不扫描,则如下:(可选) --&amp;gt;  
   &amp;lt;exclude-unlisted-classes/&amp;gt;  

   &amp;lt;!--   厂商专有属性(可选)   --&amp;gt;  
   &amp;lt;properties&amp;gt;  
    &amp;lt;!-- hibernate.hbm2ddl.auto= create-drop / create / update --&amp;gt;  
    &amp;lt;property name=&quot;hibernate.hbm2ddl.auto&quot; value=&quot;update&quot; /&amp;gt;  
    &amp;lt;property name=&quot;hibernate.show_sql&quot; value=&quot;true&quot; /&amp;gt;  
   &amp;lt;/properties&amp;gt;  

&amp;lt;/persistence-unit&amp;gt;  

&amp;lt;/persistence&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;持久化操作&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package com.javachen.spring4.jpa.entity;

import javax.persistence.EntityManager;
import javax.persistence.EntityManagerFactory;
import javax.persistence.Persistence;
import javax.persistence.Query;
import java.util.List;

public class PersonTest {

    public static void main(String[] args) {
        EntityManagerFactory emfactory = Persistence.
                createEntityManagerFactory( &quot;db1-unit&quot; );
        EntityManager entitymanager = emfactory.
                createEntityManager( );
        entitymanager.getTransaction( ).begin( );

        Person person=new Person();
        person.setAge(18);
        person.setSalary(121d);
        person.setName(&quot;zhangsan&quot;);

        entitymanager.persist( person );
        entitymanager.getTransaction( ).commit( );

        entitymanager.close();
        emfactory.close( );
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上面的代码中 &lt;code&gt;createEntityManagerFactory()&lt;/code&gt; 通过提供我们在 persistent.xml 文件提供持久化单元相同唯一的名称创建一个持久性单元。 EntityManagerFactory对象将由usingcreateEntityManager()方法创建entitymanger实例。 EntityManager对象创建 entitytransactioninstance 事务管理。通过使用 EntityManager 对象，我们可以持久化实体到数据库中。&lt;/p&gt;

&lt;h1 id=&quot;jpql&quot;&gt;3. JPQL&lt;/h1&gt;

&lt;p&gt;JPQL 代表 Java 持久化查询语言。它被用来创建针对实体的查询存储在关系数据库中。 JPQL 是基于 SQL 语法的发展。但它不会直接影响到数据库。&lt;/p&gt;

&lt;p&gt;JPQL 可以检索使用 SELECT 子句中的数据，可以使用 UPDATE 子句做批量 UPDATE 和 DELETE 子句。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3.1 标准查询结构&lt;/h2&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;EntityManagerFactory emfactory = Persistence.
                createEntityManagerFactory( &quot;db1-unit&quot; );
EntityManager entitymanager = emfactory.
        createEntityManager( );
entitymanager.getTransaction( ).begin( );

//Scalar function
Query query = entitymanager.
        createQuery(&quot;Select UPPER(e.name) from Person e&quot;);
List&amp;lt;String&amp;gt; list=query.getResultList();

for(String e:list){
    System.out.println(&quot;Person name :&quot;+e);
}

//Aggregate function
Query query1 = entitymanager.
        createQuery(&quot;Select MAX(e.salary) from Person e&quot;);
Double result=(Double) query1.getSingleResult();
System.out.println(&quot;Max Person Salary :&quot;+result);


entitymanager.close();
emfactory.close( );
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3.2 命名查询&lt;/h2&gt;

&lt;p&gt;@NamedQuery 注解被定义为一个预定义的查询字符串，它是不可改变的查询。@NamedQuery 注解加在实体之上，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Entity
@NamedQuery(query = &quot;Select e from Person e where e.id = :id&quot;,
        name = &quot;find person by id&quot;)
@Table(name = &quot;T_PERSON&quot;)
public class Person implements Serializable {
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;命名查询使用方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import java.util.List;
import javax.persistence.EntityManager;
import javax.persistence.EntityManagerFactory;
import javax.persistence.Persistence;
import javax.persistence.Query;
import com.yiibai.eclipselink.entity.Employee;

public class NamedQueries{
   public static void main( String[ ] args ){
   	EntityManagerFactory emfactory = Persistence.
   		createEntityManagerFactory( &quot;db1-unit&quot; );
   	EntityManager entitymanager = emfactory.
   		createEntityManager();
   	Query query = entitymanager.createNamedQuery(
   		&quot;find person by id&quot;);
   	query.setParameter(&quot;id&quot;, 1);
   	List&amp;lt;Person&amp;gt; list = query.getResultList( );
   	for( Person e:list ){
   		System.out.print(&quot;Person ID :&quot;+e.getId( ));
   		System.out.println(&quot;\t Person Name :&quot;+e.getName( ));
   	}
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;3.3 动态查询&lt;/h2&gt;

&lt;p&gt;下面使用简单的条件&lt;strong&gt;动态查询&lt;/strong&gt;返回数据源中的实体类的所有实例。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;EntityManager em = ...;
CriteriaBuilder cb = em.getCriteriaBuilder();
CriteriaQuery&amp;lt;Entity class&amp;gt; cq = cb.createQuery(Entity.class);
Root&amp;lt;Entity&amp;gt; from = cq.from(Entity.class);
cq.select(Entity);
TypedQuery&amp;lt;Entity&amp;gt; q = em.createQuery(cq);
List&amp;lt;Entity&amp;gt; allitems = q.getResultList();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查询演示了基本的步骤来创建一个标准。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;EntityManager&lt;/code&gt; 实例被用来创建一个 CriteriaBuilder 对象。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CriteriaQuery&lt;/code&gt; 实例是用来创建一个查询对象。这个查询对象的属性将与该查询的细节进行修改。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CriteriaQuery.form&lt;/code&gt; 方法被调用来设置查询根。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CriteriaQuery.select&lt;/code&gt; 被调用来设置结果列表类型。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;TypedQuery&amp;lt;T&amp;gt;&lt;/code&gt; 实例是用来准备一个查询执行和指定的查询结果的类型。&lt;/li&gt;
  &lt;li&gt;在 &lt;code&gt;TypedQuery&amp;lt;T&amp;gt;&lt;/code&gt; 对象 getResultList 方法来执行查询。该查询返回实体的集合，结果存储在一个列表中。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-5&quot;&gt;4. 实体映射关系&lt;/h1&gt;

&lt;h2 id=&quot;section-6&quot;&gt;4.1 注解&lt;/h2&gt;

&lt;p&gt;在实体中使用到的注解列表如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;注解&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;@Entity&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;声明类为实体或表。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@Table&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;声明表名。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@Basic&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定非约束明确的各个字段。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@Embedded&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定类或它的值是一个可嵌入的类的实例的实体的属性。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@Id&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定的类的属性，用于标识主键。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@GeneratedValue&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定主键生成方式，例如自动，手动，或从序列表中获得的值。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@Transient&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;该值永远不会存储在数据库中。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@Lob&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;将属性持久化为 Blob 或者 Clob 类型。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@Column&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定字段属性。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@SequenceGenerator&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定在 &lt;code&gt;@GeneratedValue&lt;/code&gt; 注解中指定的属性的值。它创建了一个序列。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@TableGenerator&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定在 &lt;code&gt;@GeneratedValue&lt;/code&gt; 批注指定属性的值发生器。它创造了的值生成的表。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@AccessType&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;这种类型的注释用于设置访问类型。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@JoinColumn&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定一个实体组织或实体的集合。这是用在多对一和一对多关联。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@UniqueConstraint&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定的字段和用于主要或辅助表的唯一约束。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@ColumnResult&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;参考使用 select 子句的 SQL 查询中的列名。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@ManyToMany&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义了连接表之间的多对多一对多的关系。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@ManyToOne&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义了连接表之间的多对一的关系。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@OneToMany&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义了连接表之间存在一个一对多的关系。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@OneToOne&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定义了连接表之间有一个一对一的关系。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@NamedQueries&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定命名查询的列表。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;@NamedQuery&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定使用静态名称的命名查询。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;1、 &lt;code&gt;@OneToOne&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;一对一映射注解，双向的一对一关系需要在关系维护端(owner side)的 @OneToOne 注解中添加 mappedBy 属性，建表时在关系被维护端(inverse side)建立外键指向关系维护端的主键列。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;用法：&lt;/strong&gt;&lt;code&gt;@OneToOne(optional=true,casecade=CasecadeType.ALL,mappedBy=”被维护端外键”)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;2、 &lt;code&gt;@OneToMany&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;一对多映射注解，双向一对多关系中，一端是关系维护端(owner side)，只能在一端添加 mapped 属性。多端是关系被维护端(inverse side)。建表时在关系被维护端(多端)建立外键指向关系维护端(一端)的主键列。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;用法：&lt;/strong&gt; &lt;code&gt;@OneToMany(mappedBy = &quot;维护端(一端)主键&quot;, cascade=CascadeType.ALL)&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;br /&gt;
在Hibernate中有个术语叫做维护关系反转，即由对方维护关联关系，使用 &lt;code&gt;inverse=false&lt;/code&gt; 来表示关系维护放，在JPA的注解中，mappedBy就相当于inverse=false，即由mappedBy来维护关系。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;3、&lt;code&gt;＠ManyToOne&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;多对一映射注解，在双向的一对多关系中，一端一方使用 @OneToMany 注解，多端的一方使用 @ManyToOne 注解。多对一注解用法很简单，它不用维护关系。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;用法：&lt;/strong&gt; &lt;code&gt;@ManyToOne(optional = false, fetch = FetchType.EAGER)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;4、 &lt;code&gt;@ManyToMany&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;多对多映射，采取中间表连接的映射策略，建立的中间关系表分别引入两边的主键作为外键，形成两个多对一关系。&lt;/p&gt;

&lt;p&gt;双向的多对多关系中，在关系维护端(owner side)的 @ManyToMany 注解中添加 mappedBy 属性，另一方是关系的被维护端(inverse side)，关系的被维护端不能加 mappedBy 属性，建表时，根据两个多端的主键生成一个中间表，中间表的外键是两个多端的主键。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;用法：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;关系维护端——&amp;gt; &lt;code&gt;@ManyToMany(mappedBy=&quot;另一方的关系引用属性&quot;)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;关系被维护端——&amp;gt; &lt;code&gt;@ManyToMany(cascade=CascadeType.ALL ,fetch = FetchType.Lazy)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-7&quot;&gt;4.2 实体关联映射策略&lt;/h2&gt;

&lt;h3 id=&quot;section-8&quot;&gt;4.2.1 一对一关联映射&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;(1).一对一主键关联：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;一对一关联映射中，主键关联策略不会在两个关联实体对应的数据库表中添加外键字段，两个实体的表公用同一个主键(主键相同)，其中一个实体的主键既是主键又是外键。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;主键关联映射：&lt;/strong&gt;在实体关联属性或方法上添加 @OneToOne 注解的同时添加 &lt;code&gt;@PrimaryKeyJoinColumn&lt;/code&gt; 注解(在一对一注解关联映射的任意一端实体添加即可)。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(2).一对一唯一外键关联：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;一对一关联关系映射中，唯一外键关联策略会在其中一个实体对应数据库表中添加外键字段指向另一个实体表的主键，也是一对一映射关系中最常用的映射策略。&lt;/p&gt;

&lt;p&gt;唯一外键关联：在关联属性或字段上添加 @OneToOne 注解的同时添加 &lt;code&gt;@JoinColumn(name=”数据表列名”，unique=true)&lt;/code&gt; 注解。&lt;/p&gt;

&lt;h3 id=&quot;section-9&quot;&gt;4.2.2 一对多关联映射&lt;/h3&gt;

&lt;p&gt;在JPA中两个实体之间是一对多关系的称为一对多关联关系映射，如班级和学生关系。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(1).一对多单向关联映射：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在一对多单向关联映射中，JPA 会在数据库中自动生成公有的中间表记录关联关系的情况。在一端关联集合属性或字段上添加 @OneToMany 注解即可。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(2).一对多双向关联映射：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在一对多双向关联映射中，JPA 不会在数据库中生成公有中间表。在一端关联集合属性或字段上添加 @OneToMany 注解，同时指定其 mappedBy 属性。&lt;br /&gt;
在多端关联属性或字段上添加 @ManyToOne 注解。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;一对多关系映射中，mappedBy 只能添加在 OneToMany 注解中，即在多端生成外键。&lt;/p&gt;

&lt;h3 id=&quot;section-10&quot;&gt;4.2.3 多对多关联映射&lt;/h3&gt;

&lt;p&gt;在JPA中两个实体之间是多对多关系的称为多对多关联关系映射，如学生和教师关系。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(1).多对多单向映射：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在其中任意实体一方关联属性或字段上添加 @ManyToMany 注解。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(2).多对多双向映射：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;关系维护端关联属性或字段上添加 @ManyToMany 注解，同时指定该注解的 mappedBy 属性。&lt;/p&gt;

&lt;p&gt;关系被维护端关联属性或字段上添加 @ManyToMany 注解。&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;4.3 实体继承映射策略&lt;/h2&gt;

&lt;p&gt;在 JPA 中，&lt;strong&gt;实体继承关系的映射策略共有三种：单表继承策略、Joined 策略和 TABLE_PER_CLASS 策略&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;section-12&quot;&gt;4.3.1 单表继承策略：&lt;/h3&gt;

&lt;p&gt;单表继承策略，父类实体和子类实体共用一张数据库表，在表中通过一列辨别字段来区别不同类别的实体。具体做法如下：&lt;/p&gt;

&lt;p&gt;a.在父类实体的 @Entity 注解下添加如下的注解：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Inheritance(Strategy=InheritanceType.SINGLE_TABLE)  
@DiscriminatorColumn(name=&quot;辨别字段列名&quot;)  
@DiscriminatorValue(父类实体辨别字段列值)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b.在子类实体的 @Entity 注解下添加如下的注解：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@DiscriminatorValue(子类实体辨别字段列值)  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;joined-&quot;&gt;4.3.2 Joined 策略：&lt;/h3&gt;

&lt;p&gt;Joined 策略，父类实体和子类实体分别对应数据库中不同的表，子类实体的表中只存在其扩展的特殊属性，父类的公共属性保存在父类实体映射表中。&lt;/p&gt;

&lt;p&gt;具体做法：只需在父类实体的 @Entity 注解下添加如下注解：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Inheritance(Strategy=InheritanceType.JOINED)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;子类实体不需要特殊说明。&lt;/p&gt;

&lt;h3 id=&quot;tableperclass-&quot;&gt;4.3.3 TABLE_PER_CLASS 策略：&lt;/h3&gt;

&lt;p&gt;TABLE_PER_CLASS 策略，父类实体和子类实体每个类分别对应一张数据库中的表，子类表中保存所有属性，包括从父类实体中继承的属性。&lt;/p&gt;

&lt;p&gt;具体做法：只需在父类实体的 @Entity 注解下添加如下注解：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Inheritance(Strategy=InheritanceType.TABLE_PER_CLASS)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;子类实体不需要特殊说明。&lt;/p&gt;

&lt;h1 id=&quot;section-13&quot;&gt;5. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://www.yiibai.com/jpa/&quot;&gt;JPA教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://blog.csdn.net/chjttony/article/details/6086298&quot;&gt;JPA学习笔记1——JPA基础&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/12/02/some-usages-of-jpa.html</link>
      <guid>http://blog.javachen.com/2014/12/02/some-usages-of-jpa.html</guid>
      <pubDate>2014-12-02T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hadoop集群部署权限总结</title>
      <description>&lt;p&gt;这是一篇总结的文章，主要介绍 Hadoop 集群快速部署权限的步骤以及一些注意事项。如果你想了解详细的过程，请参考本博客中其他的文章。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 开始之前&lt;/h1&gt;

&lt;p&gt;hadoop 集群一共有三个节点，每个节点的 ip、hostname、角色如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;192.168.56.121 cdh1 NameNode、kerberos-server、ldap-server、sentry-store
192.168.56.122 cdh2 DataNode、yarn、hive、impala
192.168.56.123 cdh3 DataNode、yarn、hive、impala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一些注意事项：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统为 CentOs6.2&lt;/li&gt;
  &lt;li&gt;Hadoop 版本为 CDH5.2&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;hostname 请使用小写&lt;/strong&gt;，因为 kerberos 中区分大小写，而 hadoop 中会使用 hostname 的小写替换 &lt;code&gt;_HOST&lt;/code&gt;，impala 直接使用 hostname 替换 &lt;code&gt;_HOST&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;开始之前，请确认 hadoop 集群部署安装成功，不管是否配置 HA，请规划好每个节点的角色。我这里为了简单，以三个节点的集群为例做说明，你可以参考本文并结合你的实际情况做调整。&lt;/li&gt;
  &lt;li&gt;请确认防火墙关闭，以及集群内和 kerberos 以及 ldap 服务器保持&lt;strong&gt;时钟同步&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;cdh1 为管理节点，故需要做好 cdh1 到集群所有节点的&lt;strong&gt;无密码登陆&lt;/strong&gt;，包括其本身。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群中每个节点的 hosts 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /etc/hosts
127.0.0.1       localhost

192.168.56.121 cdh1
192.168.56.122 cdh2
192.168.56.123 cdh3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了方便管理集群，使用 cdh1 作为管理节点，并在 /opt/shell 目录编写了几脚本，/opt/shell/cmd.sh 用于批量执行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /opt/shell/cmd.sh

#!/bin/sh

for node in 121 122 123;do
	echo &quot;===============&quot;192.168.56.$node&quot;===============&quot;
	ssh 192.168.56.$node $1
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/opt/shell/cmd.sh 用于批量执行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /opt/shell/syn.sh

#!/bin/sh

for node in 121 122 123;do
	echo &quot;===============&quot;192.168.56.$node&quot;===============&quot;
	scp -r $1 192.168.56.$node:$2
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/opt/shell/cluster.sh 用于批量维护集群各个服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /opt/shell/cluster.sh
#!/bin/sh
for node in 121 122 123;do
	echo &quot;===============&quot;192.168.56.$node&quot;===============&quot;
	ssh 192.168.56.$node &#39;for src in `ls /etc/init.d|grep &#39;$1&#39;`;do service $src &#39;$2&#39;; done&#39;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;kerberos&quot;&gt;2. 安装 kerberos&lt;/h1&gt;

&lt;p&gt;在 cdh1 节点修改 /etc/krb5.conf 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[logging]
 default = FILE:/var/log/krb5libs.log
 kdc = FILE:/var/log/krb5kdc.log
 admin_server = FILE:/var/log/kadmind.log

[libdefaults]
 default_realm = JAVACHEN.COM
 dns_lookup_realm = false
 dns_lookup_kdc = false
 ticket_lifetime = 24h
 renew_lifetime = 7d
 forwardable = true
 default_tgs_enctypes = aes256-cts-hmac-sha1-96
 default_tkt_enctypes = aes256-cts-hmac-sha1-96
 permitted_enctypes = aes256-cts-hmac-sha1-96
 clockskew = 120
 udp_preference_limit = 1

[realms]
 JAVACHEN.COM = {
  kdc = cdh1
  admin_server = cdh1
 }

[domain_realm]
 .javachen.com = JAVACHEN.COM
 javachen.com = JAVACHEN.COM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 /var/kerberos/krb5kdc/kdc.conf 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[kdcdefaults]
 kdc_ports = 88
 kdc_tcp_ports = 88

[realms]
 JAVACHEN.COM = {
  #master_key_type = aes256-cts
  acl_file = /var/kerberos/krb5kdc/kadm5.acl
  dict_file = /usr/share/dict/words
  max_renewable_life = 7d
  max_life = 1d
  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
  default_principal_flags = +renewable, +forwardable
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 /var/kerberos/krb5kdc/kadm5.acl  如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*/admin@JAVACHEN.COM  *
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 cdh1 上的 /etc/krb5.conf 同步到集群各个节点上：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sh /opt/shell/syn.sh /etc/krb5.conf /etc/krb5.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写了一个脚本安装和初始化 kerberos，供大家参考（详细的脚本，请参考 &lt;a href=&quot;https://github.com/javachen/hadoop-install/tree/master/shell/bin/install_kerberos.sh&quot;&gt;install_kerberos.sh&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/javachen/hadoop-install/tree/master/shell/bin/init_kerberos.sh&quot;&gt;init_kerberos.sh&lt;/a&gt; ）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# install the kerberos components
yum install -y krb5-server
yum install -y openldap-clients
yum install -y krb5-workstation

rm -rf /var/kerberos/krb5kdc/*.keytab /var/kerberos/krb5kdc/prin*

kdb5_util create -r JAVACHEN.COM -s

chkconfig --level 35 krb5kdc on
chkconfig --level 35 kadmin on
service krb5kdc restart
service kadmin restart

echo -e &quot;root\nroot&quot; | kadmin.local -q &quot;addprinc root/admin&quot;

DNS=JAVACHEN.COM
HOSTNAME=`hostname -i`

#读取/etc/host文件中ip为 192.168.56 开头的机器名称并排除自己（kerberos 服务器）
for host in  `cat /etc/hosts|grep 192.168.56|grep -v $HOSTNAME|awk &#39;{print $2}&#39;` ;do
	for user in hdfs; do
		kadmin.local -q &quot;addprinc -randkey $user/$host@$DNS&quot;
		kadmin.local -q &quot;xst -k /var/kerberos/krb5kdc/$user-un.keytab $user/$host@$DNS&quot;
	done
	for user in HTTP hive yarn mapred impala zookeeper zkcli hbase llama sentry solr hue; do
		kadmin.local -q &quot;addprinc -randkey $user/$host@$DNS&quot;
		kadmin.local -q &quot;xst -k /var/kerberos/krb5kdc/$user.keytab $user/$host@$DNS&quot;
	done
done

# 合并
cd /var/kerberos/krb5kdc/
echo -e &quot;rkt hdfs-un.keytab\nrkt HTTP.keytab\nwkt hdfs.keytab&quot; | ktutil

#kerberos 重新初始化之后，还需要添加下面代码用于集成 ldap

kadmin.local -q &quot;addprinc ldapadmin@JAVACHEN.COM&quot;
kadmin.local -q &quot;addprinc -randkey ldap/cdh1@JAVACHEN.COM&quot;
kadmin.local -q &quot;ktadd -k /etc/openldap/ldap.keytab ldap/cdh1@JAVACHEN.COM&quot;

/etc/init.d/slapd restart

#测试 ldap 是否可以正常使用
ldapsearch -x -b &#39;dc=javachen,dc=com&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行上面的脚本，然后将上面生成的 keytab 同步到其他节点并设置权限：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sh /opt/shell/syn.sh /opt/keytab/hdfs.keytab /etc/hadoop/conf/
sh /opt/shell/syn.sh /opt/keytab/mapred.keytab /etc/hadoop/conf/
sh /opt/shell/syn.sh /opt/keytab/yarn.keytab /etc/hadoop/conf/
sh /opt/shell/syn.sh /opt/keytab/hive.keytab /etc/hive/conf/
sh /opt/shell/syn.sh /opt/keytab/impala.keytab /etc/impala/conf/
sh /opt/shell/syn.sh /opt/keytab/zookeeper.keytab /etc/zookeeper/conf/
sh /opt/shell/syn.sh /opt/keytab/zkcli.keytab /etc/zookeeper/conf/
sh /opt/shell/syn.sh /opt/keytab/sentry.keytab /etc/sentry/conf/

sh /opt/shell/cmd.sh &quot;chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/*.keytab&quot;
sh /opt/shell/cmd.sh &quot;chown mapred:hadoop /etc/hadoop/conf/mapred.keytab ;chmod 400 /etc/hadoop/conf/*.keytab&quot;
sh /opt/shell/cmd.sh &quot;chown yarn:hadoop /etc/hadoop/conf/yarn.keytab ;chmod 400 /etc/hadoop/conf/*.keytab&quot;
sh /opt/shell/cmd.sh &quot;chown hive:hadoop /etc/hive/conf/hive.keytab ;chmod 400 /etc/hive/conf/*.keytab&quot;
sh /opt/shell/cmd.sh &quot;chown impala:hadoop /etc/impala/conf/impala.keytab ;chmod 400 /etc/impala/conf/*.keytab&quot;
sh /opt/shell/cmd.sh &quot;chown zookeeper:hadoop /etc/zookeeper/conf/*.keytab ;chmod 400 /etc/zookeeper/conf/*.keytab&quot;

# sentry 只安装在 cdh1 节点
chown sentry:hadoop /etc/sentry/conf/*.keytab ;chmod 400 /etc/sentry/conf/*.keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在集群中每个节点安装 kerberos 客户端：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sh /opt/shell/cmd.sh &quot;yum install krb5-workstation -y&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;批量获取 root/admin 用户的 ticket&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sh /opt/shell/cmd.sh &quot;echo root|kinit root/admin&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hadoop--kerberos&quot;&gt;3. hadoop 集成 kerberos&lt;/h1&gt;

&lt;p&gt;更新每个节点上的 JCE 文件并修改 /etc/default/hadoop-hdfs-datanode，并且修改 hdfs、yarn、mapred、hive 的配置文件。&lt;/p&gt;

&lt;p&gt;如果配置了 HA，则先配置 zookeeper 集成 kerberos。&lt;/p&gt;

&lt;p&gt;同步配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sh /opt/shell/syn.sh /etc/hadoop/conf /etc/hadoop
sh /opt/shell/syn.sh /etc/zookeeper/conf /etc/zookeeper

sh /opt/shell/cmd.sh &quot;cd /etc/hadoop/conf/; chown root:yarn container-executor.cfg ; chmod 400 container-executor.cfg&quot;

sh /opt/shell/syn.sh /etc/hive/conf /etc/hive
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来就是依次获取每个服务对应的 ticket 并启动对应的服务，我创建了一个脚本 /opt/shell/manager_cluster.sh 来做这件事：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

role=$1
dir=$role
command=$2

if [ X&quot;$role&quot; == X&quot;hdfs&quot; ];then
	dir=hadoop
fi

if [ X&quot;$role&quot; == X&quot;yarn&quot; ];then
        dir=hadoop
fi

if [ X&quot;$role&quot; == X&quot;mapred&quot; ];then
        dir=hadoop
fi

echo $dir $role $command
for node in 121 122 123 ;do
	echo &quot;========192.168.56.$node========&quot;
	ssh 192.168.56.$node &#39;
		host=`hostname -f| tr &quot;[:upper:]&quot; &quot;[:lower:]&quot;`
		path=&quot;&#39;$role&#39;/$host&quot;
		#echo $path
		principal=`klist -k /etc/&#39;$dir&#39;/conf/&#39;$role&#39;.keytab | grep $path | head -n1 | cut -d &quot; &quot; -f5`
		echo $principal
		if [ X&quot;$principal&quot; == X ]; then
			principal=`klist -k /etc/&#39;$dir&#39;/conf/&#39;$role&#39;.keytab | grep $path | head -n1 | cut -d &quot; &quot; -f4`
			echo $principal
			if [ X&quot;$principal&quot; == X ]; then
					echo &quot;Failed to get hdfs Kerberos principal&quot;
					exit 1
			fi
		fi
		kinit -r 24l -kt /etc/&#39;$dir&#39;/conf/&#39;$role&#39;.keytab $principal
		if [ $? -ne 0 ]; then
				echo &quot;Failed to login as hdfs by kinit command&quot;
				exit 1
		fi
		kinit -R
		for src in `ls /etc/init.d|grep &#39;$role&#39;`;do service $src &#39;$command&#39;; done

	&#39;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 启动 zookeeper
sh /opt/shell/manager_cluster.sh zookeeper restart

# 获取 hdfs 服务的 ticket
sh /opt/shell/manager_cluster.sh hdfs status

# 使用普通脚本依次启动 hadoop-hdfs-zkfc、hadoop-hdfs-journalnode、hadoop-hdfs-namenode、hadoop-hdfs-datanode
sh /opt/shell/cluster.sh hadoop-hdfs-zkfc restart
sh /opt/shell/cluster.sh hadoop-hdfs-journalnode restart
sh /opt/shell/cluster.sh hadoop-hdfs-namenode restart
sh /opt/shell/cluster.sh hadoop-hdfs-datanode restart

sh /opt/shell/manager_cluster.sh yarn restart
sh /opt/shell/manager_cluster.sh mapred restart

sh /opt/shell/manager_cluster.sh hive restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 impala 配置文件并同步到其他节点，然后启动 impala 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;\cp /etc/hadoop/conf/core-site.xml /etc/impala/conf/
\cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf/
\cp /etc/hive/conf/hive-site.xml /etc/impala/conf/

sh /opt/shell/syn.sh /etc/impala/conf /etc/impala/
sh /opt/shell/syn.sh /etc/default/impala /etc/default/impala
sh /opt/shell/manager_cluster.sh impala restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此，集群应该启动成功了。&lt;/p&gt;

&lt;h1 id=&quot;java--kerberos&quot;&gt;3 使用 java 代码测试 kerberos&lt;/h1&gt;

&lt;p&gt;在 hdfs 中集成 kerberos 之前，可以先使用下面代码(Krb.java)进行测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import com.sun.security.auth.module.Krb5LoginModule;

import javax.security.auth.Subject;
import java.io.File;
import java.io.FileInputStream;
import java.io.InputStream;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;

public class Krb {
    private void loginImpl(final String propertiesFileName) throws Exception {
        System.out.println(&quot;NB: system property to specify the krb5 config: [java.security.krb5.conf]&quot;);
        //System.setProperty(&quot;java.security.krb5.conf&quot;, &quot;/etc/krb5.conf&quot;);

        System.out.println(System.getProperty(&quot;java.version&quot;));

        System.setProperty(&quot;sun.security.krb5.debug&quot;, &quot;true&quot;);

        final Subject subject = new Subject();

        final Krb5LoginModule krb5LoginModule = new Krb5LoginModule();
        final Map&amp;lt;String,String&amp;gt; optionMap = new HashMap&amp;lt;String,String&amp;gt;();

        if (propertiesFileName == null) {
            //optionMap.put(&quot;ticketCache&quot;, &quot;/tmp/krb5cc_1000&quot;);
            optionMap.put(&quot;keyTab&quot;, &quot;/etc/krb5.keytab&quot;);
            optionMap.put(&quot;principal&quot;, &quot;foo&quot;); // default realm

            optionMap.put(&quot;doNotPrompt&quot;, &quot;true&quot;);
            optionMap.put(&quot;refreshKrb5Config&quot;, &quot;true&quot;);
            optionMap.put(&quot;useTicketCache&quot;, &quot;true&quot;);
            optionMap.put(&quot;renewTGT&quot;, &quot;true&quot;);
            optionMap.put(&quot;useKeyTab&quot;, &quot;true&quot;);
            optionMap.put(&quot;storeKey&quot;, &quot;true&quot;);
            optionMap.put(&quot;isInitiator&quot;, &quot;true&quot;);
        } else {
            File f = new File(propertiesFileName);
            System.out.println(&quot;======= loading property file [&quot;+f.getAbsolutePath()+&quot;]&quot;);
            Properties p = new Properties();
            InputStream is = new FileInputStream(f);
            try {
                p.load(is);
            } finally {
                is.close();
            }
            optionMap.putAll((Map)p);
        }
        optionMap.put(&quot;debug&quot;, &quot;true&quot;); // switch on debug of the Java implementation

        krb5LoginModule.initialize(subject, null, new HashMap&amp;lt;String,String&amp;gt;(), optionMap);

        boolean loginOk = krb5LoginModule.login();
        System.out.println(&quot;======= login:  &quot; + loginOk);

        boolean commitOk = krb5LoginModule.commit();
        System.out.println(&quot;======= commit: &quot; + commitOk);
        System.out.println(&quot;======= Subject: &quot; + subject);
    }

    public static void main(String[] args) throws Exception {
        System.out.println(&quot;A property file with the login context can be specified as the 1st and the only paramater.&quot;);
        final Krb krb = new Krb();
        krb.loginImpl(args.length == 0 ? null : args[0]);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个配置文件krb5.properties：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;keyTab=/etc/hadoop/conf/hdfs.keytab
principal=hdfs/cdh1@JAVACHEN.COM

doNotPrompt=true
refreshKrb5Config=true
useTicketCache=true
renewTGT=true
useKeyTab=true
storeKey=true
isInitiator=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译 java 代码并运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;
# 先销毁当前 ticket

$ kdestroy

$ javac Krb.java

$ java -cp . Krb ./krb5.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;ldap&quot;&gt;4. 安装 ldap&lt;/h1&gt;

&lt;p&gt;使用下面命令在 cdh1 节点快速安装 ldap-server：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;yum install db4 db4-utils db4-devel cyrus-sasl* krb5-server-ldap -y
yum install openldap openldap-servers openldap-clients openldap-devel compat-openldap -y

# 更新配置库：
rm -rf /var/lib/ldap/*
cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG
chown -R ldap.ldap /var/lib/ldap

# 备份原来的 slapd-conf
cp -rf /etc/openldap/slapd.d /etc/openldap/slapd.d.bak

cp /usr/share/doc/krb5-server-ldap-1.10.3/kerberos.schema /etc/openldap/schema/
touch /etc/openldap/slapd.conf

echo &quot;include /etc/openldap/schema/corba.schema
include /etc/openldap/schema/core.schema
include /etc/openldap/schema/cosine.schema
include /etc/openldap/schema/duaconf.schema
include /etc/openldap/schema/dyngroup.schema
include /etc/openldap/schema/inetorgperson.schema
include /etc/openldap/schema/java.schema
include /etc/openldap/schema/misc.schema
include /etc/openldap/schema/nis.schema
include /etc/openldap/schema/openldap.schema
include /etc/openldap/schema/ppolicy.schema
include /etc/openldap/schema/collective.schema
include /etc/openldap/schema/kerberos.schema&quot; &amp;gt; /etc/openldap/slapd.conf

echo -e &quot;pidfile /var/run/openldap/slapd.pid\nargsfile /var/run/openldap/slapd.args&quot; &amp;gt;&amp;gt; /etc/openldap/slapd.conf
slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d
chown -R ldap:ldap /etc/openldap/slapd.d &amp;amp;&amp;amp; chmod -R 700 /etc/openldap/slapd.d

#重启服务
chkconfig --add slapd
chkconfig --level 345 slapd on

/etc/init.d/slapd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集成 kerberos：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 创建管理员用户
kadmin.local -q &quot;addprinc ldapadmin@JAVACHEN.COM&quot;
kadmin.local -q &quot;addprinc -randkey ldap/cdh1@JAVACHEN.COM&quot;

rm -rf /etc/openldap/ldap.keytab
kadmin.local -q &quot;ktadd -k /etc/openldap/ldap.keytab ldap/cdh1@JAVACHEN.COM&quot;

chown -R ldap:ldap /etc/openldap/ldap.keytab
/etc/init.d/slapd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 modify.ldif 文件用于更新数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dn: olcDatabase={2}bdb,cn=config
changetype: modify
replace: olcSuffix
olcSuffix: dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
replace: olcRootDN
# Temporary lines to allow initial setup
olcRootDN: uid=ldapadmin,ou=people,dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
add: olcRootPW
olcRootPW: secret

dn: cn=config
changetype: modify
add: olcAuthzRegexp
olcAuthzRegexp: uid=([^,]*),cn=GSSAPI,cn=auth uid=$1,ou=people,dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
add: olcAccess
# Everyone can read everything
olcAccess: {0}to dn.base=&quot;&quot; by * read
# The ldapadm dn has full write access
olcAccess: {1}to * by dn=&quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; write by * read
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行下面命令更新数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ldapmodify -Y EXTERNAL -H ldapi:/// -f modify.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加用户和组，创建 setup.ldif 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dn: dc=javachen,dc=com
objectClass: top
objectClass: dcObject
objectclass: organization
o: javachen com
dc: javachen

dn: ou=people,dc=javachen,dc=com
objectclass: organizationalUnit
ou: people
description: Users

dn: ou=group,dc=javachen,dc=com
objectClass: organizationalUnit
ou: group

dn: uid=ldapadmin,ou=people,dc=javachen,dc=com
objectClass: inetOrgPerson
objectClass: posixAccount
objectClass: shadowAccount
cn: LDAP admin account
uid: ldapadmin
sn: ldapadmin
uidNumber: 1001
gidNumber: 100
homeDirectory: /home/ldap
loginShell: /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行下面命令导入到数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; -w secret -f setup.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，可以在 ldap 服务器上创建一些本地系统用户，然后将这些用户导入到 ldap 服务中。&lt;/p&gt;

&lt;p&gt;先安装 migrationtools 然后修改 /usr/share/migrationtools/migrate_common.ph 文件中的 defalut DNS domain 和 defalut base。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;
# 创建 admin 组
groupadd admin

# 创建 test 和 hive 用户，用于后面测试 sentry
useradd test hive
usermod -G admin test
usermod -G admin hive

# 将关键用户导入到 ldap
grep -E &quot;bi_|hive|test&quot; /etc/passwd  &amp;gt;/opt/passwd.txt
/usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldif
ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; -w secret -f /opt/passwd.ldif

# 将 admin 组导入到 ldap
grep -E &quot;admin&quot; /etc/group  &amp;gt;/opt/group.txt
/usr/share/migrationtools/migrate_group.pl /opt/group.txt /opt/group.ldif
ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; -w secret -f /opt/group.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，你可以依次为每个用户设置密码，使用下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ldappasswd -x -D &#39;uid=ldapadmin,ou=people,dc=javachen,dc=com&#39; -w secret &quot;uid=hive,ou=people,dc=javachen,dc=com&quot; -S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，这些用户和组都是存在于 ldap 服务器上的，需要将其远程挂载到 hadoop 的每个节点上，否则，你需要在每个节点创建对应的用户和组（目前，测试是这样的）。&lt;/p&gt;

&lt;h1 id=&quot;sentry&quot;&gt;6. 集成 sentry&lt;/h1&gt;

&lt;p&gt;这部分建议使用数据库的方式存储规则，不建议生产环境使用文件保存方式。&lt;/p&gt;

&lt;p&gt;详细的配置，请参考 &lt;a href=&quot;/2014/11/14/config-impala-and-hive-with-sentry.html&quot;&gt;Impala和Hive集成Sentry&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;通过 beeline 使用 &lt;code&gt;hive/cdh1@JAVACHEN.COM&lt;/code&gt; 连接 hive-server2 创建一些角色和组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create role admin_role;
GRANT ALL ON SERVER server1 TO ROLE admin_role;
GRANT ROLE admin_role TO GROUP admin;
GRANT ROLE admin_role TO GROUP hive;

create role test_role;
GRANT ALL ON DATABASE testdb TO ROLE test_role;
GRANT ALL ON DATABASE default TO ROLE test_role;
GRANT ROLE test_role TO GROUP test;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面 amdin 和 hive 组具有所有数据库的管理员权限，而 test 组只有 testdb 和 default 库的读写权限。&lt;/p&gt;

&lt;p&gt;在 impala-shell 中通过 ldap 的方式传入不同的用户，可以测试读写权限。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;7. 如何添加新用户并设置权限？&lt;/h1&gt;

&lt;p&gt;下面以 test2 账号为例，说明如何添加新的用户并设置访问权限。test2 需要具有以下权限&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;dw_default 库：读权限&lt;/li&gt;
  &lt;li&gt;dw_user 库 t1表：读权限&lt;/li&gt;
  &lt;li&gt;dw_user 库 t2 表：读权限&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 LDAP 服务器上 上添加 LDAP 用户并设置密码，首先添加系统用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;useradd test2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用 LDAP 工具将该用户导入到 LDAP：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;grep -E &quot;test2&quot; /etc/passwd  &amp;gt;/opt/passwd.txt
/usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldif
ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; -w secret -f /opt/passwd.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给 test2 用户生成一个随机密码，然后修改 LDAP 中 test2 的密码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ldappasswd -x -D &#39;uid=ldapadmin,ou=people,dc=javachen,dc=com&#39; -w secret &quot;uid=test2,ou=people,dc=javachen,dc=com&quot; -S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在每台datanode机器上创建 test2 用户和 secure_analyst 分组，test2 属于 secure_analyst 分组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sh /opt/shell/cmd.sh &quot;groupadd secure_analyst ; useradd test2; usermod -G secure_analyst,test2 test2&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 hive 中创建角色：&lt;/p&gt;

&lt;p&gt;运行 &lt;code&gt;beeline -u &quot;jdbc:hive2://cdh1:10000/default;principal=hive/cdh1@JAVACHEN.COM&quot;&lt;/code&gt;，然后输入下面语句在 sentry 中创建角色和授予权限给 secure_analyst 组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create role dw_default_r;
GRANT SELECT ON DATABASE dw_default TO ROLE dw_default_r;
 
create role dw_user;
GRANT SELECT ON DATABASE dw_user TO ROLE dw_user_r;
 
use dw_user;
create role dw_user_secure_r;
GRANT SELECT ON table t1 TO ROLE dw_user_secure_r;
GRANT SELECT ON table t2 TO ROLE dw_user_secure_r;
  
GRANT ROLE dw_default_r TO GROUP secure_analyst;
GRANT ROLE dw_user_secure_r TO GROUP secure_analyst;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，需要 impala 刷新元数据，然后进行测试，可能会需要一些时间 impala-catalog 才能刷新过来。&lt;/p&gt;

&lt;p&gt;最后进行测试，这部分略。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop.html</link>
      <guid>http://blog.javachen.com/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop.html</guid>
      <pubDate>2014-11-25T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spring集成JPA2.0</title>
      <description>&lt;p&gt;JPA 全称 Java Persistence API，是Java EE 5标准之一，是一个 ORM 规范，由厂商来实现该规范，目前有 Hibernate、OpenJPA、TopLink、EclipseJPA 等实现。Spring目前提供集成Hibernate、OpenJPA、TopLink、EclipseJPA四个JPA标准实现。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 集成方式&lt;/h1&gt;

&lt;p&gt;Spring提供三种方法集成JPA：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;LocalEntityManagerFactoryBean：适用于那些仅使用JPA进行数据访问的项目。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;从JNDI中获取：用于从Java EE服务器中获取指定的EntityManagerFactory，这种方式在Spring事务管理时一般要使用JTA事务管理。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;LocalContainerEntityManagerFactoryBean：适用于所有环境的FactoryBean，能全面控制EntityManagerFactory配置，非常适合那种需要细粒度定制的环境。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;localentitymanagerfactorybean&quot;&gt;1.1 LocalEntityManagerFactoryBean&lt;/h2&gt;

&lt;p&gt;仅在简单部署环境中只使用这种方式，比如独立的应用程序和集成测试。该 FactoryBean 根据 JPA PersistenceProvider自动检测配置文件进行工作，一般从 &lt;code&gt;META-INF/persistence.xml&lt;/code&gt; 读取配置信息。这种方式最简单，但是不能设置 Spring 中定义的 DataSource，且不支持 Spring 管理的全局事务，甚至，持久化类的织入（字节码转换）也是特定于提供者的，经常需要在启动时指定一个特定的JVM代理。&lt;strong&gt;这种方法实际上只适用于独立的应用程序和测试环境，不建议使用此方式。&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;entityManagerFactory&quot; class=&quot;org.springframework.orm.jpa.LocalEntityManagerFactoryBean&quot;&amp;gt;
       &amp;lt;property name=&quot;persistenceUnitName&quot; value=&quot;persistenceUnit&quot;/&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;persistenceUnit 对应 META-INF/persistence.xml 中 persistence-unit 节点的 name 属性值。&lt;/p&gt;

&lt;h2 id=&quot;jndi&quot;&gt;1.2 JNDI中获取&lt;/h2&gt;

&lt;p&gt;Spring 中的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xmlns:jee=&quot;http://www.springframework.org/schema/jee&quot;
    xsi:schemaLocation=&quot;
       http://www.springframework.org/schema/beans
       http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
       http://www.springframework.org/schema/jee
       http://www.springframework.org/schema/jee/spring-jee-3.0.xsd&quot;&amp;gt;

  &amp;lt;jee:jndi-lookup id=&quot;entityManagerFactory&quot;  jndi-name=&quot;persistence/persistenceUnit&quot;/&amp;gt;

&amp;lt;/beans&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此处需要使用 &lt;code&gt;jee&lt;/code&gt; 命名标签，且使用 &lt;code&gt;&amp;lt;jee:jndi-lookup&amp;gt;&lt;/code&gt; 标签进行 JNDI 查找，&lt;code&gt;jndi-name&lt;/code&gt; 属性用于指定 JNDI 名字。&lt;/p&gt;

&lt;p&gt;在标准的 Java EE 5启动过程中，Java EE服务器自动检测持久化单元（例如应用程序文件包中的 &lt;code&gt;META-INF/persistence.xml&lt;/code&gt;） ，以及J ava EE 部署描述符中定义给那些持久化单元命名上下文位置的环境的 &lt;code&gt;persistence-unit-ref&lt;/code&gt; 项（例如 web.xml）。&lt;/p&gt;

&lt;p&gt;在这种情况下，整个持久化单元部署，包括持久化类的织入（字码码转换）都取决于 Java EE 服务器。 JDBC DataSource 通过在 &lt;code&gt;META-INF/persistence.xml&lt;/code&gt; 文件中的 JNDI 位置进行定义；EntityManager 事务与服务器的 JTA 子系统整合。Spring 仅仅用获得的  EntityManagerFactory ，通过依赖注入将它传递给应用程序对象，并为它管理事务（一般通过 JtaTransactionManager）。&lt;/p&gt;

&lt;p&gt;注意，如果在同一个应用程序中使用了多个持久化单元，JNDI 获取的这种持久化单元的 bean 名称 应该与应用程序用来引用它们的持久化单元名称相符（例如 &lt;code&gt;@PersistenceUnit&lt;/code&gt; 和 &lt;code&gt;@PersistenceContext&lt;/code&gt; 注解）。&lt;/p&gt;

&lt;p&gt;在部署到 Java EE 5 服务器时使用该方法。关于如何将自定义 JPA 提供者部署到服务器，以及允许使用服务器提供的缺省提供者之外的 JPA 提供者，请查看服务器文档的相关说明。&lt;/p&gt;

&lt;h2 id=&quot;localcontainerentitymanagerfactorybean&quot;&gt;1.3 LocalContainerEntityManagerFactoryBean&lt;/h2&gt;

&lt;p&gt;LocalContainerEntityManagerFactoryBean 提供了对JPA EntityManagerFactory 的全面控制，非常适合那种需要细粒度定制的环境。LocalContainerEntityManagerFactoryBean 将基于 persistence.xml 文件创建 PersistenceUnitInfo 类，并提供 dataSourceLookup 策略和 loadTimeWeaver。 因此它可以在JNDI之外的用户定义的数据源之上工作，并控制织入流程。&lt;/p&gt;

&lt;p&gt;Spring 中的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;entityManagerFactory&quot; class=&quot;org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean&quot;&amp;gt;
	&amp;lt;property name=&quot;persistenceUnitName&quot; value=&quot;persistenceUnit&quot; /&amp;gt;
	&amp;lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&amp;gt;
	&amp;lt;property name=&quot;jpaVendorAdapter&quot; ref=&quot;jpaVendorAdapter&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是最为强大的JPA配置方式，允许在应用程序中灵活进行本地配置。它支持连接现有JDBC DataSource ， 支持本地事务和全局事务等等。然而，它也将需求强加到了运行时环境中，例如，如果持久化提供者需要字节码转换，则必须有织入ClassLoader的能力。&lt;/p&gt;

&lt;p&gt;注意，这个选项可能与 Java EE 5 服务器内建的 JPA 功能相冲突。因此，当运行在完全 Java EE 5 环境中时， 要考虑从 JNDI 获取 EntityManagerFactory。另一种可以替代的方法是，在 LocalContainerEntityManagerFactoryBean 定义中通过 &lt;code&gt;persistenceXmlLocation&lt;/code&gt; 指定相关位置， 例如 &lt;code&gt;META-INF/my-persistence.xml&lt;/code&gt;，并且只将包含该名称的描述符放在应用程序包文件中。因为 Java EE 5 服务器将只 查找默认的 &lt;code&gt;META-INF/persistence.xml&lt;/code&gt; 文件，它会忽略这种定制的持久化单元，因而避免与前面 Spring 驱动的 JPA 配置冲突。&lt;/p&gt;

&lt;p&gt;一个配置实例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;entityManagerFactory&quot; class=&quot;org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean&quot;&amp;gt;
	&amp;lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&amp;gt;
	&amp;lt;property name=&quot;persistenceXmlLocation&quot; value=&quot;test/persistence.xml&quot;/&amp;gt;
	&amp;lt;!-- gDickens: BOTH Persistence Unit and Packages to Scan are NOT compatible, persistenceUnit will win --&amp;gt;
	&amp;lt;property name=&quot;persistenceUnitName&quot; value=&quot;persistenceUnit&quot;/&amp;gt;
	&amp;lt;property name=&quot;packagesToScan&quot; value=&quot;com.javachen.example.springmvc&quot;/&amp;gt;
	&amp;lt;property name=&quot;persistenceProvider&quot; ref=&quot;persistenceProvider&quot;/&amp;gt;
	&amp;lt;property name=&quot;jpaVendorAdapter&quot; ref=&quot;jpaVendorAdapter&quot;/&amp;gt;
	&amp;lt;property name=&quot;jpaDialect&quot; ref=&quot;jpaDialect&quot;/&amp;gt;
	&amp;lt;property name=&quot;jpaPropertyMap&quot; ref=&quot;jpaPropertyMap&quot;/&amp;gt;
&amp;lt;/bean&amp;gt;

&amp;lt;util:map id=&quot;jpaPropertyMap&quot;&amp;gt;
	&amp;lt;entry key=&quot;dialect&quot; value=&quot;${hibernate.dialect}&quot;/&amp;gt;
	&amp;lt;entry key=&quot;hibernate.ejb.naming_strategy&quot; value=&quot;${hibernate.ejb.naming_strategy}&quot;/&amp;gt;
	&amp;lt;entry key=&quot;hibernate.hbm2ddl.auto&quot; value=&quot;${hibernate.hbm2ddl.auto}&quot;/&amp;gt;
	&amp;lt;entry key=&quot;hibernate.cache.use_second_level_cache&quot; value=&quot;false&quot;/&amp;gt;
	&amp;lt;entry key=&quot;hibernate.cache.use_query_cache&quot; value=&quot;false&quot;/&amp;gt;
	&amp;lt;entry key=&quot;hibernate.generate_statistics&quot; value=&quot;false&quot;/&amp;gt;
	&amp;lt;entry key=&quot;show_sql&quot; value=&quot;${hibernate.show_sql}&quot;/&amp;gt;
	&amp;lt;entry key=&quot;format_sql&quot; value=&quot;${hibernate.format_sql}&quot;/&amp;gt;
&amp;lt;/util:map&amp;gt;

&amp;lt;bean id=&quot;persistenceProvider&quot; class=&quot;org.hibernate.ejb.HibernatePersistence&quot;/&amp;gt;

&amp;lt;bean id=&quot;jpaVendorAdapter&quot; class=&quot;org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter&quot;&amp;gt;
	&amp;lt;property name=&quot;generateDdl&quot; value=&quot;false&quot; /&amp;gt;
	&amp;lt;property name=&quot;showSql&quot; value=&quot;false&quot; /&amp;gt;
	&amp;lt;property name=&quot;database&quot; value=&quot;HSQL&quot;/&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean id=&quot;jpaDialect&quot; class=&quot;org.springframework.orm.jpa.vendor.HibernateJpaDialect&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;`LocalContainerEntityManagerFactoryBean：指定使用本地容器管理 EntityManagerFactory，从而进行细粒度控制；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dataSource&lt;/code&gt;：属性指定使用 Spring 定义的数据源；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;persistenceXmlLocation&lt;/code&gt;：指定 JPA 配置文件为 test/persistence.xml，且该配置文件非常简单，具体配置完全在Spring中进行；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;persistenceUnitName&lt;/code&gt;：指定持久化单元名字，即 JPA 配置文件中指定的;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;packagesToScan&lt;/code&gt;：指定扫描哪个包下的类，当 persistenceUnitName 和 packagesToScan 属性同时存在时，会使用 persistenceUnitName 属性&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;persistenceProvider&lt;/code&gt;：指定 JPA 持久化提供商，此处使用 Hibernate 实现 HibernatePersistence类；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;jpaVendorAdapter&lt;/code&gt;：指定实现厂商专用特性，即 &lt;code&gt;generateDdl= false&lt;/code&gt; 表示不自动生成 DDL，&lt;code&gt;database= HSQL&lt;/code&gt; 表示使用 hsqld b数据库；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;jpaDialect&lt;/code&gt;：如果指定 jpaVendorAdapter 此属性可选，此处为 HibernateJpaDialect；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;jpaPropertyMap&lt;/code&gt;：此处指定一些属性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;处理多持久化单元&lt;/h3&gt;

&lt;p&gt;对于那些依靠多个持久化单元位置(例如存放在 classpath 中的多个 jar 中)的应用程序， Spring 提供了作为中央仓库的 PersistenceUnitManager， 避免了持久化单元查找过程。缺省实现允许指定多个位置 (默认情况下 classpath 会搜索 META-INF/persistence.xml 文件)，它们会被解析然后通过持久化单元名称被获取：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;persistenceUnitManager&quot; class=&quot;org.springframework.orm.jpa.persistenceunit.DefaultPersistenceUnitManager&quot;&amp;gt;
	&amp;lt;property name=&quot;persistenceXmlLocation&quot;&amp;gt;
	    &amp;lt;list&amp;gt;
	     &amp;lt;value&amp;gt;org/springframework/orm/jpa/domain/persistence-multi.xml&amp;lt;/value&amp;gt;
	     &amp;lt;value&amp;gt;classpath:/my/package/**/custom-persistence.xml&amp;lt;/value&amp;gt;
	     &amp;lt;value&amp;gt;classpath*:META-INF/persistence.xml&amp;lt;/value&amp;gt;
	    &amp;lt;/list&amp;gt;
	&amp;lt;/property&amp;gt;
	&amp;lt;property name=&quot;dataSources&quot;&amp;gt;
	   &amp;lt;map&amp;gt;
	    &amp;lt;entry key=&quot;localDataSource&quot; value-ref=&quot;local-db&quot;/&amp;gt;
	    &amp;lt;entry key=&quot;remoteDataSource&quot; value-ref=&quot;remote-db&quot;/&amp;gt;
	   &amp;lt;/map&amp;gt;
	&amp;lt;/property&amp;gt;
	&amp;lt;!-- if no datasource is specified, use this one --&amp;gt;
	&amp;lt;property name=&quot;defaultDataSource&quot; ref=&quot;remoteDataSource&quot;/&amp;gt;
&amp;lt;/bean&amp;gt;

&amp;lt;bean id=&quot;entityManagerFactory&quot; class=&quot;org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean&quot;&amp;gt;
	&amp;lt;property name=&quot;persistenceUnitManager&quot; ref=&quot;persistenceUnitManager&quot;/&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/11/24/spring-with-jpa2.html</link>
      <guid>http://blog.javachen.com/2014/11/24/spring-with-jpa2.html</guid>
      <pubDate>2014-11-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Zookeeper配置Kerberos认证</title>
      <description>&lt;p&gt;关于 Hadoop 集群上配置 kerberos 以及 ldap 的过程请参考本博客以下文章：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/04/config-kerberos-in-cdh-hdfs.html&quot;&gt;HDFS配置Kerberos认证&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/05/config-kerberos-in-cdh-yarn.html&quot;&gt;YARN配置Kerberos认证&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/06/config-kerberos-in-cdh-hive.html&quot;&gt;Hive配置Kerberos认证&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/06/config-kerberos-in-cdh-impala.html&quot;&gt;Impala配置Kerberos认证&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop.html&quot;&gt;Hadoop配置LDAP集成Kerberos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考 &lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt; 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、Hive、ResourceManager、HBase、impala-state-store、impala-catalog、Kerberos Server、zookeeper-server
192.168.56.122        cdh2     DataNode、SSNameNode、NodeManager、HBase、impala-server、zookeeper-server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、impala-server、zookeeper-server
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;zookeeper-server&quot;&gt;1. 配置 ZooKeeper Server&lt;/h1&gt;

&lt;h1 id=&quot;keytab&quot;&gt;1.1 生成 keytab&lt;/h1&gt;

&lt;p&gt;在 cdh1 节点，即 KDC server 节点上执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/kerberos/krb5kdc/

kadmin.local -q &quot;addprinc -randkey zookeeper/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey zookeeper/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey zookeeper/cdh3@JAVACHEN.COM &quot;

kadmin.local -q &quot;xst  -k zookeeper.keytab  zookeeper/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k zookeeper.keytab  zookeeper/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k zookeeper.keytab  zookeeper/cdh3@JAVACHEN.COM &quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拷贝 zookeeper.keytab 文件到其他节点的 /etc/zookeeper/conf 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp zookeeper.keytab cdh1:/etc/zookeeper/conf
$ scp zookeeper.keytab cdh2:/etc/zookeeper/conf
$ scp zookeeper.keytab cdh3:/etc/zookeeper/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并设置权限，分别在 cdh1、cdh2、cdh3 上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &quot;cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh2 &quot;cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh3 &quot;cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400)&lt;/p&gt;

&lt;h2 id=&quot;zookeeper-&quot;&gt;1.2 修改 zookeeper 配置文件&lt;/h2&gt;

&lt;p&gt;在 cdh1 节点上修改 /etc/zookeeper/conf/zoo.cfg 文件，添加下面内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
jaasLoginRenew=3600000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将修改的上面文件同步到其他节点：cdh2、cdh3：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp /etc/zookeeper/conf/zoo.cfg cdh2:/etc/zookeeper/conf/zoo.cfg
$ scp /etc/zookeeper/conf/zoo.cfg cdh3:/etc/zookeeper/conf/zoo.cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;jaas-&quot;&gt;1.3 创建 JAAS 配置文件&lt;/h2&gt;

&lt;p&gt;在 cdh1 的配置文件目录创建 jaas.conf 文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Server {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  keyTab=&quot;/etc/zookeeper/conf/zookeeper.keytab&quot;
  storeKey=true
  useTicketCache=false
  principal=&quot;zookeeper/cdh1@JAVACHEN.COM&quot;;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样，在 cdh2 和 cdh3 节点也创建该文件，&lt;strong&gt;注意每个节点的 principal 有所不同&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;然后，在 /etc/zookeeper/conf/ 目录创建 java.env，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export JVMFLAGS=&quot;-Djava.security.auth.login.config=/etc/zookeeper/conf/jaas.conf&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并将该文件同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp /etc/zookeeper/conf/java.env cdh2:/etc/zookeeper/conf/java.env
$ scp /etc/zookeeper/conf/java.env cdh3:/etc/zookeeper/conf/java.env
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;1.4 重启服务&lt;/h2&gt;

&lt;p&gt;依次重启，并观察日志：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/etc/init.d/zookeeper-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;zookeeper-client&quot;&gt;2. 配置 ZooKeeper Client&lt;/h1&gt;

&lt;h1 id=&quot;keytab-1&quot;&gt;2.1 生成 keytab&lt;/h1&gt;

&lt;p&gt;在 cdh1 节点，即 KDC server 节点上执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/kerberos/krb5kdc/
kadmin.local -q &quot;addprinc -randkey zkcli/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey zkcli/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey zkcli/cdh3@JAVACHEN.COM &quot;

kadmin.local -q &quot;xst  -k zkcli.keytab  zkcli/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k zkcli.keytab  zkcli/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k zkcli.keytab  zkcli/cdh3@JAVACHEN.COM &quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拷贝 zkcli.keytab 文件到其他节点的 /etc/zookeeper/conf 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp zkcli.keytab cdh1:/etc/zookeeper/conf
$ scp zkcli.keytab cdh2:/etc/zookeeper/conf
$ scp zkcli.keytab cdh3:/etc/zookeeper/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并设置权限，分别在 cdh1、cdh2、cdh3 上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &quot;cd /etc/zookeeper/conf/;chown zookeeper:hadoop zkcli.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh2 &quot;cd /etc/zookeeper/conf/;chown zookeeper:hadoop zkcli.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh3 &quot;cd /etc/zookeeper/conf/;chown zookeeper:hadoop zkcli.keytab ;chmod 400 *.keytab&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400)&lt;/p&gt;

&lt;h2 id=&quot;jaas--1&quot;&gt;2.2 创建 JAAS 配置文件&lt;/h2&gt;

&lt;p&gt;在 cdh1 的配置文件目录 /etc/zookeeper/conf/ 创建 client-jaas.conf 文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Client {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  keyTab=&quot;/etc/zookeeper/conf/zkcli.keytab&quot;
  storeKey=true
  useTicketCache=false
  principal=&quot;zkcli@JAVACHEN.COM&quot;;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp client-jaas.conf cdh2:/etc/zookeeper/conf
$ scp client-jaas.conf cdh3:/etc/zookeeper/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在 /etc/zookeeper/conf/ 目录创建或者修改  java.env，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export CLIENT_JVMFLAGS=&quot;-Djava.security.auth.login.config=/etc/zookeeper/conf/client-jaas.conf&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果，zookeeper-client 和 zookeeper-server 安装在同一个节点上，则 java.env 中的 &lt;code&gt;java.security.auth.login.config&lt;/code&gt; 参数会被覆盖，这一点从 zookeeper-client 命令启动日志可以看出来。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;并将该文件同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp /etc/zookeeper/conf/java.env cdh2:/etc/zookeeper/conf/java.env
$ scp /etc/zookeeper/conf/java.env cdh3:/etc/zookeeper/conf/java.env
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2.3 验证&lt;/h2&gt;

&lt;p&gt;启动客户端：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ zookeeper-client -server cdh1:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个 znode 节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;k: cdh1:2181(CONNECTED) 0] create /znode1 sasl:zkcli@JAVACHEN.COM:cdwra
    Created /znode1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证该节点是否创建以及其 ACL：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[zk: cdh1:2181(CONNECTED) 1] getAcl /znode1
    &#39;world,&#39;anyone
    : cdrwa
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/11/18/config-kerberos-in-cdh-zookeeper.html</link>
      <guid>http://blog.javachen.com/2014/11/18/config-kerberos-in-cdh-zookeeper.html</guid>
      <pubDate>2014-11-18T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>配置安全的Impala集群集成Sentry</title>
      <description>&lt;p&gt;本文主要记录配置安全的Impala集群集成Sentry的过程。Impala集群上配置了Kerberos认证，并且需要提前配置好Hive与Kerberos和Sentry的集成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/06/config-kerberos-in-cdh-hive.html&quot;&gt;Hive配置kerberos认证&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/06/config-kerberos-in-cdh-impala.html&quot;&gt;Impala配置kerberos认证&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/14/config-secured-hive-with-sentry.html&quot;&gt;配置安全的Hive集群集成Sentry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;JDK版本：&lt;code&gt;1.7.0_71&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群各节点角色规划为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        cdh2     DataNode、NodeManager、HBase、Hiveserver2、Impala Server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、Hiveserver2、Impala Server
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;impala&quot;&gt;2. 修改Impala配置&lt;/h1&gt;

&lt;p&gt;修改 /etc/default/impala 文件中的 &lt;code&gt;IMPALA_SERVER_ARGS&lt;/code&gt; 参数，添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-server_name=server1
-sentry_config=/etc/hive/conf/sentry-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;IMPALA_CATALOG_ARGS&lt;/code&gt; 中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-sentry_config=/etc/hive/conf/sentry-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/etc/hive/conf/sentry-site.xml 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.client.server.rpc-port&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;8038&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.client.server.rpc-address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;cdh1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.client.server.rpc-connection-timeout&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;200000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.provider&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.provider.file.HadoopGroupResourceAuthorizationProvider&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.provider.backend&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.provider.db.SimpleDBProviderBackend&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.metastore.service.users&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;&amp;lt;!--queries made by hive user (beeline) skip meta store check--&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.server&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;server1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.testing.mode&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;impala-1&quot;&gt;3. 重启Impala服务&lt;/h1&gt;

&lt;p&gt;在cdh1节点&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;4. 测试&lt;/h1&gt;

&lt;h1 id=&quot;section-2&quot;&gt;5. 其他说明&lt;/h1&gt;

&lt;p&gt;如果要使用基于文件存储的方式配置Sentry store，则需要修改 /etc/default/impala 文件中的 &lt;code&gt;IMPALA_SERVER_ARGS&lt;/code&gt; 参数，添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;-server_name=server1
-authorization_policy_file=/user/hive/sentry/sentry-provider.ini
-authorization_policy_provider_class=org.apache.sentry.provider.file.LocalGroupResourceAuthorizationProvider
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 sentry-provider.ini 文件并将其上传到 hdfs 的 /user/hive/sentry/ 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /tmp/sentry-provider.ini
[databases]
# Defines the location of the per DB policy file for the customers DB/schema
#db1 = hdfs://cdh1:8020/user/hive/sentry/db1.ini

[groups]
admin = any_operation
hive = any_operation
test = select_filtered

[roles]
any_operation = server=server1-&amp;gt;db=*-&amp;gt;table=*-&amp;gt;action=*
select_filtered = server=server1-&amp;gt;db=filtered-&amp;gt;table=*-&amp;gt;action=SELECT
select_us = server=server1-&amp;gt;db=filtered-&amp;gt;table=events_usonly-&amp;gt;action=SELECT

[users]
test = test
hive= hive

$ hdfs dfs -rm -r /user/hive/sentry/sentry-provider.ini
$ hdfs dfs -put /tmp/sentry-provider.ini /user/hive/sentry/
$ hdfs dfs -chown hive:hive /user/hive/sentry/sentry-provider.ini
$ hdfs dfs -chmod 640 /user/hive/sentry/sentry-provider.ini
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：server1 必须和 sentry-provider.ini 文件中的保持一致。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;6. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.evernote.com/tech/2014/06/09/securing-impala-for-analysts/&quot;&gt;Securing Impala for analysts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/11/14/config-secured-impala-with-sentry.html</link>
      <guid>http://blog.javachen.com/2014/11/14/config-secured-impala-with-sentry.html</guid>
      <pubDate>2014-11-14T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>配置安全的Hive集群集成Sentry</title>
      <description>&lt;p&gt;本文主要记录配置安全的Hive集群集成Sentry的过程。Hive上配置了Kerberos认证，配置的过程请参考：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2014/11/06/config-kerberos-in-cdh-hive.html&quot;&gt;Hive配置kerberos认证&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;JDK版本：&lt;code&gt;1.7.0_71&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群各节点角色规划为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        cdh2     DataNode、NodeManager、HBase、Hiveserver2、Impala Server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、Hiveserver2、Impala Server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cdh1作为master节点，其他节点作为slave节点，我们在cdh1节点安装kerberos Server，在其他节点安装kerberos client。&lt;/p&gt;

&lt;h1 id=&quot;sentry&quot;&gt;2. 安装和配置Sentry&lt;/h1&gt;

&lt;p&gt;这部分内容，请参考&lt;a href=&quot;/2015/04/30/install-and-config-sentry.html&quot;&gt;安装和配置Sentry&lt;/a&gt;，因为集群中配置了kerberos，所以需要在KDC节点上（cdh1）生成 Sentry 服务的 principal 并导出为 ticket：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /etc/sentry/conf

$ kadmin.local -q &quot;addprinc -randkey sentry/cdh1@JAVACHEN.COM &quot;
$ kadmin.local -q &quot;xst -k sentry.keytab sentry/cdh1@JAVACHEN.COM &quot;

$ chown sentry:hadoop sentry.keytab ; chmod 400 *.keytab

$ cp sentry.keytab /etc/sentry/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，修改/etc/sentry/conf/sentry-site.xml 中下面的参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.security.mode&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kerberos&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.server.principal&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;sentry/cdh1@JAVACHEN.COM&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.server.keytab&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/etc/sentry/conf/sentry.keytab&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取Sentry的ticket再启动sentry-store服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/sentry/conf/sentry.keytab sentry/cdh1@JAVACHEN.COM
$ /etc/init.d/sentry-store start
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hive&quot;&gt;3. 配置Hive&lt;/h1&gt;

&lt;h2 id=&quot;hive-metastoresentry&quot;&gt;Hive Metastore集成Sentry&lt;/h2&gt;

&lt;p&gt;需要在 /etc/hive/conf/hive-site.xml中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hive.metastore.pre.event.listeners&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;org.apache.sentry.binding.metastore.MetastoreAuthzBinding&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hive.metastore.event.listeners&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;org.apache.sentry.binding.metastore.SentryMetastorePostEventListener&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hive-server2sentry&quot;&gt;Hive-server2集成Sentry&lt;/h2&gt;

&lt;p&gt;在Hive配置了Kerberos认证之后，Hive-server2集成Sentry有以下要求：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;修改 &lt;code&gt;/user/hive/warehouse&lt;/code&gt; 权限：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.COM

$ hdfs dfs -chmod -R 770 /user/hive/warehouse
$ hdfs dfs -chown -R hive:hive /user/hive/warehouse
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;禁止 HiveServer2 impersonation：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hive.server2.enable.impersonation&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;确认 /etc/hadoop/conf/container-executor.cfg 文件中 &lt;code&gt;min.user.id=0&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改 /etc/hive/conf/hive-site.xml：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hive.server2.enable.impersonation&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.security.authorization.task.factory&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.binding.hive.SentryHiveAuthorizationTaskFactoryImpl&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.session.hook&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.binding.hive.HiveAuthzBindingSessionHook&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.sentry.conf.url&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///etc/hive/conf/sentry-site.xml&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，因为集群配置了kerberos，故需要/etc/hive/conf/sentry-site.xml添加以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.security.mode&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kerberos&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.server.principal&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;sentry/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.service.server.keytab&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/etc/sentry/conf/sentry.keytab&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考模板&lt;a href=&quot;https://github.com/cloudera/sentry/blob/cdh5-1.4.0_5.4.0/conf%2Fsentry-site.xml.hive-client.template&quot;&gt;sentry-site.xml.hive-client.template&lt;/a&gt;在 /etc/hive/conf/ 目录创建 sentry-site.xml：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.client.server.rpc-port&amp;lt;/name&amp;gt;
       &amp;lt;value&amp;gt;8038&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.client.server.rpc-address&amp;lt;/name&amp;gt;
       &amp;lt;value&amp;gt;cdh1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;sentry.service.client.server.rpc-connection-timeout&amp;lt;/name&amp;gt;
       &amp;lt;value&amp;gt;200000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!--以下是客户端配置--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.provider&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.provider.file.HadoopGroupResourceAuthorizationProvider&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.provider.backend&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.apache.sentry.provider.db.SimpleDBProviderBackend&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.metastore.service.users&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;&amp;lt;!--queries made by hive user (beeline) skip meta store check--&amp;gt;
    &amp;lt;/property&amp;gt;
      &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.server&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;server1&amp;lt;/value&amp;gt;
      &amp;lt;/property&amp;gt;
     &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;sentry.hive.testing.mode&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
     &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：这里&lt;code&gt;sentry.hive.provider.backend&lt;/code&gt;配置的是&lt;code&gt;org.apache.sentry.provider.db.SimpleDBProviderBackend&lt;/code&gt;方式，关于&lt;code&gt;org.apache.sentry.provider.file.SimpleFileProviderBackend&lt;/code&gt;的配置方法，后面再作说明。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hive添加对sentry的依赖，创建软连接：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ln -s /usr/lib/sentry/lib/sentry-binding-hive.jar /usr/lib/hive/lib/sentry-binding-hive.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hiveserver2&quot;&gt;重启HiveServer2&lt;/h2&gt;

&lt;p&gt;在cdh1上启动或重启hiveserver2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hive/conf/hive.keytab hive/cdh1@JAVACHEN.COM

$ /etc/init.d/hive-server2 restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;4. 准备测试数据&lt;/h1&gt;

&lt;p&gt;参考 &lt;a href=&quot;http://blog.evernote.com/tech/2014/06/09/securing-impala-for-analysts/&quot;&gt;Securing Impala for analysts&lt;/a&gt;，准备测试数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /tmp/events.csv
10.1.2.3,US,android,createNote
10.200.88.99,FR,windows,updateNote
10.1.2.3,US,android,updateNote
10.200.88.77,FR,ios,createNote
10.1.4.5,US,windows,updateTag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在hive中运行下面 sql 语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create database sensitive;

create table sensitive.events (
    ip STRING, country STRING, client STRING, action STRING
  ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;

load data local inpath &#39;/tmp/events.csv&#39; overwrite into table sensitive.events;
create database filtered;
create view filtered.events as select country, client, action from sensitive.events;
create view filtered.events_usonly as select * from filtered.events where country = &#39;US&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh1上通过 beeline 连接 hiveserver2，运行下面命令创建角色和组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ beeline -u &quot;jdbc:hive2://cdh1:10001/default;principal=hive/cdh1@JAVACHEN.COM&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 role、group 等等，执行下面的 sql 语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create role admin_role;
GRANT ALL ON SERVER server1 TO ROLE admin_role;
GRANT ROLE admin_role TO GROUP admin;
GRANT ROLE admin_role TO GROUP hive;

create role test_role;
GRANT ALL ON DATABASE filtered TO ROLE test_role;
GRANT ROLE test_role TO GROUP test;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面创建了两个角色：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;admin_role，具有管理员权限，可以读写所有数据库，并授权给 admin 和 hive 组（对应操作系统上的组）&lt;/li&gt;
  &lt;li&gt;test_role，只能读写 filtered 数据库，并授权给 test 组。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;5. 测试&lt;/h1&gt;

&lt;h2 id=&quot;kerberos-&quot;&gt;使用 kerberos 测试&lt;/h2&gt;

&lt;p&gt;以 test 用户为例，通过 beeline 连接 hive-server2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ su test

$ kinit -k -t test.keytab test/cdh1@JAVACHEN.COM

$ beeline -u &quot;jdbc:hive2://cdh1:10001/default;principal=test/cdh1@JAVACHEN.COM&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来运行一些sql查询，查看是否有权限。&lt;/p&gt;

&lt;h2 id=&quot;ldap-&quot;&gt;使用 ldap 用户测试&lt;/h2&gt;

&lt;p&gt;在 ldap 服务器上创建系统用户 yy_test，并使用 migrationtools 工具将该用户导入 ldap，最后设置 ldap 中该用户密码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 创建 yy_test用户
useradd yy_test

grep -E &quot;yy_test&quot; /etc/passwd  &amp;gt;/opt/passwd.txt
/usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldif
ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=lashou,dc=com&quot; -w secret -f /opt/passwd.ldif

#使用下面语句修改密码，填入上面生成的密码，输入两次：

ldappasswd -x -D &#39;uid=ldapadmin,ou=people,dc=lashou,dc=com&#39; -w secret &quot;uid=yy_test,ou=people,dc=lashou,dc=com&quot; -S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在每台 datanode 机器上创建 test 分组，并将 yy_test 用户加入到 test 分组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;groupadd test ; useradd yy_test; usermod -G test,yy_test yy_test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行 beeline 查看是否能够使用 ldap 用户连接 hiveserver2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$  beeline -u &quot;jdbc:hive2://cdh1:10001/&quot; -n yy_test -p yy_test -d org.apache.hive.jdbc.HiveDriver
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;6. 其他说明&lt;/h1&gt;

&lt;p&gt;如果要使用基于文件存储的方式配置Sentry store，则需要修改/etc/hive/conf/sentry-site.xml为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hive.sentry.server&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;server1&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;sentry.hive.provider.backend&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.sentry.provider.file.SimpleFileProviderBackend&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hive.sentry.provider&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.sentry.provider.file.LocalGroupResourceAuthorizationProvider&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hive.sentry.provider.resource&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/user/hive/sentry/sentry-provider.ini&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 sentry-provider.ini 文件并将其上传到 hdfs 的 /user/hive/sentry/ 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /tmp/sentry-provider.ini
[databases]
# Defines the location of the per DB policy file for the customers DB/schema
#db1 = hdfs://cdh1:8020/user/hive/sentry/db1.ini

[groups]
admin = any_operation
hive = any_operation
test = select_filtered

[roles]
any_operation = server=server1-&amp;gt;db=*-&amp;gt;table=*-&amp;gt;action=*
select_filtered = server=server1-&amp;gt;db=filtered-&amp;gt;table=*-&amp;gt;action=SELECT
select_us = server=server1-&amp;gt;db=filtered-&amp;gt;table=events_usonly-&amp;gt;action=SELECT

[users]
test = test
hive= hive

$ hdfs dfs -rm -r /user/hive/sentry/sentry-provider.ini
$ hdfs dfs -put /tmp/sentry-provider.ini /user/hive/sentry/
$ hdfs dfs -chown hive:hive /user/hive/sentry/sentry-provider.ini
$ hdfs dfs -chmod 640 /user/hive/sentry/sentry-provider.ini
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 sentry-provider.ini 文件的语法说明，请参考官方文档。这里我指定了 Hive 组有全部权限，并指定 Hive 用户属于 Hive 分组，而其他两个分组只有部分权限。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;7. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.evernote.com/tech/2014/06/09/securing-impala-for-analysts/&quot;&gt;Securing Impala for analysts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/11/14/config-secured-hive-with-sentry.html</link>
      <guid>http://blog.javachen.com/2014/11/14/config-secured-hive-with-sentry.html</guid>
      <pubDate>2014-11-14T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hadoop配置LDAP集成Kerberos</title>
      <description>&lt;p&gt;本文主要记录 cdh hadoop 集群集成 ldap 的过程，这里 ldap 安装的是 &lt;a href=&quot;http://www.openldap.org/&quot;&gt;OpenLDAP&lt;/a&gt; 。LDAP 用来做账号管理，Kerberos作为认证。授权一般来说是由应用来决定的，通过在 LDAP 数据库中配置一些属性可以让应用程序来进行授权判断。&lt;/p&gt;

&lt;p&gt;关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 &lt;a href=&quot;/2014/11/04/config-kerberos-in-cdh-hdfs.html&quot;&gt;HDFS配置kerberos认证&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;JDK版本：&lt;code&gt;1.7.0_71&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;OpenLDAP 版本：2.4.39&lt;/li&gt;
  &lt;li&gt;Kerberos 版本：1.10.3&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群各节点角色规划为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        cdh2     DataNode、NodeManager、HBase、Hiveserver2、Impala Server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、Hiveserver2、Impala Server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cdh1作为master节点，其他节点作为slave节点，我们在cdh1节点安装kerberos Server，在其他节点安装kerberos client。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 安装服务端&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2.1 安装&lt;/h2&gt;

&lt;p&gt;同安装 kerberos 一样，这里使用 cdh1 作为服务端安装 openldap。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install db4 db4-utils db4-devel cyrus-sasl* krb5-server-ldap -y
$ yum install openldap openldap-servers openldap-clients openldap-devel compat-openldap -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看安装的版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ rpm -qa openldap
openldap-2.4.39-8.el6.x86_64

$ rpm -qa krb5-server-ldap
krb5-server-ldap-1.10.3-33.el6.x86_64
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;openssl&quot;&gt;2.2 OpenSSL&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果，你不配置ssl，这部分内容可以略过，实际安装过程中，我也没有详细去操作这部分内容。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;OpenLDAP 默认使用 Mozilla NSS，安装后已经生成了一份证书，可使用 &lt;code&gt;certutil -d /etc/openldap/certs/ -L -n &#39;OpenLDAP Server&#39;&lt;/code&gt; 命令查看。使用如下命令生成RFC格式CA证书并分发到客户机待用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ certutil -d /etc/openldap/certs/ -L -a -n &#39;OpenLDAP Server&#39; -f /etc/openldap/certs/password &amp;gt; /etc/openldap/ldapCA.rfc

# 拷贝到其他节点
$ scp /etc/openldap/ldapCA.rfc cdh2:/tmp
$ scp /etc/openldap/ldapCA.rfc cdh3:/tmp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;附，生成自签名证书的命令供参考：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ certutil -d /etc/openldap/certs -S -n &#39;test cert&#39; -x -t &#39;u,u,u&#39; -s &#39;C=XX, ST=Default Province, L=Default City, O=Default Company Ltd, OU=Default Unit, CN=cdh1&#39; -k rsa -v 120 -f /etc/openldap/certs/password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 &lt;code&gt;/etc/sysconfig/ldap&lt;/code&gt;，开启 ldaps：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Run slapd with -h &quot;... ldaps:/// ...&quot;
#   yes/no, default: no
SLAPD_LDAPS=yes
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ldap-&quot;&gt;2.3 LDAP 服务端配置&lt;/h2&gt;

&lt;p&gt;更新配置库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;rm -rf /var/lib/ldap/*
cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG
chown -R ldap.ldap /var/lib/ldap
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在2.4以前的版本中，OpenLDAP 使用 slapd.conf 配置文件来进行服务器的配置，而2.4开始则使用 &lt;code&gt;slapd.d&lt;/code&gt; 目录保存细分后的各种配置，这一点需要注意，其数据存储位置即目录 &lt;code&gt;/etc/openldap/slapd.d&lt;/code&gt; 。尽管该系统的数据文件是透明格式的，还是建议使用 ldapadd, ldapdelete, ldapmodify 等命令来修改而不是直接编辑。&lt;/p&gt;

&lt;p&gt;默认配置文件保存在 /etc/openldap/slapd.d，将其备份：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp -rf /etc/openldap/slapd.d /etc/openldap/slapd.d.bak
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加一些基本配置，并引入 kerberos 和 openldap 的 schema：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cp /usr/share/doc/krb5-server-ldap-1.10.3/kerberos.schema /etc/openldap/schema/

$ touch /etc/openldap/slapd.conf

$ echo &quot;include /etc/openldap/schema/corba.schema
include /etc/openldap/schema/core.schema
include /etc/openldap/schema/cosine.schema
include /etc/openldap/schema/duaconf.schema
include /etc/openldap/schema/dyngroup.schema
include /etc/openldap/schema/inetorgperson.schema
include /etc/openldap/schema/java.schema
include /etc/openldap/schema/misc.schema
include /etc/openldap/schema/nis.schema
include /etc/openldap/schema/openldap.schema
include /etc/openldap/schema/ppolicy.schema
include /etc/openldap/schema/collective.schema
include /etc/openldap/schema/kerberos.schema&quot; &amp;gt; /etc/openldap/slapd.conf
$ echo -e &quot;pidfile /var/run/openldap/slapd.pid\nargsfile /var/run/openldap/slapd.args&quot; &amp;gt;&amp;gt; /etc/openldap/slapd.conf

#更新slapd.d
$ slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d

$ chown -R ldap:ldap /etc/openldap/slapd.d &amp;amp;&amp;amp; chmod -R 700 /etc/openldap/slapd.d
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;2.4 启动服务&lt;/h2&gt;

&lt;p&gt;启动 LDAP 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;chkconfig --add slapd
chkconfig --level 345 slapd on

/etc/init.d/slapd start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看状态，验证服务端口：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ps aux | grep slapd | grep -v grep
  ldap      9225  0.0  0.2 581188 44576 ?        Ssl  15:13   0:00 /usr/sbin/slapd -h ldap:/// -u ldap

$ netstat -tunlp  | grep :389
  tcp        0      0 0.0.0.0:389                 0.0.0.0:*                   LISTEN      8510/slapd
  tcp        0      0 :::389                      :::*                        LISTEN      8510/slapd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果启动失败，则运行下面命令来启动 slapd 服务并查看日志：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ slapd -h ldap://127.0.0.1 -d 481
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;待查明原因之后，停止该进程使用正常方式启动 slapd 服务。&lt;/p&gt;

&lt;h2 id=&quot;ldap--kerberos&quot;&gt;2.5 LDAP 和 Kerberos&lt;/h2&gt;

&lt;p&gt;在Kerberos安全机制里，一个principal就是realm里的一个对象，一个principal总是和一个密钥（secret key）成对出现的。&lt;/p&gt;

&lt;p&gt;这个principal的对应物可以是service，可以是host，也可以是user，对于Kerberos来说，都没有区别。&lt;/p&gt;

&lt;p&gt;Kdc(Key distribute center)知道所有principal的secret key，但每个principal对应的对象只知道自己的那个secret key。这也是 “共享密钥” 的由来。&lt;/p&gt;

&lt;p&gt;为了使 Kerberos 能够绑定到 OpenLDAP 服务器，请创建一个管理员用户和一个 principal，并生成 keytab 文件，设置该文件的权限为 LDAP 服务运行用户可读（ LDAP 服务运行用户一般为 ldap）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kadmin.local -q &quot;addprinc ldapadmin@JAVACHEN.COM&quot;
$ kadmin.local -q &quot;addprinc -randkey ldap/cdh1@JAVACHEN.COM&quot;
$ kadmin.local -q &quot;ktadd -k /etc/openldap/ldap.keytab ldap/cdh1@JAVACHEN.COM&quot;

$ chown ldap:ldap /etc/openldap/ldap.keytab &amp;amp;&amp;amp; chmod 640 /etc/openldap/ldap.keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ktadd 后面的&lt;code&gt;-k&lt;/code&gt; 指定把 key 存放在一个本地文件中。&lt;/p&gt;

&lt;p&gt;使用 ldapadmin 用户测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;kinit ldapadmin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;系统会提示输入密码，如果一切正常，那么会安静的返回。实际上，你已经通过了kerberos的身份验证，且获得了一个Service TGT(Ticket-Granting Ticket). Service TGT的意义是， 在一段时间内，你都可以用此TGT去请求某些service，比如ldap service，而不需要再次通过kerberos的认证。&lt;/p&gt;

&lt;p&gt;确保 LDAP 启动时使用上一步中创建的keytab文件，在 &lt;code&gt;/etc/sysconfig/ldap&lt;/code&gt; 增加 &lt;code&gt;KRB5_KTNAME&lt;/code&gt; 配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export KRB5_KTNAME=/etc/openldap/ldap.keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，重启 slapd 服务。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;2.6 创建数据库&lt;/h2&gt;

&lt;p&gt;进入到 /etc/openldap/slapd.d 目录，查看 &lt;code&gt;etc/openldap/slapd.d/cn\=config/olcDatabase={2}bdb.ldif&lt;/code&gt; 可以看到一些默认的配置，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;olcRootDN: cn=Manager,dc=my-domain,dc=com  
olcRootPW: secret  
olcSuffix: dc=my-domain,dc=com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来更新这三个配置，建立 modify.ldif 文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;dn: olcDatabase={2}bdb,cn=config
changetype: modify
replace: olcSuffix
olcSuffix: dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
replace: olcRootDN
# Temporary lines to allow initial setup
olcRootDN: uid=ldapadmin,ou=people,dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
add: olcRootPW
olcRootPW: secret

dn: cn=config
changetype: modify
add: olcAuthzRegexp
olcAuthzRegexp: uid=([^,]*),cn=GSSAPI,cn=auth uid=$1,ou=people,dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
add: olcAccess
# Everyone can read everything
olcAccess: {0}to dn.base=&quot;&quot; by * read
# The ldapadm dn has full write access
olcAccess: {1}to * by dn=&quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; write by * read
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;上面的密码使用的是明文密码 secret ，你也可以使用 &lt;code&gt;slappasswd -s secret&lt;/code&gt; 生成的字符串作为密码。&lt;/li&gt;
  &lt;li&gt;上面的权限中指明了只有用户 &lt;code&gt;uid=ldapadmin,ou=people,dc=javachen,dc=com&lt;/code&gt; 有写权限。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用下面命令导入更新配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ldapmodify -Y EXTERNAL -H ldapi:/// -f modify.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候数据库没有数据，需要添加数据，你可以手动编写 ldif 文件来导入一些用户和组，或者使用 migrationtools 工具来生成 ldif 模板。创建 setup.ldif 文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dn: dc=javachen,dc=com
objectClass: top
objectClass: dcObject
objectclass: organization
o: javachen com
dc: javachen

dn: ou=people,dc=javachen,dc=com
objectclass: organizationalUnit
ou: people
description: Users

dn: ou=group,dc=javachen,dc=com
objectClass: organizationalUnit
ou: group

dn: uid=ldapadmin,ou=people,dc=javachen,dc=com
objectClass: inetOrgPerson
objectClass: posixAccount
objectClass: shadowAccount
cn: LDAP admin account
uid: ldapadmin
sn: ldapadmin
uidNumber: 1001
gidNumber: 100
homeDirectory: /home/ldap
loginShell: /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用下面命令导入数据，密码是前面设置的 secret 。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; -w secret -f setup.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;-w&lt;/code&gt; 指定密码&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-x&lt;/code&gt; 是使用一个匿名的绑定&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ldap--1&quot;&gt;2.7 LDAP 的使用&lt;/h2&gt;

&lt;h3 id=&quot;section-5&quot;&gt;导入系统用户&lt;/h3&gt;

&lt;p&gt;接下来你可以从 /etc/passwd, /etc/shadow, /etc/groups 中生成 ldif 更新 ldap 数据库，这需要用到 migrationtools 工具。&lt;/p&gt;

&lt;p&gt;安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install migrationtools -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用迁移工具生成模板，先修改默认的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ vim /usr/share/migrationtools/migrate_common.ph

#line 71 defalut DNS domain
$DEFAULT_MAIL_DOMAIN = &quot;javachen.com&quot;;
#line 74 defalut base
$DEFAULT_BASE = &quot;dc=javachen,dc=com&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成模板文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/usr/share/migrationtools/migrate_base.pl &amp;gt; /opt/base.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，可以修改该文件，然后执行导入命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; -w secret -f /opt/base.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将当前节点上的用户导入到 ldap 中，可以有选择的导入指定的用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 先添加用户
$ useradd test hive
# 查找系统上的 test、hive 等用户
$ grep -E &quot;test|hive&quot; /etc/passwd  &amp;gt;/opt/passwd.txt
$ /usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldif
$ ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; -w secret -f /opt/passwd.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将用户组导入到 ldap 中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 生成用户组的 ldif 文件，然后导入到 ldap
$ grep -E &quot;test|hive&quot; /etc/group  &amp;gt;/opt/group.txt
$ /usr/share/migrationtools/migrate_group.pl /opt/group.txt /opt/group.ldif
$ ldapadd -x -D &quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; -w secret -f /opt/group.ldif
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-6&quot;&gt;查询&lt;/h3&gt;

&lt;p&gt;查询新添加的 test 用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ldapsearch -LLL -x -D &#39;uid=ldapadmin,ou=people,dc=javachen,dc=com&#39; -w secret -b &#39;dc=javachen,dc=com&#39; &#39;uid=test&#39;
  dn: uid=test,ou=people,dc=javachen,dc=com
  objectClass: inetOrgPerson
  objectClass: posixAccount
  objectClass: shadowAccount
  cn: test account
  sn: test
  uid: test
  uidNumber: 1001
  gidNumber: 100
  homeDirectory: /home/test
  loginShell: /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，通过指定 ‘uid=test’，我们只查询这个用户的数据，这个查询条件叫做filter。有关 filter 的使用可以查看 ldapsearch 的 manpage。&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;修改&lt;/h3&gt;

&lt;p&gt;用户添加好以后，需要给其设定初始密码，运行命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ldappasswd -x -D &#39;uid=ldapadmin,ou=people,dc=javachen,dc=com&#39; -w secret &quot;uid=test,ou=people,dc=javachen,dc=com&quot; -S
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-8&quot;&gt;删除&lt;/h3&gt;

&lt;p&gt;删除用户或组条目：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ldapdelete -x -w secret -D &#39;uid=ldapadmin,ou=people,dc=javachen,dc=com&#39; &quot;uid=test,ou=people,dc=javachen,dc=com&quot;
$ ldapdelete -x -w secret -D &#39;uid=ldapadmin,ou=people,dc=javachen,dc=com&#39; &quot;cn=test,ou=group,dc=javachen,dc=com&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-9&quot;&gt;3. 客户端配置&lt;/h1&gt;

&lt;p&gt;在 cdh2 和 cdh3上，使用下面命令安装openldap客户端&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install openldap-clients -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 /etc/openldap/ldap.conf 以下两个配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BASE    dc=javachen,dc=com
URI     ldap://cdh1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，运行下面命令测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#先删除 ticket
$ kdestroy

$ ldapsearch -b &#39;dc=javachen,dc=com&#39;
  SASL/GSSAPI authentication started
  ldap_sasl_interactive_bind_s: Local error (-2)
    additional info: SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure.  Minor code may provide more information (No credentials cache found)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新获取 ticket：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit root/admin
$ ldapsearch -b &#39;dc=javachen,dc=com&#39;
 # 没有报错了
$ ldapwhoami
  SASL/GSSAPI authentication started
  SASL username: root/admin@JAVACHEN.COM
  SASL SSF: 56
  SASL installing layers
  dn:uid=root/admin,ou=people,dc=javachen,dc=com
  Result: Success (0)

# 直接输入 ldapsearch 不会报错
$ ldapsearch  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 LDAP 客户端工具进行测试，这里我使用的是 LDAP Browser/Editor：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2015/hadoop-ldap-LdapBrowser.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;hive--ldap&quot;&gt;4. 配置 Hive 集成 LDAP&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;说明： CDH5.2 之前 hive-server2 支不支持集成 ldap，故需要升级 cdh 版本到高版本，如 cdh5.3，该版本支持 ldap。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-10&quot;&gt;修改配置文件&lt;/h2&gt;

&lt;p&gt;这部分内容参考自&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_sg_hiveserver2_security.html#topic_9_1_3_unique_1&quot;&gt;Using LDAP Username/Password Authentication with HiveServer2&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;我这使用的是 OpenLDAP ，故修改 hive-site.xml 配置文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.server2.authentication&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;LDAP&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.server2.authentication.ldap.url&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;ldap://cdh1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.server2.authentication.ldap.baseDN&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;ou=people,dc=javachen,dc=com&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为什么这样配置，可以参考 &lt;a href=&quot;https://svn.apache.org/repos/asf/hive/trunk/service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java&quot;&gt;LdapAuthenticationProviderImpl.java&lt;/a&gt; 源码。&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;测试&lt;/h2&gt;

&lt;p&gt;重启服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/etc/init.d/hive-server2 restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用 beeline 测试：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;beeline --verbose=true
beeline&amp;gt; !connect jdbc:hive2://cdh1:10000/default
Connecting to jdbc:hive2://cdh1:10000/default;
Enter username for jdbc:hive2://cdh1:10000/default;: hive
Enter password for jdbc:hive2://cdh1:10000/default;: ****
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;impala--ldap&quot;&gt;5. 配置 Impala 集成 LDAP&lt;/h1&gt;

&lt;h2 id=&quot;section-12&quot;&gt;修改配置文件&lt;/h2&gt;

&lt;p&gt;修改 /etc/default/impala 中的 &lt;code&gt;IMPALA_SERVER_ARGS&lt;/code&gt; 参数，添加&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-enable_ldap_auth=true \
-ldap_uri=ldaps://cdh1 \
-ldap_baseDN=ou=people,dc=javachen,dc=com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果没有开启 ssl，则添加 &lt;code&gt;-ldap_passwords_in_clear_ok=true&lt;/code&gt;，同样如果开启了 ssl，则 &lt;code&gt;ldap_uri&lt;/code&gt; 值为 &lt;code&gt;ldaps://XXXX&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;ldap_baseDN 的值是 &lt;code&gt;ou=people,dc=javachen,dc=com&lt;/code&gt;，因为 impala 会将其追加到 &lt;code&gt;uid={用户名},&lt;/code&gt; 后面&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-13&quot;&gt;测试&lt;/h2&gt;

&lt;p&gt;重启服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /etc/init.d/impala-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用 impala-shell 测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ impala-shell -l -u test
  Starting Impala Shell using LDAP-based authentication
  LDAP password for test:
  Connected to cdh1:21000
  Server version: impalad version 2.0.0-cdh5 RELEASE (build ecf30af0b4d6e56ea80297df2189367ada6b7da7)
  Welcome to the Impala shell. Press TAB twice to see a list of available commands.

  Copyright (c) 2012 Cloudera, Inc. All rights reserved.

  (Shell build version: Impala Shell v2.0.0-cdh5 (ecf30af) built on Sat Oct 11 13:56:06 PDT 2014)
  [cdh1:21000] &amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 beeline 通过 ldap 方式来连接 jdbc 进行测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ beeline -u &quot;jdbc:hive2://cdh1:21050/default;&quot; -n test -p test
  scan complete in 2ms
  Connecting to jdbc:hive2://cdh1:21050/default;
  Connected to: Impala (version 2.0.0-cdh5)
  Driver: Hive JDBC (version 0.13.1-cdh5.2.0)
  Transaction isolation: TRANSACTION_REPEATABLE_READ
  Beeline version 0.13.1-cdh5.2.0 by Apache Hive

  0: jdbc:hive2://cdh1:21050/default&amp;gt;show tables;
  +-----------------------------+--+
  |            name             |
  +-----------------------------+--+
  | t1                          |
  | tab1                        |
  | tab2                        |
  | tab3                        |
  +-----------------------------+--+
  4 rows selected (0.325 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-14&quot;&gt;6. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.tuicool.com/articles/6fy6z2r&quot;&gt;New in CDH 5.2: Impala Authentication with LDAP and Kerberos&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.clanzx.net/2013/09/27/ldap-kerberos.html&quot;&gt;使用 LDAP + Kerberos 实现集中用户认证及授权系统&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/mchina/archive/2013/01/03/2840040.html&quot;&gt;Linux NFS服务器的安装与配置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/kakane/article/details/7455922&quot;&gt;linux的LDAP认证服务器的配置及客户端pam网络验证实例&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://help.ubuntu.com/10.04/serverguide/kerberos-ldap.html&quot;&gt;Kerberos and LDAP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.sina.com.cn/s/blog_64aac6750101gwst.html&quot;&gt;RHEL6配置简单LDAP服务器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.suse.com/zh-cn/documentation/sles10/book_sle_reference/data/sec.kerbadmin.ldap.html&quot;&gt;使用 LDAP 和 Kerberos&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wenku.baidu.com/view/fe7c82757fd5360cba1adbe7.html&quot;&gt;kerberos与openldap整合&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ovirt-china.org/mediawiki/index.php/LDAP%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B&quot;&gt;LDAP配置示例&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kinggoo.com/openldapinstallconf.htm&quot;&gt;centos下yum安装配置openldap 2.4.23-32外送svn的apache下配置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.linux-mag.com/id/4765/&quot;&gt;Integrating LDAP and Kerberos: Part Two (LDAP)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://techpubs.spinlocksolutions.com/dklar/kerberos.html&quot;&gt;Debian GNU and Ubuntu: Setting up MIT Kerberos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop.html</link>
      <guid>http://blog.javachen.com/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop.html</guid>
      <pubDate>2014-11-12T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Impala配置Kerberos认证</title>
      <description>&lt;p&gt;关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 &lt;a href=&quot;/2014/11/04/config-kerberos-in-cdh-hdfs.html&quot;&gt;HDFS配置kerberos认证&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;关于 Kerberos 的安装和 YARN 配置 kerberos 认证，请参考 &lt;a href=&quot;/2014/11/05/config-kerberos-in-cdh-yarn.html&quot;&gt;YARN配置kerberos认证&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;关于 Kerberos 的安装和 Hive 配置 kerberos 认证，请参考 &lt;a href=&quot;/2014/11/06/config-kerberos-in-cdh-hive.html&quot;&gt;Hive配置kerberos认证&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;JDK版本：&lt;code&gt;1.7.0_71&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群各节点角色规划为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        cdh2     DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、Hive Server2、Impala Server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cdh1作为master节点，其他节点作为slave节点，hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 安装必须的依赖&lt;/h1&gt;

&lt;p&gt;在每个节点上运行下面的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install python-devel openssl-devel python-pip cyrus-sasl cyrus-sasl-gssapi cyrus-sasl-devel -y
$ pip-python install ssl
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;keytab&quot;&gt;3. 生成 keytab&lt;/h1&gt;

&lt;p&gt;在 cdh1 节点，即 KDC server 节点上执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/kerberos/krb5kdc/

kadmin.local -q &quot;addprinc -randkey impala/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey impala/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey impala/cdh3@JAVACHEN.COM &quot;

kadmin.local -q &quot;xst  -k impala-unmerge.keytab  impala/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k impala-unmerge.keytab  impala/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k impala-unmerge.keytab  impala/cdh3@JAVACHEN.COM &quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，如果你使用了haproxy来做负载均衡，参考官方文档&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/cloudera-impala/latest/topics/impala_proxy.html&quot;&gt;Using Impala through a Proxy for High Availability&lt;/a&gt;，还需生成 proxy.keytab：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/kerberos/krb5kdc/

# proxy 为安装了 haproxy 的机器
kadmin.local -q &quot;addprinc -randkey impala/proxy@JAVACHEN.COM &quot;

kadmin.local -q &quot;xst  -k proxy.keytab impala/proxy@JAVACHEN.COM &quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;合并 proxy.keytab 和 impala-unmerge.keytab 生成 impala.keytab：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ktutil
ktutil: rkt proxy.keytab
ktutil: rkt impala-unmerge.keytab
ktutil: wkt impala.keytab
ktutil: quit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拷贝 impala.keytab 和 proxy_impala.keytab 文件到其他节点的 /etc/impala/conf 目录&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp impala.keytab cdh1:/etc/impala/conf
$ scp impala.keytab cdh2:/etc/impala/conf
$ scp impala.keytab cdh3:/etc/impala/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并设置权限，分别在 cdh1、cdh2、cdh3 上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &quot;cd /etc/impala/conf/;chown impala:hadoop *.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh2 &quot;cd /etc/impala/conf/;chown impala:hadoop *.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh3 &quot;cd /etc/impala/conf/;chown impala:hadoop *.keytab ;chmod 400 *.keytab&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400)&lt;/p&gt;

&lt;h1 id=&quot;impala-&quot;&gt;4. 修改 impala 配置文件&lt;/h1&gt;

&lt;p&gt;修改 cdh1 节点上的 /etc/default/impala，在 &lt;code&gt;IMPALA_CATALOG_ARGS&lt;/code&gt; 、&lt;code&gt;IMPALA_SERVER_ARGS&lt;/code&gt; 和 &lt;code&gt;IMPALA_STATE_STORE_ARGS&lt;/code&gt; 中添加下面参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-kerberos_reinit_interval=60
-principal=impala/_HOST@JAVACHEN.COM
-keytab_file=/etc/impala/conf/impala.keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果使用了 HAProxy（关于 HAProxy 的配置请参考 &lt;a href=&quot;/2014/01/08/hive-ha-by-haproxy.html&quot;&gt;Hive使用HAProxy配置HA&lt;/a&gt;），则 &lt;code&gt;IMPALA_SERVER_ARGS&lt;/code&gt; 参数需要修改为（proxy为 HAProxy 机器的名称，这里我是将 HAProxy 安装在 cdh1 节点上）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-kerberos_reinit_interval=60
-be_principal=impala/_HOST@JAVACHEN.COM
-principal=impala/proxy@JAVACHEN.COM
-keytab_file=/etc/impala/conf/impala.keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;IMPALA_CATALOG_ARGS&lt;/code&gt; 中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-state_store_host=${IMPALA_STATE_STORE_HOST} \
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将修改的上面文件同步到其他节点。最后，/etc/default/impala 文件如下，这里，为了避免 hostname 存在大写的情况，使用 &lt;code&gt;hostname&lt;/code&gt; 变量替换 &lt;code&gt;_HOST&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;IMPALA_CATALOG_SERVICE_HOST=cdh1
IMPALA_STATE_STORE_HOST=cdh1
IMPALA_STATE_STORE_PORT=24000
IMPALA_BACKEND_PORT=22000
IMPALA_LOG_DIR=/var/log/impala

IMPALA_MEM_DEF=$(free -m |awk &#39;NR==2{print $2-5120}&#39;)
hostname=`hostname -f |tr &quot;[:upper:]&quot; &quot;[:lower:]&quot;`

IMPALA_CATALOG_ARGS=&quot; -log_dir=${IMPALA_LOG_DIR} -state_store_host=${IMPALA_STATE_STORE_HOST} \
    -kerberos_reinit_interval=60\
    -principal=impala/${hostname}@JAVACHEN.COM \
    -keytab_file=/etc/impala/conf/impala.keytab
&quot;

IMPALA_STATE_STORE_ARGS=&quot; -log_dir=${IMPALA_LOG_DIR} -state_store_port=${IMPALA_STATE_STORE_PORT}\
    -statestore_subscriber_timeout_seconds=15 \
    -kerberos_reinit_interval=60 \
    -principal=impala/${hostname}@JAVACHEN.COM \
    -keytab_file=/etc/impala/conf/impala.keytab
&quot;
IMPALA_SERVER_ARGS=&quot; \
    -log_dir=${IMPALA_LOG_DIR} \
    -catalog_service_host=${IMPALA_CATALOG_SERVICE_HOST} \
    -state_store_port=${IMPALA_STATE_STORE_PORT} \
    -use_statestore \
    -state_store_host=${IMPALA_STATE_STORE_HOST} \
    -be_port=${IMPALA_BACKEND_PORT} \
    -kerberos_reinit_interval=60 \
    -be_principal=impala/${hostname}@JAVACHEN.COM \
    -principal=impala/cdh1@JAVACHEN.COM \
    -keytab_file=/etc/impala/conf/impala.keytab \
    -mem_limit=${IMPALA_MEM_DEF}m
&quot;

ENABLE_CORE_DUMPS=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将修改的上面文件同步到其他节点：cdh2、cdh3：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp /etc/default/impala cdh2:/etc/default/impala
$ scp /etc/default/impala cdh3:/etc/default/impala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新 impala 配置文件下的文件并同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp /etc/hadoop/conf/core-site.xml /etc/impala/conf/
cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf/
cp /etc/hive/conf/hive-site.xml /etc/impala/conf/

scp -r /etc/impala/conf cdh2:/etc/impala
scp -r /etc/impala/conf cdh3:/etc/impala
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;5. 启动服务&lt;/h1&gt;

&lt;h2 id=&quot;impala-state-store&quot;&gt;启动 impala-state-store&lt;/h2&gt;

&lt;p&gt;impala-state-store 是通过 impala 用户启动的，故在 cdh1 上先获取 impala 用户的 ticket 再启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/impala/conf/impala.keytab impala/cdh1@JAVACHEN.COM
$ service impala-state-store start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后查看日志，确认是否启动成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tailf /var/log/impala/statestored.INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;impala-catalog&quot;&gt;启动 impala-catalog&lt;/h2&gt;

&lt;p&gt;impala-catalog 是通过 impala 用户启动的，故在 cdh1 上先获取 impala 用户的 ticket 再启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/impala/conf/impala.keytab impala/cdh1@JAVACHEN.COM
$ service impala-catalog start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后查看日志，确认是否启动成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tailf /var/log/impala/catalogd.INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;impala-server&quot;&gt;启动 impala-server&lt;/h2&gt;

&lt;p&gt;impala-server 是通过 impala 用户启动的，故在 cdh1 上先获取 impala 用户的 ticket 再启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/impala/conf/impala.keytab impala/cdh1@JAVACHEN.COM
$ service impala-server start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后查看日志，确认是否启动成功。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tailf /var/log/impala/impalad.INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;6. 测试&lt;/h1&gt;

&lt;h2 id=&quot;impala-shell&quot;&gt;测试 impala-shell&lt;/h2&gt;

&lt;p&gt;在启用了 kerberos 之后，运行 impala-shell 时，需要添加 &lt;code&gt;-k&lt;/code&gt; 参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ impala-shell -k
Starting Impala Shell using Kerberos authentication
Using service name &#39;impala&#39;
Connected to cdh1:21000
Server version: impalad version 1.3.1-cdh4 RELEASE (build 907481bf45b248a7bb3bb077d54831a71f484e5f)
Welcome to the Impala shell. Press TAB twice to see a list of available commands.

Copyright (c) 2012 Cloudera, Inc. All rights reserved.

(Shell build version: Impala Shell v1.3.1-cdh4 (907481b) built on Wed Apr 30 14:23:48 PDT 2014)
[cdh1:21000] &amp;gt;
[cdh1:21000] &amp;gt; show tables;
Query: show tables
+------+
| name |
+------+
| a    |
| b    |
| c    |
| d    |
+------+
Returned 4 row(s) in 0.08s
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;7. 排错&lt;/h1&gt;

&lt;p&gt;如果出现下面异常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[cdh1:21000] &amp;gt; select * from test limit 10;
Query: select * from test limit 10
ERROR: AnalysisException: Failed to load metadata for table: default.test
CAUSED BY: TableLoadingException: Failed to load metadata for table: test
CAUSED BY: TTransportException: java.net.SocketTimeoutException: Read timed out
CAUSED BY: SocketTimeoutException: Read timed out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则需要在 hive-site.xml 中将 &lt;code&gt;hive.metastore.client.socket.timeout&lt;/code&gt; 值设置大一些：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.metastore.client.socket.timeout&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;36000&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/11/06/config-kerberos-in-cdh-impala.html</link>
      <guid>http://blog.javachen.com/2014/11/06/config-kerberos-in-cdh-impala.html</guid>
      <pubDate>2014-11-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive配置Kerberos认证</title>
      <description>&lt;p&gt;关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 &lt;a href=&quot;/2014/11/04/config-kerberos-in-cdh-hdfs.html&quot;&gt;HDFS配置kerberos认证&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;关于 Kerberos 的安装和 YARN 配置 kerberos 认证，请参考 &lt;a href=&quot;/2014/11/05/config-kerberos-in-cdh-yarn.html&quot;&gt;YARN配置kerberos认证&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;JDK版本：&lt;code&gt;1.7.0_71&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群各节点角色规划为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        cdh2     DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、Hive Server2、Impala Server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cdh1作为master节点，其他节点作为slave节点，hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。&lt;/p&gt;

&lt;h1 id=&quot;keytab&quot;&gt;2. 生成 keytab&lt;/h1&gt;

&lt;p&gt;在 cdh1 节点，即 KDC server 节点上执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/kerberos/krb5kdc/

kadmin.local -q &quot;addprinc -randkey hive/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey hive/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey hive/cdh3@JAVACHEN.COM &quot;

kadmin.local -q &quot;xst  -k hive.keytab  hive/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k hive.keytab  hive/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k hive.keytab  hive/cdh3@JAVACHEN.COM &quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拷贝 hive.keytab 文件到其他节点的 /etc/hive/conf 目录&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp hive.keytab cdh1:/etc/hive/conf
$ scp hive.keytab cdh2:/etc/hive/conf
$ scp hive.keytab cdh3:/etc/hive/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并设置权限，分别在 cdh1、cdh2、cdh3 上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &quot;cd /etc/hive/conf/;chown hive:hadoop hive.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh2 &quot;cd /etc/hive/conf/;chown hive:hadoop hive.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh3 &quot;cd /etc/hive/conf/;chown hive:hadoop hive.keytab ;chmod 400 *.keytab&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400)&lt;/p&gt;

&lt;h1 id=&quot;hive-&quot;&gt;3. 修改 hive 配置文件&lt;/h1&gt;

&lt;p&gt;修改 hive-site.xml，添加下面配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.server2.authentication&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;KERBEROS&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.server2.authentication.kerberos.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hive/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.server2.authentication.kerberos.keytab&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hive/conf/hive.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.metastore.sasl.enabled&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.metastore.kerberos.keytab.file&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hive/conf/hive.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.metastore.kerberos.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hive/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 core-site.xml 中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.hive.hosts&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.hive.groups&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.hdfs.hosts&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.hdfs.groups&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.HTTP.hosts&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.HTTP.groups&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记住将修改的上面文件同步到其他节点：cdh2、cdh3，并再次一一检查权限是否正确。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp /etc/hive/conf/hive-site.xml cdh2:/etc/hive/conf/
$ scp /etc/hive/conf/hive-site.xml cdh3:/etc/hive/conf/
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;4. 启动服务&lt;/h1&gt;

&lt;h2 id=&quot;hive-metastore&quot;&gt;启动 Hive MetaStore&lt;/h2&gt;

&lt;p&gt;hive-metastore 是通过 hive 用户启动的，故在 cdh1 上先获取 hive 用户的 ticket 再启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hive/conf/hive.keytab hive/cdh1@JAVACHEN.COM
$ service hive-metastore start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后查看日志，确认是否启动成功。&lt;/p&gt;

&lt;h2 id=&quot;hive-server2&quot;&gt;启动 Hive Server2&lt;/h2&gt;

&lt;p&gt;hive-server2 是通过 hive 用户启动的，故在 cdh2 和 cdh3 上先获取 hive 用户的 ticket 再启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hive/conf/hive.keytab hive/cdh1@JAVACHEN.COM
$ service hive-server2 start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后查看日志，确认是否启动成功。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;5. 测试&lt;/h1&gt;

&lt;h2 id=&quot;hive-cli&quot;&gt;Hive CLI&lt;/h2&gt;

&lt;p&gt;在没有配置 kerberos 之前，想要通过 hive 用户运行 hive 命令需要执行sudo，现在配置了 kerberos 之后，不再需要 &lt;code&gt;sudo&lt;/code&gt; 了，hive 会通过 ticket 中的用户去执行该命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ klist
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: hdfs/dn5.h.lashou-inc.com@lashou_hadoop

Valid starting     Expires            Service principal
11/06/14 11:39:09  11/07/14 11:39:09  krbtgt/lashou_hadoop@lashou_hadoop
  renew until 11/08/14 11:39:09


Kerberos 4 ticket cache: /tmp/tkt0
klist: You have no tickets cached
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行Hive cli：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hive
hive&amp;gt; set system:user.name;
system:user.name=root
hive&amp;gt; create table t(id int);
OK
Time taken: 2.183 seconds
hive&amp;gt; show tables;
OK
t
Time taken: 1.349 seconds
hive&amp;gt; select * from t;
OK
Time taken: 1.116 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到在获取了 hdfs 用户的 ticket 之后，进入 hive cli 可以执行查看表、查询数据等命令。当然，你也可以获取 hive 的 ticket 之后再来运行 hive 命令。&lt;/p&gt;

&lt;p&gt;另外，如果你想通过普通用户来访问 hive，则需要 kerberos 创建规则和导出 ticket，然后把这个 ticket 拷贝到普通用户所在的家目录，在获取 ticket 了之后，再运行 hive 命令即可。&lt;/p&gt;

&lt;h2 id=&quot;jdbc-&quot;&gt;JDBC 客户端&lt;/h2&gt;

&lt;p&gt;客户端通过 jdbc 代码连结 hive-server2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;String url = &quot;jdbc:hive2://cdh1:10000/default;principal=hive/cdh1@JAVACHEN.COM&quot;
Connection con = DriverManager.getConnection(url);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;beeline&quot;&gt;Beeline&lt;/h2&gt;

&lt;p&gt;Beeline 连结 hive-server2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ beeline
beeline&amp;gt; !connect jdbc:hive2://cdh1:10000/default;principal=hive/cdh1@JAVACHEN.COM
scan complete in 4ms
Connecting to jdbc:hive2://localhost:10000/default;principal=hive/cdh1@JAVACHEN.COM;
Enter username for jdbc:hive2://localhost:10000/default;principal=hive/cdh1@JAVACHEN.COM;:
Enter password for jdbc:hive2://localhost:10000/default;principal=hive/cdh1@JAVACHEN.COM;:
Connected to: Apache Hive (version 0.14.0)
Driver: Hive (version 0.14.0-cdh5.4.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://cdh1:10000/default&amp;gt; select * from t;
+-------+--+
| t.id  |
+-------+--+
+-------+--+
No rows selected (1.575 seconds)
0: jdbc:hive2://cdh1:10000/default&amp;gt; desc t;
+-----------+------------+----------+--+
| col_name  | data_type  | comment  |
+-----------+------------+----------+--+
| id        | int        |          |
+-----------+------------+----------+--+
1 row selected (0.24 seconds)
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/11/06/config-kerberos-in-cdh-hive.html</link>
      <guid>http://blog.javachen.com/2014/11/06/config-kerberos-in-cdh-hive.html</guid>
      <pubDate>2014-11-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>YARN配置Kerberos认证</title>
      <description>&lt;p&gt;关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 &lt;a href=&quot;/2014/11/04/config-kerberos-in-cdh-hdfs.html&quot;&gt;HDFS配置kerberos认证&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;JDK版本：&lt;code&gt;1.7.0_71&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群各节点角色规划为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        cdh2     DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、Hive Server2、Impala Server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cdh1作为master节点，其他节点作为slave节点，hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。&lt;/p&gt;

&lt;h1 id=&quot;keytab&quot;&gt;2. 生成 keytab&lt;/h1&gt;

&lt;p&gt;在 cdh1 节点，即 KDC server 节点上执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd /var/kerberos/krb5kdc/

kadmin.local -q &quot;addprinc -randkey yarn/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey yarn/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey yarn/cdh3@JAVACHEN.COM &quot;

kadmin.local -q &quot;addprinc -randkey mapred/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey mapred/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;addprinc -randkey mapred/cdh3@JAVACHEN.COM &quot;

kadmin.local -q &quot;xst  -k yarn.keytab  yarn/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k yarn.keytab  yarn/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k yarn.keytab  yarn/cdh3@JAVACHEN.COM &quot;

kadmin.local -q &quot;xst  -k mapred.keytab  mapred/cdh1@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k mapred.keytab  mapred/cdh2@JAVACHEN.COM &quot;
kadmin.local -q &quot;xst  -k mapred.keytab  mapred/cdh3@JAVACHEN.COM &quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拷贝 yarn.keytab 和 mapred.keytab 文件到其他节点的 &lt;code&gt;/etc/hadoop/conf&lt;/code&gt; 目录&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp yarn.keytab mapred.keytab cdh1:/etc/hadoop/conf
$ scp yarn.keytab mapred.keytab cdh2:/etc/hadoop/conf
$ scp yarn.keytab mapred.keytab cdh3:/etc/hadoop/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并设置权限，分别在 cdh1、cdh2、cdh3 上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &quot;cd /etc/hadoop/conf/;chown yarn:hadoop yarn.keytab;chown mapred:hadoop mapred.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh2 &quot;cd /etc/hadoop/conf/;chown yarn:hadoop yarn.keytab;chown mapred:hadoop mapred.keytab ;chmod 400 *.keytab&quot;
$ ssh cdh3 &quot;cd /etc/hadoop/conf/;chown yarn:hadoop yarn.keytab;chown mapred:hadoop mapred.keytab ;chmod 400 *.keytab&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(&lt;code&gt;0400&lt;/code&gt;)&lt;/p&gt;

&lt;h1 id=&quot;yarn-&quot;&gt;3. 修改 YARN 配置文件&lt;/h1&gt;

&lt;p&gt;修改 yarn-site.xml，添加下面配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.resourcemanager.keytab&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hadoop/conf/yarn.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.resourcemanager.principal&amp;lt;/name&amp;gt; 
  &amp;lt;value&amp;gt;yarn/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.nodemanager.keytab&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hadoop/conf/yarn.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.nodemanager.principal&amp;lt;/name&amp;gt; 
  &amp;lt;value&amp;gt;yarn/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt; 
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.nodemanager.container-executor.class&amp;lt;/name&amp;gt;  
  &amp;lt;value&amp;gt;org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt; 
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.nodemanager.linux-container-executor.group&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要 YARN 开启 SSL，则添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;yarn.http.policy&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;HTTPS_ONLY&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 mapred-site.xml，添加如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;mapreduce.jobhistory.keytab&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hadoop/conf/mapred.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt; 
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;mapreduce.jobhistory.principal&amp;lt;/name&amp;gt; 
  &amp;lt;value&amp;gt;mapred/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要 mapreduce jobhistory 开启 SSL，则添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;mapreduce.jobhistory.http.policy&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;HTTPS_ONLY&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;/etc/hadoop/conf&lt;/code&gt; 目录下创建 container-executor.cfg 文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;#configured value of yarn.nodemanager.linux-container-executor.group
yarn.nodemanager.linux-container-executor.group=yarn
#comma separated list of users who can not run applications
banned.users=bin
#Prevent other super-users
min.user.id=0
#comma separated list of system users who CAN run applications
allowed.system.users=root,nobody,impala,hive,hdfs,yarn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置该文件权限：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ chown root:yarn container-executor.cfg
$ chmod 400 container-executor.cfg

$ ll container-executor.cfg
-r-------- 1 root yarn 354 11-05 14:14 container-executor.cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;container-executor.cfg&lt;/code&gt; 文件读写权限需设置为 &lt;code&gt;400&lt;/code&gt;，所有者为 &lt;code&gt;root:yarn&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.nodemanager.linux-container-executor.group&lt;/code&gt; 要同时配置在 yarn-site.xml 和 container-executor.cfg，且其值需要为运行 NodeManager 的用户所在的组，这里为 yarn。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;banned.users&lt;/code&gt; 不能为空，默认值为 &lt;code&gt;hfds,yarn,mapred,bin&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;min.user.id&lt;/code&gt; 默认值为 1000，在有些 centos 系统中，用户最小 id 为500，则需要修改该值&lt;/li&gt;
  &lt;li&gt;确保 &lt;code&gt;yarn.nodemanager.local-dirs&lt;/code&gt; 和 &lt;code&gt;yarn.nodemanager.log-dirs&lt;/code&gt; 对应的目录权限为 &lt;code&gt;755&lt;/code&gt; 。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;设置 /usr/lib/hadoop-yarn/bin/container-executor 读写权限为 &lt;code&gt;6050&lt;/code&gt; 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ chown root:yarn /usr/lib/hadoop-yarn/bin/container-executor
$ chmod 6050 /usr/lib/hadoop-yarn/bin/container-executor

$ ll /usr/lib/hadoop-yarn/bin/container-executor
---Sr-s--- 1 root yarn 333 11-04 19:11 container-executor
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试是否配置正确：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /usr/lib/hadoop-yarn/bin/container-executor --checksetup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果提示错误，则查看 NodeManger 的日志，然后对照 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_sg_other_hadoop_security.html?scroll=topic_18_unique_2&quot;&gt;YARN ONLY: Container-executor Error Codes&lt;/a&gt; 查看错误对应的问题说明。&lt;/p&gt;

&lt;p&gt;关于 LinuxContainerExecutor 的详细说明，可以参考 &lt;a href=&quot;http://hadoop.apache.org/docs/r2.5.0/hadoop-project-dist/hadoop-common/SecureMode.html#LinuxContainerExecutor&quot;&gt;http://hadoop.apache.org/docs/r2.5.0/hadoop-project-dist/hadoop-common/SecureMode.html#LinuxContainerExecutor&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;记住将修改的上面文件同步到其他节点：cdh2、cdh3，并再次一一检查权限是否正确。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /etc/hadoop/conf/

$ scp yarn-site.xml mapred-site.xml container-executor.cfg  cdh2:/etc/hadoop/conf/
$ scp yarn-site.xml mapred-site.xml container-executor.cfg  cdh3:/etc/hadoop/conf/

$ ssh cdh2 &quot;cd /etc/hadoop/conf/; chown root:yarn container-executor.cfg ; chmod 400 container-executor.cfg&quot;
$ ssh cdh3 &quot;cd /etc/hadoop/conf/; chown root:yarn container-executor.cfg ; chmod 400 container-executor.cfg&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;4. 启动服务&lt;/h1&gt;

&lt;h2 id=&quot;resourcemanager&quot;&gt;启动 ResourceManager&lt;/h2&gt;

&lt;p&gt;resourcemanager 是通过 yarn 用户启动的，故在 cdh1 上先获取 yarn 用户的 ticket 再启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hadoop/conf/yarn.keytab yarn/cdh1@JAVACHEN.COM
$ service hadoop-yarn-resourcemanager start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后查看日志，确认是否启动成功。&lt;/p&gt;

&lt;h2 id=&quot;nodemanager&quot;&gt;启动 NodeManager&lt;/h2&gt;

&lt;p&gt;resourcemanager 是通过 yarn 用户启动的，故在 cdh2 和 cdh3 上先获取 yarn 用户的 ticket 再启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh2 &quot;kinit -k -t /etc/hadoop/conf/yarn.keytab yarn/cdh2@JAVACHEN.COM ;service hadoop-yarn-nodemanager start&quot;
$ ssh cdh3 &quot;kinit -k -t /etc/hadoop/conf/yarn.keytab yarn/cdh3@JAVACHEN.COM ;service hadoop-yarn-nodemanager start&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;mapreduce-job-history-server&quot;&gt;启动 MapReduce Job History Server&lt;/h2&gt;

&lt;p&gt;resourcemanager 是通过 mapred 用户启动的，故在 cdh1 上先获取 mapred 用户的 ticket 再启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hadoop/conf/mapred.keytab mapred/cdh1@JAVACHEN.COM
$ service hadoop-mapreduce-historyserver start
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;5. 测试&lt;/h1&gt;

&lt;p&gt;检查 web 页面是否可以访问：http://cdh1:8088/cluster&lt;/p&gt;

&lt;p&gt;运行一个 mapreduce 的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ klist
  Ticket cache: FILE:/tmp/krb5cc_1002
  Default principal: yarn/cdh1@JAVACHEN.COM

  Valid starting     Expires            Service principal
  11/10/14 11:18:55  11/11/14 11:18:55  krbtgt/cdh1@JAVACHEN.COM
    renew until 11/17/14 11:18:55


  Kerberos 4 ticket cache: /tmp/tkt1002
  klist: You have no tickets cached

$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 10 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果没有报错，则说明配置成功。最后运行的结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;Job Finished in 54.56 seconds
Estimated value of Pi is 3.14120000000000000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果出现下面错误，请检查环境变量中 &lt;code&gt;HADOOP_YARN_HOME&lt;/code&gt; 是否设置正确，并和 &lt;code&gt;yarn.application.classpath&lt;/code&gt; 中的保持一致。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;14/11/13 11:41:02 INFO mapreduce.Job: Job job_1415849491982_0003 failed with state FAILED due to: Application application_1415849491982_0003 failed 2 times due to AM Container for appattempt_1415849491982_0003_000002 exited with  exitCode: 1 due to: Exception from container-launch.
Container id: container_1415849491982_0003_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1:
  at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
  at org.apache.hadoop.util.Shell.run(Shell.java:455)
  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:702)
  at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:281)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:299)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:81)
  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
  at java.util.concurrent.FutureTask.run(FutureTask.java:138)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:662)

Shell output: main : command provided 1
main : user is yarn
main : requested yarn user is yarn


Container exited with a non-zero exit code 1
.Failing this attempt.. Failing the application.
14/11/13 11:41:02 INFO mapreduce.Job: Counters: 0
Job Finished in 13.428 seconds
java.io.FileNotFoundException: File does not exist: hdfs://cdh1:8020/user/yarn/QuasiMonteCarlo_1415850045475_708291630/out/reduce-out
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2014/11/05/config-kerberos-in-cdh-yarn.html</link>
      <guid>http://blog.javachen.com/2014/11/05/config-kerberos-in-cdh-yarn.html</guid>
      <pubDate>2014-11-05T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HDFS配置Kerberos认证</title>
      <description>&lt;p&gt;本文主要记录 CDH Hadoop 集群上配置 HDFS 集成 Kerberos 的过程，包括 Kerberos 的安装和 Hadoop 相关配置修改说明。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;JDK版本：&lt;code&gt;1.7.0_71&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群各节点角色规划为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        cdh2     DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、Hive Server2、Impala Server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cdh1作为master节点，其他节点作为slave节点，我们在cdh1节点安装kerberos Server，在其他节点安装kerberos client。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 准备工作&lt;/h1&gt;

&lt;p&gt;确认添加主机名解析到 &lt;code&gt;/etc/hosts&lt;/code&gt; 文件中。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /etc/hosts
127.0.0.1       localhost

192.168.56.121 cdh1
192.168.56.122 cdh2
192.168.56.123 cdh3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。&lt;/p&gt;

&lt;h1 id=&quot;kerberos&quot;&gt;3. 安装 Kerberos&lt;/h1&gt;

&lt;p&gt;在 cdh1 上安装包 krb5、krb5-server 和 krb5-client。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install krb5-server -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在其他节点（cdh1、cdh2、cdh3）安装 krb5-devel、krb5-workstation ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#使用无密码登陆
$ ssh cdh1 &quot;yum install krb5-devel krb5-workstation -y&quot;
$ ssh cdh2 &quot;yum install krb5-devel krb5-workstation -y&quot;
$ ssh cdh3 &quot;yum install krb5-devel krb5-workstation -y&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;4. 修改配置文件&lt;/h1&gt;

&lt;p&gt;kdc 服务器涉及到三个配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/etc/krb5.conf
/var/kerberos/krb5kdc/kdc.conf
/var/kerberos/krb5kdc/kadm5.acl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置 Kerberos 的一种方法是编辑配置文件 /etc/krb5.conf。默认安装的文件中包含多个示例项。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[logging]
 default = FILE:/var/log/krb5libs.log
 kdc = FILE:/var/log/krb5kdc.log
 admin_server = FILE:/var/log/kadmind.log

[libdefaults]
 default_realm = JAVACHEN.COM
 dns_lookup_realm = false
 dns_lookup_kdc = false
 ticket_lifetime = 24h
 renew_lifetime = 7d
 forwardable = true
 default_tgs_enctypes = aes256-cts-hmac-sha1-96
 default_tkt_enctypes = aes256-cts-hmac-sha1-96
 permitted_enctypes = aes256-cts-hmac-sha1-96
 clockskew = 120
 udp_preference_limit = 1

[realms]
 JAVACHEN.COM = {
  kdc = cdh1
  admin_server = cdh1
 }

[domain_realm]
 .javachen.com = JAVACHEN.COM
 javachen.com = JAVACHEN.COM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;[logging]&lt;/code&gt;：表示 server 端的日志的打印位置&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;[libdefaults]&lt;/code&gt;：每种连接的默认配置，需要注意以下几个关键的小配置
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;default_realm = JAVACHEN.COM&lt;/code&gt;：设置 Kerberos 应用程序的默认领域。如果您有多个领域，只需向 [realms] 节添加其他的语句。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ticket_lifetime&lt;/code&gt;： 表明凭证生效的时限，一般为24小时。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;renew_lifetime&lt;/code&gt;： 表明凭证最长可以被延期的时限，一般为一个礼拜。当凭证过期之后，对安全认证的服务的后续访问则会失败。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;clockskew&lt;/code&gt;：时钟偏差是不完全符合主机系统时钟的票据时戳的容差，超过此容差将不接受此票据。通常，将时钟扭斜设置为 300 秒（5 分钟）。这意味着从服务器的角度看，票证的时间戳与它的偏差可以是在前后 5 分钟内。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;udp_preference_limit= 1&lt;/code&gt;：禁止使用 udp 可以防止一个 Hadoop 中的错误&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;[realms]&lt;/code&gt;：列举使用的 realm。
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;kdc&lt;/code&gt;：代表要 kdc 的位置。格式是 &lt;code&gt;机器:端口&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;admin_server&lt;/code&gt;：代表 admin 的位置。格式是 &lt;code&gt;机器:端口&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;default_domain&lt;/code&gt;：代表默认的域名&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;[appdefaults]&lt;/code&gt;：可以设定一些针对特定应用的配置，覆盖默认配置。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改 &lt;code&gt;/var/kerberos/krb5kdc/kdc.conf&lt;/code&gt; ，该文件包含 Kerberos 的配置信息。例如，KDC 的位置，Kerbero 的 admin 的realms 等。需要所有使用的 Kerberos 的机器上的配置文件都同步。这里仅列举需要的基本配置。详细介绍参考：&lt;a href=&quot;http://web.mit.edu/~kerberos/krb5-devel/doc/admin/conf_files/krb5_conf.html&quot;&gt;krb5conf&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;[kdcdefaults]
 kdc_ports = 88
 kdc_tcp_ports = 88

[realms]
 JAVACHEN.COM = {
  #master_key_type = aes256-cts
  acl_file = /var/kerberos/krb5kdc/kadm5.acl
  dict_file = /usr/share/dict/words
  max_renewable_life = 7d
  max_life = 1d
  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
  default_principal_flags = +renewable, +forwardable
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;JAVACHEN.COM&lt;/code&gt;： 是设定的 realms。名字随意。Kerberos 可以支持多个 realms，会增加复杂度。大小写敏感，一般为了识别使用全部大写。这个 realms 跟机器的 host 没有大关系。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;master_key_type&lt;/code&gt;：和 &lt;code&gt;supported_enctypes&lt;/code&gt; 默认使用 &lt;code&gt;aes256-cts&lt;/code&gt;。JAVA 使用 &lt;code&gt;aes256-cts&lt;/code&gt; 验证方式需要安装 JCE 包，见下面的说明。为了简便，你可以不使用 &lt;code&gt;aes256-cts&lt;/code&gt; 算法，这样就不需要安装 JCE 。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;acl_file&lt;/code&gt;：标注了 admin 的用户权限，需要用户自己创建。文件格式是：&lt;code&gt;Kerberos_principal permissions [target_principal]  [restrictions]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;supported_enctypes&lt;/code&gt;：支持的校验方式。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;admin_keytab&lt;/code&gt;：KDC 进行校验的 keytab。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;关于AES-256加密&lt;/strong&gt;：&lt;/p&gt;

  &lt;p&gt;对于使用 centos5. 6 及以上的系统，默认使用 &lt;code&gt;AES-256&lt;/code&gt; 来加密的。这就需要集群中的所有节点上安装 JCE，如果你使用的是 JDK1.6 ，则到&lt;br /&gt;
 &lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html&quot;&gt;Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for JDK/JRE 6&lt;/a&gt; 页面下载，如果是 JDK1.7，则到 &lt;a href=&quot;http://www.oracle.com/technetwork/java/embedded/embedded-se/downloads/jce-7-download-432124.html&quot;&gt;Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for JDK/JRE 7&lt;/a&gt; 下载。&lt;/p&gt;

  &lt;p&gt;下载的文件是一个 zip 包，解开后，将里面的两个文件放到下面的目录中：&lt;code&gt;$JAVA_HOME/jre/lib/security&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了能够不直接访问 KDC 控制台而从 Kerberos 数据库添加和删除主体，请对 Kerberos 管理服务器指示允许哪些主体执行哪些操作。通过编辑文件 /var/lib/kerberos/krb5kdc/kadm5.acl 完成此操作。ACL（访问控制列表）允许您精确指定特权。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /var/kerberos/krb5kdc/kadm5.acl
  */admin@JAVACHEN.COM *
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;5. 同步配置文件&lt;/h1&gt;

&lt;p&gt;将 kdc 中的 &lt;code&gt;/etc/krb5.conf&lt;/code&gt; 拷贝到集群中其他服务器即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp /etc/krb5.conf cdh2:/etc/krb5.conf
$ scp /etc/krb5.conf cdh3:/etc/krb5.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请确认集群如果关闭了 selinux。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;6. 创建数据库&lt;/h1&gt;

&lt;p&gt;在 cdh1 上运行初始化数据库命令。其中 &lt;code&gt;-r&lt;/code&gt; 指定对应 realm。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kdb5_util create -r JAVACHEN.COM -s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出现 &lt;code&gt;Loading random data&lt;/code&gt; 的时候另开个终端执行点消耗CPU的命令如 &lt;code&gt;cat /dev/sda &amp;gt; /dev/urandom&lt;/code&gt; 可以加快随机数采集。该命令会在 &lt;code&gt;/var/kerberos/krb5kdc/&lt;/code&gt; 目录下创建 principal 数据库。&lt;/p&gt;

&lt;p&gt;如果遇到数据库已经存在的提示，可以把 &lt;code&gt;/var/kerberos/krb5kdc/&lt;/code&gt; 目录下的 principal 的相关文件都删除掉。默认的数据库名字都是 principal。可以使用 &lt;code&gt;-d&lt;/code&gt; 指定数据库名字。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;7. 启动服务&lt;/h1&gt;

&lt;p&gt;在 cdh1 节点上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ chkconfig --level 35 krb5kdc on
$ chkconfig --level 35 kadmin on
$ service krb5kdc start
$ service kadmin start
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;kerberos-&quot;&gt;8. 创建 kerberos 管理员&lt;/h1&gt;

&lt;p&gt;关于 kerberos 的管理，可以使用 &lt;code&gt;kadmin.local&lt;/code&gt; 或 &lt;code&gt;kadmin&lt;/code&gt;，至于使用哪个，取决于账户和访问权限：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果有访问 kdc 服务器的 root 权限，但是没有 kerberos admin 账户，使用 &lt;code&gt;kadmin.local&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;如果没有访问 kdc 服务器的 root 权限，但是用 kerberos admin 账户，使用 &lt;code&gt;kadmin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 cdh1 上创建远程管理的管理员：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#手动输入两次密码，这里密码为 root
$ kadmin.local -q &quot;addprinc root/admin&quot;

# 也可以不用手动输入密码
$ echo -e &quot;root\nroot&quot; | kadmin.local -q &quot;addprinc root/admin&quot;

# 或者运行下面命令
$ kadmin.local &amp;lt;&amp;lt;eoj
addprinc -pw root root/admin
eoj
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;系统会提示输入密码，密码不能为空，且需妥善保存。&lt;/p&gt;

&lt;h1 id=&quot;kerberos-1&quot;&gt;9. 测试 kerberos&lt;/h1&gt;

&lt;p&gt;查看当前的认证用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 查看principals
$ kadmin: list_principals

  # 添加一个新的 principal
  kadmin:  addprinc user1
    WARNING: no policy specified for user1@JAVACHEN.COM; defaulting to no policy
    Enter password for principal &quot;user1@JAVACHEN.COM&quot;:
    Re-enter password for principal &quot;user1@JAVACHEN.COM&quot;:
    Principal &quot;user1@JAVACHEN.COM&quot; created.

  # 删除 principal
  kadmin:  delprinc user1
    Are you sure you want to delete the principal &quot;user1@JAVACHEN.COM&quot;? (yes/no): yes
    Principal &quot;user1@JAVACHEN.COM&quot; deleted.
    Make sure that you have removed this principal from all ACLs before reusing.

  kadmin: exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以直接通过下面的命令来执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 提示需要输入密码
$ kadmin -p root/admin -q &quot;list_principals&quot;
$ kadmin -p root/admin -q &quot;addprinc user2&quot;
$ kadmin -p root/admin -q &quot;delprinc user2&quot;

# 不用输入密码
$ kadmin.local -q &quot;list_principals&quot;
$ kadmin.local -q &quot;addprinc user2&quot;
$ kadmin.local -q &quot;delprinc user2&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个测试用户 test，密码设置为 test：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo -e &quot;test\ntest&quot; | kadmin.local -q &quot;addprinc test&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取 test 用户的 ticket：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 通过用户名和密码进行登录
$ kinit test
Password for test@JAVACHEN.COM:

$ klist  -e
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: test@JAVACHEN.COM

Valid starting     Expires            Service principal
11/07/14 15:29:02  11/08/14 15:29:02  krbtgt/JAVACHEN.COM@JAVACHEN.COM
  renew until 11/17/14 15:29:02, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96

Kerberos 4 ticket cache: /tmp/tkt0
klist: You have no tickets cached
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;销毁该 test 用户的 ticket：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kdestroy

$ klist
klist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_0)

Kerberos 4 ticket cache: /tmp/tkt0
klist: You have no tickets cached
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新 ticket：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit root/admin
  Password for root/admin@JAVACHEN.COM:

$  klist
  Ticket cache: FILE:/tmp/krb5cc_0
  Default principal: root/admin@JAVACHEN.COM

  Valid starting     Expires            Service principal
  11/07/14 15:33:57  11/08/14 15:33:57  krbtgt/JAVACHEN.COM@JAVACHEN.COM
    renew until 11/17/14 15:33:57

  Kerberos 4 ticket cache: /tmp/tkt0
  klist: You have no tickets cached

$ kinit -R

$ klist
  Ticket cache: FILE:/tmp/krb5cc_0
  Default principal: root/admin@JAVACHEN.COM

  Valid starting     Expires            Service principal
  11/07/14 15:34:05  11/08/14 15:34:05  krbtgt/JAVACHEN.COM@JAVACHEN.COM
    renew until 11/17/14 15:33:57

  Kerberos 4 ticket cache: /tmp/tkt0
  klist: You have no tickets cached
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;抽取密钥并将其储存在本地 keytab 文件 /etc/krb5.keytab 中。这个文件由超级用户拥有，所以您必须是 root 用户才能在 kadmin shell 中执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kadmin.local -q &quot;ktadd kadmin/admin&quot;

$ klist -k /etc/krb5.keytab
  Keytab name: FILE:/etc/krb5.keytab
  KVNO Principal
  ---- --------------------------------------------------------------------------
     3 kadmin/admin@LASHOU-INC.COM
     3 kadmin/admin@LASHOU-INC.COM
     3 kadmin/admin@LASHOU-INC.COM
     3 kadmin/admin@LASHOU-INC.COM
     3 kadmin/admin@LASHOU-INC.COM
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hdfs--kerberos&quot;&gt;10. HDFS 上配置 kerberos&lt;/h1&gt;

&lt;h2 id=&quot;section-6&quot;&gt;10.1 创建认证规则&lt;/h2&gt;

&lt;p&gt;在 Kerberos 安全机制里，一个 principal 就是 realm 里的一个对象，一个 principal 总是和一个密钥（secret key）成对出现的。&lt;/p&gt;

&lt;p&gt;这个 principal 的对应物可以是 service，可以是 host，也可以是 user，对于 Kerberos 来说，都没有区别。&lt;/p&gt;

&lt;p&gt;Kdc(Key distribute center) 知道所有 principal 的 secret key，但每个 principal 对应的对象只知道自己的那个 secret key 。这也是“共享密钥“的由来。&lt;/p&gt;

&lt;p&gt;对于 hadoop，principals 的格式为 &lt;code&gt;username/fully.qualified.domain.name@YOUR-REALM.COM&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;通过 yum 源安装的 cdh 集群中，NameNode 和 DataNode 是通过 hdfs 启动的，故为集群中每个服务器节点添加两个principals：hdfs、HTTP。&lt;/p&gt;

&lt;p&gt;在 KCD server 上（这里是 cdh1）创建 hdfs principal：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;kadmin.local -q &quot;addprinc -randkey hdfs/cdh1@JAVACHEN.COM&quot;
kadmin.local -q &quot;addprinc -randkey hdfs/cdh2@JAVACHEN.COM&quot;
kadmin.local -q &quot;addprinc -randkey hdfs/cdh3@JAVACHEN.COM&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;-randkey&lt;/code&gt; 标志没有为新 principal 设置密码，而是指示 kadmin 生成一个随机密钥。之所以在这里使用这个标志，是因为此 principal 不需要用户交互。它是计算机的一个服务器帐户。&lt;/p&gt;

&lt;p&gt;创建 HTTP principal：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;kadmin.local -q &quot;addprinc -randkey HTTP/cdh1@JAVACHEN.COM&quot;
kadmin.local -q &quot;addprinc -randkey HTTP/cdh2@JAVACHEN.COM&quot;
kadmin.local -q &quot;addprinc -randkey HTTP/cdh3@JAVACHEN.COM&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建完成后，查看：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kadmin.local -q &quot;listprincs&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;keytab&quot;&gt;10.2 创建keytab文件&lt;/h2&gt;

&lt;p&gt;keytab 是包含 principals 和加密 principal key 的文件。keytab 文件对于每个 host 是唯一的，因为 key 中包含 hostname。keytab 文件用于不需要人工交互和保存纯文本密码，实现到 kerberos 上验证一个主机上的 principal。因为服务器上可以访问 keytab 文件即可以以 principal 的身份通过 kerberos 的认证，所以，keytab 文件应该被妥善保存，应该只有少数的用户可以访问。&lt;/p&gt;

&lt;p&gt;创建包含 hdfs principal 和 host principal 的 hdfs keytab：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;xst -norandkey -k hdfs.keytab hdfs/fully.qualified.domain.name host/fully.qualified.domain.name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建包含 mapred principal 和 host principal 的 mapred keytab：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;xst -norandkey -k mapred.keytab mapred/fully.qualified.domain.name host/fully.qualified.domain.name
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：&lt;br /&gt;
上面的方法使用了xst的norandkey参数，有些kerberos不支持该参数。&lt;br /&gt;
当不支持该参数时有这样的提示：&lt;code&gt;Principal -norandkey does not exist.&lt;/code&gt;，需要使用下面的方法来生成keytab文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 cdh1 节点，即 KDC server 节点上执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/kerberos/krb5kdc/

kadmin.local -q &quot;xst  -k hdfs-unmerged.keytab  hdfs/cdh1@JAVACHEN.COM&quot;
kadmin.local -q &quot;xst  -k hdfs-unmerged.keytab  hdfs/cdh2@JAVACHEN.COM&quot;
kadmin.local -q &quot;xst  -k hdfs-unmerged.keytab  hdfs/cdh3@JAVACHEN.COM&quot;

kadmin.local -q &quot;xst  -k HTTP.keytab  HTTP/cdh1@JAVACHEN.COM&quot;
kadmin.local -q &quot;xst  -k HTTP.keytab  HTTP/cdh2@JAVACHEN.COM&quot;
kadmin.local -q &quot;xst  -k HTTP.keytab  HTTP/cdh3@JAVACHEN.COM&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，就会在 &lt;code&gt;/var/kerberos/krb5kdc/&lt;/code&gt; 目录下生成 &lt;code&gt;hdfs-unmerged.keytab&lt;/code&gt; 和 &lt;code&gt;HTTP.keytab&lt;/code&gt; 两个文件，接下来使用 &lt;code&gt;ktutil&lt;/code&gt; 合并者两个文件为 &lt;code&gt;hdfs.keytab&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/kerberos/krb5kdc/

$ ktutil
ktutil: rkt hdfs-unmerged.keytab
ktutil: rkt HTTP.keytab
ktutil: wkt hdfs.keytab
ktutil: exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 klist 显示 hdfs.keytab 文件列表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ klist -ket  hdfs.keytab
Keytab name: FILE:hdfs.keytab
KVNO Timestamp         Principal
---- ----------------- --------------------------------------------------------
   2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (aes256-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (aes128-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (des3-cbc-sha1)
   2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (arcfour-hmac)
   2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (des-hmac-sha1)
   2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (des-cbc-md5)
   2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (aes256-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (aes128-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (des3-cbc-sha1)
   2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (arcfour-hmac)
   2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (des-hmac-sha1)
   2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (des-cbc-md5)
   2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (aes256-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (aes128-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (des3-cbc-sha1)
   2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (arcfour-hmac)
   2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (des-hmac-sha1)
   2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (des-cbc-md5)
   2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (aes256-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (aes128-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (des3-cbc-sha1)
   2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (arcfour-hmac)
   2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (des-hmac-sha1)
   2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (des-cbc-md5)
   2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (aes256-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (aes128-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (des3-cbc-sha1)
   2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (arcfour-hmac)
   2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (des-hmac-sha1)
   2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (des-cbc-md5)
   2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (aes256-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (aes128-cts-hmac-sha1-96)
   2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (des3-cbc-sha1)
   2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (arcfour-hmac)
   2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (des-hmac-sha1)
   2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (des-cbc-md5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证是否正确合并了key，使用合并后的keytab，分别使用hdfs和host principals来获取证书。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t hdfs.keytab hdfs/cdh1@JAVACHEN.COM
$ kinit -k -t hdfs.keytab HTTP/cdh1@JAVACHEN.COM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果出现错误：&lt;code&gt;kinit: Key table entry not found while getting initial credentials&lt;/code&gt;，&lt;br /&gt;
则上面的合并有问题，重新执行前面的操作。&lt;/p&gt;

&lt;h2 id=&quot;kerberos-keytab&quot;&gt;10.3 部署kerberos keytab文件&lt;/h2&gt;

&lt;p&gt;拷贝 hdfs.keytab 文件到其他节点的 /etc/hadoop/conf 目录&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/kerberos/krb5kdc/

$ scp hdfs.keytab cdh1:/etc/hadoop/conf
$ scp hdfs.keytab cdh2:/etc/hadoop/conf
$ scp hdfs.keytab cdh3:/etc/hadoop/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并设置权限，分别在 cdh1、cdh2、cdh3 上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &quot;chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab&quot;
$ ssh cdh2 &quot;chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab&quot;
$ ssh cdh3 &quot;chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改kdc中的principal的密码，则该keytab就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400)&lt;/p&gt;

&lt;h2 id=&quot;hdfs-&quot;&gt;10.4 修改 hdfs 配置文件&lt;/h2&gt;

&lt;p&gt;先停止集群：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ for x in `cd /etc/init.d ; ls hive-*` ; do sudo service $x stop ; done
$ for x in `cd /etc/init.d ; ls impala-*` ; do sudo service $x stop ; done
$ for x in `cd /etc/init.d ; ls hadoop-*` ; do sudo service $x stop ; done
$ for x in `cd /etc/init.d ; ls zookeeper-*` ; do sudo service $x stop ; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在集群中所有节点的 core-site.xml 文件中添加下面的配置:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.security.authentication&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;kerberos&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.security.authorization&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在集群中所有节点的 hdfs-site.xml 文件中添加下面的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.block.access.token.enable&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;  
  &amp;lt;name&amp;gt;dfs.datanode.data.dir.perm&amp;lt;/name&amp;gt;  
  &amp;lt;value&amp;gt;700&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.namenode.keytab.file&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hadoop/conf/hdfs.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.namenode.kerberos.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hdfs/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.namenode.kerberos.https.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;HTTP/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.datanode.address&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;0.0.0.0:1004&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.datanode.http.address&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;0.0.0.0:1006&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.datanode.keytab.file&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hadoop/conf/hdfs.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.datanode.kerberos.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hdfs/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.datanode.kerberos.https.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;HTTP/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想开启 SSL，请添加（本文不对这部分做说明）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.http.policy&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;HTTPS_ONLY&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 HDFS 配置了 QJM HA，则需要添加（另外，你还要在 zookeeper 上配置 kerberos）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.journalnode.keytab.file&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hadoop/conf/hdfs.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.journalnode.kerberos.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hdfs/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.journalnode.kerberos.internal.spnego.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;HTTP/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果配置了 WebHDFS，则添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.webhdfs.enabled&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.web.authentication.kerberos.principal&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;HTTP/_HOST@JAVACHEN.COM&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.web.authentication.kerberos.keytab&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/etc/hadoop/conf/hdfs.keytab&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置中有几点要注意的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;dfs.datanode.address&lt;/code&gt;表示 data transceiver RPC server 所绑定的 hostname 或 IP 地址，如果开启 security，端口号必须小于 &lt;code&gt;1024&lt;/code&gt;(privileged port)，否则的话启动 datanode 时候会报 &lt;code&gt;Cannot start secure cluster without privileged resources&lt;/code&gt; 错误&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;principal 中的 instance 部分可以使用 &lt;code&gt;_HOST&lt;/code&gt; 标记，系统会自动替换它为全称域名&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;如果开启了 security, hadoop 会对 hdfs block data(由 &lt;code&gt;dfs.data.dir&lt;/code&gt; 指定)做 permission check，方式用户的代码不是调用hdfs api而是直接本地读block data，这样就绕过了kerberos和文件权限验证，管理员可以通过设置 &lt;code&gt;dfs.datanode.data.dir.perm&lt;/code&gt; 来修改 datanode 文件权限，这里我们设置为700&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hdfs--1&quot;&gt;10.5 检查集群上的 HDFS 和本地文件的权限&lt;/h2&gt;

&lt;p&gt;请参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_sg_users_groups_verify.html&quot;&gt;Verify User Accounts and Groups in CDH 5 Due to Security&lt;/a&gt; 或者 &lt;a href=&quot;http://hadoop.apache.org/docs/r2.5.0/hadoop-project-dist/hadoop-common/SecureMode.html&quot;&gt;Hadoop in Secure Mode&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;namenode&quot;&gt;10.6 启动 NameNode&lt;/h2&gt;

&lt;p&gt;启动之前，请确认 JCE jar 已经替换，请参考前面的说明。&lt;/p&gt;

&lt;p&gt;在每个节点上获取 root 用户的 ticket，这里 root 为之前创建的 root/admin 的密码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &quot;echo root|kinit root/admin&quot;
$ ssh cdh1 &quot;echo root|kinit root/admin&quot;
$ ssh cdh1 &quot;echo root|kinit root/admin&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取 cdh1的 ticket：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.COM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果出现下面异常 &lt;code&gt;kinit: Password incorrect while getting initial credentials&lt;/code&gt;，则重新导出 keytab 再试试。&lt;/p&gt;

&lt;p&gt;然后启动服务，观察日志：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /etc/init.d/hadoop-hdfs-namenode start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证 NameNode 是否启动，一是打开 web 界面查看启动状态，一是运行下面命令查看 hdfs：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -ls /
Found 4 items
drwxrwxrwx   - yarn hadoop          0 2014-06-26 15:24 /logroot
drwxrwxrwt   - hdfs hadoop          0 2014-11-04 10:44 /tmp
drwxr-xr-x   - hdfs hadoop          0 2014-08-10 10:53 /user
drwxr-xr-x   - hdfs hadoop          0 2013-05-20 22:52 /var
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果在你的凭据缓存中没有有效的 kerberos ticket，执行上面命令将会失败，将会出现下面的错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;14/11/04 12:08:12 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException:
GSS initiate failed [Caused by GS***ception: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
Bad connection to FS. command aborted. exception: Call to cdh1/192.168.56.121:8020 failed on local exception: java.io.IOException:
javax.security.sasl.SaslException: GSS initiate failed [Caused by GS***ception: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;datanode&quot;&gt;10.7 启动DataNode&lt;/h2&gt;

&lt;p&gt;DataNode 需要通过 JSVC 启动。首先检查是否安装了 JSVC 命令，然后配置环境变量。&lt;/p&gt;

&lt;p&gt;在 cdh1 节点查看是否安装了 JSVC：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ls /usr/lib/bigtop-utils/
bigtop-detect-classpath  bigtop-detect-javahome  bigtop-detect-javalibs  jsvc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后编辑 &lt;code&gt;/etc/default/hadoop-hdfs-datanode&lt;/code&gt;，取消对下面的注释并添加一行设置 &lt;code&gt;JSVC_HOME&lt;/code&gt;，修改如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export HADOOP_SECURE_DN_USER=hdfs
export HADOOP_SECURE_DN_PID_DIR=/var/run/hadoop-hdfs
export HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfs

export JSVC_HOME=/usr/lib/bigtop-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将该文件同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp /etc/default/hadoop-hdfs-datanode cdh2:/etc/default/hadoop-hdfs-datanode
$ scp /etc/default/hadoop-hdfs-datanode cdh3:/etc/default/hadoop-hdfs-datanode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分别在 cdh2、cdh3 获取 ticket 然后启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#root 为 root/admin 的密码
$ ssh cdh1 &quot;kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.COM; service hadoop-hdfs-datanode start&quot;
$ ssh cdh2 &quot;kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh2@JAVACHEN.COM; service hadoop-hdfs-datanode start&quot;
$ ssh cdh3 &quot;kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh3@JAVACHEN.COM; service hadoop-hdfs-datanode start&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;观看 cdh1 上 NameNode 日志，出现下面日志表示 DataNode 启动成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;14/11/04 17:21:41 INFO security.UserGroupInformation:
Login successful for user hdfs/cdh2@JAVACHEN.COM using keytab file /etc/hadoop/conf/hdfs.keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-7&quot;&gt;11. 总结&lt;/h1&gt;

&lt;p&gt;本文介绍了 CDH Hadoop 集成 kerberos 认证的过程，其中主要需要注意以下几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;配置 hosts，&lt;code&gt;hostname&lt;/code&gt; 请使用小写&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;确保 kerberos 客户端和服务端连通&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;替换 JRE 自带的 JCE jar 包&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;为 DataNode 设置运行用户并配置 &lt;code&gt;JSVC_HOME&lt;/code&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;启动服务前，先获取 ticket 再运行相关命令&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面的过程比较繁琐，我总结了上面的过程并写了一些自动化的脚本方便快速安装、配置以及管理 kerberos，请参考&lt;a href=&quot;/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop.html&quot;&gt;Hadoop集群部署权限总结&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-8&quot;&gt;12. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2015/03/how-to-quickly-configure-kerberos-for-your-apache-hadoop-cluster/&quot;&gt;How-to: Quickly Configure Kerberos for Your Apache Hadoop Cluster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zouhc/MyHadoop/blob/master/doc/Hadoop%E7%9A%84kerberos%E7%9A%84%E5%AE%9E%E8%B7%B5%E9%83%A8%E7%BD%B2.md&quot;&gt;Hadoop的kerberos的实践部署&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.chinaunix.net/uid-1838361-id-3243243.html&quot;&gt;hadoop 添加kerberos认证&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/lalaguozhe/article/details/11570009&quot;&gt;YARN &amp;amp; HDFS2 安装和配置Kerberos&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.godatadriven.com/kerberos_kdc_install.html&quot;&gt;Kerberos basics and installing a KDC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.wuzesheng.com/?p=2345&quot;&gt;Hadoop, Hbase, Zookeeper安全实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/11/04/config-kerberos-in-cdh-hdfs.html</link>
      <guid>http://blog.javachen.com/2014/11/04/config-kerberos-in-cdh-hdfs.html</guid>
      <pubDate>2014-11-04T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Mac上使用homebrew安装PostgreSql</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;安装&lt;/h1&gt;

&lt;p&gt;brew 安装 postgresql ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install postgresql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看安装的版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pg_ctl -V
pg_ctl (PostgreSQL) 9.3.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装成功之后，安装路径为：/usr/local/var/postgres&lt;/p&gt;

&lt;p&gt;接下来，初始化数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ initdb /usr/local/var/postgres
The files belonging to this database system will be owned by user &quot;june&quot;.
This user must also own the server process.

The database cluster will be initialized with locale &quot;zh_CN.UTF-8&quot;.
The default database encoding has accordingly been set to &quot;UTF8&quot;.
initdb: could not find suitable text search configuration for locale &quot;zh_CN.UTF-8&quot;
The default text search configuration will be set to &quot;simple&quot;.

Data page checksums are disabled.

creating directory /usr/local/var/postgres ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
creating configuration files ... ok
creating template1 database in /usr/local/var/postgres/base/1 ... ok
initializing pg_authid ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects&#39; descriptions ... ok
creating collations ... ok
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok
syncing data to disk ... ok

WARNING: enabling &quot;trust&quot; authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.

Success. You can now start the database server using:

    postgres -D /usr/local/var/postgres
or
    pg_ctl -D /usr/local/var/postgres -l logfile start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置开机登陆（可选）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p ~/Library/LaunchAgents
$ cp /usr/local/Cellar/postgresql/9.3.5_1/homebrew.mxcl.postgresql.plist ~/Library/LaunchAgents/
$ launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;手动启动 postgresql&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看状态：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log stop -s -m fast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看进程：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ps auxwww | grep postgres
june            56126   0.0  0.0  2432772    644 s000  S+    5:01下午   0:00.00 grep postgres
june            56058   0.0  0.0  2467360    584   ??  Ss    5:00下午   0:00.00 postgres: stats collector process
june            56057   0.0  0.0  2611808   1744   ??  Ss    5:00下午   0:00.00 postgres: autovacuum launcher process
june            56056   0.0  0.0  2611676    696   ??  Ss    5:00下午   0:00.00 postgres: wal writer process
june            56055   0.0  0.0  2611676    944   ??  Ss    5:00下午   0:00.01 postgres: writer process
june            56054   0.0  0.0  2611676    756   ??  Ss    5:00下午   0:00.00 postgres: checkpointer process
june            56044   0.0  0.2  2611676  14096 s000  S     5:00下午   0:00.02 /usr/local/Cellar/postgresql/9.3.5_1/bin/postgres -D /usr/local/var/postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建用户和数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#createuser will prompt you for a password, enter it twice.
$ createuser  -P test
$ createdb -Otest -Eutf8 test_db
$ psql
postgres=# GRANT ALL PRIVILEGES ON test TO test;
postgres=# \q
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入命令行模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ psql -U test test_db -h localhost -W
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果出现 &lt;code&gt;FATAL: Ident authentication failed for user&lt;/code&gt;，是因为：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is because by default PostgreSQL uses ‘ident’ authentication i.e it checks if the username exists on the system. You need to change authentication mode to ‘trust’ as we do not want to add a system user.&lt;br /&gt;
Modify the settings in “pg_hba.conf” to use ‘trust’ authentication.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;请修改 /usr/local/var/postgres/pg_hba.conf 为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;host    all             all             127.0.0.1/32            trust
# IPv6 local connections:
host    all             all             ::1/128                 trust
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装 pgadmin，下载地址：&lt;a href=&quot;http://www.pgadmin.org/download/macosx.php&quot;&gt;http://www.pgadmin.org/download/macosx.php&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;卸载&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew uninstall postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果配置了开机登陆：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist
$ rm -rf ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/10/30/install-postgresql-on-mac-using-homebrew.html</link>
      <guid>http://blog.javachen.com/2014/10/30/install-postgresql-on-mac-using-homebrew.html</guid>
      <pubDate>2014-10-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Django中的模板</title>
      <description>&lt;p&gt;通过《&lt;a href=&quot;/2014/01/11/how-to-create-a-django-site.html&quot;&gt;如何创建一个Django网站&lt;/a&gt;》大概清楚了如何创建一个简单的 Django 网站，这篇文章主要是在此基础上介绍 Django 中模板相关的用法。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;视图中使用模板&lt;/h1&gt;

&lt;p&gt;在《&lt;a href=&quot;/2014/01/11/how-to-create-a-django-site.html&quot;&gt;如何创建一个Django网站&lt;/a&gt;》中使用模板的方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.http import HttpResponse
from django.template import RequestContext, loader

from polls.models import Question

def index(request):
    latest_question_list = Question.objects.order_by(&#39;-pub_date&#39;)[:5]
    template = loader.get_template(&#39;polls/index.html&#39;)
    context = RequestContext(request, {
        &#39;latest_question_list&#39;: latest_question_list,
    })
    return HttpResponse(template.render(context))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;通过 loader 来加载模板页面，如前面提到的，这里是相对 polls/templates 目录&lt;/li&gt;
  &lt;li&gt;创建上下文，将需要传递到页面的变量放入上下文变量 context&lt;/li&gt;
  &lt;li&gt;使用 template 通过上下文来渲染页面&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 settings.py 中有一个 TEMPLATE_LOADERS 属性，并且有一个默认值 django.template.loaders.app_directories.Loader，该值定义了从每一个安装的 app 的 templates 目录下寻找模板。当然，你也可以编辑 settings.py 文件中的 TEMPLATE_DIRS属性手动指定模板路径：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;TEMPLATE_DIRS = (
    # Put strings here, like &quot;/home/html/django_templates&quot; or &quot;C:/www/django/templates&quot;.
    # Always use forward slashes, even on Windows.
    # Don&#39;t forget to use absolute paths, not relative paths.
    &#39;/home/django/mysite/templates&#39;,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面配置的是系统绝对路径，最好的方式是使用代码动态构建这个路径，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os

BASE_DIR = os.path.dirname(os.path.dirname(__file__))

TEMPLATE_DIRS = (
    os.path.join(BASE_DIR,&#39;templates&#39;).replace(&#39;\\&#39;,&#39;/&#39;),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;这个例子使用了神奇的 Python 内部变量 &lt;code&gt;__file__&lt;/code&gt; ，该变量被自动设置为代码所在的 Python 模块文件名。 &lt;code&gt;os.path.dirname(__file__)&lt;/code&gt;将会获取自身所在的文件，然后由 &lt;code&gt;os.path.join&lt;/code&gt; 这个方法将这目录与 templates 进行连接。如果在 windows下，它会智能地选择正确的后向斜杠’/’进行连接，而不是前向斜杠’/’。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;完成 &lt;code&gt;TEMPLATE_DIRS&lt;/code&gt; 设置后，下一步就是修改视图代码，让它使用 Django 模板加载功能而不是对模板路径硬编码。&lt;/p&gt;

&lt;p&gt;举例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.template.loader import get_template
from django.template import Context
from django.http import HttpResponse
import datetime

def current_datetime(request):
    now = datetime.datetime.now()
    t = get_template(&#39;current_datetime.html&#39;)
    html = t.render(Context({&#39;current_date&#39;: now}))
    return HttpResponse(html)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一种更加简洁的编码方式是使用 &lt;code&gt;render_to_response()&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.shortcuts import render_to_response
import datetime

def current_datetime(request):
    now = datetime.datetime.now()
    return render_to_response(&#39;current_datetime.html&#39;, {&#39;current_date&#39;: now})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面两个方法都会传入 Context 上下文，将页面需要的变量传到前台去，如果变量很多，需要一一输入。还有一种更加简单的方式，那就是可以利用 Python 的内建函数 &lt;code&gt;locals()&lt;/code&gt; 。它返回的字典对所有局部变量的名称与值进行映射。 因此，前面的视图可以重写成下面这个样子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def current_datetime(request):
    current_date = datetime.datetime.now()
    return render_to_response(&#39;current_datetime.html&#39;, locals())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 &lt;code&gt;locals()&lt;/code&gt; 时要注意是它将包括所有的局部变量，它们可能比你想让模板访问的要多。 在前例中，&lt;code&gt;locals()&lt;/code&gt; 还包含了 request。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;模板标签&lt;/h1&gt;

&lt;p&gt;上面的例子中提到了模板 &lt;code&gt;current_datetime.html&lt;/code&gt; ，其中的内容如何定义呢？这个就要使用模板标签了。Django 的模板系统带有内置的标签和过滤器。&lt;/p&gt;

&lt;h2 id=&quot;ifelse&quot;&gt;if/else&lt;/h2&gt;

&lt;p&gt;if 标签检查一个变量，如果这个变量为真（即，变量存在，非空，不是布尔值假），系统会显示在 if 和 endif 之间的任何内容，例如：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% if today_is_weekend %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;Welcome to the weekend&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% endif %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;else 标签是可选的：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% if today_is_weekend %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;Welcome to the weekend&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% else %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;Get back to work.&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% endif %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;一些注意事项：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;if 标签接受 and ，or 或者 not 关键字来对多个变量做判断，或者对变量取反。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;if 标签不允许在同一个标签中同时使用 and 和 or ，因为逻辑上可能模糊的。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;系统不支持用圆括号来组合比较操作。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;多次使用同一个逻辑操作符是没有问题的，但是我们不能把不同的操作符组合起来。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;并没有 elif 标签， 请使用嵌套的 if 标签来达成同样的效果。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;一定要用 endif 关闭每一个 if 标签。&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;for&quot;&gt;for&lt;/h2&gt;

&lt;p&gt;允许我们在一个序列上迭代。 与 Python 的 for 语句的情形类似，循环语法是 &lt;code&gt;for X in Y &lt;/code&gt;，Y 是要迭代的序列而X是在每一个特定的循环中使用的变量名称。 每一次循环中，模板系统会渲染在 for 和 endfor 之间的所有内容。&lt;/p&gt;

&lt;p&gt;给定一个运动员列表 &lt;code&gt;athlete_list&lt;/code&gt; 变量，我们可以使用下面的代码来显示这个列表：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;ul&amp;gt;&lt;/span&gt;
{% for athlete in athlete_list %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;li&amp;gt;&lt;/span&gt;{{ athlete.name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
{% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/ul&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;给标签增加一个 &lt;code&gt;reversed&lt;/code&gt; 使得该列表被反向迭代：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% for athlete in athlete_list reversed %}
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;li&amp;gt;&lt;/span&gt;{{ athlete.name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
{% endfor %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;在执行循环之前先检测列表的大小是一个通常的做法，当列表为空时输出一些特别的提示。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% if athlete_list %}
    {% for athlete in athlete_list %}
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;{{ athlete.name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
    {% endfor %}
{% else %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;There are no athletes. Only computer programmers.&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% endif %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;因为这种做法十分常见，所以 &lt;code&gt;for&lt;/code&gt; 标签支持一个可选的 empty 分句，通过它我们可以定义当列表为空时的输出内容 下面的例子与之前那个等价：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% for athlete in athlete_list %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;{{ athlete.name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% empty %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;There are no athletes. Only computer programmers.&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% endfor %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Django不支持退出循环操作。&lt;/p&gt;

&lt;p&gt;在每个 for 循环里有一个称为&lt;code&gt;forloop&lt;/code&gt; 的模板变量。这个变量有一些提示循环进度信息的属性。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;forloop.counter 总是一个表示当前循环的执行次数的整数计数器。 这个计数器是从1开始的，所以在第一次循环时 forloop.counter 将会被设置为1。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% for item in todo_list %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;{{ forloop.counter }}: {{ item }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% endfor %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;forloop.counter0 类似于 forloop.counter ，但是它是从0计数的。 第一次执行循环时这个变量会被设置为0。&lt;/p&gt;

  &lt;p&gt;forloop.revcounter 是表示循环中剩余项的整型变量。 在循环初次执行时 forloop.revcounter 将被设置为序列中项的总数。 最后一次循环执行中，这个变量将被置1。&lt;/p&gt;

  &lt;p&gt;forloop.revcounter0 类似于 forloop.revcounter ，但它以0做为结束索引。 在第一次执行循环时，该变量会被置为序列的项的个数减1。&lt;/p&gt;

  &lt;p&gt;forloop.first 是一个布尔值，如果该迭代是第一次执行，那么它被置为 True&lt;/p&gt;

  &lt;p&gt;forloop.last 是一个布尔值；在最后一次执行循环时被置为True&lt;/p&gt;

  &lt;p&gt;forloop.parentloop 是一个指向当前循环的上一级循环的 forloop 对象的引用（在嵌套循环的情况下）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;ifequalifnotequal&quot;&gt;ifequal/ifnotequal&lt;/h2&gt;

&lt;p&gt;ifequal 标签比较两个值，当他们相等时，显示在 ifequal 和 endifequal 之中所有的值。参数可以是硬编码的字符串，随便用单引号或者双引号引起来。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% ifequal user currentuser %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;h1&amp;gt;&lt;/span&gt;Welcome&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;
{% endifequal %}

{% ifequal section &amp;quot;community&amp;quot; %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;h1&amp;gt;&lt;/span&gt;Community&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;
{% endifequal %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;和 if 类似，ifequal 支持可选的 else 标签。&lt;/p&gt;

&lt;p&gt;只有模板变量、字符串、整数和小数可以作为 ifequal 标签的参数。&lt;/p&gt;

&lt;h2 id=&quot;cycle&quot;&gt;cycle&lt;/h2&gt;

&lt;p&gt;每当我们使用一次这个标签后，标签中的值就会变化，如上，每使用一次下面的 cycle 标签，输出的就会在 row1 和 row2 之间切换。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% for o in some_list %}  
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;tr&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% cycle &amp;#39;row1&amp;#39; &amp;#39;row2&amp;#39; %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;  
        ...  
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/tr&amp;gt;&lt;/span&gt;  
{% endfor %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;一些情况下，我们希望将cycle当做一个变量一样来使用，那么我们可以这样：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;tr&amp;gt;&lt;/span&gt;  
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;td&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% cycle &amp;#39;row1&amp;#39; &amp;#39;row2&amp;#39; as rowcolors %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;...&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;  
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;td&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{{ rowcolors }}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;...&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;  
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/tr&amp;gt;&lt;/span&gt;  
&lt;span class=&quot;nt&quot;&gt;&amp;lt;tr&amp;gt;&lt;/span&gt;  
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;td&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% cycle rowcolors %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;...&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;  
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;td&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{{ rowcolors }}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;...&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;  
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/tr&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;最后，当出现我们不希望 cycle 主动输出的时候，也就是我么只希望它作为一个变量的时候，我们可以这样设置。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cycle&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;row1&amp;#39;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;row2&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rowcolors&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;silent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;url&quot;&gt;url&lt;/h2&gt;

&lt;p&gt;Django 中的 url 标签是用来简化 url 的定义，其可以通过唯一的名称引用 urls.py 中定义的 url。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;urlpatterns = patterns(&#39;&#39;,    
    url(r&#39;^hello/$&#39;, hello,name=&#39;hello&#39;),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于上面的定义，可以在模板里通过 hello 名称来应用 &lt;code&gt;hello/&lt;/code&gt; 这个 url。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% url &amp;#39;hello&amp;#39; %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Hello World&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;这样使用的好处是，无论你怎么修改 urlpatterns 的地址，Template 都会随着改变，省事了不少。&lt;strong&gt;在模版中调用url标签的时候，需要添加下面代码：&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;future&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;需要注意的是：name是全局的，你整个 urlpatterns 里只能一个唯一的name。&lt;/p&gt;

&lt;p&gt;如果想在试图中使用该名称，可以使用项目代码（关键在于 &lt;code&gt;reverse&lt;/code&gt; 函数）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.core.urlresolvers import reverse

HttpResponseRedirect(reverse(&quot;news_index&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当遇到urlpatterns的地址包含有参数的时候，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;(r&#39;^(?P&amp;lt;year&amp;gt;\d{4})/(?P&amp;lt;month&amp;gt;\d{1,2})/$&#39;,&#39;news_list&#39;,name=&#39;news_archive&#39; ),
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有两个参数，最终的地址如归档的地址 http://blog.javachen.com/2014/10 ，这时候的标签使用如下：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{%url &amp;#39;news_archive&amp;#39; 2014  10%}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;2014年10月&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{%url &amp;#39;news_archive&amp;#39; year=2014  month=10%}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;2014年10月&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;当然，在你后台的 views.py 中的方法上也必须有这两个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def news_list(request,year,month):
    print &#39;year:&#39;,year
    print &#39;monty:&#39;,month
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而在试图里写法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.core.urlresolvers import reverse
......
reverse(&quot;news_archive&quot;,kwargs={&quot;year&quot;:2014,&quot;month&quot;:10})
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;autoescape&quot;&gt;autoescape&lt;/h2&gt;

&lt;p&gt;控制HTML转义，参数是：on 或 off。效果和使用 safe 或 escape 过滤器相同。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autoescape&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;body&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endautoescape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;csrftoken&quot;&gt;csrf_token&lt;/h2&gt;

&lt;p&gt;防止跨站请求伪造。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;form&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;action=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;.&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;method=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{% csrf_token %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;debug&quot;&gt;debug&lt;/h2&gt;

&lt;p&gt;输出完整的调试信息，包括当前的上下文及导入的模块信息。&lt;/p&gt;

&lt;h2 id=&quot;filter&quot;&gt;filter&lt;/h2&gt;

&lt;p&gt;通过可变过滤器过滤变量的内容。&lt;/p&gt;

&lt;p&gt;过滤器也可以相互传输，它们也可以有参数，就像变量的语法一样。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;force_escape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;This&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;will&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;be&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HTML&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;escaped&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;will&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appear&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lowercase&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endfilter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意： &lt;br /&gt;
escape 和safe 过滤器不能接受参数，而使用 autoescape 标签用来管理模板代码块的自动转移。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;firstof&quot;&gt;firstof&lt;/h2&gt;

&lt;p&gt;输出传入的第一个不是 False 的变量，如果被传递变量都是 False ，则什么也不输出。例：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;firstof&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;等价于:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;safe&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;safe&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;safe&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;extends&quot;&gt;extends&lt;/h2&gt;

&lt;p&gt;extends 标签声明这个模板继承的父模板。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% extends &amp;quot;base.html&amp;quot; %}
{% load staticfiles %}

{% block css %} 
{{ block.super }}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;link&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% static &amp;#39;css/page/index.css&amp;#39; %}&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;rel=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;stylesheet&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;text/css&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
{% endblock %}

{% block content %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;Hello world&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% include &amp;quot;partial/index/product_list.html&amp;quot; %}
{% endblock %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;上面代码中，首先用extends标签声明这个模板继承的父模板（必须保证其为模板中的第一个模板标记）。接着重定义 css 这个 block，在包含了父模板的基础上（block.super）引入了新的 CSS 文件；替换了 content 这个 block。这就是子模板所做的全部工作。&lt;/p&gt;

&lt;h2 id=&quot;block&quot;&gt;block&lt;/h2&gt;

&lt;p&gt;所有的 block 标签告诉模板引擎，子模板可以重载这些部分，如果子模板不重载这些部分，则将按默认的内容显示。例如：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% load staticfiles %}
&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;head&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;title&amp;gt;&lt;/span&gt;
        {% block title %} The Default Title {% endblock %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;

    {% block css %} 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;link&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% static &amp;#39;css/base.css&amp;#39; %}&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;rel=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;stylesheet&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;text/css&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    {% endblock %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;wrapper&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
        {% block header %}
        {% include &amp;#39;partial/header.html&amp;#39; %}
        {% endblock %}

        {% block content %} {% endblock %}

        {% block footer %}
        {% include &amp;#39;partial/footer.html&amp;#39; %}
        {% endblock %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

    {% block js %} 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% static &amp;#39;js/jquery.min.js&amp;#39; %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
    {% endblock %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;顺便提一下，模板中的 &lt;code&gt;load staticfiles&lt;/code&gt; 表示加载静态资源，这个一般用于加载 CSS、JS 等静态文件时用到&lt;/p&gt;

&lt;h2 id=&quot;include&quot;&gt;include&lt;/h2&gt;

&lt;p&gt;该标签允许在（模板中）包含其它的模板的内容。标签的参数是所要包含的模板名称，可以是一个变量，也可以是用单/双引号硬编码的字符串。每当在多个模板中出现相同的代码时，就应该考虑是否要使用 &lt;code&gt;include&lt;/code&gt; 来减少重复，这也是为了提高代码的可重用性。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;注释&lt;/h2&gt;

&lt;p&gt;注释使用：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# This is a comment #}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;如果要实现多行注释，可以使用 comment 模板标签。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;过滤器&lt;/h1&gt;

&lt;p&gt;模板过滤器是在变量被显示前修改它的值的一个简单方法，过滤器使用管道字符，例如：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;显示的内容是变量  被过滤器 lower 处理后的结果，它功能是转换文本为小写。&lt;/p&gt;

&lt;p&gt;过滤管道可以 &lt;em&gt;套接&lt;/em&gt; ：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upper&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;有些过滤器有参数。 过滤器的参数跟随冒号之后并且总是以双引号包含。 例如：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;truncatewords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;30&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;这个将显示变量 bio 的前30个词。&lt;/p&gt;

&lt;p&gt;当我们使用了网页编辑器的时候，我们通过编辑器得到的是一串 HTML 代码，如果字节输出，那么 django 会将它默认输出为字符串，从而不能显示出样式。这时候，我们可以使用过滤器来实现我们想要的。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myHtml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;safe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;这样，我们的字符串就被当做 HTML 代码来输出了。safe 表示这段代码是安全的。&lt;/p&gt;

&lt;p&gt;一些常见的过滤器：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;addslashes：添加反斜杠到任何反斜杠、单引号或者双引号前面。 这在处理包含 JavaScript 的文本时是非常有用的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;date：按指定的格式字符串参数格式化 date 或者 datetime 对象，范例：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pub_date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;F j, Y&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1 id=&quot;section-4&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://djangobook.py3k.cn/2.0/chapter04/&quot;&gt;The Django Book-第四章 模板&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yihaomen.com/article/python/355.htm&quot;&gt;Django url 标签的使用&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://raytaylorlin.com/Tech/Script/Python/django-note-3/&quot;&gt;Django学习笔记（3）——Django的模板&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/10/30/django-template.html</link>
      <guid>http://blog.javachen.com/2014/10/30/django-template.html</guid>
      <pubDate>2014-10-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Impala查询功能测试</title>
      <description>&lt;p&gt;关于 Impala 使用方法的一些测试，包括加载数据、查看数据库、聚合关联查询、子查询等等。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 准备测试数据&lt;/h1&gt;

&lt;p&gt;以下测试以 impala 用户来运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ su - impala
-bash-4.1$ whoami
impala
$ hdfs dfs -ls /user
Found 5 items
drwxr-xr-x   - hdfs   hadoop          0 2014-09-22 18:36 /user/hdfs
drwxrwxrwt   - mapred hadoop          0 2014-07-23 21:37 /user/history
drwxr-xr-x   - hive   hadoop          0 2014-08-04 16:57 /user/hive
drwxr-xr-x   - impala hadoop          0 2014-10-24 10:13 /user/impala
drwxr-xr-x   - root   hadoop          0 2014-09-22 10:22 /user/root
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;准备一些测试数据，tab1.csv 文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1,true,123.123,2012-10-24 08:55:00 
2,false,1243.5,2012-10-25 13:40:00
3,false,24453.325,2008-08-22 09:33:21.123
4,false,243423.325,2007-05-12 22:32:21.33454
5,true,243.325,1953-04-22 09:11:33
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tab1.csv 文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1,true,12789.123
2,false,1243.5
3,false,24453.325
4,false,2423.3254
50,true,243.325
60,false,243565423.325
70,true,243.325
80,false,243423.325
90,true,243.325
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将这两个表上传到 hdfs：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hdfs dfs -mkdir -p sample_data/tab1 sample_data/tab2

$ hdfs dfs -put tab1.csv /user/impala/sample_data/tab1
$ hdfs dfs -ls /user/impala/sample_data/tab1
Found 1 items
-rw-r--r--   3 impala hadoop        193 2014-10-24 10:13 /user/impala/sample_data/tab1/tab1.csv

$ hdfs dfs -put tab2.csv /user/impala/sample_data/tab2
$ hdfs dfs -ls /user/impala/sample_data/tab2
Found 1 items
-rw-r--r--   3 impala hadoop        158 2014-10-24 10:13 /user/impala/sample_data/tab2/tab2.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 impala 中建表，建表语句如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;DROP TABLE IF EXISTS tab1;
CREATE EXTERNAL TABLE tab1 (
   id INT,
   col_1 BOOLEAN,
   col_2 DOUBLE,
   col_3 TIMESTAMP
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LOCATION &#39;/user/impala/sample_data/tab1&#39;;

DROP TABLE IF EXISTS tab2;
CREATE EXTERNAL TABLE tab2 (
   id INT,
   col_1 BOOLEAN,
   col_2 DOUBLE
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LOCATION &#39;/user/impala/sample_data/tab2&#39;;

DROP TABLE IF EXISTS tab3;
CREATE TABLE tab3 (
   id INT,
   col_1 BOOLEAN,
   col_2 DOUBLE,
   month INT,
   day INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 tab1 和 tab2 都是外部表，tab3 是内部表。&lt;/p&gt;

&lt;p&gt;将上面 sql 保存在 init.sql 语句，然后运行下面命令进行创建表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ impala-shell -i localhost -f init.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以进入到 impala-shell 命令行模式，直接运行 sql 语句。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 查看表结构&lt;/h1&gt;

&lt;p&gt;查看所有数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; show databases;
Query: show databases
+------------------+
| name             |
+------------------+
| _impala_builtins |
| default          |          
| testdb           |
+------------------+
Returned 3 row(s) in 0.05s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看默认数据库下的所有表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; show tables;
Query: show tables
+------+
| name |
+------+
| tab1 |
| tab2 |
| tab3 |
+------+
Returned 3 row(s) in 0.01s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 tab1 表结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; describe tab1;
Query: describe tab1
+-------+-----------+---------+
| name  | type      | comment |
+-------+-----------+---------+
| id    | int       |         |
| col_1 | boolean   |         |
| col_2 | double    |         |
| col_3 | timestamp |         |
+-------+-----------+---------+
Returned 4 row(s) in 0.07s
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;impala-shell-&quot;&gt;3. impala-shell 命令&lt;/h1&gt;

&lt;p&gt;使用 impala-shell 进入命令行交互模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ impala-shell -i localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;传入一个文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ impala-shell -i localhost -f init.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行指定的 sql：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ impala-shell -i localhost -q &#39;select count(*) from tab1;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;4. 导入数据并查询&lt;/h1&gt;

&lt;p&gt;导入数据：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;准备数据&lt;/li&gt;
  &lt;li&gt;创建表&lt;/li&gt;
  &lt;li&gt;加数据导入到创建的表&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;查询数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[192.168.56.121:21000] &amp;gt; SELECT * FROM tab1;
Query: select * FROM tab1
+----+-------+------------+-------------------------------+
| id | col_1 | col_2      | col_3                         |
+----+-------+------------+-------------------------------+
| 1  | true  | 123.123    | 2012-10-24 08:55:00           |
| 2  | false | 1243.5     | 2012-10-25 13:40:00           |
| 3  | false | 24453.325  | 2008-08-22 09:33:21.123000000 |
| 4  | false | 243423.325 | 2007-05-12 22:32:21.334540000 |
| 5  | true  | 243.325    | 1953-04-22 09:11:33           |
+----+-------+------------+-------------------------------+
Returned 5 row(s) in 0.24s
[192.168.56.121:21000] &amp;gt; SELECT * FROM tab2;
Query: select * FROM tab2
+----+-------+---------------+
| id | col_1 | col_2         |
+----+-------+---------------+
| 1  | true  | 12789.123     |
| 2  | false | 1243.5        |
| 3  | false | 24453.325     |
| 4  | false | 2423.3254     |
| 50 | true  | 243.325       |
| 60 | false | 243565423.325 |
| 70 | true  | 243.325       |
| 80 | false | 243423.325    |
| 90 | true  | 243.325       |
+----+-------+---------------+
Returned 9 row(s) in 0.44s
[192.168.56.121:21000] &amp;gt; SELECT * FROM tab2 LIMIT 5;
Query: select * FROM tab2 LIMIT 5
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 2  | false | 1243.5    |
| 3  | false | 24453.325 |
| 4  | false | 2423.3254 |
| 50 | true  | 243.325   |
+----+-------+-----------+
Returned 5 row(s) in 0.44s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;带 OFFSET 语句查询&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;带 OFFSET 语句查询，需要和 order by 一起使用，起始编号从 0 开始往后偏移，offset 为 0 时，其结果和去掉 offset 的 limit 结果一致。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;测试如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; SELECT * FROM tab2 order by id LIMIT 3 offset 0;
Query: select * FROM tab2 order by id LIMIT 3 offset 0
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 2  | false | 1243.5    |
| 3  | false | 24453.325 |
+----+-------+-----------+
Returned 3 row(s) in 0.45s
[192.168.56.121:21000] &amp;gt; SELECT * FROM tab2 order by id LIMIT 3 offset 2;
Query: select * FROM tab2 order by id LIMIT 3 offset 2
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 3  | false | 24453.325 |
| 4  | false | 2423.3254 |
| 50 | true  | 243.325   |
+----+-------+-----------+
Returned 3 row(s) in 0.45s
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;join-&quot;&gt;5. join 连接查询&lt;/h1&gt;

&lt;h2 id=&quot;section-3&quot;&gt;5.1 左外连接：&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 LEFT OUTER JOIN tab2 USING (id);
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 2  | false | 1243.5    |
| 3  | false | 24453.325 |
| 4  | false | 2423.3254 |
| 5  | true  | NULL      |
+----+-------+-----------+
Returned 5 row(s) in 1.12s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上 SQL 语句等同于下面语句，用法同样适用于多个字段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 LEFT OUTER JOIN tab2 where tab1.id=tab2.id;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由上可以看到左边表 tab1 的记录都查询出来了，右边表 tab2 只查询出跟 tab1 关联的记录。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;5.2 内连接&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 INNER JOIN tab2 USING (id);
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 2  | false | 1243.5    |
| 3  | false | 24453.325 |
| 4  | false | 2423.3254 |
+----+-------+-----------+
Returned 4 row(s) in 0.53s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上语句可以修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;-- 下面语句都是内连接
SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 JOIN tab2 USING (id);
SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2 where tab1.id=tab2.id ;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查询结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2 where tab1.id=tab2.id ;
Query: select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2 where tab1.id=tab2.id
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 2  | false | 1243.5    |
| 3  | false | 24453.325 |
| 4  | false | 2423.3254 |
+----+-------+-----------+
Returned 4 row(s) in 0.38s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果去掉 where 语句，会提示错误：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2;
Query: select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2
ERROR: NotImplementedException: Join with &#39;default.tab2&#39; requires at least one conjunctive equality predicate. To perform a Cartesian product between two tables, use a CROSS JOIN.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;5.3 自连接&lt;/h2&gt;

&lt;p&gt;impala 允许自连接，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;-- Combine fields from both parent and child rows.
SELECT lhs.id, rhs.parent, lhs.c1, rhs.c2 FROM tree_data lhs, tree_data rhs WHERE lhs.id = rhs.parent;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;5.4 交叉连接&lt;/h2&gt;

&lt;p&gt;为了避免产生大量的结果集，impala 不允许下面形式的笛卡尔连接：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT ... FROM t1 JOIN t2;
SELECT ... FROM t1, t2;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果，你的确想使用笛卡尔连接，建议使用 cross join：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 CROSS JOIN tab2 where tab1.id&amp;lt;3;
Query: select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 CROSS JOIN tab2 where tab1.id&amp;lt;3
+----+-------+---------------+
| id | col_1 | col_2         |
+----+-------+---------------+
| 1  | true  | 12789.123     |
| 1  | true  | 1243.5        |
| 1  | true  | 24453.325     |
| 1  | true  | 2423.3254     |
| 1  | true  | 243.325       |
| 1  | true  | 243565423.325 |
| 1  | true  | 243.325       |
| 1  | true  | 243423.325    |
| 1  | true  | 243.325       |
| 2  | false | 12789.123     |
| 2  | false | 1243.5        |
| 2  | false | 24453.325     |
| 2  | false | 2423.3254     |
| 2  | false | 243.325       |
| 2  | false | 243565423.325 |
| 2  | false | 243.325       |
| 2  | false | 243423.325    |
| 2  | false | 243.325       |
+----+-------+---------------+
Returned 18 row(s) in 0.41s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-7&quot;&gt;5.5 等值连接和非等值连接&lt;/h2&gt;

&lt;p&gt;默认地，impala的两表连接需要一个等值的比较，或者使用 ON、USING、WHERE 语句。在Impala 1.2.2 之后，非等值连接也支持。同样需要避免因为产生大量的结果集而造成内存溢出。一旦你想使用非等值连接，建议使用 cross 连接并增加额外的 where 语句。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; select tab1.id,tab1.col_1,tab1.col_2,tab2.col_2 FROM tab1 CROSS JOIN tab2 where tab1.col_2 &amp;gt;tab2.col_2 ;
+----+-------+------------+-----------+
| id | col_1 | col_2      | col_2     |
+----+-------+------------+-----------+
| 2  | false | 1243.5     | 243.325   |
| 2  | false | 1243.5     | 243.325   |
| 2  | false | 1243.5     | 243.325   |
| 3  | false | 24453.325  | 12789.123 |
| 3  | false | 24453.325  | 1243.5    |
| 3  | false | 24453.325  | 2423.3254 |
| 3  | false | 24453.325  | 243.325   |
| 3  | false | 24453.325  | 243.325   |
| 3  | false | 24453.325  | 243.325   |
| 4  | false | 243423.325 | 12789.123 |
| 4  | false | 243423.325 | 1243.5    |
| 4  | false | 243423.325 | 24453.325 |
| 4  | false | 243423.325 | 2423.3254 |
| 4  | false | 243423.325 | 243.325   |
| 4  | false | 243423.325 | 243.325   |
| 4  | false | 243423.325 | 243.325   |
+----+-------+------------+-----------+
Returned 16 row(s) in 0.41s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查询出来的结果会有一些重复的记录，这个时候可以通过 distinct 去重。&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;5.6 半连接&lt;/h2&gt;

&lt;p&gt;左半连接是为了实现 in 语句，左边的记录会查询出来，而不管右边表有多少匹配的记录。Impala 2.0版本之后，支持右半连接。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 LEFT SEMI JOIN tab2 USING (id);
Query: select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 LEFT SEMI JOIN tab2 USING (id)
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 2  | false | 1243.5    |
| 3  | false | 24453.325 |
| 4  | false | 2423.3254 |
+----+-------+-----------+
Returned 4 row(s) in 0.41s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-9&quot;&gt;5.7 自然连接（不支持）&lt;/h2&gt;

&lt;p&gt;Impala 不支持 NATURAL JOIN 操作，以避免产生不一致或者大量的结果。自然连接不适应 ON 和 USING 语句，而是自动的关联所有列相同值的记录。这种连接是不建议的，特别是当表结构发生变化的时候，如添加或者删除列的时候，会产生不一样的结果集。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;-- &#39;NATURAL&#39; is interpreted as an alias for &#39;t1&#39; and Impala attempts an inner join,
-- resulting in an error because inner joins require explicit comparisons between columns.
SELECT t1.c1, t2.c2 FROM t1 NATURAL JOIN t2;
ERROR: NotImplementedException: Join with &#39;t2&#39; requires at least one conjunctive equality predicate.
  To perform a Cartesian product between two tables, use a CROSS JOIN.

-- If you expect the tables to have identically named columns with matching values,
-- list the corresponding column names in a USING clause.
SELECT t1.c1, t2.c2 FROM t1 JOIN t2 USING (id, type_flag, name, address);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;impala-20--cdh-52-&quot;&gt;5.8 反连接（Impala 2.0 / CDH 5.2 以上版本）&lt;/h2&gt;

&lt;p&gt;Impala 2.0 / CDH 5.2 以上版本中支持反连接，包括左反连接和右反连接。左反连接的意思是返回左边表不在右边表中的记录。&lt;/p&gt;

&lt;p&gt;找出 tab2 的 id 不在 tab1 中的记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; SELECT tab2.id FROM tab2 LEFT ANTI JOIN tab1 USING (id);
+----+
| id |
+----+
| 50 |
| 60 |
| 70 |
| 80 |
| 90 |
+----+
Returned 5 row(s) in 0.41s
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-10&quot;&gt;6. 聚合查询&lt;/h1&gt;

&lt;p&gt;聚合关联查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; select tab1.col_1, MAX(tab2.col_2), MIN(tab2.col_2) FROM tab2 JOIN tab1 USING (id) GROUP BY col_1 ORDER BY 1 LIMIT 5 ;
+-------+-----------------+-----------------+
| col_1 | max(tab2.col_2) | min(tab2.col_2) |
+-------+-----------------+-----------------+
| false | 24453.325       | 1243.5          |
| true  | 12789.123       | 12789.123       |
+-------+-----------------+-----------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;聚合关联子查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; select tab2.* FROM tab2, (SELECT tab1.col_1, MAX(tab2.col_2) AS max_col2 FROM tab2, tab1 WHERE tab1.id = tab2.id GROUP BY col_1) subquery1 WHERE subquery1.max_col2 = tab2.col_2 ;
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 3  | false | 24453.325 |
+----+-------+-----------+
Returned 2 row(s) in 0.54s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Impala 2版本中，支持where 条件子查询，包括 IN 、EXISTS 和比较符的子查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;select tab2.* from tab2 where tab2.id IN (select max(id) from tab1)
select tab2.* from tab2 where tab2.id EXISTS (select max(id) from tab1)
select tab2.* from tab2 where tab2.id &amp;gt; (select max(id) from tab1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;插入查询：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; insert OVERWRITE TABLE tab3 SELECT id, col_1, col_2, MONTH(col_3), DAYOFMONTH(col_3) FROM tab1 WHERE YEAR(col_3) = 2012 ;
Inserted 2 rows in 0.44s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候查询 tab3 的记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[192.168.56.121:21000] &amp;gt; SELECT * FROM tab3;
+----+-------+---------+-------+-----+
| id | col_1 | col_2   | month | day |
+----+-------+---------+-------+-----+
| 1  | true  | 123.123 | 10    | 24  |
| 2  | false | 1243.5  | 10    | 25  |
+----+-------+---------+-------+-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-11&quot;&gt;7. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/impala_joins.html&quot;&gt;Cloudera Impala Guide - Impala Joins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/10/24/impala-query-table-tutorial.html</link>
      <guid>http://blog.javachen.com/2014/10/24/impala-query-table-tutorial.html</guid>
      <pubDate>2014-10-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>当前数据仓库建设过程</title>
      <description>&lt;p&gt;一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 数据采集和存储&lt;/h1&gt;

&lt;p&gt;采集数据之前，先要定义数据如何存放在 hadoop 以及一些相关约束。约束如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;所有的日志数据都存放在 hdfs 上的 &lt;code&gt;/logroot&lt;/code&gt; 路径下面&lt;/li&gt;
  &lt;li&gt;hive 中数据库命名方式为 &lt;code&gt;dw_XXXX&lt;/code&gt;，例如：dw_srclog 存放外部来源的原始数据，dw_stat 存放统计结果的数据&lt;/li&gt;
  &lt;li&gt;原始数据都加工成为结构化的文本文件，字段分隔符统一使用制表符，并在 lzo 压缩之后上传到 hdfs 中。&lt;/li&gt;
  &lt;li&gt;hive 中使用外部表保存数据，数据存放在 &lt;code&gt;/logroot&lt;/code&gt; 下，如果不是分区表，则文件名为表名；如果是分区表，则按月和天分区，每天分区下的文件名为&lt;code&gt;表名_日期&lt;/code&gt;，例如：&lt;code&gt;test_20141023&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据采集的来源可能是关系数据库或者一些系统日志，采集工具可以是日志采集系统，例如：flume、sqoop 、storm以及一些 ETL 工具等等。&lt;/p&gt;

&lt;p&gt;目前，主要是从 mysql 中导出数据然后在导入到 hdfs 中，对于存储不需要按天分区的表，这部分过程代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

if [ &quot;$1&quot; ]; then 
  DAY=&quot;$1&quot;
else 
  DAY=&quot;yesterday&quot;
fi

datestr=`date +%Y-%m-%d -d&quot;$DAY&quot;`;
logday=`date +%Y%m%d -d&quot;$DAY&quot;`;
logmonth=`date +%Y%m -d&quot;$DAY&quot;`

#hive table
table=test
#mysql db config file
srcdb=db_name

sql=&quot;select * from test&quot;

hql=&quot;
use dw_srclog;
create external table if not exists test (
  id int,
  name int
)ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;
STORED AS INPUTFORMAT
  &#39;com.hadoop.mapred.DeprecatedLzoTextInputFormat&#39;
OUTPUTFORMAT
  &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;
LOCATION
  &#39;/logroot/test&#39;;
&quot;

#begin
chmod +x $srcdb.sql
. ./$srcdb.sql

file=&quot;${table}&quot;
sql_var=&quot; -r -quick --default-character-set=utf8  --skip-column&quot;

mysql $sql_var -h${db_host} -u${db_user} -p${db_pass} -P${db_port} -D${db_name} -e &quot;$sql&quot; | sed &quot;s/NULL/\\\\N/g&quot;&amp;gt; $file 2&amp;gt;&amp;amp;1

lzop -U $file
hadoop fs -mkdir -p /logroot/$table
hadoop fs -ls /logroot/$table |grep lzo|awk &#39;{print $8}&#39;|xargs -i hadoop fs -rm {} 
hadoop fs -moveFromLocal $file.lzo /logroot/$table/
hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer  /logroot/$table/$file.lzo 2&amp;gt;&amp;amp;1

echo &quot;create table if not exists&quot;
hive -v -e &quot;$hql;&quot; 2&amp;gt;&amp;amp;1 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面 bash 代码逻辑如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、判断是否输入参数，如果没有参数，则取昨天，意思是每天读取 mysql 数据库中昨天的数据。&lt;/li&gt;
  &lt;li&gt;2、定义 mysql 中 select 查询语句&lt;/li&gt;
  &lt;li&gt;3、定义 hive 中建表语句&lt;/li&gt;
  &lt;li&gt;4、读取 mysql 数据库连接信息，上例中为从 db_name.sql 中读取 &lt;code&gt;db_host&lt;/code&gt;、&lt;code&gt;db_user&lt;/code&gt;、&lt;code&gt;db_pass&lt;/code&gt;、&lt;code&gt;db_port&lt;/code&gt;、&lt;code&gt;db_name&lt;/code&gt; 五个变量&lt;/li&gt;
  &lt;li&gt;5、运行 mysql 命令导出指定 sql 查询的结果，并将结果中的 NULL 字段转换为 &lt;code&gt;\\N&lt;/code&gt;，因为 &lt;code&gt;\&lt;/code&gt; 在 bash 中是转义字符，故需要使用两个 &lt;code&gt;\&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;6、lzo 压缩文件并上传到 hdfs，并且创建 lzo 索引&lt;/li&gt;
  &lt;li&gt;7、最后删除本地文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于分区表来说，建表语句如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;use dw_srclog;
create external table if not exists test_p (
  id int,
  name int
)
partitioned by (key_ym int, key_ymd int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;
STORED AS INPUTFORMAT
  &#39;com.hadoop.mapred.DeprecatedLzoTextInputFormat&#39;
OUTPUTFORMAT
  &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;
LOCATION
  &#39;/logroot/test_p&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 mysql 导出文件并上传到 hdfs 命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#begin
chmod +x $srcdb.sql
. ./$srcdb.sql

file=&quot;${table}_$logday&quot;
sql_var=&quot; -r -quick --default-character-set=utf8  --skip-column&quot;

mysql $sql_var -h${db_host} -u${db_user} -p${db_pass} -P${db_port} -D${db_name} -e &quot;$sql&quot; | sed &quot;s/NULL/\\\\N/g&quot;&amp;gt; $file 2&amp;gt;&amp;amp;1

lzop -U $file
hadoop fs -mkdir -p /logroot/$table/key_ym=$logmonth/key_ymd=$logday
hadoop fs -ls /logroot/$table/key_ym=$logmonth/key_ymd=$logday/ |grep lzo|awk &#39;{print $8}&#39;|xargs -i hadoop fs -rm {} 2&amp;gt;&amp;amp;1
hadoop fs -moveFromLocal $file.lzo /logroot/$table/key_ym=$logmonth/key_ymd=$logday/
hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer  /logroot/$table/key_ym=$logmonth/key_ymd=$logday/$file.lzo 2&amp;gt;&amp;amp;1

hive -v -e &quot;$hql;ALTER TABLE $table ADD IF NOT EXISTS PARTITION(key_ym=$logmonth,key_ymd=$logday) location &#39;/logroot/$table/key_ym=$logmonth/key_ymd=$logday&#39; &quot; 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过上面的两个命令就可以实现将 mysql 中的数据导入到 hdfs 中。&lt;/p&gt;

&lt;p&gt;这里需要注意以下几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、 hive 中原始日志使用默认的 textfile 方式存储，是为了保证日志的可读性，方便以后从 hdfs 下载来之后能够很方便的转换为结构化的文本文件并能浏览文件内容。&lt;/li&gt;
  &lt;li&gt;2、使用 lzo 压缩是为了节省存储空间&lt;/li&gt;
  &lt;li&gt;3、使用外包表建表，在删除表结构之后数据不会删，方便修改表结构和分区。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sqoop&quot;&gt;使用 Sqoop&lt;/h2&gt;

&lt;p&gt;使用 sqoop 主要是用于从 oracle 中通过 jdbc 方式导出数据到 hdfs，sqoop 命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sqoop import --connect jdbc:oracle:thin:@192.168.56.121:2154:db --username bi_user_limit --password &#39;XXXX&#39; --query &quot;select * from test where  \$CONDITIONS&quot; --split-by id  -m 5 --fields-terminated-by &#39;\t&#39; --lines-terminated-by &#39;\n&#39;  --null-string &#39;\\N&#39; --null-non-string &#39;\\N&#39; --target-dir &quot;/logroot/test/key_ymd=20140315&quot;  --delete-target-dir
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 数据加工&lt;/h1&gt;

&lt;p&gt;对于数据量比较小任务可以使用 impala 处理，对于数据量大的任务使用 hive hql 来处理。&lt;/p&gt;

&lt;p&gt;impala 处理数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;impala-shell -i &#39;192.168.56.121:21000&#39; -r -q &quot;$sql;&quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有时候需要使用 impala 导出数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;impala-shell -i &#39;192.168.56.121:21000&#39; -r -q &quot;$sql;&quot; -B --output_delimiter=&quot;\t&quot; -o $file
sed -i &#39;1d&#39; $file  #导出的第一行有不可见的字符
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 hive 处理数据生成结果表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

if [ &quot;$1&quot; ]; then 
  DAY=&quot;$1&quot;
else 
  DAY=&quot;yesterday&quot;
fi

echo &quot;DAY=$DAY&quot;

datestr=`date +%Y-%m-%d -d&quot;$DAY&quot;`;
logday=`date +%Y%m%d -d&quot;$DAY&quot;`;
logmonth=`date +%Y%m -d&quot;$DAY&quot;`

#target table
table=stat_test_p
sql=&quot;use dw_srclog;insert OVERWRITE table stat_test_p partition(key_ym=$logmonth,key_ymd=$logday)
select id,count(name) from test_p where key_ymd=$logday group by id
&quot;

hql=&quot;
use dw_web;
create external table if not exists goods_sales_info_day (
  id int,
  count int
) partitioned by (key_ym int, key_ymd int)
STORED AS RCFILE
LOCATION &#39;/logroot/stat_test_p&#39;;
&quot;
#begin
hive -v -e &quot;
$hql;
SET hive.exec.compress.output=true;
SET mapreduce.input.fileinputformat.split.maxsize=128000000;
SET mapred.output.compression.type=BLOCK;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
$sql&quot; 2&amp;gt;&amp;amp;1 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里主要是先判断是否创建外包表（外包表存储为 RCFILE 格式），然后设置 map 的输出结果使用 snappy 压缩，并设置每个 map 的大小，最后运行 insert 语句。结果表存储为 RCFILE 的原因是，&lt;em&gt;在 CDH 5.2 之前，该格式的表可以被 impala 读取&lt;/em&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;任务调度&lt;/h2&gt;

&lt;p&gt;当任务多了之后，每个任务之间会有一些依赖，为了保证任务的先后执行顺序，这里使用的是 azkaban 任务调度框架。&lt;/p&gt;

&lt;p&gt;该框架的使用方式很简单：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;首先创建一个 bi_etl 目录，用于存放执行脚本。&lt;/li&gt;
  &lt;li&gt;在 bi_etl 目录下创建一个 properties 文件，文件名称任意，文件内容为：&lt;code&gt;DAY=yesterday&lt;/code&gt;，这是一个系统默认参数，即默认 DAY 变量的值为 yesterday，该变量在运行时可以被覆盖：在 azkaban 的 web 管理界面，运行一个 Flow 时，添加一个 &lt;code&gt;Flow Parameters&lt;/code&gt; 参数，Name 为 DAY，Value 为你想要指定的值，例如：20141023。&lt;/li&gt;
  &lt;li&gt;创建一个 bash 脚本 test.sh，文件内容如第一章节内容，需要注意的是该脚本中会判断是否有输出参数。&lt;/li&gt;
  &lt;li&gt;针对 bash 脚本，创建 azkaban 需要的 job 文件，文件内容如下（azkaban 运行该 job 时候，会替换 &lt;code&gt;${DAY}&lt;/code&gt; 变量为实际的值 ）：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;type=command
command=sh test.sh ${DAY}
failure.emails=XXX@163.com
dependencies=xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;最后，将 bi_etl 目录打包成 zip 文件，然后上传到 azkaban 管理界面上去，就可以运行或者是设置调度任务了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用上面的方式编写 bash 脚本和 azkaban 的 job 的好处是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;azkaban 的 job 可以指定参数来控制运行哪一天的任务&lt;/li&gt;
  &lt;li&gt;job 中实际上运行的是 bash 脚本，这些脚本脱离了 azkaban 也能正常运行，同样也支持传参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;3. 数据展现&lt;/h1&gt;

&lt;p&gt;目前是将 hive 或者 impala 的处理结果推送到关系数据库中，由传统的 BI 报表工具展示数据或者直接通过 impala 查询数据生成报表并发送邮件。&lt;/p&gt;

&lt;p&gt;为了保证报表的正常发送，需要监控任务的正常运行，当任务失败的时候能够发送邮件，这部分通过 azkaban 可以做到。另外，还需要监控每天运行的任务同步的记录数，下面脚本是统计记录数为0的任务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

if [ &quot;$1&quot; ]; then
  DAY=&quot;$1&quot;
else
  DAY=&quot;yesterday&quot;
fi

echo &quot;DAY=$DAY&quot;

datestr=`date +%Y-%m-%d -d&quot;$DAY&quot;`;
logday=`date +%Y%m%d -d&quot;$DAY&quot;`;
logmonth=`date +%Y%m -d&quot;$DAY&quot;`
datemod=`date +%w -d &quot;yesterday&quot;`

rm -rf /tmp/stat_table_day_count_$logday
touch /tmp/stat_table_day_count_$logday
for db in `hadoop fs -ls /user/hive/warehouse|grep -vE &#39;testdb|dw_etl&#39;|grep &#39;.db&#39;|awk &#39;{print $8}&#39;|awk -F &#39;/&#39; &#39;{print $5}&#39; |awk -F &#39;.&#39; &#39;{print $1}&#39;`;do
    for table in `hive -S -e &quot;set hive.cli.print.header=false; use $db;show tables&quot; ` ;do
        count_new=&quot;&quot;
        result=`hive -S -e &quot;set hive.cli.print.header=false; use $db;show create table $table;&quot;  2&amp;gt;&amp;amp;1 | grep PARTITIONED`
        if [ ${#result} -gt 0 ];then
      is_part=1
      count_new=`impala-shell -k -i 10.168.35.127:21089 --quiet -B --output_delimiter=&quot;\t&quot; -q &quot;select count(1) from ${db}.$table where key_ymd=$logday &quot;`
        else
      is_part=0
      count_new=`impala-shell -k -i 10.168.35.127:21089 --quiet -B --output_delimiter=&quot;\t&quot; -q &quot;select count(1) from ${db}.$table; &quot;`
        fi
        echo &quot;$db,$table,$is_part,$count_new&quot; &amp;gt;&amp;gt; /tmp/stat_table_day_count_$logday
    done
done

#mail -s &quot;The count of the table between old and new cluster in $datestr&quot; -c $mails &amp;lt; /tmp/stat_table_day_count_$logday

sed -i &#39;s/1034h//g&#39; /tmp/stat_table_day_count_$logday
sed -i &#39;s/\[//g&#39; /tmp/stat_table_day_count_$logday
sed -i &#39;s/\?//g&#39; /tmp/stat_table_day_count_$logday
sed -i &#39;s/\x1B//g&#39; /tmp/stat_table_day_count_$logday

res=`cat /tmp/stat_table_day_count_$logday|grep -E &#39;1,0|0,0&#39;|grep -v stat_table_day_count`

echo $res

hive -e &quot;use dw_default;
LOAD DATA LOCAL INPATH &#39;/tmp/stat_table_day_count_$logday&#39; overwrite INTO TABLE stat_table_day_count  PARTITION (key_ym=$logmonth,key_ymd=$logday)
&quot;
python mail.py &quot;Count is 0 in $datestr&quot; &quot;$res&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;4. 总结&lt;/h1&gt;

&lt;p&gt;上面介绍了数据采集、加工和任务调度的过程，有些地方还可以改进：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;引入 ETL 工具实现关系数据库导入到 hadoop，例如：Kettle 工具&lt;/li&gt;
  &lt;li&gt;目前是每天一次从 mysql 同步数据到 hadoop，以后需要修改同步频率，做到更实时&lt;/li&gt;
  &lt;li&gt;hive 和 impala 在字段类型、存储方式、函数的兼容性上存在一些问题&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/10/23/hive-warehouse-in-2014.html</link>
      <guid>http://blog.javachen.com/2014/10/23/hive-warehouse-in-2014.html</guid>
      <pubDate>2014-10-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>CDH 5.2.0 的改变</title>
      <description>&lt;p&gt;最近 CDH 5.2.0 发布了，想看看其做了哪些改进、带来哪些不兼容以及是否有必要升级现有的 hadoop 集群。&lt;/p&gt;

&lt;h1 id=&quot;cdh-520-&quot;&gt;1. CDH 5.2.0 新特性&lt;/h1&gt;

&lt;h2 id=&quot;apache-avro&quot;&gt;1.1. Apache Avro&lt;/h2&gt;

&lt;p&gt;Avro 版本使用1.7.6，重要的一些改变：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-1398&quot;&gt;AVRO-1398&lt;/a&gt;。增加同步间隔，从16k 调整到64k，该参数可以在 mapreduce 的配置参数中通过 &lt;code&gt;avro.mapred.sync.interval&lt;/code&gt; 参数来设置&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-1355&quot;&gt;AVRO-1355&lt;/a&gt;。schema 中不能包括相同的 field 名称。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-hadoop&quot;&gt;1.2. Apache Hadoop&lt;/h2&gt;

&lt;h3 id=&quot;hdfs&quot;&gt;HDFS&lt;/h3&gt;

&lt;p&gt;提供新的功能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;HDFS Data at Rest Encryption。hdfs 数据的加密，该功能在5.2.0中还有一些限制，尚不能用于生产环境。&lt;/li&gt;
  &lt;li&gt;每一个 HDFS 文件上添加了一个新的属性：XAttrs，详见：&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2006&quot;&gt;https://issues.apache.org/jira/browse/HDFS-2006&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;使用 HTTP proxy server时的Authentication改进&lt;/li&gt;
  &lt;li&gt;增加了一个新的 Metrics sink，允许直接将监控数据写到 Graphite&lt;/li&gt;
  &lt;li&gt;Specification for Hadoop Compatible Filesystem effort&lt;/li&gt;
  &lt;li&gt;增加 OfflineImageViewer 通过 WebHDFS API 浏览 fsimage&lt;/li&gt;
  &lt;li&gt;对 NFS 支持的改进&lt;/li&gt;
  &lt;li&gt;hdfs daemons 的 web ui 改进&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mapreduce&quot;&gt;MapReduce&lt;/h3&gt;

&lt;p&gt;CDH 5.2 提供了一个 mapper 端 shuffle 的优化实现，使用该实现需要修改原来的实现类，默认未开启该实现。&lt;/p&gt;

&lt;p&gt;可以修改 ­&lt;code&gt;mapreduce.job.map.output.collector.class&lt;/code&gt; 参数为 &lt;code&gt;org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator&lt;/code&gt;来开启该特性。&lt;/p&gt;

&lt;p&gt;使用了自定义的可写的类型或者比较器时，无法使用该特性。&lt;/p&gt;

&lt;h3 id=&quot;yarn&quot;&gt;YARN&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fair Scheduler 新特性：&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;允许为每个队列设置 &lt;code&gt;fairsharePreemptionThreshold&lt;/code&gt; 属性，该值在 fair-scheduler.xml 中设置，默认值为0.5&lt;/li&gt;
  &lt;li&gt;允许为每个队列设置 &lt;code&gt;fairsharePreemptionTimeout&lt;/code&gt; 属性，该值在 fair-scheduler.xml 中设置&lt;/li&gt;
  &lt;li&gt;在 web ui 中可以显示 Steady Fair Share&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fair Scheduler 改进：&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Fair Scheduler uses Instantaneous Fair Share (fairshare that considers only active queues) for scheduling decisions to improve the time to achieve steady state (fairshare).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;maxAMShare 默认值设为0.5，意思是只有一半的集群资源可以被  Application Master 使用。该参数可以在 fair-scheduler.xml 中设置。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;YARN 的 rest api 支持提交和杀掉 application 。&lt;/li&gt;
  &lt;li&gt;YARN 的 timeline store 和 Kerberos 集成&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-crunch&quot;&gt;1.3 Apache Crunch&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://crunch.apache.org/scrunch.html&quot;&gt;Scrunch&lt;/a&gt; 改进：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;新的 join API&lt;/li&gt;
  &lt;li&gt;新的 aggregation API，支持  &lt;a href=&quot;https://github.com/twitter/algebird&quot;&gt;Algebird&lt;/a&gt;-based aggregation&lt;/li&gt;
  &lt;li&gt;增加新的模块 crunch-hive，用于使用 Crunch 读写 ORC 文件。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-flume&quot;&gt;1.4 Apache Flume&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;集成 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/cloudera-kafka/latest/PDF/cloudera-kafka.pdf&quot;&gt;Kafka&lt;/a&gt;，添加 KafkaSource 和 KafkaSink。&lt;/li&gt;
  &lt;li&gt;Kite Sink 可以写数据到 hive 和 hbase。&lt;/li&gt;
  &lt;li&gt;Flume agent 可以通过 zookeeper 配置（试验中）。&lt;/li&gt;
  &lt;li&gt;嵌入式的 agent 支持拦截器。&lt;/li&gt;
  &lt;li&gt;syslog source 支持配置那个字段可以保留。&lt;/li&gt;
  &lt;li&gt;File Channel replay 速度变快&lt;/li&gt;
  &lt;li&gt;添加新的正则表达式查询替换拦截器&lt;/li&gt;
  &lt;li&gt;Backup checkpoint 可以可选的被压缩。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hue&quot;&gt;1.5 Hue&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;添加新的应用修改数据和表上的 Sentry 的角色和权限&lt;/li&gt;
  &lt;li&gt;arch App&lt;/li&gt;
  &lt;li&gt;添加 Heatmap, Tree, Leaflet 组件&lt;/li&gt;
  &lt;li&gt;Micro-analysis of fields&lt;/li&gt;
  &lt;li&gt;Exclusion facets&lt;/li&gt;
  &lt;li&gt;Oozie Dashboard: bulk actions, faster display&lt;/li&gt;
  &lt;li&gt;File Browser: drag-and-drop upload, history, ACLs edition&lt;/li&gt;
  &lt;li&gt;Hive and Impala: LDAP pass-through, query expiration, SSL (Hive), new graphs&lt;/li&gt;
  &lt;li&gt;Job Browser: YARN kill application button&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-hbase&quot;&gt;1.6 Apache HBase&lt;/h2&gt;

&lt;p&gt;HBase 版本升级到 0.98.6&lt;/p&gt;

&lt;h2 id=&quot;apache-hive&quot;&gt;1.7 Apache Hive&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;hive 版本升级到 0.13，增加如下特性：&lt;/li&gt;
  &lt;li&gt;where 语句支持子查询&lt;/li&gt;
  &lt;li&gt;Common table expressions&lt;/li&gt;
  &lt;li&gt;Parquet 支持 timestamp&lt;/li&gt;
  &lt;li&gt;HiveServer2 可以配置 hiverc 文件，当连接的时候，自动执行该文件内容&lt;/li&gt;
  &lt;li&gt;Permanent UDFs&lt;/li&gt;
  &lt;li&gt;HiveServer2 添加 session 和操作超时&lt;/li&gt;
  &lt;li&gt;Beeline 接受一个 &lt;code&gt;-i&lt;/code&gt; 参数执行初始化的 sql 文件&lt;/li&gt;
  &lt;li&gt;新的 join 语法(implicit joins)&lt;/li&gt;
  &lt;li&gt;建表语句支持 AVRO 存储格式&lt;/li&gt;
  &lt;li&gt;Hive 支持额外的数据类型：&lt;/li&gt;
  &lt;li&gt;hive 可以读 hive 和 impala 创建的 char 和 varchar 数据类型&lt;/li&gt;
  &lt;li&gt;Impala 可以读 hive 和 impala 创建的 char 和 varchar 数据类型&lt;/li&gt;
  &lt;li&gt;DESCRIBE DATABASE  命令添加两个新属性：owner_name 和 owner_type。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;impala&quot;&gt;1.7 Impala&lt;/h2&gt;

&lt;p&gt;impala 版本升级到 2.0，改进包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;添加 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/impala_scalability.html#spill_to_disk_unique_1&quot;&gt;spill to disk&lt;/a&gt; 支持，当内存不够时自动转换到磁盘上进行处理。&lt;/li&gt;
  &lt;li&gt;子查询改进：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;WHERE&lt;/code&gt; 语句中支持子查询，可以用于 &lt;code&gt;in&lt;/code&gt; 查询&lt;/li&gt;
  &lt;li&gt;支持 &lt;code&gt;EXISTS&lt;/code&gt; 和  &lt;code&gt;NOT EXISTS&lt;/code&gt; 操作&lt;/li&gt;
  &lt;li&gt;子查询中可以使用 &lt;code&gt;IN&lt;/code&gt; 和 &lt;code&gt;NOT IN&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;where 语句可以使用如下语句： &lt;code&gt;WHERE column = (SELECT MAX(some_other_column FROM table)&lt;/code&gt; 或者 &lt;code&gt;WHERE column IN (SELECT some_other_column FROM table WHERE conditions)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Correlated subqueries let you cross-reference values from the outer query block and the subquery.&lt;/li&gt;
  &lt;li&gt;Scalar subqueries let you substitute the result of single-value aggregate functions such as MAX(), MIN(), COUNT(), or AVG(), where you would normally use a numeric value in a WHERE clause.&lt;/li&gt;
  &lt;li&gt;添加几个聚合函数： &lt;code&gt;RANK()&lt;/code&gt;, &lt;code&gt;LAG()&lt;/code&gt;, &lt;code&gt;LEAD(&lt;/code&gt;), &lt;code&gt;FIRST_VALUE()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;添加新的数据类型：&lt;/li&gt;
  &lt;li&gt;VARCHAR&lt;/li&gt;
  &lt;li&gt;char&lt;/li&gt;
  &lt;li&gt;Security方面的改进：&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/impala_mixed_security.html#mixed_security&quot;&gt;Using Multiple Authentication Methods with Impala&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;GRANT&lt;/li&gt;
  &lt;li&gt;REVOKE&lt;/li&gt;
  &lt;li&gt;CREATE ROLE&lt;/li&gt;
  &lt;li&gt;DROP ROLE&lt;/li&gt;
  &lt;li&gt;SHOW ROLES&lt;/li&gt;
  &lt;li&gt;–disk_spill_encryption&lt;/li&gt;
  &lt;li&gt;Impala 可以读取 gzip, bzip, 或 Snappy 的压缩数据&lt;/li&gt;
  &lt;li&gt;Query hints can now use comment notation, /* +hint_name */ or – +hint_name，更详细说明见：&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/impala_hints.html#hints&quot;&gt;Hints&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;QUERY_TIMEOUT_S&lt;/code&gt; 用于设置查询超时时间。&lt;/li&gt;
  &lt;li&gt;添加 &lt;code&gt;VAR_SAMP()&lt;/code&gt; 和 &lt;code&gt;VAR_POP()&lt;/code&gt;，分别为 &lt;code&gt;VARIANCE_SAMP()&lt;/code&gt; 和 &lt;code&gt;VARIANCE_POP()&lt;/code&gt; 别名&lt;/li&gt;
  &lt;li&gt;添加新的日期和时间类型函数：DATE_PART()&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/impala_appx_count_distinct.html#appx_count_distinct&quot;&gt;APPX_COUNT_DISTINCT&lt;/a&gt;，如果设置该参数，impala 会用 &lt;code&gt;NDV()&lt;/code&gt; 函数代替 &lt;code&gt;COUNT(DISTINCT)&lt;/code&gt;，加快查询速度并支持多个 &lt;code&gt;COUNT(DISTINCT)&lt;/code&gt; 操作&lt;/li&gt;
  &lt;li&gt;添加 &lt;code&gt;DECODE()&lt;/code&gt; 函数，是 &lt;code&gt;CASE()&lt;/code&gt; 函数的一种简写方式，详细参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/impala_conditional_functions.html#conditional_functions&quot;&gt;Impala Conditional Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;STDDEV(), STDDEV_POP(), STDDEV_SAMP(), VARIANCE(), VARIANCE_POP(), VARIANCE_SAMP(), NDV() 返回 double 类型&lt;/li&gt;
  &lt;li&gt;Parquet 块大小默认值由 1G 改为256M，也可以通过 &lt;code&gt;PARQUET_FILE_SIZE&lt;/code&gt; 参数设置&lt;/li&gt;
  &lt;li&gt;支持 Anti-joins，可以使用 &lt;code&gt;LEFT ANTI JOIN&lt;/code&gt; 和 &lt;code&gt;RIGHT ANTI JOIN&lt;/code&gt; 语句&lt;/li&gt;
  &lt;li&gt;impala-shell 中可以执行 set 语句，可以设置 &lt;code&gt;PARQUET_FILE_SIZE&lt;/code&gt;、&lt;code&gt;MEM_LIMIT&lt;/code&gt;和 &lt;code&gt;SYNC_DDL&lt;/code&gt; 等参数，详细说明见：&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/impala_set.html#set&quot;&gt;SET Statement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;impala-shell 可以读取 &lt;code&gt;$HOME/.impalarc&lt;/code&gt; 中的配置，详细说明见：&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/impala_shell_options.html#shell_options&quot;&gt;impala-shell Configuration Options&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kite&quot;&gt;1.8 Kite&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/guide/&quot;&gt;Kite&lt;/a&gt; is an open source set of libraries, references, tutorials, and code samples for building data-oriented systems and applications.&lt;/p&gt;

&lt;h2 id=&quot;apache-parquet-incubating&quot;&gt;1.9 Apache Parquet (incubating)&lt;/h2&gt;

&lt;p&gt;版本更新： 5.2 Parquet is rebased on Parquet 1.5 and Parquet-format 2.1.0.&lt;/p&gt;

&lt;h2 id=&quot;apache-spark&quot;&gt;1.10 Apache Spark&lt;/h2&gt;

&lt;p&gt;Apache Spark/Streaming 版本使用 1.1&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;稳定性和性能改进&lt;/li&gt;
  &lt;li&gt;新的 sort-based shuffle 实现，默认未开启。&lt;/li&gt;
  &lt;li&gt;Spark UI 更好的监控性能改进&lt;/li&gt;
  &lt;li&gt;PySpark 支持 Hadoop InputFormats&lt;/li&gt;
  &lt;li&gt;改进 Yarn 的支持，并修复一些 bug&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-sqoop&quot;&gt;1.11 Apache Sqoop&lt;/h2&gt;

&lt;p&gt;CDH 5.2 Sqoop 1 is rebased on Sqoop 1.4.5&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mainframe connector added.&lt;/li&gt;
  &lt;li&gt;Parquet support added.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;2.0 不兼容改变&lt;/h1&gt;

&lt;h2 id=&quot;hdfs-1&quot;&gt;2.1 HDFS&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;当没有快照目录时，getSnapshottableDirListing() 方法返回 null&lt;/li&gt;
  &lt;li&gt;NameNode ` -finalize&lt;code&gt; 启动参数被删除，为了完成集群的升级，应该使用 &lt;/code&gt;hdfs dfsadmin -finalizeUpgrade` 命令&lt;/li&gt;
  &lt;li&gt;libhdfs 函数返回正确的错误码&lt;/li&gt;
  &lt;li&gt;HDFS balancer 命令运行错误时候返回0，运行成功返回1&lt;/li&gt;
  &lt;li&gt;Disable symlinks temporarily&lt;/li&gt;
  &lt;li&gt;Files named &lt;code&gt;.snapshot&lt;/code&gt; or &lt;code&gt;.reserved&lt;/code&gt; must not exist within HDFS.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Change in High-Availability Support&lt;/em&gt;：&lt;/p&gt;

&lt;p&gt;CDH5 中唯一的 HA 实现是基于 Quorum-based storage，使用 NFS 的共享存储不再支持。&lt;/p&gt;

&lt;h2 id=&quot;mapreduce-1&quot;&gt;2.2 MapReduce&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CATALINA_BASE 变量不再用于决定一个组件是否配置为 YARN 或者 MRv1&lt;/li&gt;
  &lt;li&gt;YARN Fair Scheduler ACL change. Root queue defaults to everybody, and other queues default to nobody.&lt;/li&gt;
  &lt;li&gt;YARN 高可用配置参数修改了 key 名称&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;YARN_HOME&lt;/code&gt; 改为 &lt;code&gt;HADOOP_YARN_HOME&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;yarn-site.xml 中的以下参数改名：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.shuffle&lt;/code&gt; 改为 &lt;code&gt;mapreduce_shuffle&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/code&gt; 改为 &lt;code&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.resourcemanager.resourcemanager.connect.max.wait.secs&lt;/code&gt; 改为 &lt;code&gt;yarn.resourcemanager.connect.max-wait.secs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.resourcemanager.resourcemanager.connect.retry_interval.secs&lt;/code&gt; 改名为 &lt;code&gt;yarn.resourcemanager.connect.retry-interval.secs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.resourcemanager.am.max-retries&lt;/code&gt; 改名为 &lt;code&gt;yarn.resourcemanager.am.max-attempts&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hbase&quot;&gt;2.3 HBase&lt;/h2&gt;

&lt;p&gt;HBase 版本变化太大，这里不做说明。&lt;/p&gt;

&lt;h2 id=&quot;hive&quot;&gt;2.4 Hive&lt;/h2&gt;

&lt;p&gt;CDH 5 提供一个新的离线命令用于升级元数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;schemaTool -d &amp;lt;dbType&amp;gt; -upgradeSchema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CDH 4.x 和 CDH 5 中不兼容的地方：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CDH 4 JDBC 客户端和 CDH5 HiveServer2 不兼容&lt;/li&gt;
  &lt;li&gt;连接 HiveServer2 需要 CDH5 的 jar 包&lt;/li&gt;
  &lt;li&gt;因为权限和并发问题，hive 命令行和 hiveserver1 将删除不再使用，建议使用 HiveServer2 和 Beeline&lt;/li&gt;
  &lt;li&gt;CDH 5 Hue 不能用于 CDH 4 的HiveServer2&lt;/li&gt;
  &lt;li&gt;删除 npath 函数&lt;/li&gt;
  &lt;li&gt;Cloudera recommends that custom ObjectInspectors created for use with custom SerDes have a no-argument constructor in addition to their normal constructors, for serialization purposes. See HIVE-5380 for more details.&lt;/li&gt;
  &lt;li&gt;The SerDe interface has been changed which requires the custom SerDe modules to be reworked.&lt;/li&gt;
  &lt;li&gt;The decimal data type format has changed as of CDH 5 Beta 2 and is not compatible with CDH 4.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CDH 5 和 CDH 5.2.x 中不兼容的地方：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The CDH 5.2 Hive JDBC driver is not wire-compatible with the CDH 5.1&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-spark-1&quot;&gt;2.4 Apache Spark&lt;/h2&gt;

&lt;h1 id=&quot;section-1&quot;&gt;3. 性能改进&lt;/h1&gt;

&lt;p&gt;1、Disabling Transparent Hugepage Compaction&lt;/p&gt;

&lt;p&gt;查看是否开启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /sys/kernel/mm/redhat_transparent_hugepage/defrag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关闭该特性，并将其加入到 /etc/rc.local&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo never &amp;gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、设置 swap 交换&lt;/p&gt;

&lt;p&gt;查看是否开启：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /proc/sys/vm/swappiness
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On most systems, it is set to 60 by default. This is not suitable for Hadoop clusters nodes, because it can cause processes to get swapped out even when there is free memory available. This can affect stability and performance, and may cause problems such as lengthy garbage collection pauses for important system daemons.&lt;/p&gt;

&lt;p&gt;建议修改为0：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysctl -w vm.swappiness=0 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Improving Performance in Shuffle Handler and IFile Reader&lt;/p&gt;

&lt;p&gt;Shuffle Handler，开启预先读取数据：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于 YARN，设置 &lt;code&gt;mapreduce.shuffle.readahead.bytes&lt;/code&gt;，默认值为4MB&lt;/li&gt;
  &lt;li&gt;对于 MRv1，设置 &lt;code&gt;mapred.tasktracker.shuffle.readahead.bytes&lt;/code&gt;，默认值为4MB&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;IFile Reader，开启预先读取IFile文件可以改进合并文件性能，开启该特性，请设置 &lt;code&gt;mapreduce.ifile.readahead property&lt;/code&gt; 为 true，默认值为 true，更进一步，可以设置 &lt;code&gt;mapreduce.ifile.readahead.bytes&lt;/code&gt; 参数值，该值默认为4MB&lt;/p&gt;

&lt;p&gt;4、MapReduce配置最佳实践&lt;/p&gt;

&lt;p&gt;设置 &lt;code&gt;mapreduce.tasktracker.outofband.heartbeat&lt;/code&gt; 为 true，该值默认为 false&lt;/p&gt;

&lt;p&gt;在一个小集群中，设置 JobTracker heartbeat 间隔到一个更小的值，参数为 &lt;code&gt;apreduce.jobtracker.heartbeat.interval.min&lt;/code&gt; ，默认值为10&lt;/p&gt;

&lt;p&gt;5、立即启动 MapReduce 的 JVM&lt;/p&gt;

&lt;p&gt;对于小任务，设置 &lt;code&gt;mapred.reduce.slowstart.completed.maps&lt;/code&gt; 值为0，对于比较大的任务，最大设置为 50%&lt;/p&gt;

&lt;p&gt;6、调整 MRv1 日志级别&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.map.log.level&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.reduce.log.level&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;4. 存在的问题&lt;/h1&gt;

&lt;h2 id=&quot;apache-flume-1&quot;&gt;4.1 Apache Flume&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Flume does not provide a native sink that stores the data that can be directly consumed by Hive.&lt;/li&gt;
  &lt;li&gt;Fast Replay does not work with encrypted File Channel&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-hadoop-1&quot;&gt;4.2 Apache Hadoop&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DistCp between unencrypted and encrypted locations fails&lt;/li&gt;
  &lt;li&gt;NameNode - KMS communication fails after long periods of inactivity&lt;/li&gt;
  &lt;li&gt;Spark fails when the KMS is configured to use SSL&lt;/li&gt;
  &lt;li&gt;Files inside encryption zones cannot be read in Hue&lt;/li&gt;
  &lt;li&gt;Cannot move encrypted files to trash&lt;/li&gt;
  &lt;li&gt;No error when changing permission to 777 on .snapshot directory&lt;/li&gt;
  &lt;li&gt;Snapshots do not retain directories’ quotas settings&lt;/li&gt;
  &lt;li&gt;NameNode cannot use wildcard address in a secure cluster&lt;/li&gt;
  &lt;li&gt;Permissions for dfs.namenode.name.dir incorrectly set.&lt;/li&gt;
  &lt;li&gt;hadoop fsck -move does not work in a cluster with host-based Kerberos&lt;/li&gt;
  &lt;li&gt;HttpFS cannot get delegation token without prior authenticated request.&lt;/li&gt;
  &lt;li&gt;DistCp does not work between a secure cluster and an insecure cluster in some cases&lt;/li&gt;
  &lt;li&gt;Using DistCp with Hftp on a secure cluster using SPNEGO requires that the dfs.https.port property be configured&lt;/li&gt;
  &lt;li&gt;Offline Image Viewer (OIV) tool regression: missing Delimited outputs.&lt;/li&gt;
  &lt;li&gt;Snapshot operations are not supported by ViewFileSystem&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mapreduce-2&quot;&gt;4.3 MapReduce&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Starting an unmanaged ApplicationMaster may fail&lt;/li&gt;
  &lt;li&gt;No JobTracker becomes active if both JobTrackers are migrated to other hosts&lt;/li&gt;
  &lt;li&gt;Hadoop Pipes may not be usable in an MRv1 Hadoop installation done through tarballs&lt;/li&gt;
  &lt;li&gt;Task-completed percentage may be reported as slightly under 100% in the web UI, even when all of a job’s tasks have successfully completed.&lt;/li&gt;
  &lt;li&gt;Encrypted shuffle in MRv2 does not work if used with LinuxContainerExecutor and encrypted web UIs.&lt;/li&gt;
  &lt;li&gt;Link from ResourceManager to Application Master does not work when the Web UI over HTTPS feature is enabled.&lt;/li&gt;
  &lt;li&gt;Hadoop client JARs don’t provide all the classes needed for clean compilation of client code&lt;/li&gt;
  &lt;li&gt;The ulimits setting in /etc/security/limits.conf is applied to the wrong user if security is enabled.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-hive-1&quot;&gt;4.4 Apache Hive&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Hive’s Timestamp type cannot be stored in Parquet&lt;/li&gt;
  &lt;li&gt;Hive’s Decimal type cannot be stored in Parquet and Avro&lt;/li&gt;
  &lt;li&gt;Hive creates an invalid table if you specify more than one partition with alter table&lt;/li&gt;
  &lt;li&gt;PostgreSQL 9.0+ requires additional configuration，需要设置 &lt;code&gt;standard_conforming_strings&lt;/code&gt; 为 off&lt;/li&gt;
  &lt;li&gt;Setting hive.optimize.skewjoin to true causes long running queries to fail&lt;/li&gt;
  &lt;li&gt;JDBC - executeUpdate does not returns the number of rows modified&lt;/li&gt;
  &lt;li&gt;Hive Auth (Grant/Revoke/Show Grant) statements do not support fully qualified table names (default.tab1)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apache-parquet-incubating-1&quot;&gt;4.5 Apache Parquet (incubating)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Parquet file writes run out of memory if (number of partitions) times (block size) exceeds available memory&lt;/li&gt;
  &lt;li&gt;Hive cannot read arrays in Parquet written by parquet-avro or parquet-thrift&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;5. 总结&lt;/h1&gt;

&lt;p&gt;本篇文章主要是翻译了 cloudera 官网上关于 CDH5.2 的新特性、不兼容变化、性能改进以及可能存在的问题等相关文档，以便清楚的了解 hadoop 各组件的特性并为是否升级 hadoop 版本做出决策支持。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/10/20/cdh5.2-release.html</link>
      <guid>http://blog.javachen.com/2014/10/20/cdh5.2-release.html</guid>
      <pubDate>2014-10-20T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spring源码整体架构</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;前言&lt;/h1&gt;

&lt;p&gt;Spring 是一个开源框架，是为了解决企业应用程序开发复杂性而创建的。框架的主要优势之一就是其分层架构，分层架构允许您选择使用哪一个组件，同时为 J2EE 应用程序开发提供集成的框架。&lt;/p&gt;

&lt;p&gt;从这篇文章开始，我讲开始阅读并介绍 Spring 源码的设计思想，希望能改对 Spring 框架有一个初步的全面的认识，并且学习其架构设计方面的一些理念和方法。&lt;/p&gt;

&lt;p&gt;Spring 源码地址：&lt;a href=&quot;https://github.com/spring-projects/spring-framework&quot;&gt;https://github.com/spring-projects/spring-framework&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;概述&lt;/h1&gt;

&lt;h2 id=&quot;spring&quot;&gt;Spring的整体架构&lt;/h2&gt;

&lt;p&gt;Spring 总共有十几个组件，其中核心组件只有三个：Core、Context 和 Beans。以下是 Spring3的总体架构图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/spring/spring3-modules.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;组成 Spring 框架的每个模块（或组件）都可以单独存在，或者与其他一个或多个模块联合实现。每个模块的功能如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;核心容器：核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转 （IOC） 模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。&lt;/li&gt;
  &lt;li&gt;Spring 上下文：Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如：JNDI、EJB、电子邮件、国际化、校验和调度功能。&lt;/li&gt;
  &lt;li&gt;Spring AOP：通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理的任何对象支持 AOP。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。&lt;/li&gt;
  &lt;li&gt;Spring DAO：JDBC DAO 抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理和不同数据库供应商抛出的错误消息。异常层次结构简化了错误处理，并且极大地降低了需要编写的异常代码数量（例如打开和关闭连接）。Spring DAO 的面向 JDBC 的异常遵从通用的 DAO 异常层次结构。&lt;/li&gt;
  &lt;li&gt;Spring ORM：Spring 框架插入了若干个 ORM 框架，从而提供了 ORM 的对象关系工具，其中包括 JDO、Hibernate 和 iBatis SQL Map。所有这些都遵从 Spring 的通用事务和 DAO 异常层次结构。&lt;/li&gt;
  &lt;li&gt;Spring Web 模块：Web 上下文模块建立在应用程序上下文模块之上，为基于 Web 的应用程序提供了上下文。所以，Spring 框架支持与 Jakarta Struts 的集成。Web 模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。&lt;/li&gt;
  &lt;li&gt;Spring MVC 框架：MVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架变成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从下图（该图来自&lt;a href=&quot;http://www.javastar.org/?p=847&quot;&gt;SPRING 3.2.X 源代码分析之二: SPRING源码的包结构&lt;/a&gt;）可以看出 Spring 各个模块之间的依赖关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/spring/spring-packages.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看出，IOC 的实现包 spring-beans 和 AOP 的实现包 spring-aop 也是整个框架的基础，而 spring-core 是整个框架的核心，基础的功能都在这里。&lt;/p&gt;

&lt;p&gt;在此基础之上，spring-context 提供上下文环境，为各个模块提供粘合作用。&lt;/p&gt;

&lt;p&gt;在 spring-context 基础之上提供了 spring-tx 和 spring-orm包，而web部分的功能，都是要依赖spring-web来实现的。&lt;/p&gt;

&lt;h2 id=&quot;spring-&quot;&gt;Spring 的设计理念&lt;/h2&gt;

&lt;p&gt;Spring 是面向 Bean 的编程（BOP,Bean Oriented Programming），Bean 在 Spring 中才是真正的主角。Bean 在 Spring 中作用就像 Object 对 OOP 的意义一样，没有对象的概念就像没有面向对象编程，Spring 中没有 Bean 也就没有 Spring 存在的意义。Spring 提供了 IOC 容器通过配置文件或者注解的方式来管理对象之间的依赖关系。&lt;/p&gt;

&lt;p&gt;控制反转模式（也称作依赖性介入）的基本概念是：不创建对象，但是描述创建它们的方式。在代码中不直接与对象和服务连接，但在配置文件中描述哪一个组件需要哪一项服务。容器 （在 Spring 框架中是 IOC 容器） 负责将这些联系在一起。&lt;/p&gt;

&lt;p&gt;在典型的 IOC 场景中，容器创建了所有对象，并设置必要的属性将它们连接在一起，决定什么时间调用方法。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;面向方面的编程&lt;/h2&gt;

&lt;p&gt;面向方面的编程，即 AOP，是一种编程技术，它允许程序员对横切关注点或横切典型的职责分界线的行为（例如日志和事务管理）进行模块化。AOP 的核心构造是方面，它将那些影响多个类的行为封装到可重用的模块中。&lt;/p&gt;

&lt;p&gt;AOP 和 IOC 是补充性的技术，它们都运用模块化方式解决企业应用程序开发中的复杂问题。在典型的面向对象开发方式中，可能要将日志记录语句放在所有方法和 Java 类中才能实现日志功能。在 AOP 方式中，可以反过来将日志服务模块化，并以声明的方式将它们应用到需要日志的组件上。当然，优势就是 Java 类不需要知道日志服务的存在，也不需要考虑相关的代码。所以，用 Spring AOP 编写的应用程序代码是松散耦合的。&lt;/p&gt;

&lt;p&gt;AOP 的功能完全集成到了 Spring 事务管理、日志和其他各种特性的上下文中。&lt;/p&gt;

&lt;h2 id=&quot;ioc-&quot;&gt;IOC 容器&lt;/h2&gt;

&lt;p&gt;Spring 设计的核心是 org.springframework.beans 包，它的设计目标是与 JavaBean 组件一起使用。这个包通常不是由用户直接使用，而是由服务器将其用作其他多数功能的底层中介。下一个最高级抽象是 BeanFactory 接口，它是工厂设计模式的实现，允许通过名称创建和检索对象。BeanFactory 也可以管理对象之间的关系。&lt;/p&gt;

&lt;p&gt;BeanFactory 支持两个对象模型。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;单例。 模型提供了具有特定名称的对象的共享实例，可以在查询时对其进行检索。Singleton 是默认的也是最常用的对象模型。对于无状态服务对象很理想。&lt;/li&gt;
  &lt;li&gt;原型。 模型确保每次检索都会创建单独的对象。在每个用户都需要自己的对象时，原型模型最适合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;bean 工厂的概念是 Spring 作为 IOC 容器的基础。IOC 将处理事情的责任从应用程序代码转移到框架。正如我将在下一个示例中演示的那样，Spring 框架使用 JavaBean 属性和配置数据来指出必须设置的依赖关系&lt;/p&gt;

&lt;h2 id=&quot;spring4-&quot;&gt;Spring4 的系统架构图&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/spring/spring4-modules.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spring 4.0.x对比Spring3.2.x的系统架构变化（以下文字摘抄于&lt;a href=&quot;http://www.javastar.org/?p=872&quot;&gt;SPRING 3.2.X 源代码分析之三: SPRING源码的整体架构分析&lt;/a&gt;）:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;从图中可以看出，总体的层次结构没有太大变化，变化的是 Spring 4.0.3去掉了 struts 模块(spring-struts包)。现在的 Spring mvc的确已经足够优秀了，大量的 web 应用均已经使用了 Spring mvc。而 struts1.x 的架构太落后了，struts2.x 是 struts 自身提供了和 Spring 的集成包，但是由于之前版本的 struts2 存在很多致命的安全漏洞，所以，大大影响了其使用度，好在最新的2.3.16版本的 struts 安全有所改善，希望不会再出什么大乱子。&lt;/p&gt;

  &lt;p&gt;web 部分去掉了 struts 模块，但是增加 WebSocket 模块(spring-websocket包)，增加了对 WebSocket、SockJS 以及 STOMP 的支持，它与 JSR-356 Java WebSocket API 兼容。另外，还提供了基于 SockJS（对 WebSocket 的模拟）的回调方案，以适应不支持 WebSocket 协议的浏览器。&lt;/p&gt;

  &lt;p&gt;同时，增加了 messaging 模块(spring-messaging)，提供了对 STOMP 的支持，以及用于路由和处理来自 WebSocket 客户端的 STOMP 消息的注解编程模型。spring-messaging 模块中还 包含了 Spring Integration 项目中的核心抽象类，如 Message、MessageChannel、MessageHandler。&lt;/p&gt;

  &lt;p&gt;如果去看源代码的话，还可以发现还有一个新增的包，加强了 beans 模块，就是 spring-beans-groovy。应用可以部分或完全使用 Groovy 编写。借助于 Spring 4.0，能够使用 Groovy DSL 定义外部的 Bean 配置，这类似于 XML Bean 声明，但是语法更为简洁。使用Groovy还能够在启动代码中直接嵌入Bean的声明。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;还有一些：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;删除过时的包和方法。具体API变动可以参考&lt;a href=&quot;http://docs.spring.io/spring-framework/docs/3.2.4.RELEASE_to_4.0.0.RELEASE/&quot;&gt;变动报告&lt;/a&gt;，第三方类库至少使用2010/2011年发布的版本，尤其是Hibernate 3.6+, EhCache 2.1+, Quartz 1.8+, Groovy 1.8+, and Joda-Time 2.0+。Hibernate Validator要求使用4.3+，Jackson 2.0+。&lt;/li&gt;
  &lt;li&gt;Java 8支持。当然也支持Java6和Java7，但最好在使用Spring框架3.X或4.X时，将JDK升级到Java7，因为有些版本至少需要Java7。&lt;/li&gt;
  &lt;li&gt;Java EE 6和7。使用Spring4.x时Java EE版本至少要6或以上，且需要JPA 2.0和Servlet 3.0 的支持，所以服务器，web容器需要做相应的升级。一个更具前瞻性的注意是，Spring4.0支持J2EE 7的适用级规范，比如JMS 2.0， JTA 1.2， JPA 2.1， Bean Validation 1.1和JSR-236并发工具包，在选择这些jar包时需要注意版本。&lt;/li&gt;
  &lt;li&gt;使用Groovy DSL定义外部Bean。&lt;/li&gt;
  &lt;li&gt;核心容器提升。&lt;/li&gt;
  &lt;li&gt;1、支持Bean的泛型注入，比如：&lt;code&gt;@Autowired Repository&amp;lt;Customer&amp;gt; customerRepository&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;2、使用元注解开发暴露指定内部属性的自定义注解。&lt;/li&gt;
  &lt;li&gt;3、通过 &lt;code&gt;@Ordered&lt;/code&gt; 注解或&lt;code&gt;Ordered&lt;/code&gt; 接口对注入集合或数组的 Bean 进行排序。&lt;/li&gt;
  &lt;li&gt;4、&lt;code&gt;@Lazy&lt;/code&gt; 注解可以用在注入点或 &lt;code&gt;@Bean&lt;/code&gt; 定义上。&lt;/li&gt;
  &lt;li&gt;5、为开发者引入 &lt;code&gt;@Description&lt;/code&gt; 注解。&lt;/li&gt;
  &lt;li&gt;6、引入 &lt;code&gt;@Conditional&lt;/code&gt; 注解进行有条件的 Bea n过滤。&lt;/li&gt;
  &lt;li&gt;7、基于 CGLIB 的代理类不需要提供默认构造器，因为 Spring 框架将 CGLIB 整合到内部了。&lt;/li&gt;
  &lt;li&gt;8、框架支持时区管理，比如 LocalContext。&lt;/li&gt;
  &lt;li&gt;Web提升。&lt;/li&gt;
  &lt;li&gt;1、增加新的 &lt;code&gt;@RestController&lt;/code&gt; 注解，这样就不需要在每个 &lt;code&gt;@RequestMapping&lt;/code&gt; 方法中添加 &lt;code&gt;@ResponseBody&lt;/code&gt; 注解。&lt;/li&gt;
  &lt;li&gt;2、添加 AsyncRestTemplate，在开发 REST 客户端时允许非阻塞异步支持。&lt;/li&gt;
  &lt;li&gt;3、为 Spring MVC 应用程序开发提供全面的时区支持。&lt;/li&gt;
  &lt;li&gt;WebSocket，SockJS 和 STOMP 消息。&lt;/li&gt;
  &lt;li&gt;测试提升。&lt;/li&gt;
  &lt;li&gt;1、spring-test 模块里的几乎所有注解都能被用做元注解去创建自定义注解，来减少跨测试集时的重复配置。&lt;/li&gt;
  &lt;li&gt;2、活跃的 bean 定义配置文件可以编程方式解析。&lt;/li&gt;
  &lt;li&gt;3、spring-core 模块里引入一个新的 SocketUtils 类，用于扫描本地可使用的 TCP 和 UDP 服务端口。一般用于测试需要 socket 的情况，比如测试开启内存 SMTP 服务，FTP 服务，Servlet 容器等。&lt;/li&gt;
  &lt;li&gt;4、由于 Spring4.0 的原因，org.springframework.mock.web 包现在基于 Servlet 3.0 API。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;阅读过程&lt;/h1&gt;

&lt;p&gt;因为 Spring 是分模块的，所以阅读 Spring 3.2.11 版本的源码过程打算先从最底层的模块开始，然后再由下向上分析每一个模块的实现过程。在阅读过程中，随着对代码的理解加深，也会重新阅读已经读过的代码。&lt;/p&gt;

&lt;p&gt;大概的阅读顺序：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;spring-core：了解 Spring 提供哪些工具类以及一些基础的功能，如对资源文件的封装、对 Propertis 文件操作的封装、对 Environment 的封装等等。&lt;/li&gt;
  &lt;li&gt;spring-context：通过分析 Spring 的启动方式，了解 Spring xml 文件的解析过程，bean 的初始化过程&lt;/li&gt;
  &lt;li&gt;spring-orm&lt;/li&gt;
  &lt;li&gt;spring-tx&lt;/li&gt;
  &lt;li&gt;spring-web&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：这里使用的Spring 版本是 3.2.11，下载地址在：&lt;a href=&quot;https://github.com/spring-projects/spring-framework/releases&quot;&gt;https://github.com/spring-projects/spring-framework/releases&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;搭建测试环境&lt;/h1&gt;

&lt;p&gt;Spring 源码编译过程，这里不做说明。&lt;/p&gt;

&lt;p&gt;为了测试简单，可以单独创建 java 项目，编写一些测试用例或者例子来测试 Spring 的功能。我使用的测试环境是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Idea 13&lt;/li&gt;
  &lt;li&gt;Maven 3.0.5&lt;/li&gt;
  &lt;li&gt;JDK1.6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;生成 maven 项目命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn archetype:generate -DgroupId=com.javachen.spring.core -DartifactId=Spring3Example 
	-DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转换成 idea 项目：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mvn idea:idea
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，根据使用情况添加 Spring 的依赖包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; 
	xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
	xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 
	http://maven.apache.org/maven-v4_0_0.xsd&quot;&amp;gt;
	&amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;
	&amp;lt;groupId&amp;gt;com.javachen.spring.core&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;Spring3Example&amp;lt;/artifactId&amp;gt;
	&amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;
	&amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt;
	&amp;lt;name&amp;gt;Spring3Example&amp;lt;/name&amp;gt;
	&amp;lt;url&amp;gt;http://maven.apache.org&amp;lt;/url&amp;gt;
 
	&amp;lt;properties&amp;gt;
		&amp;lt;spring.version&amp;gt;3.2.11.RELEASE&amp;lt;/spring.version&amp;gt;
	&amp;lt;/properties&amp;gt;
 
	&amp;lt;dependencies&amp;gt;
 
		&amp;lt;!-- Spring 3 dependencies --&amp;gt;
		&amp;lt;dependency&amp;gt;
			&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
			&amp;lt;artifactId&amp;gt;spring-core&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
		&amp;lt;/dependency&amp;gt;
 
		&amp;lt;dependency&amp;gt;
			&amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt;
			&amp;lt;artifactId&amp;gt;spring-context&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt;
		&amp;lt;/dependency&amp;gt;
 
	&amp;lt;/dependencies&amp;gt;
&amp;lt;/project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记下来就可以编写测试类进行测试并关联 Spring 源代码跟踪调试代码。&lt;/p&gt;

&lt;h1 id=&quot;spring--1&quot;&gt;Spring 的一些教程&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.mkyong.com/tutorials/spring-tutorials/&quot;&gt;http://www.mkyong.com/tutorials/spring-tutorials/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.springbyexample.org/examples/index.html&quot;&gt;http://www.springbyexample.org/examples/index.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.dzone.com/tutorials/java/spring/spring-tutorial/spring-tutorial.html&quot;&gt;http://www.dzone.com/tutorials/java/spring/spring-tutorial/spring-tutorial.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://viralpatel.net/blogs/category/spring/&quot;&gt;http://viralpatel.net/blogs/category/spring/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-5&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ibm.com/developerworks/cn/java/j-lo-spring-principle/&quot;&gt;Spring 框架的设计理念与设计模式分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ibm.com/developerworks/cn/java/wa-spring1/&quot;&gt;Spring 系列: Spring 框架简介&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.javastar.org/?p=847&quot;&gt;SPRING 3.2.X 源代码分析之二: SPRING源码的包结构&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.javastar.org/?p=872&quot;&gt;SPRING 3.2.X 源代码分析之三: SPRING源码的整体架构分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/09/29/spring-source-codes.html</link>
      <guid>http://blog.javachen.com/2014/09/29/spring-source-codes.html</guid>
      <pubDate>2014-09-29T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>编译Dubbo源码并测试</title>
      <description>&lt;p&gt;Dubbo是阿里巴巴内部的SOA服务化治理方案的核心框架，每天为2000+ 个服务提供3,000,000,000+ 次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。Dubbo自2011年开源后，已被许多非阿里系公司使用。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;项目主页：&lt;a href=&quot;http://alibaba.github.io/dubbo-doc-static/Home-zh.htm&quot;&gt;http://alibaba.github.io/dubbo-doc-static/Home-zh.htm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;项目源码：&lt;a href=&quot;https://github.com/alibaba/dubbo&quot;&gt;https://github.com/alibaba/dubbo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装&lt;/h1&gt;

&lt;p&gt;首先从 github 下载源代码并阅读 readme.md ，参考该文档，首先下载 &lt;a href=&quot;https://github.com/alibaba/opensesame&quot;&gt;opensesame&lt;/a&gt;，并编译：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git@github.com:alibaba/opensesame.git
$ cd opensesame
$ mvn install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，下载 dubbo 并编译：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone git@github.com:alibaba/dubbo.git
$ cd dubbo
$ mvn clean install -Dmaven.test.skip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译成功之后，生成 idea 相关配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn idea:idea
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，将代码通过 maven 的方式导入到 idea ide 中。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 测试&lt;/h1&gt;

&lt;p&gt;安装之后，现在来搭一个测试环境。搭建一个测试环境，需要下面三个角色：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;消息提供者&lt;/strong&gt;，示例工程见：dubbo-demo-provider&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;消息注册中心&lt;/strong&gt;，有四种类型：multicast、zookeeper、redis、dubbo&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;消息消费者&lt;/strong&gt;，示例工程见：dubbo-demo-consumer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;作为测试，这里消息注册中心使用 Multicast 注册中心，以下操作是在 idea 中运行。&lt;/p&gt;

&lt;p&gt;首先，修改 Dubbo/dubbo-demo/dubbo-demo-provider/src/test/resources/dubbo.properties 文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;dubbo.container=log4j,spring
dubbo.application.name=demo-provider
dubbo.application.owner=
dubbo.registry.address=multicast://224.5.6.7:1234?unicast=false
#dubbo.registry.address=zookeeper://127.0.0.1:2181
#dubbo.registry.address=redis://127.0.0.1:6379
#dubbo.registry.address=dubbo://10.1.19.41:20880
#dubbo.monitor.protocol=registry
dubbo.protocol.name=dubbo
dubbo.protocol.port=20880
dubbo.service.loadbalance=roundrobin
#dubbo.log4j.file=logs/dubbo-demo-consumer.log
#dubbo.log4j.level=WARN
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;消息提供者和消息消费者建议在不同机器上运行&lt;/strong&gt;，如果在同一机器上，需设置 unicast=false：即：&lt;code&gt;multicast://224.5.6.7:1234?unicast=false&lt;/code&gt;，否则发给消费者的单播消息可能被提供者抢占，两个消费者在同一台机器也一样，只有 multicast 注册中心有此问题。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后，修改 Dubbo/dubbo-demo/dubbo-demo-consumer/src/test/resources/dubbo.properties 文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;dubbo.container=log4j,spring
dubbo.application.name=demo-consumer
dubbo.application.owner=
dubbo.registry.address=multicast://224.5.6.7:1234?unicast=false
#dubbo.registry.address=zookeeper://127.0.0.1:2181
#dubbo.registry.address=redis://127.0.0.1:6379
#dubbo.registry.address=dubbo://10.1.19.41:20880
dubbo.monitor.protocol=registry
#dubbo.log4j.file=logs/dubbo-demo-consumer.log
#dubbo.log4j.level=WARN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，就可以运行 dubbo-demo-provider 和 dubbo-demo-consumer 了。&lt;/p&gt;

&lt;p&gt;在 idea 中右键运行 Dubbo/dubbo-demo/dubbo-demo-provider/src/test/java/com/alibaba/dubbo/demo/provider/DemoProvider.java 类，以启动 dubbo-demo-provider 。&lt;/p&gt;

&lt;p&gt;在 idea 中右键运行 Dubbo/dubbo-demo/dubbo-demo-consumer/src/test/java/com/alibaba/dubbo/demo/consumer/DemoConsumer.java 类，以启动 dubbo-demo-consumer 。&lt;/p&gt;

&lt;p&gt;最后，观察终端输出的日志，dubbo-demo-provider 中输出如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[17:13:19] Hello world458, request from consumer: /10.1.19.41:57319, cookie:iamsorry
[17:13:21] Hello world459, request from consumer: /10.1.19.41:57319, cookie:iamsorry
[17:13:23] Hello world460, request from consumer: /10.1.19.41:57319, cookie:iamsorry
[17:13:25] Hello world461, request from consumer: /10.1.19.41:57319, cookie:iamsorry
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而 dubbo-demo-consumer 中输出如下内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[17:13:17] Hello world458, response form provider: 10.1.19.41:20880
cookie-&amp;gt;iamsorry
abc-&amp;gt;17:13:19
Key 1-&amp;gt;1
Key 2-&amp;gt;2
codec-&amp;gt;neg
output-&amp;gt;135
[17:13:20] Hello world459, response form provider: 10.1.19.41:20880
cookie-&amp;gt;iamsorry
abc-&amp;gt;17:13:21
Key 1-&amp;gt;1
Key 2-&amp;gt;2
codec-&amp;gt;neg
output-&amp;gt;135
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，你可以试试使用其他的消息注册方式。&lt;/p&gt;

&lt;p&gt;使用类似的方式，你也可以启动 dubbo-admin 和 dubbo-monitor-simple，需要注意的是，&lt;strong&gt;如果你是在一台机器上启动这两个服务，则需要修改 dubbo.properties 中的端口以避免端口冲突&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 其他&lt;/h1&gt;

&lt;p&gt;简单谈谈个人对 dubbo 项目的看法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;项目导入到 IDE 之后，使用的是 jdk 1.5 进行编译，需要手动一个一个地修改为 1.6。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;项目没有使用统一的 code-template ，代码风格不统一。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;文档不够规范，缺少一些能够快速上手的用户文档。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;dubbo 是获取第一个网卡的 ip 地址，当有多个网卡或者使用 VPN 时候会存在问题。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;dubbo 依赖的 Spring 和 Netty 版本都较低&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;有些类和注解中的属性过多，显得比较臃肿，当然，这是强迫性症了。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上仅仅代表个人意见。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2014/09/24/compile-and-test-dubbo.html</link>
      <guid>http://blog.javachen.com/2014/09/24/compile-and-test-dubbo.html</guid>
      <pubDate>2014-09-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Mahout推荐引擎介绍</title>
      <description>&lt;p&gt;Mahout 是一个来自 Apache 的、开源的机器学习软件库，他主要关注于推荐引擎（协同过滤）、聚类和分类。&lt;/p&gt;

&lt;p&gt;推荐一般是基于物品或者用户进行推荐相关。&lt;/p&gt;

&lt;p&gt;聚类是讲大量的事物组合为拥有类似属性的簇，借以在一些规模较大或难于理解的数据集上发现层次结构和顺序，以揭示一些有用的模式或让数据集更易于理解。&lt;/p&gt;

&lt;p&gt;分类有助于判断一个新的输入或新的事物是否于以前观察到的模式相匹配，它通常还被用于筛选异常的行为或模式，来检测可疑的网络活动或欺骗行为。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;推荐系统&lt;/h1&gt;

&lt;p&gt;推荐引擎算法应用最广的两大类：基于用户和基于物品的推荐。这两者都是协同过滤的范畴：仅仅通过了解用户于物品之间的关系进行推荐。这些技术无需了解物品自身的属性。&lt;br /&gt;
还有基于内容的推荐技术，这需要和特定的领域相结合，mahout 中没有讨论此类算法。&lt;/p&gt;

&lt;p&gt;推荐引擎分为5个主要部分组成：数据模型，相似度算法，近邻算法，推荐算法，算法评分器。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;数据模型：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GenericDataModel：用户ID，物品ID，用户对物品的打分(UserID,ItemID,PreferenceValue)&lt;/li&gt;
  &lt;li&gt;GenericBooleanPrefDataModel: 用户ID，物品ID (UserID,ItemID)，这种方式表达用户是否浏览过该物品，但并未对物品进行打分。&lt;/li&gt;
  &lt;li&gt;内存级 DataModel&lt;/li&gt;
  &lt;li&gt;基于文件的 DataModel&lt;/li&gt;
  &lt;li&gt;基于数据库的 DataModel&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;相似度算法&lt;/h2&gt;

&lt;h3 id=&quot;pearsoncorrelationsimilarity-&quot;&gt;PearsonCorrelationSimilarity: 皮尔逊相似度&lt;/h3&gt;

&lt;p&gt;原理：其值为两个序列协方差与二者方差乘积的比值。它度量两个用户针对同一物品的偏好值变化趋势的一致性：都偏高或都偏低。&lt;/p&gt;

&lt;p&gt;范围：[-1,1]，绝对值越大，说明相关性越强，负相关对于推荐的意义小。&lt;/p&gt;

&lt;p&gt;问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.没有考虑两个用户同时给出偏好值的物品数目。&lt;/li&gt;
  &lt;li&gt;2.如果两个用户的交集仅包含一个物品，则无法计算相关性。&lt;/li&gt;
  &lt;li&gt;3.只要任何一个序列中出现偏好值相同的情况，相关系数都是未定义的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;该相似度并不是最好的选择，也不是最坏的选择，只是因为其容易理解，在早期研究中经常被提起。使用Pearson线性相关系数必须假设数据是成对地从正态分布中取得的，并且数据至少在逻辑范畴内必须是等间距的数据。Mahout中，为皮尔森相关计算提供了一个扩展，通过增加一个权重来使得重叠数也成为计算相似度的影响因子。&lt;/p&gt;

&lt;h3 id=&quot;euclideandistancesimilarity-&quot;&gt;EuclideanDistanceSimilarity: 欧氏距离相似度&lt;/h3&gt;

&lt;p&gt;原理：利用欧式距离d定义的相似度s，s=1 / (1+d)。&lt;/p&gt;

&lt;p&gt;范围：[0,1]，值越大，说明d越小，也就是距离越近，则相似度越大。&lt;/p&gt;

&lt;p&gt;说明：同皮尔森相似度一样，该相似度也没有考虑重叠数对结果的影响，同样地，Mahout通过增加一个枚举类型（Weighting）的参数来使得重叠数也成为计算相似度的影响因子。&lt;/p&gt;

&lt;h3 id=&quot;uncenteredcosinesimilarity-&quot;&gt;UncenteredCosineSimilarity: 余弦相似度&lt;/h3&gt;

&lt;p&gt;原理：多维空间两点与所设定的点形成夹角的余弦值。&lt;/p&gt;

&lt;p&gt;范围：[-1,1]，值越大，说明夹角越大，两点相距就越远，相似度就越小。&lt;/p&gt;

&lt;p&gt;说明：在数学表达中，如果对两个项的属性进行了数据中心化，计算出来的余弦相似度和皮尔森相似度是一样的，在mahout中，实现了数据中心化的过程，所以皮尔森相似度值也是数据中心化后的余弦相似度。另外在新版本中，Mahout提供了UncenteredCosineSimilarity类作为计算非中心化数据的余弦相似度。&lt;/p&gt;

&lt;h3 id=&quot;spearmancorrelationsimilarity-&quot;&gt;SpearmanCorrelationSimilarity: 斯皮尔曼相关系数相似度&lt;/h3&gt;

&lt;p&gt;原理：斯皮尔曼相关系数通常被认为是排名后的变量之间的皮尔逊线性相关系数。&lt;/p&gt;

&lt;p&gt;范围：{-1.0,1.0}，当一致时为1.0，不一致时为-1.0。&lt;/p&gt;

&lt;p&gt;说明：&lt;strong&gt;计算非常慢，有大量排序&lt;/strong&gt;。针对推荐系统中的数据集来讲，用斯皮尔曼相关系数作为相似度量是不合适的。&lt;/p&gt;

&lt;h3 id=&quot;tanimotocoefficientsimilarity-&quot;&gt;TanimotoCoefficientSimilarity: 忽略偏好值基于谷本系数相似度&lt;/h3&gt;

&lt;p&gt;原理：又名广义Jaccard系数，是对Jaccard系数的扩展。它是两个偏好物品之间的交集大小与并集大小的比值。&lt;/p&gt;

&lt;p&gt;范围：[0,1]，完全重叠时为1，无重叠项时为0，越接近1说明越相似。&lt;/p&gt;

&lt;p&gt;说明：处理无打分的偏好数据。&lt;/p&gt;

&lt;h3 id=&quot;cityblocksimilarity-&quot;&gt;CityBlockSimilarity: 曼哈顿距离相似度&lt;/h3&gt;

&lt;p&gt;原理：曼哈顿距离的实现，同欧式距离相似，都是用于多维数据空间距离的测度&lt;/p&gt;

&lt;p&gt;范围：[0,1]，同欧式距离一致，值越小，说明距离值越大，相似度越大。&lt;/p&gt;

&lt;p&gt;说明：比欧式距离计算量少，性能相对高。&lt;/p&gt;

&lt;h3 id=&quot;loglikelihoodsimilarity-&quot;&gt;LogLikelihoodSimilarity: 对数似然相似度&lt;/h3&gt;

&lt;p&gt;原理：重叠的个数，不重叠的个数，都没有的个数，计算&lt;code&gt;发生重叠的非偶然概率&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;范围：具体可去百度文库中查找论文《Accurate Methods for the Statistics of Surprise and Coincidence》&lt;/p&gt;

&lt;p&gt;说明：处理无打分的偏好数据，比Tanimoto系数的计算方法更为智能。&lt;/p&gt;

&lt;p&gt;总结，无偏好的相似度：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TanimotoCoefficientSimilarity: 忽略偏好值基于谷本系数相似度&lt;/li&gt;
  &lt;li&gt;LogLikelihoodSimilarity: 对数似然相似度&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;近邻算法&lt;/h2&gt;

&lt;p&gt;分为2种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NearestNUserNeighborhood:指定N的个数，比如，选出前10最相似的用户。&lt;/li&gt;
  &lt;li&gt;ThresholdUserNeighborhood:指定比例，比如，选择前10%最相似的用户。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在小数据量情况下，基于物品的推荐算法比基于用户的推荐算法要快。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;推荐算法&lt;/h2&gt;

&lt;h3 id=&quot;slop-one-&quot;&gt;Slop-one 推荐算法&lt;/h3&gt;

&lt;p&gt;Slop-one 算法假设两个物品之间存在着某种线性关系，由物品 x 的偏好值可以估计出物品 y 的偏好值。Slop-one 的吸引力在于其算法的在线部分执行很快。&lt;/p&gt;

&lt;h3 id=&quot;svd-&quot;&gt;基于SVD 奇异值分解的推荐算法&lt;/h3&gt;

&lt;h3 id=&quot;section-5&quot;&gt;基于聚类的推荐算法&lt;/h3&gt;

&lt;p&gt;运行时的推荐很快，因为几乎一切都预先计算好了。但是，这种推荐只能基于一个群组做推荐而不是为个人提供的。另外，聚类的过程非常花时间，在用户数较少时效果很好。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1]&lt;a href=&quot;http://blog.fens.me/mahout-recommend-engine/&quot;&gt;从源代码剖析Mahout推荐引擎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/09/22/mahout-recommend-engine.html</link>
      <guid>http://blog.javachen.com/2014/09/22/mahout-recommend-engine.html</guid>
      <pubDate>2014-09-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Gradle构建项目</title>
      <description>&lt;p&gt;Gradle 是一款基于 Groovy 语言、免费开源的构建工具，它既保持了 Maven 的优点，又通过使用 Groovy 定义的 DSL 克服了 Maven 中使用 XML 繁冗以及不灵活的缺点。&lt;/p&gt;

&lt;p&gt;Gradle 官方网站：&lt;a href=&quot;http://www.gradle.org/downloads&quot;&gt;http://www.gradle.org/downloads&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;安装&lt;/h1&gt;

&lt;p&gt;一种方式是从 &lt;a href=&quot;http://www.gradle.org/downloads&quot;&gt;官方&lt;/a&gt; 下载解压然后配置环境变量。&lt;/p&gt;

&lt;p&gt;Mac 上安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew install gradle
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试是否安装成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gradle -v
------------------------------------------------------------
Gradle 2.0
------------------------------------------------------------

Build time:   2014-07-01 07:45:34 UTC
Build number: none
Revision:     b6ead6fa452dfdadec484059191eb641d817226c

Groovy:       2.3.3
Ant:          Apache Ant(TM) version 1.9.3 compiled on December 23 2013
JVM:          1.7.0_60 (Oracle Corporation 24.60-b09)
OS:           Mac OS X 10.9.4 x86_64
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;简单使用&lt;/h1&gt;

&lt;p&gt;Gradle 使用 &lt;a href=&quot;http://zh.wikipedia.org/wiki/%E7%BA%A6%E5%AE%9A%E4%BC%98%E4%BA%8E%E9%85%8D%E7%BD%AE&quot;&gt;约定优于配置&lt;/a&gt;(Convention over Configuration)的理念。使用与maven兼容的目录结构布局。完全按照约定的目录结构来布置工程文件，会大大简化编译配置文件。&lt;/p&gt;

&lt;p&gt;除了常见的src/main/java等目录，默认的 web 应用程序根目录为 src/main/webapp，也就是包含 WEB-INF 目录的上一级目录。如果工程没有完全依照约定布局，可以通过脚本文件指定相应的路径。&lt;/p&gt;

&lt;p&gt;Gradle 中有两个最基本的对象：project 和 task。每个 Gradle 的构建由一个 project 对象来表达，它代表着需要被构建的组件或者构建的整个项目。每个 project 对象由一个或者多个 task 对象组成。&lt;/p&gt;

&lt;p&gt;Gradle 已经自带了很多 pugins，可以满足大部分的常见构建任务。&lt;/p&gt;

&lt;p&gt;Gradle 的默认构建脚本文件为工程根目录下的 build.gradle。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;创建项目&lt;/h2&gt;

&lt;p&gt;作为测试，创建一个 test 目录，然后通过下面命令来初始化一个项目：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir test
$ cd test
$ gradle init
:wrapper
:init

BUILD SUCCESSFUL

Total time: 3.058 secs

$ ls
build.gradle    gradle          gradlew         gradlew.bat     settings.gradle
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到生成了 gradle 的一些配置文件。接下来在 build.gradle 文件中添加下面代码，可以支持生成 jar 包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin: &#39;java&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是你定义一个Java项目所需要做的一切。这就会在你项目里使用Java插件，该插件会给你的项目增加很多任务。&lt;/p&gt;

&lt;p&gt;Gradle 期望在 src/main/java 路径下找到你项目的源代码，并且测试在 src/test/java 路径下的代码。同时，在src/main/resources 路径下的文件也会作为资源文件包含在JAR包中，并且 src/test/resources 下的所有文件会包含在 classpath 下以运行测试程序。所有的输出文件都生成在 build 目录下，JAR 包生成在 build/libs 目录下。&lt;/p&gt;

&lt;p&gt;运行下面命令即可生成 jar 包：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gradle jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 jar 包中有一个包含 main 方法的主类，想让其打包之后能够运行其 main 方法，则需要添加下面代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin: &#39;java&#39;

jar {
    manifest {
        attributes &#39;Main-Class&#39;: &#39;com.javachen.gradle.HelloWorld&#39;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常，JAR 包需要被发布到某个地方。为了完成这个功能，你需要告诉 Gradle 把 JAR 包发布到哪里。在 Gradle 中，如 JAR 之类的压缩包都被发布到库中。在我们的样例中，我们将会发布到本地仓库。你也可以发布到一个或多个远端地址。&lt;/p&gt;

&lt;p&gt;发布 jar 包添加如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;uploadArchives {
    repositories {
       flatDir {
           dirs &#39;repos&#39;
    } }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外，你还可以添加下面代码，引入 Eclipse 插件以支持生成 Eclipse 工程：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin: &#39;eclipse&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加Maven库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;repositories {
    mavenCentral()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想添加依赖，也是非常简单，例如添加 spring 依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin: &#39;java&#39;
dependencies {
    compile &#39;org.springframework:spring-context:3.2.6.RELEASE&#39;
    testCompile group: &#39;junit&#39;, name: &#39;junit&#39;, version: &#39;4.+&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;gradle 便会自动地到 maven 服务器下载 spring-context-3.2.6.RELEASE.jar，以及它所依赖的 jar 包。&lt;/p&gt;

&lt;p&gt;常用的依赖：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;（1）compile：编译生产代码的依赖环境，即src/main/下&lt;/li&gt;
  &lt;li&gt;（2）runtime：生产代码运行时的依赖（包含编译生产代码时的依赖）&lt;/li&gt;
  &lt;li&gt;（3）testCompile：编译测试代码的依赖环境，即src/test下&lt;/li&gt;
  &lt;li&gt;（4）testRuntime：测试代码运行时的依赖（包含编译测试代码时的依赖）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然，也可以直接依赖本地的 jar 包，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin: &#39;java&#39;
dependencies {
    compile fileTree(dir: &#39;libs&#39;, include: &#39;*.jar&#39;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以通过 &lt;code&gt;buildscript{}&lt;/code&gt; 中添加依赖的方式，将相关 jar 包加入到 classpath 中，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;buildscript {  
    repositories {  
        mavenCentral()  
    }  
    dependencies {  
        classpath group: &#39;commons-codec&#39;, name: &#39;commons-codec&#39;, version: &#39;1.2&#39;  
    }  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;综上，完成的配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin: &#39;java&#39;
apply plugin: &#39;eclipse&#39;

jar {
    manifest {
        attributes &#39;Main-Class&#39;: &#39;com.javachen.gradle.HelloWorld&#39;
    }
}

repositories {
    mavenLocal()
    mavenCentral()
    mavenRepo urls: &quot;http://repository.sonatype.org/content/groups/forge/&quot;
}

dependencies {
    compile &#39;org.springframework:spring-context:3.2.6.RELEASE&#39;
    testCompile group: &#39;junit&#39;, name: &#39;junit&#39;, version: &#39;4.+&#39;
    compile fileTree(dir: &#39;libs&#39;, include: &#39;*.jar&#39;)
}

buildscript {  
    repositories {  
        mavenLocal()
        mavenCentral()
        mavenRepo urls: &quot;http://repository.sonatype.org/content/groups/forge/&quot;
    }  
    dependencies {  
        classpath group: &#39;commons-codec&#39;, name: &#39;commons-codec&#39;, version: &#39;1.2&#39;  
    }  
}  

uploadArchives {
    repositories {
        flatDir {
            dirs &#39;repos&#39;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;其他使用&lt;/h1&gt;

&lt;h2 id=&quot;section-4&quot;&gt;创建项目目录结构&lt;/h2&gt;

&lt;p&gt;gradle 不像 maven 那样有固定的项目结构，gradle 原声 API 是不支持的，要想做到这一点，我们可以自定义一个 task。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin: &#39;idea&#39;
apply plugin: &#39;java&#39;
apply plugin: &#39;war&#39;

task createJavaProject &amp;lt;&amp;lt; {
  sourceSets*.java.srcDirs*.each { it.mkdirs() }
  sourceSets*.resources.srcDirs*.each { it.mkdirs()}
}

task createWebProject(dependsOn: &#39;createJavaProject&#39;) &amp;lt;&amp;lt; {
  def webAppDir = file(&quot;$webAppDirName&quot;)
  webAppDir.mkdirs()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后运行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gradle createJavaProject
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，还可以使用 &lt;a href=&quot;http://tellurianring.com/wiki/gradle/templates&quot;&gt;gradle templates&lt;/a&gt; 创建项目目录结构，这里不做研究。&lt;/p&gt;

&lt;p&gt;更加标准的方法是使用 gradle 自带的插件创建项目目录结构，例如创建 java 项目结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gradle init --type java-library
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候的目录结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tree -L 4
.
├── build.gradle
├── gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
├── settings.gradle
└── src
    ├── main
    │   └── java
    │       └── Library.java
    └── test
        └── java
            └── LibraryTest.java

7 directories, 8 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要导入到 idea 中，先执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gradle idea
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候的 build.gradle 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;apply plugin: &#39;idea&#39;
apply plugin: &#39;java&#39;
apply plugin: &#39;war&#39;

task createJavaProject &amp;lt;&amp;lt; {
      sourceSets*.java.srcDirs*.each { it.mkdirs() }
        sourceSets*.resources.srcDirs*.each { it.mkdirs()}
}

task createWebProject(dependsOn: &#39;createJavaProject&#39;) &amp;lt;&amp;lt; {
      def webAppDir = file(&quot;$webAppDirName&quot;)
        webAppDir.mkdirs()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;javamavengradle&quot;&gt;将Java项目从maven迁移到gradle&lt;/h2&gt;

&lt;p&gt;如何将一个 java 项目从maven迁移到 gradle 呢？gradle 集成了一个很方便的插件：&lt;code&gt;Build Init Plugin&lt;/code&gt;，使用这个插件可以很方便地创建一个新的 gradle 项目，或者将其它类型的项目转换为 gradle 项目。&lt;/p&gt;

&lt;p&gt;要将 maven 项目转换为 gradle 项目，只需要在项目的 pom 文件所在的目录下执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gradle init --type pom
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的命令会根据 pom 文件自动生成 gradle 项目所需的文件和配置，然后以 gradle 项目重新导入即可。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;参考&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://openwares.net/java/gradle_intro.html&quot;&gt;Gradle安装与简单使用&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://sulong.me/2011/01/26/greate_gradle&quot;&gt;gradle太好用了&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/09/15/build-project-with-gradle.html</link>
      <guid>http://blog.javachen.com/2014/09/15/build-project-with-gradle.html</guid>
      <pubDate>2014-09-15T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Groovy操作文件</title>
      <description>&lt;p&gt;Java 读写文件比较麻烦，那 Groovy 操作文件又如何呢？&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 读文件&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;读文件内容&lt;/h2&gt;

&lt;p&gt;在groovy中输出文件的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;println new File(&quot;tmp.csv&quot;).text  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码非常简单，没有流的出现，没有资源关闭的出现，也没有异常控制的出现，所有的这些groovy已经搞定了。&lt;/p&gt;

&lt;p&gt;读取每一行内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;File file = new File(&#39;tmp.csv&#39;)
assert file.name == &#39;tmp.csv&#39;
assert ! file.isAbsolute()
assert file.path == &#39;tmp.csv&#39;
assert file.parent == null

//使用系统默认的编码处理文件流  
file.eachLine {println it }  
//指定处理流的编码
file.eachLine(&quot;UTF-8&quot;) { println it }

file.eachLine(&quot;UTF-8&quot;,10) {str,no-&amp;gt;  
    println str  
    println no }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对文件中每一行的内容做处理：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;file.splitEachLine(&quot;\t&quot;) { println it  }

//以大写行式输出文件内容  
lineList = file.readLines();  
liineList.each {  
  println it.toUpperCase();  
}

file.filterLine {String str-&amp;gt;  
    if (str.contains(&#39;code&#39;))  
        println str  
}.writeTo(new PrintWriter(System.out)) 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;xml-&quot;&gt;解析 xml 文件&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt; 
&amp;lt;customers&amp;gt; 
  &amp;lt;corporate&amp;gt; 
    &amp;lt;customer name=&quot;bill gates&quot; company=&quot;microsoft&quot;&amp;gt;&amp;lt;/customer&amp;gt; 
    &amp;lt;customer name=&quot;steve jobs&quot; company=&quot;apple&quot;&amp;gt;&amp;lt;/customer&amp;gt; 
    &amp;lt;customer name=&quot;bill dyh&quot; company=&quot;sun&quot;&amp;gt;&amp;lt;/customer&amp;gt; 
  &amp;lt;/corporate&amp;gt; 
  &amp;lt;consumer&amp;gt; 
    &amp;lt;customer name=&quot;jone Doe&quot;&amp;gt;&amp;lt;/customer&amp;gt; 
    &amp;lt;customer name=&quot;jane Doe&quot;&amp;gt;&amp;lt;/customer&amp;gt;    
  &amp;lt;/consumer&amp;gt; 
&amp;lt;/customers&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;def customers = new XmlSlurper().parse(new File(&quot;customers.xml&quot;)) 
/*对文件进行解析*/ 
for(customer in customers.corporate.customer){ 
    println &quot;${customer.@name} works for${customer.@company}&quot;; 
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;propeties-&quot;&gt;解析 propeties 文件&lt;/h2&gt;

&lt;p&gt;参考 &lt;a href=&quot;http://stackoverflow.com/questions/2055959/groovy-how-to-access-to-properties-file&quot;&gt;groovy: How to access to properties file?&lt;/a&gt;，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;def props = new Properties()
new File(&quot;message.properties&quot;).withInputStream { 
  stream -&amp;gt; props.load(stream) 
}
// accessing the property from Properties object using Groovy&#39;s map notation
println &quot;capacity.created=&quot; + props[&quot;capacity.created&quot;]

def config = new ConfigSlurper().parse(props)
// accessing the property from ConfigSlurper object using GPath expression
println &quot;capacity.created=&quot; + config.capacity.created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外一种方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;def config = new ConfigSlurper().parse(new File(&quot;message.groovy&quot;).toURL())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;message.groovy 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;capacity {
  created=&quot;x&quot;
  modified=&quot;y&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;2. 操作目录&lt;/h1&gt;

&lt;p&gt;列出目录所有文件（包含子文件夹，子文件夹内文件） ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;def dir = new File(dirName)  
if (dir.isDirectory()) {  
    dir.eachFileRecurse { file -&amp;gt;  
        println file  
    }  
} 

dir.eachFileMatch(~/.*\.txt/) {File it-&amp;gt; println it.name  } //使正则表达式匹配文件名  
dir.eachFileMatch(FILES, ~/.*\.txt/) { File it-&amp;gt; println it.name  }   
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;3. 写文件&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;import java.io.File  
  
def writeFile(fileName) {  
    def file = new File(fileName)  
      
    if (file.exists())   
        file.delete()  
          
    def printWriter = file.newPrintWriter() //   
      
    printWriter.write(&#39;The first content of file&#39;)  
    printWriter.write(&#39;\n&#39;)  
    printWriter.write(&#39;The first content of file&#39;)  
      
    printWriter.flush()  
    printWriter.close()  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了 &lt;code&gt;file.newPrintWriter()&lt;/code&gt; 可以得到一个 PrintWriter，类似方法还有 &lt;code&gt;file.newInputStream()&lt;/code&gt;、&lt;br /&gt;
&lt;code&gt;file.newObjectInputStream()&lt;/code&gt;等。&lt;/p&gt;

&lt;p&gt;更简洁写法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;new File(fileName).withPrintWriter { printWriter -&amp;gt;  
     printWriter.println(&#39;The first content of file&#39;)  
}  
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/09/12/file-operation-in-groovy.html</link>
      <guid>http://blog.javachen.com/2014/09/12/file-operation-in-groovy.html</guid>
      <pubDate>2014-09-12T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Llama的使用</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 介绍&lt;/h1&gt;

&lt;p&gt;Llama (Low Latency Application MAster) 是一个 Yarn 的  Application Master，用于协调 Impala 和 Yarn 之间的集群资源的管理和监控。Llama 使 Impala 能够获取、使用和释放资源配额，而不需要 Impala 使用 Yarn 管理的 container 进程。Llama 提供了 Thrift API 来和 Yarn 交互。&lt;/p&gt;

&lt;p&gt;个人理解，Llama 的作用就是使 Impala 能够工作在 YARN 之上，使得 Impala 和 YARN 共享集群资源，提供低延迟的查询。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Llama 官网地址：&lt;a href=&quot;http://cloudera.github.io/llama/&quot;&gt;http://cloudera.github.io/llama/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Llama 源码：&lt;a href=&quot;https://github.com/cloudera/llama&quot;&gt;https://github.com/cloudera/llama&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 架构&lt;/h1&gt;

&lt;h1 id=&quot;llama-&quot;&gt;3. Llama 安装&lt;/h1&gt;

&lt;h2 id=&quot;llama&quot;&gt;3.1 安装 llama&lt;/h2&gt;

&lt;p&gt;Llama 需要安装在装有 Yarn 的节点上。&lt;/p&gt;

&lt;p&gt;在 rhel 系统上安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum install llama-master
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3.2 配置&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Llama 只能和 Yarn 配合工作，不能用于 MRv1。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Llama 的配置文件在 /etc/llama/conf/ 目录，llama-site.xml 默认配置在 &lt;a href=&quot;http://cloudera.github.io/llama/llama-site.html&quot;&gt;http://cloudera.github.io/llama/llama-site.html&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3.3 启动和停止&lt;/h2&gt;

&lt;p&gt;启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service llama start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service llama stop
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ha&quot;&gt;3.4 配置 HA&lt;/h2&gt;

&lt;p&gt;Llama 使用 Zookeeper 来实现 HA，任一时刻，只有一个 Llama-master 实例是 active的以确保资源不会被分区。&lt;/p&gt;

&lt;p&gt;为了从 Yarn 获取资源，Llama 启动 YARN application 并且运行未管理的ApplicationMaster。当一个 Llama 实例宕掉的时候，分配给该实例启动的 application 的所有资源将会被回首，直到这些 application 超时（默认超时时间为10分钟）。当 Llama 运行失败的时候，这些资源将会被杀掉他启动的application的 Llama 回收。&lt;/p&gt;

&lt;p&gt;HA 相关配置参数在 /etc/llama/conf/llama-site.xml：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;属性&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;描述&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;默认值&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;llama.am.cluster.id&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Cluster ID of the Llama pair, used to differentiate between different Llamas&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;llama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;llama.am.ha.enabled&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Whether to enable Llama HA&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;false&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;llama.am.ha.zk-quorum&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ZooKeeper quorum to use for leader election and fencing&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;llama.am.ha.zk-base&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Base znode for leader election and fencing data&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;/llama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;llama.am.ha.zk-timeout-ms&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;The session timeout, in milliseconds, for connections to ZooKeeper quorum&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;llama.am.ha.zk-ac&lt;/code&gt;l&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ACLs to control access to ZooKeeper&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;world:anyone:rwcda&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;llama.am.ha.zk-auth&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Authorization information to go with the ACLs&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;上面必填的两个参数为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;llama.am.ha.enabled&lt;/code&gt; ： true&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;llama.am.ha.zk-quorum&lt;/code&gt; ： cdh1:21088,cdh2:21088&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;impala-&quot;&gt;3.5 修改 Impala 启动参数&lt;/h2&gt;

&lt;p&gt;使用 jdbc 方式提交查询到 Impala 时，会出现 &lt;code&gt;number of running queries 20 is over limit 20&lt;/code&gt; 的异常，这时候在 impala的 源代码中搜索关键字 &lt;code&gt;number of running queries&lt;/code&gt;，可以找到&lt;a href=&quot;https://github.com/cloudera/Impala/blob/cdh5-1.4_5.1.2/be/src/scheduling/admission-controller.cc&quot;&gt;https://github.com/cloudera/Impala/blob/cdh5-1.4_5.1.2/be/src/scheduling/admission-controller.cc&lt;/a&gt;，从源代码中可以看到出现该问题和 Llama 有关系，在找不到 llama 的相关配置时，impala 一个队列中能够接受的最大请求数为 20。代码见:&lt;a href=&quot;https://github.com/cloudera/Impala/blob/c5c475712f88244e15160befaf4e99d6e165a148/fe/src/main/java/com/cloudera/impala/util/RequestPoolService.java&quot;&gt;RequestPoolService.java&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@VisibleForTesting
  TPoolConfigResult getPoolConfig(String pool) {
    TPoolConfigResult result = new TPoolConfigResult();
    int maxMemoryMb = allocationConf_.get().getMaxResources(pool).getMemory();
    result.setMem_limit(
        maxMemoryMb == Integer.MAX_VALUE ? -1 : (long) maxMemoryMb * ByteUnits.MEGABYTE);
    if (llamaConf_ == null) {												//llama配置为空
      result.setMax_requests(LLAMA_MAX_PLACED_RESERVATIONS_DEFAULT);
      result.setMax_queued(LLAMA_MAX_QUEUED_RESERVATIONS_DEFAULT);
    } else {
      // Capture the current llamaConf_ in case it changes while we&#39;re using it.
      Configuration currentLlamaConf = llamaConf_;
      result.setMax_requests(getLlamaPoolConfigValue(currentLlamaConf, pool,
          LLAMA_MAX_PLACED_RESERVATIONS_KEY,
          LLAMA_MAX_PLACED_RESERVATIONS_DEFAULT));  //20
      result.setMax_queued(getLlamaPoolConfigValue(currentLlamaConf, pool,
          LLAMA_MAX_QUEUED_RESERVATIONS_KEY,
          LLAMA_MAX_QUEUED_RESERVATIONS_DEFAULT));
    }
    LOG.trace(&quot;getPoolConfig(pool={}): mem_limit={}, max_requests={}, max_queued={}&quot;,
        new Object[] { pool, result.mem_limit, result.max_requests, result.max_queued });
    return result;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前，参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_admission.html&quot;&gt;Admission Control and Query Queuing&lt;/a&gt;，在不安装和使用 llama 情况下，找到的一种解决办法是：&lt;/p&gt;

&lt;p&gt;修改 impala 启动参数（/etc/default/impala），添加 ` -default_pool_max_requests=-1`，该参数设置每一个队列的最大请求数，如果为-1，则表示不做限制。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;4. 使用&lt;/h1&gt;

&lt;h2 id=&quot;llama-application-master&quot;&gt;4.1 Llama Application Master&lt;/h2&gt;

&lt;h2 id=&quot;llama-admin-command-line-tool&quot;&gt;4.2 Llama Admin Command Line tool&lt;/h2&gt;

&lt;h2 id=&quot;llama-node-manager-auxiliary-service&quot;&gt;4.3 Llama Node Manager Auxiliary Service&lt;/h2&gt;

&lt;h1 id=&quot;section-5&quot;&gt;5. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://cloudera.github.io/llama&quot;&gt;http://cloudera.github.io/llama&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_admission.html&quot;&gt;Admission Control and Query Queuing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/09/09/llama.html</link>
      <guid>http://blog.javachen.com/2014/09/09/llama.html</guid>
      <pubDate>2014-09-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>从零开始创建Grails应用</title>
      <description>&lt;p&gt;本篇文章主要介绍如何从零开始一步一步创建一个 Grails 应用程序。整个过程中，你将学到如何改变 Grails 运行的端口，了解 Grails 应用的基础组成部分(领域类、控制器和视图)、指定字段的缺省值，以及其他许多内容。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装&lt;/h1&gt;

&lt;p&gt;下载压缩包然后解压或者通过rpm、deb发行包安装。&lt;/p&gt;

&lt;p&gt;这里，我在 mac 上安装 grails ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install grails
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 创建应用&lt;/h1&gt;

&lt;p&gt;以 blog 为例，创建一个应用程序，在命令行输入 &lt;code&gt;grails create-app blog&lt;/code&gt; ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grails create-app blog
2014-9-9 10:43:29 org.codehaus.groovy.runtime.m12n.MetaInfExtensionModule newModule
警告: Module [groovy-all] - Unable to load extension class [org.codehaus.groovy.runtime.NioGroovyMethods]
2014-9-9 10:43:29 org.codehaus.groovy.runtime.m12n.MetaInfExtensionModule newModule
| Created Grails Application at /Users/june/workspace/groovy/blog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，就创建了一个空的应用程序，进入到 blog 目录，输入 &lt;code&gt;grails run-app&lt;/code&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd blog
$ grails run-app
...
2014-9-9 10:45:31 org.codehaus.groovy.runtime.m12n.MetaInfExtensionModule newModule
警告: Module [groovy-all] - Unable to load extension class [org.codehaus.groovy.runtime.NioGroovyMethods]
| Server running. Browse to http://localhost:8080/blog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一切顺刟癿话，你应该可以在浏觅器中访问 &lt;code&gt;http://localhost:8080/blog/&lt;/code&gt; 并查看欢迎页面。&lt;/p&gt;

&lt;p&gt;如果发现端口冲突，你可以&lt;a href=&quot;http://tilt.lib.tsinghua.edu.cn/node/624&quot;&gt;修改默认端口&lt;/a&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在启动时添加参数：&lt;code&gt;grails -Dserver.port=9090 run-app&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;在 grails-app/conf 目录修改 BuildConfig.groovy，添加 &lt;code&gt;grails.server.port.http=9090&lt;/code&gt; 或 &lt;code&gt;server.port=9090&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;修改用户的默认设置。修改 &lt;code&gt;~/.grails/settings.groovy&lt;/code&gt;，添加 &lt;code&gt;grails.server.port.http = 9000&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;修改Grails程序的默认设置。在 &lt;code&gt;$GRAILS_HOME/scripts/_GrailsSettings.groovy&lt;/code&gt; 中添加：&lt;code&gt;serverPort = getPropertyValue(&quot;server.port&quot;, 9000).toInteger()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;测试应用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;grails test-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想部署应用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;grails war
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面命令默认使用的是 production 环境，也可以添加参数使用 dev 环境：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;grails dev war
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当部署应用时候，最后是设置 jvm 内存：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-server -Xmx512M -XX:MaxPermSize=256m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 blog 目录查看应用目录结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ blog  tree -L 2
.
├── application.properties
├── grails-app
│   ├── assets
│   ├── conf			# 配置文件(如数据源、URL 映射、遗留的 Spring 和 Hibernate 配置文件)
│   ├── controllers		# 控制器(MVC 中的“C”)
│   ├── domain			# 领域类(MVC 中的模型或“M”。该目弽中 癿每个文件在数据库中都有对应癿表。)
│   ├── i18n			# 国际化
│   ├── migrations		# 迁移
│   ├── services		# 服务类
│   ├── taglib			# 自定义标签库
│   ├── utils			# 工具
│   └── views			# 视图
├── grailsw
├── grailsw.bat
├── lib					# 存放第三方 jar
├── scripts				# Gant 脚本
├── src 				# 源代码文件
│   ├── groovy
│   └── java
├── target
│   ├── classes
│   ├── stacktrace.log
│   └── work
├── test				# 单元测试
│   ├── integration
│   └── unit
├── web-app				# web 资源文件
│   ├── META-INF
│   ├── WEB-INF
│   ├── css
│   ├── images
│   └── js
└── wrapper
    ├── grails-wrapper-runtime-2.4.3.jar
    ├── grails-wrapper.properties
    └── springloaded-1.2.0.RELEASE.jar

29 directories, 7 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Grails 非常强调惯例优于配置(convention over configuration)。这意味着 Grails 并不是靠配置(configuration)文件来把应用各部分组织在一起，相反，它靠的是惯例 (convention)。所有领域类都存放在 domain 目录，控制器保存在 controllers 目录，视图则=放在 views 目录，等等。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 创建领域类&lt;/h1&gt;

&lt;p&gt;Grails 接受这些简单的类，并用它们完成许多工作。相应的数据库表会自劢为每个领域 类创建，控制器和视图会从关联的领域类中派生出相应的类。领域类还是存放验证规则、定义 “一对多”关系，以及包含其他许多信息的地方。&lt;/p&gt;

&lt;p&gt;创建 User 领域类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grails create-domain-class User
...
| Created file grails-app/domain/blog/User.groovy
| Compiling 1 source files
| Created file test/unit/blog/UserSpec.groovy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你会发现 Grails 同时创建了一个领域类和一个测试类，领域类中包名为 blog，其实，你也可以在创建领域类的时候自定义包名。&lt;/p&gt;

&lt;p&gt;编辑 &lt;code&gt;grails-app/domain/blog/User.groovy&lt;/code&gt;，添加一些属性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User { 
	String name
	Integer age
	String sex
	Date dateCreated
	Date lastUpdated

	static mapping = { 
		autoTimestamp false
	}

	static constraints = {

	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上只是添加了一些简单的属性，你还可以添加两个特殊的属性。如果你定义了一个名为 dateCreated 的日期字段，Grails 将自动在第一次向数据库保存实例的时候填上这个值。要是你创建了&lt;br /&gt;
另一个名为 lastUpdated 的日期字段，Grails 将在每次把更新后的记录存回数据库的时候填充这个日期。这个可以在 static mapping 代码块中来配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User { 

	Date dateCreated
	Date lastUpdated

	static mapping = { 
		autoTimestamp false
	}

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，你也可以定义一些领域类的生命周期事件来做一些复杂的事情：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User { 
	// ...
	def beforeInsert = {
		// your code goes here
	}
	def beforeUpdate = {
		// your code goes here 
	}
	def beforeDelete = {
		// your code goes here
	}
	def onLoad = {
		// your code goes here 
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;static mapping 还可以做一些其他事情，如制定列表排列顺序：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class User {
	static mapping = {
		sort &quot;name&quot; 
	}
	// ... 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关亍 static mapping 代码块更多的信息，请参见：&lt;a href=&quot;http://grails.org/GORM+-+Mapping+DSL&quot;&gt;http://grails.org/GORM+-+Mapping+DSL&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4. 创建控制器和视图&lt;/h1&gt;

&lt;p&gt;在命令行下，输入 &lt;code&gt;grails create-controller User&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grails create-controller User
| Compiling 1 source files
| Created file grails-app/controllers/blog/UserController.groovy
| Created file grails-app/views/user
| Compiling 1 source files
| Created file test/unit/blog/UserControllerSpec.groovy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 User 的控制器 grails-app/controllers/blog/UserController.groovy，你会发现没有多少内容 ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;package blog

class UserController {

    def index() { }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改一下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;package blog

class UserController {

    def index={
		render &quot;Hello World&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候启动应用，访问 &lt;a href=&quot;http://localhost:8080/blog/user&quot;&gt;http://localhost:8080/blog/user&lt;/a&gt; 你会看到 “Hello World”。&lt;/p&gt;

&lt;p&gt;在控制器中定义的任何一个闭包都会暴露成一个 url。&lt;/p&gt;

&lt;p&gt;重新修改控制器类代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;package blog

class UserController {
	def scaffold = User
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当 Grails 看到控制器中的 scaffold 属性,它会动态地产生针对指定领域类的控制器逻辑和必要的CRUD视图。所有这些只需要这一行代码。&lt;/p&gt;

&lt;p&gt;进入 &lt;a href=&quot;http://localhost:8080/blog/user/create&quot;&gt;http://localhost:8080/blog/user/create&lt;/a&gt; 页面，你会发现表单字段排列顺序没有按照预想的情况排列，这时候可在 static constraints 代码块中指定表单顺序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User { 
	String name
	Integer age
	String sex
	Date dateCreated
	Date lastUpdated

	static mapping = { 
		autoTimestamp false
		sort &quot;name&quot;
	}

	static constraints = {
		name()
		age()
		sex()
		dateCreated()
		lastUpdated()
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，也可以增加一些约束条件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User { 
	String name
	Integer age
	String sex
	Date activeDate
	Date dateCreated
	Date lastUpdated

	static mapping = { 
		autoTimestamp false
		sort &quot;name&quot;
	}

	static constraints = {
		name(blank:false, maxSize:50)
		age(min:0)
		sex(inList:[&quot;F&quot;, &quot;M&quot;])
		activeDate()
		dateCreated()
		lastUpdated()
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Grails 所有可用的验证选项：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;blank、nullable&lt;/li&gt;
  &lt;li&gt;creditCard&lt;/li&gt;
  &lt;li&gt;display&lt;/li&gt;
  &lt;li&gt;email&lt;/li&gt;
  &lt;li&gt;password&lt;/li&gt;
  &lt;li&gt;inList&lt;/li&gt;
  &lt;li&gt;matches&lt;/li&gt;
  &lt;li&gt;min, max&lt;/li&gt;
  &lt;li&gt;minSize,maxSize,size&lt;/li&gt;
  &lt;li&gt;notEqual&lt;/li&gt;
  &lt;li&gt;range&lt;/li&gt;
  &lt;li&gt;scale&lt;/li&gt;
  &lt;li&gt;unique&lt;/li&gt;
  &lt;li&gt;url&lt;/li&gt;
  &lt;li&gt;validator&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而校验失败的国际化消息保存在 grails-app/i18n 目录下的 messages.properties 文件里。&lt;/p&gt;

&lt;p&gt;你可以通过 validator 来指定自定义的校验器，例如，startDate要大于当前时间：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;static constraints = {
	// ...
	activeDate(validator: {return (it &amp;gt; new Date())}) // ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在 messages.properties 文件里添加一行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;user.activeDate.validator.invalid=Sorry, but the date is the past. 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5. 对象关联映射&lt;/h1&gt;

&lt;p&gt;不管愿不愿意,我们都已经创建了领域类，从来没有操心过这些数据的保存和位置。之所以我们能够如此享福，这都得感谢 GORM。Grails 对象-关系映射(Grails Object-Relational Mapping)API 得以让我们放心地以对象方式去思考问题——而不至于陷入到关系数据库相关的 SQL 当中。&lt;/p&gt;

&lt;h2 id=&quot;datasource&quot;&gt;5.1 DataSource&lt;/h2&gt;

&lt;p&gt;GORM 缺醒设置在 grails-app/conf/DataSource.groovy：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;dataSource {
    pooled = true
    jmxExport = true
    driverClassName = &quot;org.h2.Driver&quot;
    username = &quot;sa&quot;
    password = &quot;&quot;
}
hibernate {
    cache.use_second_level_cache = true
    cache.use_query_cache = false
//    cache.region.factory_class = &#39;net.sf.ehcache.hibernate.EhCacheRegionFactory&#39; // Hibernate 3
    cache.region.factory_class = &#39;org.hibernate.cache.ehcache.EhCacheRegionFactory&#39; // Hibernate 4
    singleSession = true // configure OSIV singleSession mode
    flush.mode = &#39;manual&#39; // OSIV session flush mode outside of transactional context
}

// environment specific settings
environments {
    development {
        dataSource {
            dbCreate = &quot;create-drop&quot; // one of &#39;create&#39;, &#39;create-drop&#39;, &#39;update&#39;, &#39;validate&#39;, &#39;&#39;
            url = &quot;jdbc:h2:mem:devDb;MVCC=TRUE;LOCK_TIMEOUT=10000;DB_CLOSE_ON_EXIT=FALSE&quot;
        }
    }
    test {
        dataSource {
            dbCreate = &quot;update&quot;
            url = &quot;jdbc:h2:mem:testDb;MVCC=TRUE;LOCK_TIMEOUT=10000;DB_CLOSE_ON_EXIT=FALSE&quot;
        }
    }
    production {
        dataSource {
            dbCreate = &quot;update&quot;
            url = &quot;jdbc:h2:prodDb;MVCC=TRUE;LOCK_TIMEOUT=10000;DB_CLOSE_ON_EXIT=FALSE&quot;
            properties {
               // See http://grails.org/doc/latest/guide/conf.html#dataSource for documentation
               jmxEnabled = true
               initialSize = 5
               maxActive = 50
               minIdle = 5
               maxIdle = 25
               maxWait = 10000
               maxAge = 10 * 60000
               timeBetweenEvictionRunsMillis = 5000
               minEvictableIdleTimeMillis = 60000
               validationQuery = &quot;SELECT 1&quot;
               validationQueryTimeout = 3
               validationInterval = 15000
               testOnBorrow = true
               testWhileIdle = true
               testOnReturn = false
               jdbcInterceptors = &quot;ConnectionState&quot;
               defaultTransactionIsolation = java.sql.Connection.TRANSACTION_READ_COMMITTED
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面可以看到，Grails 的环境分为三种模式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;development：开发模式&lt;/li&gt;
  &lt;li&gt;test：测试模式&lt;/li&gt;
  &lt;li&gt;production：生产模式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并且每种模式下的数据源配置有些许差异，如：dbCreate 值不一样， Grails 默认使用的是 H2内存数据库来保存数据，并三种模式下使用的 JDBC url（内存或者文件）不一样，等等。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果在 DataSource 上设置dbCreate属性为”update”, “create” or “create-drop”, Grails 会为你自动生成/修改数据表格。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;你也可以修改该文件，使用其他的数据库，从而清楚的看到 Grails 创建的表以及其中每一个字段。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;dataSource {
    pooled = true
    jmxExport = true
    driverClassName = &quot;com.mysql.jdbc.Driver&quot;
    username = &quot;grails&quot;
    password = &quot;grails&quot;
}
hibernate {
    cache.use_second_level_cache = true
    cache.use_query_cache = false
//    cache.region.factory_class = &#39;net.sf.ehcache.hibernate.EhCacheRegionFactory&#39; // Hibernate 3
    cache.region.factory_class = &#39;org.hibernate.cache.ehcache.EhCacheRegionFactory&#39; // Hibernate 4
    singleSession = true // configure OSIV singleSession mode
    flush.mode = &#39;manual&#39; // OSIV session flush mode outside of transactional context
}

// environment specific settings
environments {
    development {
        dataSource {
            dbCreate = &quot;create-drop&quot; // one of &#39;create&#39;, &#39;create-drop&#39;, &#39;update&#39;, &#39;validate&#39;, &#39;&#39;
            url = &quot;jdbc:mysql://localhost:3306/grails?autoreconnect=true&quot;
        }
    }
    test {
        dataSource {
            dbCreate = &quot;update&quot;
            url = &quot;jdbc:mysql://localhost:3306/grails?autoreconnect=true&quot;
        }
    }
    production {
        dataSource {
            dbCreate = &quot;update&quot;
            url = &quot;jdbc:mysql://localhost:3306/grails?autoreconnect=true&quot;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，你还需要在 mysql 中创建 grails 用户和 grails 数据库，并将 mysql 的 jdbc 驱动拷贝到 lib 目录下。&lt;/p&gt;

&lt;p&gt;然后，启动应用观察日志中是否有报错。&lt;/p&gt;

&lt;h2 id=&quot;one-to-many&quot;&gt;5.2 One-to-many&lt;/h2&gt;

&lt;p&gt;创建 Blog 领域类，并设置 User 和 Blog 的 &lt;code&gt;一对多&lt;/code&gt; 的关系。&lt;/p&gt;

&lt;p&gt;先创建 Blog 领域类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ grails create-domain-class Blog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后修改 User 领域类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User { 
  // ...
  static hasMany = [blogs:Blog] 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述代码创建了一个名为 blogs 的新字段，类型是 java.util.Set。如果有多个关系，可以用逗号分隔。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Grails中默认使用的fetch策略是 “lazy”, 意思就是集合将被延迟初始化。 如果你不小心，这会导致 &lt;a href=&quot;http://www.javalobby.org/java/forums/t20533.html&quot;&gt;n+1&lt;/a&gt; 问题 。&lt;br /&gt;
如果需要”eager” 抓取 ，需要使用 &lt;a href=&quot;http://www.cjsdn.net/doc/jvm/grails/docs/1.1/guide/single.html#5.5.2 Custom ORM Mapping&quot;&gt;ORM DSL&lt;/a&gt; 或者指定立即抓取作为query的一部分&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;默认的级联行为是级联保存和更新，但不删除，除非 belongsTo 被指定:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class Blog { 
  // ...
  static belongsTo = [user:User] 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这不仅形成了闭环，它还强制了级联更新和删除。&lt;/p&gt;

&lt;p&gt;如果在one-to-many的多方拥有2个同类型的属性，必须使用mappedBy 指定哪个集合被映射：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User {
        static hasMany = [blogs:Blog]
        static mappedBy = [blogs:&quot;users1&quot;]
}
class Blog {
        User users1
        User users2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果多方拥有多个集合被映射到不同的属性，也是一样的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User {
        static hasMany = [blogs1:Blog, blogs2:Blog]
        static mappedBy = [blogs1:&quot;users1&quot;, blogs2:&quot;users2&quot;]
}
class Blog {
        User users1
        User users2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，为了代码的可读性，我们可以修改领域类的 &lt;code&gt;toString()&lt;/code&gt; 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class User { 
  // ...
  String toString(){
    return &quot;${name}, ${activeDate.format(&#39;MM/dd/yyyy&#39;)}&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;many-to-many&quot;&gt;5.3 Many-to-many&lt;/h2&gt;

&lt;p&gt;Grails 支持 many-to-many 关联，通过在关联双方定义 hasMany ，并在关联拥有方定义 belongsTo ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class Book {
   static belongsTo = Author
   static hasMany = [authors:Author]
   String title
}
class Author {
   static hasMany = [books:Book]
   String name
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Grails 在数据库层使用一个连接表来映射 many-to-many，在这种情况下，Author 负责持久化关联，并且是唯一可以级联保存另一端的一方 。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;5.4 数据初始化&lt;/h2&gt;

&lt;p&gt;在 grails-app/conf 目录下有一个文件叫做 BootStrap.groovy，可以用来做一些初始化的工作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class BootStrap {
  def init = { servletContext -&amp;gt; }
  def destroy = {} 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;init 闭包会每次在 Grails 启动时被调用; destroy 闭包会每次在 Grails 停止时被调用。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;6. 参考&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://www.cjsdn.net/doc/jvm/grails/docs/1.1/&quot;&gt;The Grails Framework - Reference Documentation-v1.1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://grails.asia/grails-tutorial-for-beginners/&quot;&gt;Grails Tutorial for Beginners&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://tilt.lib.tsinghua.edu.cn/docs/program/grails/j-grails/section5.html&quot;&gt;使用 Grails 快速开发 Web 应用程序&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/09/09/create-a-grails-app-step-by-step.html</link>
      <guid>http://blog.javachen.com/2014/09/09/create-a-grails-app-step-by-step.html</guid>
      <pubDate>2014-09-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Groovy语法介绍</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 介绍&lt;/h1&gt;

&lt;p&gt;Groovy 是基于 JRE 的脚本语言，和Perl，Python等等脚本语言一样，它能以快速简洁的方式来完成一些工作：如访问数据库，编写单元测试用例，快速实现产品原型等等。&lt;/p&gt;

&lt;p&gt;Groovy 是由James Strachan 和 Bob McWhirter 这两位天才发明的（JSR 241 2004 年 3 月）。Groovy 完全以Java API为基础，使用了Java开发人员最熟悉的功能和库。Groovy 的语法近似Java，并吸收了 Ruby 的一些特点，因此 Groovy 在某些场合可以扮演一种 “咖啡伴侣”的角色。&lt;/p&gt;

&lt;p&gt;官网地址：&lt;a href=&quot;http://groovy.codehaus.org/&quot;&gt;http://groovy.codehaus.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Groovy的主要特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Closure（闭包）的支持&lt;/li&gt;
  &lt;li&gt;本地的 List 和 Map 语法&lt;/li&gt;
  &lt;li&gt;Groovy 标记：支持多种标记语言，如 XML、HTML、SAX、W3C DOM&lt;/li&gt;
  &lt;li&gt;Groovy Path 表达式语言：类似 Xpath&lt;/li&gt;
  &lt;li&gt;Groovlet：用简单的 Groovy 脚本实现 Servlet&lt;/li&gt;
  &lt;li&gt;Groovy SQL：使得和 SQL 一起工作更简单&lt;/li&gt;
  &lt;li&gt;Groovy Bean：和 Bean 一起工作的简单语法&lt;/li&gt;
  &lt;li&gt;Groovy模版引擎：简单使用，集成了 Gpath 和编译成字节码&lt;/li&gt;
  &lt;li&gt;Ant 脚本化&lt;/li&gt;
  &lt;li&gt;正则表达式：简洁的脚本语法使用正则表达式&lt;/li&gt;
  &lt;li&gt;操作符重载：使 Collection 和 Map 的数据类型简单化&lt;/li&gt;
  &lt;li&gt;多形式的 iteration 和 Autoboxing&lt;/li&gt;
  &lt;li&gt;直接编译成 Java 字节码，很干净的和所有已存在的 Java 对象和类库一起工作&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Groovy 可以作为 javac 的一种可选编译器来生成标准的 Java 字节码，在任何 Java 工程 中使用。Groovy 可以作为一种动态的可选语言，如脚本化 Java对 象、模版化、编写单元测试用例。&lt;/p&gt;

&lt;p&gt;工具介绍：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Groovy ：运行 groovy 脚本文件。&lt;/li&gt;
  &lt;li&gt;Groovyc：编译 groovy 脚本成 java 的 bytecode 文件（.class）&lt;/li&gt;
  &lt;li&gt;Groovysh：运行命令行的控制台，可以输入 groovy 语句直接执行&lt;/li&gt;
  &lt;li&gt;GroovyConsole：GUI 形式的控制台，相当于简单的编辑器&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 使用&lt;/h1&gt;

&lt;p&gt;使用压缩包安装，下载地址为 &lt;a href=&quot;http://groovy.codehaus.org/Download&quot;&gt;http://groovy.codehaus.org/Download&lt;/a&gt;，下载然后解压配置环境变量。&lt;/p&gt;

&lt;p&gt;在 mac 上安装 groovy：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install groovy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Groovy 脚本可以直接用 Groovy 解析执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;groovy hello.groovy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译为字节码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;groovyc -d classes hello.groovy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行编译好的groovy脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -cp %GROOVY_HOM E%/embeddable/groovy-all.jar;classes hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你会发现其实就是运行 java 的 class 文件。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 语法&lt;/h1&gt;

&lt;p&gt;Groovy 的语法融合了 Ruby、Python 和 Smalltalk 的一些最有用的功能，同时保留了基于 Java 语言的核心语法。对于Java 开发人员，Groovy 提供了更简单的替代语言，且几乎不需要学习时间。&lt;/p&gt;

&lt;p&gt;Groovy在语法上跟java有几点不同：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Groovy中，”= =”等同于java中的equals方法；”= = =”等同于java中的”= =”。&lt;/li&gt;
  &lt;li&gt;Groovy中缺省的标志符是public。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3.1 关键字&lt;/h2&gt;

&lt;p&gt;在 Groovy 可以用 def 定义无类型的变量(定义变量方面 def 与 JavaScript 中的 var 相似)，和返回值为无类型的方法&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;class Man {
  def name = &quot;javachen&quot;
  def introduce() {
    return &quot;I&#39;m $name&quot; // return可以省略
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;3.2 语句&lt;/h2&gt;

&lt;p&gt;Groovy的语句和Java类似，但是有一些特殊的地方。例如语句的分号是可选的。如果每行一个语句，就可以省略分号；如果一行上有多个语句，则需要用分号来分隔。&lt;/p&gt;

&lt;p&gt;另外return关键字在方法的最后是可选的；同样，返回类型也是可选（缺省是Object）。&lt;/p&gt;

&lt;p&gt;调用方法时可以不用括号，只要有参数，并且没有歧义。&lt;/p&gt;

&lt;p&gt;一个示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;package com.javachen.groovy.test

class Hello{
    static main(args){
        println &quot;hello world&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和Java一样，程序会从这个类的main方法开始执行，和Java的区别是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;class 前省略 public 修饰；&lt;/li&gt;
  &lt;li&gt;main 方法前省略 public 修饰；&lt;/li&gt;
  &lt;li&gt;main 方法省略返回值类型 void；&lt;/li&gt;
  &lt;li&gt;main 方法形参列表省略类型 String[]；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然，这只是 Groovy 代码的一种写法，实际上执行 Groovy 代码完全可以不必需要一个类或 main 方法，所以更简单的写法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;package com.javachen.groovy.test

println &quot;hello world&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上述代码保存为 hello.groovy，然后运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ groovy hello.groovy
hello world
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到正确的输出了 “hello world”&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;3.3 变量和类型&lt;/h2&gt;

&lt;p&gt;像其他 Script 一样，Groovy 不需要显式声明类型。在 Groovy 中，一个对象的类型是在运行时动态发现的，这极大地减少了要编写的代码数量。&lt;/p&gt;

&lt;p&gt;在 Groovy 中，类型对于值(varibles)、属性(properties)、方法(method)和闭包(closure)参数、返回值都是可有可无的，只有在给定值的时候，才会决定它的类型，(当然声明了类型的除外）。&lt;/p&gt;

&lt;p&gt;Groovy 对 boolean 类型放宽了限制：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;常量true和false分别表示“真”和“假”；&lt;/li&gt;
  &lt;li&gt;null表示false，非null表示true；&lt;/li&gt;
  &lt;li&gt;空字符串”“表示false，非空字符串表示true；&lt;/li&gt;
  &lt;li&gt;0表示false，非0表示true；&lt;/li&gt;
  &lt;li&gt;空集合表示false，非空集合表示true；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-6&quot;&gt;3.4 字符串&lt;/h2&gt;

&lt;p&gt;Groovy 中的字符串允许使用双引号和单引号。   当使用双引号时，可以在字符串内嵌入一些运算式，Groovy 允许您使用 与 bash 类似的 &lt;code&gt;${expression}&lt;/code&gt; 语法进行替换。可以在字符串中包含任意的 Groovy 表达式。&lt;/p&gt;

&lt;p&gt;大块文本：&lt;/p&gt;

&lt;p&gt;如果有一大块文本（例如 HTML 和 XML）不想编码，你可以使用Here-docs。 here-docs 是创建格式化字符串的一种便利机制。它需要类似 Python 的三重引号(“”“)开头，并以三重引号结尾。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;package com.javachen.groovy.test

// groovy中对字符串的使用做了大量的简化

// 获取字符串中的字符
s = &quot;Hello&quot;
println s[0]		// 输出&#39;H&#39;
// 遍历字符串中的所有字符
s.each {
	print it + &quot;, &quot;		// 遍历字符串中的所有字符
}
println &quot;&quot;

// 截取字符串
s1 = s[1..3]		// 截取s字符串标号从1到3的3个字符，组成新的字符串赋予s1
					// 该语法是String类的substring方法的简化
println s1

// 模板式字符串
n = 100
s1 = &quot;The number n is ${n}&quot;		// ${n}表示将变量n的值放在字符串该位置
println s1

// 带格式的长字符串
// &quot;&quot;&quot;和&quot;&quot;&quot;之间的所有字符都会被算做字符串内容，包括// /*以及回车，制表符等
s = &quot;&quot;&quot;
大家好
欢迎大家学习Groovy编程
Groovy is a better Java
&quot;&quot;&quot;
println s

// groovy中单引号的作用

// 在不定义类型时，单引号也表示字符串
c1 = &#39;A&#39;
println c1.getClass().getName()

// 要明确的定义字符类型，需要给变量增加定义
char c2 = &#39;A&#39;
println c2.getClass().getName()

// 取消转义字符
s = &#39;c:\\windows\\system&#39;
println s
s = /c:\windows\system/		// 利用/字符串/定义的字符串
println s

// 字符串运算
s = &quot;hello&quot;
s = s + &quot; world&quot;			// +运算符用于连接字符串
println s
s -= &quot;world&quot;				// -可以从字符串中去掉一部分
println s
s = s * 2					// *可以让字符串重复n次
println s

// 字符串比较
s1 = &quot;Abc&quot;
s2 = &quot;abc&quot;

println s1 == s2 ? &quot;Same&quot; : &quot;Different&quot;		// 执行s1.equals(s2)
println s1 != s2 ? &quot;Different&quot; : &quot;Same&quot;		// 执行!s1.equals(s2)
println s1 &amp;gt; s2 ? &quot;Great&quot; : &quot;Less&quot;			// 执行s1.compareTo(s2) &amp;gt; 0
println s1 &amp;lt; s2 ? &quot;Less&quot; : &quot;Great&quot;			// 执行s1.compareTo(s2) &amp;lt; 0
println s1 &amp;lt;=&amp;gt; s2 == 1 ? &quot;Same&quot; : &quot;Different&quot;		// 执行s1.compareTo(s2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Groovy增加了对字符串的如下操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;集合操作，Groovy 将字符串看为字符的集合，可以通过 [n] 运算符直接访问字符串内的字符，也可以通过 each 循环遍历字符串的每一个字符；&lt;/li&gt;
  &lt;li&gt;截取子字符串的 substring 方法被简化为使用数值范围来进行截取，”hello”[1..3]表示截取字符串 “hello” 从下标为1到下标为3的部分，结果为 “ell”；&lt;/li&gt;
  &lt;li&gt;Groovy 增加了一个新的字符串类型 GString，这种字符串可以进行格式化，在 GString 字符串中使用 ${变量}，可以将该变量的值放入字符串的相应位置；&lt;/li&gt;
  &lt;li&gt;带格式的字符串，使用 “&quot;”字符串内容””“(连续的三个引号)，这种字符串中可以包含直接输入的回车，TAB键，//或/*等字符，而这些在 Java 原本的字符串里，都必须通过转义字符来表示，例如只能用 \n 表示回车；&lt;/li&gt;
  &lt;li&gt;单引号问题，和 Javascript 和 PHP 类似，Groovy 中无论是单引号还是双引号都表示是字符串类型，例如 ‘a’ 和”a”都是字符串类型，所以如果要确定存储一个 char 类型变量，就必须使用 char 类型定义强类型变量；实际上 Groovy 认为 char 类型并不是必须的，大部分时候字符串类型更方便一些；&lt;/li&gt;
  &lt;li&gt;用 / 包围的字符串，即 /字符串内容/，可以避免在字符串中使用转义字符，但 \n 字符不包含在内；&lt;/li&gt;
  &lt;li&gt;Java 中对字符串的运算只有+运算，在 Groovy 中，字符串还可以使用 - 运算 和 * 运算，减法运算可以从一个字符串中删除一部分，乘法运算可以将一个字符串重复n次；&lt;/li&gt;
  &lt;li&gt;Groovy 还为字符串加入了所有关系运算符，包括 ==, !=, &amp;gt;, &amp;lt;, &amp;gt;=, &amp;lt;=，这要归功于 Groovy 允许运算符重载，对于 == 和 !=，将调用 String 类的 equals 方法，对于 &amp;gt;, &amp;gt;=, &amp;lt;, &amp;lt;=，将调用 String 类的 compareTo 方法；Groovy 还增加了一个特殊的运算符&amp;lt;=&amp;gt;，这个运算符也会调用 compareTo 方法，返回 compareTo 方法的返回值；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;switch&quot;&gt;3.5 switch语句&lt;/h2&gt;

&lt;p&gt;Groovy 的 switch 语句兼容 Java 代码，但是更灵活，Groovy 的 switch 语句能够处理各种类型的 switch 值，可以做各种类型的匹配：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;case 值为类名，匹配 switch 值为类实例&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;case 值为正则表达式，匹配 switch 值的字符串匹配该正则表达式&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;case 值为集合，匹配 switch 值包含在集合中，包括 ranges&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;除了上面的，case值与switch值相等才匹配。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Switch 语句的工作原理：switch 语句在做 case 值匹配时，会调用 &lt;code&gt;isCase(switchValue)&lt;/code&gt; 方法，Groovy 提供了各种类型，如类，正则表达式、集合等等的重载。可以创建自定义的匹配类，增加 &lt;code&gt;isCase(switchValue)&lt;/code&gt; 方法来提供自定义的匹配类型。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;3.6 循环&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-groovy&quot;&gt;package com.javachen.groovy.loops

public class LoopTest{
  public static void main(args){
  	def list = [&quot;Lars&quot;, &quot;Ben&quot;, &quot;Jack&quot;]
    // using a variable assignment
    list.each{firstName-&amp;gt;
      println firstName
    }
    // using the it variable
    list.each{println it}

	5.times {println &quot;Times + $it &quot;}
	1.upto(3) {println &quot;Up + $it &quot;}
	4.downto(1) {print &quot;Down + $it &quot;}
	def sum = 0
	1.upto(100) {sum += 1}
	print sum
	
	(1..6).each {print &quot;Range $it&quot;}

	for (i in 0..9) {
      println (&quot;Hello $i&quot;)
    }
  }  
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Groovy对Java循环结构作了如下的修整：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于 for 循环：除了传统三表达式的 for 循环和用于迭代的 for each 循环外，Groovy 允许 for 循环遍历一个范围（Range），例如 &lt;code&gt;for (i in 1..10)&lt;/code&gt;，表示循环10次，i在1至10之间取值；&lt;/li&gt;
  &lt;li&gt;对于整数，Groovy 增加了如下几个方法来进行循环：
    &lt;ul&gt;
      &lt;li&gt;upto：&lt;code&gt;n.upto(m)&lt;/code&gt; 函数，表示循环 m- n 次，并且会有一个循环变量it，从 n 开始，每次循环增加1，直到 m。循环体写在upto方法之后大括号中，表示一个闭包，在闭包中，it 作为循环变量，值从 a 增长到 n；&lt;/li&gt;
      &lt;li&gt;times：&lt;code&gt;n.times&lt;/code&gt; 函数，表示循环 n 次，循环变量 it 从0开始到n结束。&lt;/li&gt;
      &lt;li&gt;step：&lt;code&gt;n.step(x, y)&lt;/code&gt; 函数，表示循环变量从 n 开始到 x 结束，每次循环后循环变量增加 y，所以整个循环次数为 &lt;code&gt;(x - n) / y &lt;/code&gt;次；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-8&quot;&gt;3.7 集合&lt;/h2&gt;

&lt;h1 id=&quot;section-9&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://blog.csdn.net/mousebaby808/article/details/7093946&quot;&gt;Groovy基本语法(1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://blog.csdn.net/mousebaby808/article/details/7093950&quot;&gt;Groovy基本语法(2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://blog.csdn.net/mousebaby808/article/details/7097114&quot;&gt;Groovy基本语法(3)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[4] &lt;a href=&quot;http://www.vogella.com/tutorials/Groovy/article.html&quot;&gt;Groovy with Eclipse - Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/09/05/about-groovy.html</link>
      <guid>http://blog.javachen.com/2014/09/05/about-groovy.html</guid>
      <pubDate>2014-09-05T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>安装Azkaban</title>
      <description>&lt;p&gt;Azkaban 是由 Linkedin 开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban 定义了一种 KV 文件格式来建立任务之间的依赖关系，并提供一个易于使用的 web 用户界面维护和跟踪你的工作流。&lt;/p&gt;

&lt;p&gt;Azkaban 官网地址：&lt;a href=&quot;http://azkaban.github.io/&quot;&gt;http://azkaban.github.io/&lt;/a&gt;&lt;br /&gt;
Azkaban 的下载地址：&lt;a href=&quot;http://azkaban.github.io/downloads.html&quot;&gt;http://azkaban.github.io/downloads.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Azkaban 包括三个关键组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;关系数据库&lt;/code&gt;：使用 Mysql数据库，主要用于保存流程、权限、任务状态、任务计划等信息。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;AzkabanWebServer&lt;/code&gt;：为用户提供管理留存、任务计划、权限等功能。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;AzkabanExecutorServer&lt;/code&gt;：执行任务，并把任务执行的输出日志保存到 Mysql；可以同时启动多个 AzkabanExecutorServer ，他们通过 mysql 获取流程状态来协调工作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://azkaban.github.io/azkaban/docs/2.5/images/azkaban2overviewdesign.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在 2.5 版本之后，Azkaban 提供了两种模式来安装：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一种是 standalone 的 “solo-server” 模式；&lt;/li&gt;
  &lt;li&gt;另一种是两个 server 的模式，分别为 AzkabanWebServer 和 AzkabanExecutorServer。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里主要介绍第二种模式的安装方法。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装过程&lt;/h1&gt;

&lt;h2 id=&quot;mysql&quot;&gt;1.1 安装 MySql&lt;/h2&gt;

&lt;p&gt;目前 Azkaban 只支持 MySql ，故需安装 MySql 服务器，安装 MySql 的过程这里不作介绍。&lt;/p&gt;

&lt;p&gt;安装之后，创建 azkaban 数据库，并创建 azkaban 用户，密码为 azkaban，并设置权限。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;# Example database creation command, although the db name doesn&#39;t need to be &#39;azkaban&#39;
mysql&amp;gt; CREATE DATABASE azkaban;
# Example database creation command. The user name doesn&#39;t need to be &#39;azkaban&#39;
mysql&amp;gt; CREATE USER &#39;azkaban&#39;@&#39;%&#39; IDENTIFIED BY &#39;azkaban&#39;;
# Replace db, username with the ones created by the previous steps.
mysql&amp;gt; GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to &#39;azkaban&#39;@&#39;%&#39; WITH GRANT OPTION;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 &lt;code&gt;/etc/my.cnf&lt;/code&gt; 文件，设置 &lt;code&gt;max_allowed_packet&lt;/code&gt; 值：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mysqld]
...
max_allowed_packet=1024M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后重启 MySql。&lt;/p&gt;

&lt;p&gt;解压缩 azkaban-sql-2.5.0.tar.gz文件，并进入到 azkaban-sql-script目录，然后进入 mysql 命令行模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mysql -uazkaban -pazkaban
mysql&amp;gt; use azkaban
mysql&amp;gt; source create-all-sql-2.5.0.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;azkaban-web-server&quot;&gt;1.2 安装 azkaban-web-server&lt;/h2&gt;

&lt;p&gt;解压缩 azkaban-web-server-2.5.0.tar.gz，创建 SSL 配置，命令：&lt;code&gt;keytool -keystore keystore -alias jetty -genkey -keyalg RSA&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;完成上述工作后，将在当前目录生成 keystore 证书文件，将 keystore 考贝到 azkaban web 目录中。&lt;/p&gt;

&lt;p&gt;修改 azkaban web 服务器配置，主要包括：&lt;/p&gt;

&lt;p&gt;a. 修改时区和首页名称:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;# Azkaban Personalization Settings
azkaban.name=ETL Task
azkaban.label=By BI
azkaban.color=#FF3601
azkaban.default.servlet.path=/index
web.resource.dir=web/
default.timezone.id=Asia/Shanghai
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b. 修改 MySql 数据库配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;database.type=mysql
mysql.port=3306
mysql.host=localhost
mysql.database=azkaban
mysql.user=azkaban
mysql.password=azkaban
mysql.numconnections=100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c. 修改 Jetty 服务器属性，包括 keystore 的相关配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;# Azkaban Jetty server properties.
jetty.hostname=0.0.0.0
jetty.maxThreads=25
jetty.ssl.port=8443
jetty.port=8081
jetty.keystore=keystore
jetty.password=redhat
jetty.keypassword=redhat
jetty.truststore=keystore
jetty.trustpassword=redhat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;d. 修改邮件设置（可选）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;# mail settings
mail.sender=admin@javachen.com
mail.host=javachen.com
mail.user=admin
mail.password=admin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;azkaban-executor-server&quot;&gt;1.3 安装 azkaban-executor-server&lt;/h2&gt;

&lt;p&gt;解压缩 azkaban-executor-server-2.5.0.tar.gz，然后修改配置文件，包括：&lt;/p&gt;

&lt;p&gt;a. 修改时区为：&lt;code&gt;default.timezone.id=Asia/Shanghai&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;b. 修改 MySql 数据库配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;database.type=mysql
mysql.port=3306
mysql.host=localhost
mysql.database=azkaban
mysql.user=azkaban
mysql.password=azkaban
mysql.numconnections=100
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1.4 用户设置&lt;/h2&gt;

&lt;p&gt;进入 azkaban web 服务器 conf 目录，修改 azkaban-users.xml ，增加管理员用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;azkaban-users&amp;gt;
        &amp;lt;user username=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; groups=&quot;azkaban&quot; /&amp;gt;
        &amp;lt;user username=&quot;metrics&quot; password=&quot;metrics&quot; roles=&quot;metrics&quot;/&amp;gt;
        &amp;lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot; /&amp;gt;
        &amp;lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot; /&amp;gt;
        &amp;lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&amp;gt;
&amp;lt;/azkaban-users&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;1.5 启动服务&lt;/h2&gt;

&lt;p&gt;azkaban-web-server，需要在 azkaban-web-server 目录下执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sh bin/azkaban-web-start.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;azkaban-executor-server，需要在 azkaban-executor-server 目录下执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sh bin/azkaban-executor-start.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;1.6 配置插件&lt;/h2&gt;

&lt;p&gt;下载 &lt;a href=&quot;https://s3.amazonaws.com/azkaban2/azkaban-plugins/2.5.0/azkaban-hdfs-viewer-2.5.0.tar.gz&quot;&gt;HDFS Browser&lt;/a&gt; 插件，解压然后重命名为 hdfs，然后将其拷贝到 azkaban-web-server/plugins/viewer 目录下。&lt;/p&gt;

&lt;p&gt;下载 &lt;a href=&quot;https://s3.amazonaws.com/azkaban2/azkaban-plugins/2.5.0/azkaban-jobtype-2.5.0.tar.gz&quot;&gt;Job Types Plugins&lt;/a&gt; 插件，解压然后重命名为 jobtype，然后将其拷贝到 azkaban-executor-server/plugins/ 目录下，然后修改以下文件，设置 &lt;code&gt;hive.home&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;plugins/jobtypes/commonprivate.properties&lt;/li&gt;
  &lt;li&gt;plugins/jobtypes/common.properties&lt;/li&gt;
  &lt;li&gt;plugins/jobtypes/hive/plugin.properties&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;说明：&lt;/p&gt;

  &lt;p&gt;在实际使用中，这些插件都没有配置成功，故最后的生产环境没有使用这些插件，而是基于最基本的 command 或 script 方式来编写作业。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-4&quot;&gt;1.7 生产环境使用&lt;/h2&gt;

&lt;p&gt;这部分内容详细说明见 &lt;a href=&quot;/2014/10/23/hive-warehouse-in-2014.html&quot;&gt;当前数据仓库建设过程&lt;/a&gt; 一文中的任务调度这一章节内容。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/08/25/install-azkaban.html</link>
      <guid>http://blog.javachen.com/2014/08/25/install-azkaban.html</guid>
      <pubDate>2014-08-25T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>升级cdh4到cdh5</title>
      <description>&lt;p&gt;本文主要记录从CDH4升级到CDH5的过程和遇到的问题，当然本文同样适用于CDH5低版本向最新版本的升级。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 不兼容的变化&lt;/h1&gt;

&lt;p&gt;升级前，需要注意 cdh5 有哪些不兼容的变化，具体请参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_rn_incompatible_changes.html&quot;&gt;Apache Hadoop Incompatible Changes&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 升级过程&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2.1. 备份数据和停止所有服务&lt;/h2&gt;

&lt;h3 id=&quot;namenode-&quot;&gt;2.1.1 让 namenode 进入安全模式&lt;/h3&gt;

&lt;p&gt;在NameNode或者配置了 HA 中的 active NameNode上运行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hdfs dfsadmin -safemode enter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保存 fsimage：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hdfs dfsadmin -saveNamespace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果使用了kerberos，则先获取hdfs用户凭证，再执行下面代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.COM
$ hdfs dfsadmin -safemode enter
$ hdfs dfsadmin -saveNamespace
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.1.2 备份配置文件、数据库和其他重要文件&lt;/h3&gt;

&lt;p&gt;根据你安装的cdh组件，可能需要备份的配置文件包括：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/etc/hadoop/conf
/etc/hive/conf
/etc/hbase/conf
/etc/zookeeper/conf
/etc/impala/conf
/etc/spark/conf
/etc/sentry/conf
/etc/default/impala
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-4&quot;&gt;2.1.3 停止所有服务&lt;/h3&gt;

&lt;p&gt;在每个节点上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for x in `cd /etc/init.d ; ls hadoop-*` ; do sudo service $x stop ; done
for x in `cd /etc/init.d ; ls hbase-*` ; do sudo service $x stop ; done
for x in `cd /etc/init.d ; ls hive-*` ; do sudo service $x stop ; done
for x in `cd /etc/init.d ; ls zookeeper-*` ; do sudo service $x stop ; done
for x in `cd /etc/init.d ; ls hadoop-*` ; do sudo service $x stop ; done
for x in `cd /etc/init.d ; ls impala-*` ; do sudo service $x stop ; done
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-5&quot;&gt;2.1.4 在每个节点上查看进程&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ps -aef | grep java
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hdfs-&quot;&gt;2.2. 备份 hdfs 元数据（可选，防止在操作过程中对数据的误操作）&lt;/h2&gt;

&lt;p&gt;a，查找本地配置的文件目录（属性名为 &lt;code&gt;dfs.name.dir&lt;/code&gt; 或者 &lt;code&gt;dfs.namenode.name.dir或者hadoop.tmp.dir&lt;/code&gt; ）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;grep -C1 hadoop.tmp.dir /etc/hadoop/conf/hdfs-site.xml

#或者
grep -C1 dfs.namenode.name.dir /etc/hadoop/conf/hdfs-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过上面的命令，可以看到类似以下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;/data/dfs/nn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b，对hdfs数据进行备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd /data/dfs/nn
tar -cvf /root/nn_backup_data.tar .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;yum-&quot;&gt;2.3. 更新 yum 源&lt;/h2&gt;

&lt;p&gt;如果你使用的是官方的远程yum源，则下载 &lt;a href=&quot;http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo&quot;&gt;cloudera-cdh5.repo&lt;/a&gt; 文件到 /etc/yum.repos.d 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo -P /etc/yum.repos.d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你使用本地yum，则需要从 http://archive-primary.cloudera.com/cdh5/repo-as-tarball 下载最新的压缩包文件，然后解压到对于目录，以 CDH5.4 版本为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /var/ftp/pub
$ rm -rf cdh
$ wget http://archive-primary.cloudera.com/cdh5/repo-as-tarball/5.4.0/cdh5.4.0-centos6.tar.gz
$ tar zxvf cdh5.4.0-centos6.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在 /etc/yum.repos.d 目录创建一个 repos 文件，指向本地yum源即可，详细过程请自行百度。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;2.4. 升级组件&lt;/h2&gt;

&lt;p&gt;在所有节点上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo yum update hadoop* hbase* hive* zookeeper* bigtop* impala* spark* llama* lzo* sqoop* parquet* sentry* avro* mahout* -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动ZooKeeper集群，如果配置了 HA，则在原来的所有 Journal Nodes 上启动 hadoop-hdfs-journalnode：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 在安装zookeeper-server的节点上运行
$ /etc/init.d/zookeeper-server start

# 在安装zkfc的节点上运行
$ /etc/init.d/hadoop-hdfs-zkfc

# 在安装journalnode的节点上运行
$ /etc/init.d/hadoop-hdfs-journalnode start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hdfs--1&quot;&gt;2.5. 更新 hdfs 元数据&lt;/h2&gt;

&lt;p&gt;在NameNode或者配置了 HA 中的 active NameNode上运行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo service hadoop-hdfs-namenode upgrade
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看日志，检查是否完成升级，例如查看日志中是否出现&lt;code&gt;/var/lib/hadoop-hdfs/cache/hadoop/dfs/&amp;lt;name&amp;gt; is complete&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo tail -f /var/log/hadoop-hdfs/hadoop-hdfs-namenode-&amp;lt;hostname&amp;gt;.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果配置了 HA，在另一个 NameNode 节点上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 输入 Y
$ sudo -u hdfs hdfs namenode -bootstrapStandby
=====================================================
Re-format filesystem in Storage Directory /data/dfs/nn ? (Y or N)
$ sudo service hadoop-hdfs-namenode start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动所有的 DataNode：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo service hadoop-hdfs-datanode start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开 web 界面查看 hdfs 文件是否都存在。&lt;/p&gt;

&lt;p&gt;待集群稳定运行一段时间，可以完成升级：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop dfsadmin -finalizeUpgrade
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;yarn&quot;&gt;2.6. 更新 YARN&lt;/h2&gt;

&lt;p&gt;更新 YARN 需要注意以下节点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;yarn-site.xml&lt;/code&gt; 中做如下改变：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;yarn.nodemanager.aux-services&lt;/code&gt;的值从&lt;code&gt;mapreduce.shuffle&lt;/code&gt; 修改为  &lt;code&gt;mapreduce_shuffle&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/code&gt; 改名为 &lt;code&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;yarn.resourcemanager.resourcemanager.connect.max.wait.secs&lt;/code&gt; 修改为 &lt;code&gt;yarn.resourcemanager.connect.max-wait.secs&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;yarn.resourcemanager.resourcemanager.connect.retry_interval.secs&lt;/code&gt; 修改为 &lt;code&gt;yarn.resourcemanager.connect.retry-interval.secs&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.resourcemanager.am. max-retries&lt;/code&gt; 修改为 &lt;code&gt;yarn.resourcemanager.am.max-attempts&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;yarn.application.classpath&lt;/code&gt; 中的环境变量 &lt;code&gt;YARN_HOME&lt;/code&gt; 属性修改为&lt;code&gt; HADOOP_YARN_HOME&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后在启动 YARN 的相关服务。&lt;/p&gt;

&lt;h2 id=&quot;hbase&quot;&gt;2.7. 更新 HBase&lt;/h2&gt;

&lt;p&gt;升级 HBase 之前，先启动 zookeeper。&lt;/p&gt;

&lt;p&gt;在启动hbase-master进程和hbase-regionserver进程之前，更新 HBase：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hbase upgrade -execute
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你使用了 phoenix，则请删除 HBase lib 目录下对应的 phoenix 的 jar 包。&lt;/p&gt;

&lt;p&gt;启动 HBase：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service hbase-master start
$ service hbase-regionserver start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hive&quot;&gt;2.8. 更新 hive&lt;/h2&gt;

&lt;p&gt;在启动hive之前，进入 &lt;code&gt;/usr/lib/hive/bin&lt;/code&gt; 执行下面命令升级元数据（这里元数据使用的是postgres数据库）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /usr/lib/hive/bin
# ./schematool -dbType 数据库类型 -upgradeSchemaFrom 版本号
# 升级之前 hive 版本为 0.14.0，下面命令会运行  /usr/lib/hive/scripts/metastore/upgrade/postgres/upgrade-0.14.0-to-1.1.0.postgres.sql
$ ./schematool -dbType postgres -upgradeSchemaFrom 0.14.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确认 /etc/hive/conf/hive-site.xml 和 /etc/hive/conf/hive-env.sh 是否需要修改，例如 /etc/hive/conf/hive-env.sh 配置了如下参数，需要修改到 cdh-5.2 对应的版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 请修改到 cdh5.4对应的 jar 包
$ export HIVE_AUX_JARS_PATH=/usr/lib/hive/lib/hive-contrib-1.1.0-cdh5.4.0.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改完之后，请同步到其他节点。&lt;/p&gt;

&lt;p&gt;然后启动 hive 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service hive-metastore start
$ service hive-server2 start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sentry&quot;&gt;2.9 更新Sentry&lt;/h2&gt;

&lt;p&gt;如果你从 CDH 5.2.0 以前的版本更新到 CDH 5.2.0 以后的版本，请更新sentry的元数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bin/sentry --command schema-tool --conffile &amp;lt;sentry-site.xml&amp;gt; --dbType &amp;lt;db-type&amp;gt; --upgradeSchema
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-7&quot;&gt;3. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_ig_upgrade_command_line.html&quot;&gt;Upgrading Unmanaged CDH Using the Command Line&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_rn_incompatible_changes.html&quot;&gt;CDH Incompatible Changes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/08/19/upgrading-from-cdh4-to-cdh5.html</link>
      <guid>http://blog.javachen.com/2014/08/19/upgrading-from-cdh4-to-cdh5.html</guid>
      <pubDate>2014-08-19T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Sqoop导入关系数据库到Hive</title>
      <description>&lt;p&gt;Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。本文以 mysql 数据库为例，实现关系数据库导入到 hdfs 和 hive。&lt;/p&gt;

&lt;h1 id=&quot;sqoop&quot;&gt;1. 安装 Sqoop&lt;/h1&gt;

&lt;p&gt;使用 rpm 安装即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install sqoop sqoop-metastore -y
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;安装完之后需要下载 mysql jar 包到 sqoop 的 lib 目录。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里使用 hive 的 metastore 的 mysql 数据库作为关系数据库，以 TBLS 表为例，该表结构和数据如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;mysql&amp;gt; select * from TBLS limit 3;
+------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+
|TBL_ID|CREATE_TIME|DB_ID|LAST_ACCESS_TIME|OWNER|RETENTI | SD_ID| TBL_NAME| TBL_TYPE       |VIEW_EXPANDED_TEXT| VIEW_ORIGINAL_TEXT|
+------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+
|    34|1406784308 |    8|               0|root |       0|    45| test1   | EXTERNAL_TABLE | NULL             | NULL              |
|    40|1406797005 |    9|               0|root |       0|    52| test2   | EXTERNAL_TABLE | NULL             | NULL              |
|    42|1407122307 |    7|               0|root |       0|    59| test3   | EXTERNAL_TABLE | NULL             | NULL              |
+------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;2. 使用&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2.1 命令说明&lt;/h2&gt;

&lt;p&gt;查看 sqoop 命令说明：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop help
usage: sqoop COMMAND [ARGS]

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  version            Display version information

See &#39;sqoop help COMMAND&#39; for information on a specific command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以查看某一个命令的使用说明：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --help
$ sqoop help import
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以使用别名来代替 &lt;code&gt;sqoop (toolname)&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop-import
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sqoop import 的一个示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你还可以使用 &lt;code&gt;--options-file&lt;/code&gt; 来传入一个文件，使用这种方式可以重用一些配置参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop --options-file /users/homer/work/import.txt --table TEST
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/users/homer/work/import.txt 文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import
--connect
jdbc:mysql://192.168.56.121:3306/metastore
--username
hiveuser
--password 
redhat
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hdfs&quot;&gt;2.2 导入数据到 hdfs&lt;/h1&gt;

&lt;p&gt;使用 sqoop-import 命令可以从关系数据库导入数据到 hdfs。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --target-dir /user/hive/result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mysql jdbc url 请使用 ip 地址&lt;/li&gt;
  &lt;li&gt;如果重复执行，会提示目录已经存在，可以手动删除&lt;/li&gt;
  &lt;li&gt;如果不指定 &lt;code&gt;--target-dir&lt;/code&gt;，导入到用户家目录下的 TBLS 目录&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;你还可以指定其他的参数：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--append&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--as-avrodatafile&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;将数据导入到一个Avro数据文件中&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--as-sequencefile&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;将数据导入到一个sequence文件中&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--as-textfile&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--boundary-query &amp;lt;statement&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：&lt;code&gt;--boundary-query &#39;select id,no from t where id = 3&#39;&lt;/code&gt;，表示导入的数据为id=3的记录，或者 &lt;code&gt;select min(&amp;lt;split-by&amp;gt;), max(&amp;lt;split-by&amp;gt;) from &amp;lt;table name&amp;gt;&lt;/code&gt;，注意查询的字段中不能有数据类型为字符串的字段，否则会报错&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--columns&amp;lt;col,col&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;指定要导入的字段值，格式如：&lt;code&gt;--columns id,username&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--direct&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--direct-split-size&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--inline-lob-limit&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;设定大对象数据类型的最大值&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-m,--num-mappers&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--query，-e &amp;lt;sql&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;从查询结果中导入数据，该参数使用时必须指定&lt;code&gt;–target-dir&lt;/code&gt;、&lt;code&gt;–hive-table&lt;/code&gt;，在查询语句中一定要有where条件且在where条件中需要包含 &lt;code&gt;\$CONDITIONS&lt;/code&gt;，示例：&lt;code&gt;--query &#39;select * from t where \$CONDITIONS &#39; --target-dir /tmp/t –hive-table t &lt;/code&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--split-by &amp;lt;column&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;表的列名，用来切分工作单元，一般后面跟主键ID&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--table &amp;lt;table-name&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;关系数据库表名，数据从该表中获取&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--delete-target-dir&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;删除目标目录&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--target-dir &amp;lt;dir&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;指定hdfs路径&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--warehouse-dir &amp;lt;dir&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;与 &lt;code&gt;--target-dir&lt;/code&gt; 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--where&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;从关系数据库导入数据时的查询条件，示例：&lt;code&gt;--where &quot;id = 2&quot;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;-z,--compress&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--compression-codec&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Hadoop压缩编码，默认是gzip&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--null-string &amp;lt;null-string&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;可选参数，如果没有指定，则字符串null将被使用&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--null-non-string &amp;lt;null-string&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;可选参数，如果没有指定，则字符串null将被使用&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;示例程序：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --columns &quot;tbl_id,create_time&quot; --where &quot;tbl_id &amp;gt; 1&quot; --target-dir /user/hive/result
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;sql-&quot;&gt;使用 sql 语句&lt;/h3&gt;

&lt;p&gt;参照上表，使用 sql 语句查询时，需要指定 &lt;code&gt;$CONDITIONS&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --query &#39;SELECT * from TBLS where \$CONDITIONS &#39; --split-by tbl_id -m 4 --target-dir /user/hive/result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面命令通过 &lt;code&gt;-m 1&lt;/code&gt; 控制并发的 map 数。&lt;/p&gt;

&lt;h3 id=&quot;direct-&quot;&gt;使用 direct 模式：&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --delete-target-dir --direct --default-character-set UTF-8 --target-dir /user/hive/result
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-2&quot;&gt;指定文件输出格式：&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; --delete-target-dir  --target-dir /user/hive/result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候查看 hdfs 中数据(观察分隔符是否为制表符)：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -ls result
Found 5 items
-rw-r--r--   3 root hadoop          0 2014-08-04 16:07 result/_SUCCESS
-rw-r--r--   3 root hadoop         69 2014-08-04 16:07 result/part-m-00000
-rw-r--r--   3 root hadoop          0 2014-08-04 16:07 result/part-m-00001
-rw-r--r--   3 root hadoop        142 2014-08-04 16:07 result/part-m-00002
-rw-r--r--   3 root hadoop         62 2014-08-04 16:07 result/part-m-00003

$ hadoop fs -cat result/part-m-00000
34	1406784308	8	0	root	0	45	test1	EXTERNAL_TABLE	null	null	null

$ hadoop fs -cat result/part-m-00002
40	1406797005	9	0	root	0	52	test2	EXTERNAL_TABLE	null	null	null
42	1407122307	7	0	root	0	59	test3	EXTERNAL_TABLE	null	null	null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定空字符串：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; --delete-target-dir --null-string &#39;\\N&#39; --null-non-string &#39;\\N&#39; --target-dir /user/hive/result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果需要指定压缩：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; --delete-target-dir --null-string &#39;\\N&#39; --null-non-string &#39;\\N&#39; --compression-codec &quot;com.hadoop.compression.lzo.LzopCodec&quot; --target-dir /user/hive/result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;附：可选的文件参数如下表。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--enclosed-by &amp;lt;char&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;给字段值前后加上指定的字符，比如双引号，示例：&lt;code&gt;--enclosed-by &#39;\&quot;&#39;&lt;/code&gt;，显示例子：”3”,”jimsss”,”dd@dd.com”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--escaped-by &amp;lt;char&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;给双引号作转义处理，如字段值为”测试”，经过 &lt;code&gt;--escaped-by &quot;\\&quot;&lt;/code&gt; 处理后，在hdfs中的显示值为：&lt;code&gt;\&quot;测试\&quot;&lt;/code&gt;，对单引号无效&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--fields-terminated-by &amp;lt;char&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;设定每个字段是以什么符号作为结束的，默认是逗号，也可以改为其它符号，如句号&lt;code&gt;.&lt;/code&gt;，示例如：&lt;code&gt;--fields-terminated-by&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--lines-terminated-by &amp;lt;char&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;设定每条记录行之间的分隔符，默认是换行串，但也可以设定自己所需要的字符串，示例如：&lt;code&gt;--lines-terminated-by &quot;#&quot;&lt;/code&gt; 以#号分隔&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--mysql-delimiters&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Mysql默认的分隔符设置，字段之间以&lt;code&gt;,&lt;/code&gt;隔开，行之间以换行&lt;code&gt;\n&lt;/code&gt;隔开，默认转义符号是&lt;code&gt;\&lt;/code&gt;，字段值以单引号&lt;code&gt;&#39;&lt;/code&gt;包含起来。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--optionally-enclosed-by &amp;lt;char&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;enclosed-by是强制给每个字段值前后都加上指定的符号，而&lt;code&gt;--optionally-enclosed-by&lt;/code&gt;只是给带有双引号或单引号的字段值加上指定的符号，故叫可选的&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;hive-&quot;&gt;2.3 创建 hive 表&lt;/h1&gt;

&lt;p&gt;生成与关系数据库表的表结构对应的HIVE表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop create-hive-table --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS 
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--hive-home &amp;lt;dir&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hive的安装目录，可以通过该参数覆盖掉默认的hive目录&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--hive-overwrite&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;覆盖掉在hive表中已经存在的数据&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--create-hive-table&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;默认是false，如果目标表已经存在了，那么创建任务会失败&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--hive-table&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;后面接要创建的hive表&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--table&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定关系数据库表名&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;hive&quot;&gt;2.4 导入数据到 hive&lt;/h1&gt;

&lt;p&gt;执行下面的命令会将 mysql 中的数据导入到 hdfs 中，然后创建一个hive 表，最后再将 hdfs 上的文件移动到 hive 表的目录下面。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --delete-target-dir
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;可以在 hive 的表名前面指定数据库名称&lt;/li&gt;
  &lt;li&gt;可以通过 &lt;code&gt;--create-hive-table&lt;/code&gt; 创建表，如果表已经存在则会执行失败&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接下来可以查看 hive 中的数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hive -e &#39;select * from dw_srclog.tbls&#39;
34	1406784308	8	0	root	0	45	test1	EXTERNAL_TABLE	null	null	NULL
40	1406797005	9	0	root	0	52	test2	EXTERNAL_TABLE	null	null	NULL
42	1407122307	7	0	root	0	59	test3	EXTERNAL_TABLE	null	null	NULL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接查看文件内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hadoop fs -cat /user/hive/warehouse/dw_srclog.db/tbls/part-m-00000
34140678430880root045go_goodsEXTERNAL_TABLEnullnullnull
40140679700590root052merchantEXTERNAL_TABLEnullnullnull
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面可见，数据导入到 hive 中之后分隔符为默认分隔符，参考上文你可以通过设置参数指定其他的分隔符。&lt;/p&gt;

&lt;p&gt;另外，Sqoop 默认地导入空值（NULL）为 null 字符串，而 hive 使用 \N 去标识空值（NULL），故你在 import 或者 export 时候，需要做相应的处理。在 import 时，使用如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import  ... --null-string &#39;\\N&#39; --null-non-string &#39;\\N&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在导出时，使用下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import  ... --input-null-string &#39;&#39; --input-null-non-string &#39;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个完整的例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --null-string &#39;\\N&#39; --null-non-string &#39;\\N&#39; --compression-codec &quot;com.hadoop.compression.lzo.LzopCodec&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;2.5 增量导入&lt;/h1&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--check-column (col)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;用来作为判断的列名，如id&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--incremental (mode)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--last-value (value)&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;hdfs-&quot;&gt;2.6 合并 hdfs 文件&lt;/h1&gt;

&lt;p&gt;将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中，示例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sqoop merge –new-data /test/p1/person –onto /test/p2/person –target-dir /test/merged –jar-file /opt/data/sqoop/person/Person.jar –class-name Person –merge-key id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中，&lt;code&gt;–class-name&lt;/code&gt; 所指定的 class 名是对应于 Person.jar 中的 Person 类，而 Person.jar 是通过 Codegen 生成的&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;参数&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--new-data &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能优先保留的，原则上一般是存放越新数据的目录就对应这个参数。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--onto &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能被更新数据替换掉的，原则上一般是存放越旧数据的目录就对应这个参数。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--merge-key &amp;lt;col&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;合并键，一般是主键ID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--jar-file &amp;lt;file&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;合并时引入的jar包，该jar包是通过Codegen工具生成的jar包&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--class-name &amp;lt;class&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;对应的表名或对象名，该class类是包含在jar包中的。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;code&gt;--target-dir &amp;lt;path&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;合并后的数据在HDFS里的存放目录&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;section-4&quot;&gt;3. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zihou.me/html/2014/01/28/9114.html&quot;&gt;Sqoop中文手册&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/08/04/import-data-to-hive-with-sqoop.html</link>
      <guid>http://blog.javachen.com/2014/08/04/import-data-to-hive-with-sqoop.html</guid>
      <pubDate>2014-08-04T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>2014年7月总结</title>
      <description>&lt;p&gt;在休息了将近三个月之后，7月9日终于开始上班了，新的工作还是和 hadoop 相关。7月主要的工作内容如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;搭建新的 hadoop 集群，hadoop 版本为 CDH4.7.0，并配置 NameNode 的 QJM HA 方案。配置 HA 方法见 &lt;a href=&quot;/2014/07/18/install-hdfs-ha-in-cdh.html&quot;&gt;CDH 中配置 HDFS HA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;购买了三本书：
    &lt;ul&gt;
      &lt;li&gt;mahout实战&lt;/li&gt;
      &lt;li&gt;机器学习实战&lt;/li&gt;
      &lt;li&gt;这才是搜索引擎&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;调研了 &lt;a href=&quot;&quot;&gt;flume-ng&lt;/a&gt; 日志采集方案
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2013-12-18/2817838-big-data-practice-in-dianping&quot;&gt;大众点评的大数据实践&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/&quot;&gt;analyzing-twitter-data-with-hadoop&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://cuddletech.com/?p=795&quot;&gt;Hadoop Analysis of Apache Logs Using Flume-NG, Hive and Pig&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.lopakalogic.com/articles/hadoop-articles/log-files-flume-hive/&quot;&gt;Log Files with Flume and Hive&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://sskaje.me/2013/12/solving-small-files-problem-cdh4/#.U8I48Y2SywI&quot;&gt;Solving Small Files Problem on CDH4&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/weijonathan/article/details/18301321&quot;&gt;flume-ng+Kafka+Storm+HDFS 实时系统搭建&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://gdcsy.blog.163.com/blog/static/127343609201452532339253/&quot;&gt;flume-ng+Hadoop实现日志收集&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://tech.meituan.com/mt-log-system-arch.html&quot;&gt;基于Flume的美团日志收集系统(一)架构和设计&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;查看关系数据库数据同步到 hadoop 的相关方法&lt;br /&gt;
 	- a. 查看sqoop 和 sqoop2 的相关文档&lt;br /&gt;
	- b. 测试使用kettle 连接 hdfs 和 hive&lt;/li&gt;
  &lt;li&gt;熟悉原来 hadoop 数据同步和调度的代码逻辑，查看一些开源的任务调度框架：
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/alibaba/zeus&quot;&gt;https://github.com/alibaba/zeus&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/thieman/dagobah&quot;&gt;https://github.com/thieman/dagobah&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/azkaban/azkaban&quot;&gt;https://github.com/azkaban/azkaban&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://demo.gethue.com/&quot;&gt;http://demo.gethue.com/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;熟悉 phoenix 用法（phoenix 为 HBase 提供 sql 支持），Phoenix 快速入门见&lt;a href=&quot;/2014/07/28/phoenix-quick-start.html&quot;&gt;Phoenix Quick Start&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;测试 impala 是否支持 hive 的自定义文件格式，见&lt;a href=&quot;/2014/07/29/new-features-in-impala.html&quot;&gt;Impala 新特性&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;完善下载日志文件并上传到 hive 的 python 脚本，见&lt;a href=&quot;/2014/07/25/collect-log-to-hive.html&quot;&gt;采集日志到 hive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外萌生了一些想法，例如，开发一个java web 项目，支持 hive 查询、任务调度、查看 hdfs 等功能。可参考的资源：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dianping/hiveweb&quot;&gt;https://github.com/dianping/hiveweb&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dianping/polestar&quot;&gt;https://github.com/dianping/polestar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/07/31/summary-of-july-in-2014.html</link>
      <guid>http://blog.javachen.com/2014/07/31/summary-of-july-in-2014.html</guid>
      <pubDate>2014-07-31T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Impala新特性</title>
      <description>&lt;p&gt;本文主要整理一下 Impala 每个版本的新特性，方便了解 Impala 做了哪些改进、修复了哪些 bug。&lt;/p&gt;

&lt;p&gt;Impala 目前最新版本为 1.4.0，其下载地址为：&lt;a href=&quot;http://archive.cloudera.com/impala/redhat/6/x86_64/impala/&quot;&gt;http://archive.cloudera.com/impala/redhat/6/x86_64/impala/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;不得不说的事情：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.3.1 用于 CDH4&lt;/li&gt;
  &lt;li&gt;1.4.0 用于 CDH5&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;1.4.0&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_decimal.html#decimal&quot;&gt;CDH5&lt;/a&gt; 中增加 DECIMAL 数据类型，可以设置精度，其语法为：&lt;code&gt;DECIMAL[(precision[,scale])]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;CDH5 中，impala 可以使用 HDFS 缓存特性加快频繁访问的数据的速度，减少 cpu 使用率。当数据缓存到 hdfs cache 中时，impala 可以直接从缓存中读取数据而不需要读磁盘并且减少额外的内存拷贝。
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_hdfs_caching.html&quot;&gt;Centralized Cache Management in HDFS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;impala 中使用 HDFS Caching，参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_perf_hdfs_caching.html#hdfs_caching&quot;&gt;sing HDFS Caching with Impala (CDH 5 Only)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Impala 可以使用基于 Sentry 的授权策略，详细说明可以参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_authorization.html#authorization&quot;&gt;Enabling Sentry Authorization for Impala&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Impala 支持其他 hadoop 组件创建的 Parquet 格式的文件，你可以在建表语句中指定 Parquet 格式，Impala 中创建 parquet 格式的表，请参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_parquet.html#parquet_ddl_unique_1&quot;&gt;Using the Parquet File Format with Impala Tables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ORDER BY 查询不再要求一个 limit 语句，如果需要排序的结果集的大小超过了内存限制，则会使用临时的磁盘空间用于排序，ORDER BY 语法为：&lt;code&gt;ORDER BY col1 [, col2 ...] [ASC | DESC] [NULLS FIRST | NULLS LAST]&lt;/code&gt;，详细说明见：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_order_by.html#order_by&quot;&gt;ORDER BY Clause&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LDAP 连接可以使用 SSL 或者 TLS 加密，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_ldap.html#ldap&quot;&gt;Enabling LDAP Authentication for Impala&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;增加以下内建函数：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;EXTRACT()&lt;/code&gt;，用于从一个 TIMESTAMP 字段返回一个 date 或者 time 的字段，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_datetime_functions.html#datetime_functions&quot;&gt;Date and Time Functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;TRUNC()&lt;/code&gt;，用于将一个 date/time 类型的字段裁剪为一个特定格式的值，如年、月、日、小时等等，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_datetime_functions.html#datetime_functions&quot;&gt;Date and Time Functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ADD_MONTHS()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;ROUND()&lt;/code&gt;，对 DECIMAL 类型的值四舍五入，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_math_functions.html#math_functions&quot;&gt;Mathematical Functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_stddev.html#stddev&quot;&gt;  &lt;code&gt;STDDEV&lt;/code&gt;, &lt;code&gt;STDDEV_SAMP&lt;/code&gt;, &lt;code&gt;STDDEV_POP&lt;/code&gt; Functions&lt;/a&gt; 和 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_variance.html#variance&quot;&gt;&lt;code&gt;VARIANCE&lt;/code&gt;, &lt;code&gt;VARIANCE_SAMP&lt;/code&gt;, &lt;code&gt;VARIANCE_POP&lt;/code&gt; Functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;MAX_INT()&lt;/code&gt;、&lt;code&gt;MIN_SMALLINT()&lt;/code&gt;等，用于判断数组是否超过最大值和最小值。&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;IS_INF()&lt;/code&gt; 和 &lt;code&gt;IS_NAN()&lt;/code&gt;，用于判断是否为数值。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;SHOW PARTITIONS&lt;/code&gt; 语句用于查看分区情况，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_show.html#show&quot;&gt;SHOW Statement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;添加 impalad 进程设置参数让你设置所有查询的初始化内存值，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_resource_management.html#resource_management&quot;&gt;Using YARN Resource Management with Impala (CDH 5 Only)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;CDH 5.1 中可以利用 Llama 高可用的特性，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_resource_management.html#llama_ha_unique_2&quot;&gt;Using Impala with a Llama High Availability Configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CREATE TABLE&lt;/code&gt; 语句支持 &lt;code&gt;STORED AS AVRO&lt;/code&gt;，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_avro.html#avro&quot;&gt;Using the Avro File Format with Impala Tables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;impala-shell 中添加 &lt;code&gt;SUMMARY&lt;/code&gt; 命令用于查看摘要信息，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_explain_plan.html#perf_summary_unique_1&quot;&gt;Using the SUMMARY Report for Performance Tuning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;COMPUTE STATS&lt;/code&gt; 语句性能改进：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;NDV&lt;/code&gt; 函数通过生成本地代码加快速度&lt;/li&gt;
      &lt;li&gt;在 1.4.0 或者更高版本，不再统计 NULL 值，其值被看做为 -1，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_perf_stats.html#perf_stats&quot;&gt;How Impala Uses Statistics for Query Optimization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;分区性能改进。之前只能处理3000个分区，现在没有这个限制，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_partitioning.html#partition_pruning_unique_1&quot;&gt;Partition Pruning for Queries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;impala-shell 支持 UTF-8 字符的输入和输出，可以通过参数 &lt;code&gt;--strict_unicode&lt;/code&gt; 控制是否忽略不合法的 Unicode 字符。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;1.3.1&lt;/h1&gt;

&lt;p&gt;该版本主要是 bug 修复，可以在 CDH 4 和 CDH 5 中使用。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 impalad 启动参数中，添加 &lt;code&gt;--insert_inherit_permissions&lt;/code&gt; 参数用于设置创建分区的用户。默认的，INSERT 会使用 HDFS 权限为新分区创建目录，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_insert.html#insert&quot;&gt;INSERT Statement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;SHOW&lt;/code&gt; 函数显示每个函数的返回类型，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_show.html#show&quot;&gt;SHOW Statement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CREATE TABLE&lt;/code&gt; 语句可以使用 ` FIELDS TERMINATED BY ‘\0’` 语句，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_txtfile.html#txtfile&quot;&gt;Using Text Data Files with Impala Tables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;在 1.3.1 以及更高版本后，&lt;code&gt;REGEXP&lt;/code&gt; 和 &lt;code&gt;RLIKE&lt;/code&gt; 的语义进行修正，和数据库中的语义进行兼容，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_operators.html#regexp_unique_1&quot;&gt;REGEXP Operator&lt;/a&gt;。&lt;code&gt;regexp_extract()&lt;/code&gt; 和 &lt;code&gt;regexp_replace()&lt;/code&gt; 可以不再使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;1.3.0&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_admission.html#admission_control&quot;&gt;Admission Control and Query Queuing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;EXPLAIN&lt;/code&gt; 以一种更容易读的格式显示更加详细的内容，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_explain.html#explain&quot;&gt;EXPLAIN Statement&lt;/a&gt; 和 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_explain_plan.html#explain_plan&quot;&gt; Understanding Impala Query Performance - EXPLAIN Plans and Query Profiles&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;UNIX_TIMESTAMP&lt;/code&gt; 、&lt;code&gt;FROM_UNIXTIME&lt;/code&gt; 和 &lt;code&gt;INTERVAL&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;增加条件函数： &lt;code&gt;NULLIF()&lt;/code&gt;、&lt;code&gt;NULLIFZERO()&lt;/code&gt;、 &lt;code&gt;ZEROIFNULL()&lt;/code&gt;，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_conditional_functions.html#conditional_functions&quot;&gt;Conditional Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;添加新的功能函数：&lt;code&gt;CURRENT_DATABASE()&lt;/code&gt;，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_misc_functions.html#misc_functions&quot;&gt;Miscellaneous Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;和 yarn 集成，只在 CDH5 中可用，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_resource_management.html#resource_management&quot;&gt;Using YARN Resource Management with Impala (CDH 5 Only)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;1.2.4&lt;/h1&gt;

&lt;p&gt;该版本用于 CDH4，主要针对 1.2.3 做了一些 bug 修复。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;增加 &lt;code&gt;INVALIDATE METADATA table_name&lt;/code&gt; 语法刷新新建的一个表&lt;/li&gt;
  &lt;li&gt;添加 catalogd 启动参数：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;--load_catalog_in_background&lt;/code&gt;，是否后台运行&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--num_metadata_loading_threads&lt;/code&gt;，并行加载线程&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-4&quot;&gt;1.2.3&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Impala 1.2.3 works with CDH 4 and with CDH 5 beta 2. The resource management feature requires CDH 5 beta.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;该版本主要是在 1.2.2 基础上修复 Parquet 兼容性，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Cloudera-Impala-Release-Notes/cirn_known_issues.html#known_issues&quot;&gt;Known Issues and Workarounds in Impala&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;1.2.2&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Impala 1.2.2 works with CDH 4. Its feature set is a superset of features in the Impala 1.2.0 beta, with the exception of resource management, which relies on CDH 5.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Join order optimizations&lt;/code&gt;，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_perf_joins.html#perf_joins&quot;&gt;Performance Considerations for Join Queries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;COMPUTE STATS&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;STRAIGHT_JOIN&lt;/code&gt;，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_perf_joins.html#straight_join_unique_1&quot;&gt;Overriding Join Reordering with STRAIGHT_JOIN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CROSS JOIN&lt;/code&gt;，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_tutorial.html#tut_cross_join_unique_2&quot;&gt;Cross Joins and Cartesian Products with the CROSS JOIN Operator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LDAP 支持&lt;/li&gt;
  &lt;li&gt;添加 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_string_functions.html#string_functions__group_concat&quot;&gt;&lt;code&gt;GROUP_CONCAT()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;INSERT&lt;/code&gt; 语句可以添加 &lt;code&gt;SHUFFLE&lt;/code&gt; 或者 &lt;code&gt;NOSHUFFLE&lt;/code&gt;，主要是用在插入数据到 Parquet 表的分区的时候。&lt;/li&gt;
  &lt;li&gt;添加 &lt;code&gt;CAST()&lt;/code&gt; 用于类型转换&lt;/li&gt;
  &lt;li&gt;添加 &lt;code&gt;fnv_hash()&lt;/code&gt; 用于计算 hash 值，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_math_functions.html#math_functions&quot;&gt;Mathematical Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;支持 &lt;code&gt;STORED AS PARQUET&lt;/code&gt; 语句。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-6&quot;&gt;1.2.1&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;添加 &lt;code&gt;SHOW TABLE STATS table_name&lt;/code&gt; 和 &lt;code&gt;SHOW COLUMN STATS table_name&lt;/code&gt; 语法&lt;/li&gt;
  &lt;li&gt;添加 &lt;code&gt;CREATE TABLE AS SELECT&lt;/code&gt; 语法&lt;/li&gt;
  &lt;li&gt;支持 &lt;code&gt;OFFSET&lt;/code&gt; 语句，用于分页查询&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ORDER BY&lt;/code&gt; 语句中添加 &lt;code&gt;NULLS FIRST&lt;/code&gt; 和 &lt;code&gt;NULLS LAST&lt;/code&gt; 语法支持&lt;/li&gt;
  &lt;li&gt;添加&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_functions.html#functions&quot;&gt;内置函数&lt;/a&gt;： &lt;code&gt;least()&lt;/code&gt;, &lt;code&gt;greatest()&lt;/code&gt;, &lt;code&gt;initcap()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;添加 &lt;code&gt;ndv()&lt;/code&gt; 函数来计算 &lt;code&gt;COUNT(DISTINCT col)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;LIMIT&lt;/code&gt; 语句接受数值表达式作为参数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;SHOW CREATE TABLE&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;添加两个参数：&lt;code&gt;--idle_query_timeout&lt;/code&gt; 和 &lt;code&gt;--idle_session_timeout&lt;/code&gt;，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_timeouts.html#timeouts&quot;&gt;Setting Timeout Periods for Daemons, Queries, and Sessions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;支持 UDFs，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_create_function.html#create_function&quot;&gt;CREATE FUNCTION Statement&lt;/a&gt; 和 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_drop_function.html#drop_function&quot;&gt;DROP FUNCTION Statement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;添加新的同步元数据的机制，详细参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_concepts.html#intro_catalogd_unique_2&quot;&gt;The Impala Catalog Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;添加 &lt;code&gt;CREATE TABLE ... AS SELECT&lt;/code&gt; 语法&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CREATE TABLE&lt;/code&gt; 和 &lt;code&gt;ALTER TABLE&lt;/code&gt; 支持 &lt;code&gt;TBLPROPERTIES&lt;/code&gt; 和 &lt;code&gt;WITH SERDEPROPERTIES&lt;/code&gt; 语句，详细说明参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_create_table.html#create_table&quot;&gt;CREATE TABLE Statement&lt;/a&gt; 和 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_alter_table.html#alter_table&quot;&gt;ALTER TABLE Statement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;EXPLAIN&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;SHOW CREATE TABLE&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;LIMIT&lt;/code&gt; 语句支持算术表达式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外，impala 的一些不兼容的变化，请参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Cloudera-Impala-Release-Notes/cirn_incompatible_changes.html&quot;&gt;Incompatible Changes in Impala&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Impala 一些已知的问题：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Cloudera-Impala-Release-Notes/cirn_known_issues.html&quot;&gt;Known Issues and Workarounds in Impala&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;已经修复的问题：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Cloudera-Impala-Release-Notes/cirn_fixed_issues.html&quot;&gt;Fixed Issues in Impala&lt;/a&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/07/29/new-features-in-impala.html</link>
      <guid>http://blog.javachen.com/2014/07/29/new-features-in-impala.html</guid>
      <pubDate>2014-07-29T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Phoenix Quick Start</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 介绍&lt;/h1&gt;

&lt;p&gt;Phoenix 是 Salesforce.com 开源的一个 Java 中间件，可以让开发者在Apache HBase 上执行 SQL 查询。Phoenix完全使用Java编写，代码位于 GitHub 上，并且提供了一个客户端可嵌入的 JDBC 驱动。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;根据项目所述，Phoenix 被 Salesforce.com 内部使用，对于简单的低延迟查询，其量级为毫秒；对于百万级别的行数来说，其量级为秒。Phoenix 并不是像 HBase 那样用于 map-reduce job 的，而是通过标准化的语言来访问 HBase 数据的。&lt;/p&gt;

  &lt;p&gt;Phoenix 为 HBase 提供 SQL 的查询接口，它在客户端解析SQL语句，然后转换为 HBase native 的客户端语言，并行执行查询然后生成标准的JDBC结果集。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Phoenix &lt;a href=&quot;http://phoenix-hbase.blogspot.com/2013/01/announcing-phoenix-sql-layer-over-hbase.html&quot;&gt;最值得关注的一些特性&lt;/a&gt; 有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;嵌入式的JDBC驱动，实现了大部分的java.sql接口，包括元数据API&lt;/li&gt;
  &lt;li&gt;可以通过多部行键或是键/值单元对列进行建模&lt;/li&gt;
  &lt;li&gt;完善的查询支持，可以使用多个谓词以及优化的扫描键&lt;/li&gt;
  &lt;li&gt;DDL支持：通过CREATE TABLE、DROP TABLE及ALTER TABLE来添加/删除列&lt;/li&gt;
  &lt;li&gt;版本化的模式仓库：当写入数据时，快照查询会使用恰当的模式&lt;/li&gt;
  &lt;li&gt;DML支持：用于逐行插入的UPSERT VALUES、用于相同或不同表之间大量数据传输的UPSERT SELECT、用于删除行的DELETE&lt;/li&gt;
  &lt;li&gt;通过客户端的批处理实现的有限的事务支持&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://phoenix.apache.org/joins.html&quot;&gt;表连接&lt;/a&gt; 和&lt;a href=&quot;http://phoenix.apache.org/secondary_indexing.htm&quot;&gt;二级索引&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;紧跟ANSI SQL标准&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SQL Support 可以参考 &lt;a href=&quot;http://phoenix.apache.org/language/index.html&quot;&gt;language reference&lt;/a&gt;，Phoenix 当前不支持：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;完全的事物支持&lt;/li&gt;
  &lt;li&gt;嵌套查询&lt;/li&gt;
  &lt;li&gt;关联操作: Union、Intersect、Minus&lt;/li&gt;
  &lt;li&gt;各种各样的内建函数。可以参考&lt;a href=&quot;http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html&quot;&gt;这篇文章&lt;/a&gt;添加自定义函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 安装&lt;/h1&gt;

&lt;p&gt;HBase 兼容性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Phoenix 2.x - HBase 0.94.x&lt;/li&gt;
  &lt;li&gt;Phoenix 3.x - HBase 0.94.x&lt;/li&gt;
  &lt;li&gt;Phoenix 4.x - HBase 0.98.1+&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;安装已经编译好的 phoenix ：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;下载对应你 hbase 版本的 phoenix-[version]-incubating.tar 并解压，下载地址：&lt;a href=&quot;http://www.apache.org/dyn/closer.cgi/incubator/phoenix/&quot;&gt;http://www.apache.org/dyn/closer.cgi/incubator/phoenix/&lt;/a&gt;。&lt;/li&gt;
  &lt;li&gt;添加 phoenix-core-[version]-incubating.jar 到 HBase region server 的 classpath 中，或者直接将其加载到 hbase 的 lib 目录&lt;/li&gt;
  &lt;li&gt;重启 HBase region server&lt;/li&gt;
  &lt;li&gt;添加 phoenix-[version]-incubating-client.jar 到 hadoop 客户端的 lib 目录。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 使用&lt;/h1&gt;

&lt;h2 id=&quot;jdbc&quot;&gt;3.1 JDBC&lt;/h2&gt;

&lt;p&gt;Java 客户端连接 jdbc 代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Connection conn = DriverManager.getConnection(&quot;jdbc:phoenix:server1,server2:2181&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;jdbc 的 url 类似为 &lt;code&gt;jdbc:phoenix [ :&amp;lt;zookeeper quorum&amp;gt; [ :&amp;lt;port number&amp;gt; ] [ :&amp;lt;root node&amp;gt; ] ]&lt;/code&gt;，需要引用三个参数：&lt;code&gt;hbase.zookeeper.quorum&lt;/code&gt;、&lt;code&gt;hbase.zookeeper.property.clientPort&lt;/code&gt;、&lt;code&gt;and zookeeper.znode.parent&lt;/code&gt;，这些参数可以缺省不填而在 hbase-site.xml 中定义。&lt;/p&gt;

&lt;h2 id=&quot;sqlline-&quot;&gt;3.2 sqlline 命令行&lt;/h2&gt;

&lt;p&gt;进入解压后的 bin 目录，执行下面命令可以进入一个命令行模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sqlline.py localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入之后，可以查看表和列：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;sqlline version 1.1.2
0: jdbc:phoenix:localhost&amp;gt; !tables
+------------+-------------+------------+------------+------------+------------+---------------------------+----------------+--------+
| TABLE_CAT  | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE |  REMARKS   | TYPE_NAME  | SELF_REFERENCING_COL_NAME | REF_GENERATION | INDEX_ |
+------------+-------------+------------+------------+------------+------------+---------------------------+----------------+--------+
| null       | SYSTEM      | CATALOG    | SYSTEM TABLE | null       | null       | null                      | null           | null |
| null       | SYSTEM      | SEQUENCE   | SYSTEM TABLE | null       | null       | null                      | null           | null |
+------------+-------------+------------+------------+------------+------------+---------------------------+----------------+--------+
0: jdbc:phoenix:localhost&amp;gt; !columns sequence
+------------+-------------+------------+-------------+------------+------------+-------------+---------------+----------------+-----+
| TABLE_CAT  | TABLE_SCHEM | TABLE_NAME | COLUMN_NAME | DATA_TYPE  | TYPE_NAME  | COLUMN_SIZE | BUFFER_LENGTH | DECIMAL_DIGITS | NUM |
+------------+-------------+------------+-------------+------------+------------+-------------+---------------+----------------+-----+
| null       | SYSTEM      | SEQUENCE   | TENANT_ID   | 12         | VARCHAR    | null        | null          | null           | nul |
+------------+-------------+------------+-------------+------------+------------+-------------+---------------+----------------+-----+
0: jdbc:phoenix:localhost&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面可以看出来，phoenix 中存在两个系统表：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SYSTEM.CATALOG&lt;/li&gt;
  &lt;li&gt;SYSTEM.SEQUENCE&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过 HBase Master 的 web 页面，可以看到上面两个表的建表语句，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;SYSTEM.CATALOG&#39;, {METHOD =&amp;gt; &#39;table_att&#39;, coprocessor$1 =&amp;gt; &#39;|org.apache.phoenix.coprocessor.ScanRegionObserver|1|&#39;, coprocessor$2 =&amp;gt; &#39;|org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|1|&#39;, coprocessor$3 =&amp;gt; &#39;|org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver|1|&#39;, coprocessor$4 =&amp;gt; &#39;|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|1|&#39;, coprocessor$5 =&amp;gt; &#39;|org.apache.phoenix.coprocessor.MetaDataEndpointImpl|1|&#39;, coprocessor$6 =&amp;gt; &#39;|org.apache.phoenix.coprocessor.MetaDataRegionObserver|2|&#39;, CONFIG =&amp;gt; {&#39;SPLIT_POLICY&#39; =&amp;gt; &#39;org.apache.phoenix.schema.MetaDataSplitPolicy&#39;, &#39;UpgradeTo30&#39; =&amp;gt; &#39;true&#39;}}, {NAME =&amp;gt; &#39;0&#39;, DATA_BLOCK_ENCODING =&amp;gt; &#39;FAST_DIFF&#39;, VERSIONS =&amp;gt; &#39;1000&#39;, KEEP_DELETED_CELLS =&amp;gt; &#39;true&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以用下面脚本执行一个 sql 语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./sqlline.py localhost ../examples/STOCK_SYMBOL.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;1/4          /*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* &quot;License&quot;); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

-- creates stock table with single row
CREATE TABLE IF NOT EXISTS STOCK_SYMBOL (SYMBOL VARCHAR NOT NULL PRIMARY KEY, COMPANY VARCHAR);
No rows affected (1.714 seconds)
2/4          UPSERT INTO STOCK_SYMBOL VALUES (&#39;CRM&#39;,&#39;SalesForce.com&#39;);
1 row affected (0.035 seconds)
3/4          SELECT * FROM STOCK_SYMBOL;
+------------+------------+
|   SYMBOL   |  COMPANY   |
+------------+------------+
| CRM        | SalesForce.com |
+------------+------------+
1 row selected (0.117 seconds)
4/4
Closing: org.apache.phoenix.jdbc.PhoenixConnection
sqlline version 1.1.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;../examples/STOCK_SYMBOL.sql 文件主要包括三个 sql 语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;CREATE TABLE IF NOT EXISTS STOCK_SYMBOL (SYMBOL VARCHAR NOT NULL PRIMARY KEY, COMPANY VARCHAR);
UPSERT INTO STOCK_SYMBOL VALUES (&#39;CRM&#39;,&#39;SalesForce.com&#39;);
SELECT * FROM STOCK_SYMBOL;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;第一个语句创建表&lt;/li&gt;
  &lt;li&gt;第二个语句插入一条记录&lt;/li&gt;
  &lt;li&gt;第三个语句查询数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 hbase shell 中查看存在的表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hbase(main):001:0&amp;gt; list
TABLE
STOCK_SYMBOL
SYSTEM.CATALOG
SYSTEM.SEQUENCE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 &lt;code&gt;STOCK_SYMBOL&lt;/code&gt; 表中数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hbase(main):004:0&amp;gt; scan &#39;STOCK_SYMBOL&#39;
ROW                                COLUMN+CELL
 CRM                               column=0:COMPANY, timestamp=1406535419510, value=SalesForce.com
 CRM                               column=0:_0, timestamp=1406535419510, value=
1 row(s) in 0.0210 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到插入了一行记录，rowkey 为 CRM，而列族名称为 0 ，存在两列，一列为指定的COMPANY，另一列为 phoenix 插入的 &lt;code&gt;_0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;然后，也可以通过 sqlline.py 来查看数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:phoenix:localhost&amp;gt; select * from STOCK_SYMBOL;
+------------+------------+
|   SYMBOL   |  COMPANY   |
+------------+------------+
| CRM        | SalesForce.com |
+------------+------------+
1 row selected (0.144 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意到：从上面查询看到的就只有两列，没有看到 &lt;code&gt;_0&lt;/code&gt; 这一列。&lt;/p&gt;

&lt;p&gt;从上可以知道，Phoenix 是构建在 HBase 之上的 SQL 中间层，向 HBase 发送标准 sql 语句，对 HBase 进行操作。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3.3 加载数据&lt;/h2&gt;

&lt;p&gt;你可以使用 bin/psql.py 来加载 CSV 数据 或者执行 SQL 脚本，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./psql.py localhost ../examples/WEB_STAT.sql ../examples/WEB_STAT.csv ../examples/WEB_STAT_QUERIES.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其输出结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;no rows upserted
Time: 1.528 sec(s)

csv columns from database.
CSV Upsert complete. 39 rows upserted
Time: 0.122 sec(s)

DOMAIN     AVERAGE_CPU_USAGE AVERAGE_DB_USAGE
---------- ----------------- ----------------
Salesforce.com       260.727          257.636
Google.com           212.875           213.75
Apple.com            114.111          119.556
Time: 0.062 sec(s)

DAY                 TOTAL_CPU_USAGE MIN_CPU_USAGE MAX_CPU_USAGE
------------------- --------------- ------------- -------------
2013-01-01 00:00:00              35            35            35
2013-01-02 00:00:00             150            25           125
2013-01-03 00:00:00              88            88            88
2013-01-04 00:00:00              26             3            23
2013-01-05 00:00:00             550            75           475
2013-01-06 00:00:00              12            12            12
2013-01-08 00:00:00             345           345           345
2013-01-09 00:00:00             390            35           355
2013-01-10 00:00:00             345           345           345
2013-01-11 00:00:00             335           335           335
2013-01-12 00:00:00               5             5             5
2013-01-13 00:00:00             355           355           355
2013-01-14 00:00:00               5             5             5
2013-01-15 00:00:00             720            65           655
2013-01-16 00:00:00             785           785           785
2013-01-17 00:00:00            1590           355          1235
Time: 0.045 sec(s)

HOST TOTAL_ACTIVE_VISITORS
---- ---------------------
EU                     150
NA                       1
Time: 0.058 sec(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WEB_STAT.sql 中 sql 语句为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;CREATE TABLE IF NOT EXISTS WEB_STAT (
     HOST CHAR(2) NOT NULL,
     DOMAIN VARCHAR NOT NULL,
     FEATURE VARCHAR NOT NULL,
     DATE DATE NOT NULL,
     USAGE.CORE BIGINT,  -- 指定了列族： USAGE
     USAGE.DB BIGINT,	-- 指定了列族： USAGE
     STATS.ACTIVE_VISITOR INTEGER ,  --指定了列族： STATS
     CONSTRAINT PK PRIMARY KEY (HOST, DOMAIN, FEATURE, DATE)
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 sql 语句上可以看出来，HOST、DOMAIN、FEATURE、DATE 这四列前面并没有指定列族，并且通过约束设置这四列组成 hbase 的 rowkey，其他三列都指定了列族。&lt;/p&gt;

&lt;p&gt;通过 sqlline.py 查询第一条记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:phoenix:localhost&amp;gt; select * from WEB_STAT limit 1;
+------+------------+------------+---------------------+------------+------------+----------------+
| HOST |   DOMAIN   |  FEATURE   |        DATE         |    CORE    |     DB     | ACTIVE_VISITOR |
+------+------------+------------+---------------------+------------+------------+----------------+
| EU   | Apple.com  | Mac        | 2013-01-01          | 35         | 22         | 34             |
+------+------------+------------+---------------------+------------+------------+----------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而通过 hbase shell 查询 &lt;code&gt;WEB_STAT&lt;/code&gt; 表第一条记录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; EUApple.com\x00Mac\x00\x80\x00\x0 column=STATS:ACTIVE_VISITOR, timestamp=1406535785946, value=\x80\x00\x00&quot;
 1;\xF3\xA04\xC8
 EUApple.com\x00Mac\x00\x80\x00\x0 column=USAGE:CORE, timestamp=1406535785946, value=\x80\x00\x00\x00\x00\x00\x00#
 1;\xF3\xA04\xC8
 EUApple.com\x00Mac\x00\x80\x00\x0 column=USAGE:DB, timestamp=1406535785946, value=\x80\x00\x00\x00\x00\x00\x00\x16
 1;\xF3\xA04\xC8
 EUApple.com\x00Mac\x00\x80\x00\x0 column=USAGE:_0, timestamp=1406535785946, value=
 1;\xF3\xA04\xC8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过上面对比知道：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;phoenix 对用户屏蔽了 rowkey 的设计细节&lt;/li&gt;
  &lt;li&gt;USAGE 列族中存在一列为 &lt;code&gt;_0&lt;/code&gt;，而 STATS 列族中却没有，&lt;strong&gt;这是为什么？&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Phoenix 把 rowkey 内化为 table 的 PRIMARY KEY 处理，由 HOST、DOMAIN、FEATURE、DATE 这四列拼接在一起，组成了 rowkey&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其他可选的加载数据的方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用 &lt;a href=&quot;http://phoenix.apache.org/bulk_dataload.html&quot;&gt;map-reduce based CSV loader&lt;/a&gt; 加载更大的数据&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://phoenix.apache.org/index.html#Mapping-to-an-Existing-HBase-Table&quot;&gt;映射一个存在的 HBase 表到 Phoenix 表&lt;/a&gt; 以及使用  &lt;a href=&quot;http://phoenix.apache.org/language/index.html#upsert_select&quot;&gt;UPSERT SELECT&lt;/a&gt; 来创建一个新表&lt;/li&gt;
  &lt;li&gt;使用 &lt;a href=&quot;http://phoenix.apache.org/language/index.html#upsert_values&quot;&gt;UPSERT VALUES&lt;/a&gt; 插入记录&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hbase-&quot;&gt;3.4 映射到存在的 HBase 表&lt;/h2&gt;

&lt;p&gt;创建一张hbase表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create &#39;t1&#39;, &#39;f&#39;

put &#39;t1&#39;, &quot;row1&quot;, &#39;f:q&#39;, 1
put &#39;t1&#39;, &quot;row2&quot;, &#39;f:q&#39;, 2
put &#39;t1&#39;, &quot;row3&quot;, &#39;f:q&#39;, 3
put &#39;t1&#39;, &quot;row4&quot;, &#39;f:q&#39;, 4
put &#39;t1&#39;, &quot;row5&quot;, &#39;f:q&#39;, 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在phoenix建一张同样的表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;./sqlline.py localhost 

CREATE TABLE IF NOT EXISTS &quot;t1&quot; (
     row VARCHAR NOT NULL,
     &quot;f&quot;.&quot;q&quot; VARCHAR
     CONSTRAINT PK PRIMARY KEY (row)
);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;t1、f、q 需要用双引号括起来，原因主要是大小写的问题，参考 phoenix 的 &lt;a href=&quot;https://github.com/forcedotcom/phoenix/wiki&quot;&gt;wiki&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;在这里， phoenix 会修改 table 的 Descriptor，然后添加 coprocessor，所以会先 disable，在 modify，最后 enable 表。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;接下来就可以查询了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;0: jdbc:phoenix:localhost&amp;gt; select * from &quot;t1&quot;;
+------------+------------+
|    ROW     |     q      |
+------------+------------+
| row1       | 1          |
| row2       | 2          |
| row3       | 3          |
| row4       | 4          |
| row5       | 5          |
+------------+------------+
5 rows selected (0.101 seconds)
0: jdbc:phoenix:localhost&amp;gt; select count(1) from &quot;t1&quot;;
+------------+
|  COUNT(1)  |
+------------+
| 5          |
+------------+
1 row selected (0.068 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;4. 总结&lt;/h1&gt;

&lt;p&gt;这篇文章主要是介绍了什么是 Phoenix 、如何安装以及他的一些特性，然后介绍了他的使用方法，主要包括命令行使用、加载数据以及如何映射存在的 HBase 表，通过该篇文章对 Phoenix 有了一个初步的认识。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/07/28/phoenix-quick-start.html</link>
      <guid>http://blog.javachen.com/2014/07/28/phoenix-quick-start.html</guid>
      <pubDate>2014-07-28T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>采集日志到Hive</title>
      <description>&lt;p&gt;我们现在的需求是需要将线上的日志以小时为单位采集并存储到 hive 数据库中，方便以后使用  mapreduce 或者 impala 做数据分析。为了实现这个目标调研了 flume 如何采集数据到 hive，其他的日志采集框架尚未做调研。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;日志压缩&lt;/h1&gt;

&lt;p&gt;flume中有个 HdfsSink 组件，其可以压缩日志进行保存，故首先想到我们的日志应该以压缩的方式进行保存，遂选择了 lzo 的压缩格式，HdfsSink 的配置如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;agent-1.sinks.sink_hdfs.channel = ch-1
agent-1.sinks.sink_hdfs.type = hdfs
agent-1.sinks.sink_hdfs.hdfs.path = hdfs://cdh1:8020/user/root/events/%Y-%m-%d
agent-1.sinks.sink_hdfs.hdfs.filePrefix = logs
agent-1.sinks.sink_hdfs.hdfs.inUsePrefix = .
agent-1.sinks.sink_hdfs.hdfs.rollInterval = 30
agent-1.sinks.sink_hdfs.hdfs.rollSize = 0
agent-1.sinks.sink_hdfs.hdfs.rollCount = 0
agent-1.sinks.sink_hdfs.hdfs.batchSize = 1000
agent-1.sinks.sink_hdfs.hdfs.fileType = CompressedStream
agent-1.sinks.sink_hdfs.hdfs.codeC = lzop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hive 目前是支持 lzo 压缩的，但是要想在 mapreduce 中 lzo 文件可以拆分，需要通过 hadoop 的 api 进行手动创建索引：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ lzop a.txt
$ hadoop fs -put a.txt.lzo /log/dw_srclog/sp_visit_log/ptd_ymd=20140720
​$ hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /log/sp_visit_log/ptd_ymd=20140720/a.txt.lzo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;impala 目前也是在支持 lzo 压缩格式的文件的，故采用 lzo 压缩方式存储日志文件似乎是个可行方案。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;自定义分隔符&lt;/h1&gt;

&lt;p&gt;Hive默认创建的表字段分隔符为：&lt;code&gt;\001(ctrl-A)&lt;/code&gt;，也可以通过 &lt;code&gt;ROW FORMAT DELIMITED FIELDS TERMINATED BY&lt;/code&gt; 指定其他字符，但是该语法只支持单个字符。&lt;/p&gt;

&lt;p&gt;目前，我们的日志中几乎任何单个字符都被使用了，故没法使用单个字符作为 hive 表字段的分隔符，只能使用多个字符，例如：“|||”。&lt;br /&gt;
使用多字符来分隔字段，则需要你自定义InputFormat来实现。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package org.apache.hadoop.mapred;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileSplit;
import org.apache.hadoop.mapred.InputSplit;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.LineRecordReader;
import org.apache.hadoop.mapred.RecordReader;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.TextInputFormat;

public class MyDemoInputFormat extends TextInputFormat {

	@Override
	public RecordReader&amp;lt;LongWritable, Text&amp;gt; getRecordReader(
			InputSplit genericSplit, JobConf job, Reporter reporter)
			throws IOException {
		reporter.setStatus(genericSplit.toString());
		MyDemoRecordReader reader = new MyDemoRecordReader(
				new LineRecordReader(job, (FileSplit) genericSplit));
		return reader;
	}

	public static class MyDemoRecordReader implements
			RecordReader&amp;lt;LongWritable, Text&amp;gt; {

		LineRecordReader reader;
		Text text;

		public MyDemoRecordReader(LineRecordReader reader) {
			this.reader = reader;
			text = reader.createValue();
		}

		@Override
		public void close() throws IOException {
			reader.close();
		}

		@Override
		public LongWritable createKey() {
			return reader.createKey();
		}

		@Override
		public Text createValue() {
			return new Text();
		}

		@Override
		public long getPos() throws IOException {
			return reader.getPos();
		}

		@Override
		public float getProgress() throws IOException {
			return reader.getProgress();
		}

		@Override
		public boolean next(LongWritable key, Text value) throws IOException {
			Text txtReplace;
			while (reader.next(key, text)) {
				txtReplace = new Text();
				txtReplace.set(text.toString().toLowerCase().replaceAll(&quot;\\|\\|\\|&quot;, &quot;\001&quot;));
				value.set(txtReplace.getBytes(), 0, txtReplace.getLength());
				return true;

			}
			return false;
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候的建表语句是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create external table IF NOT EXISTS  test(
id string,
name string
)partitioned by (day string) 
STORED AS INPUTFORMAT  
  &#39;org.apache.hadoop.mapred.MyDemoInputFormat&#39;  
OUTPUTFORMAT  
  &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;
LOCATION &#39;/log/dw_srclog/test&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是，这样建表的话，是不能识别 lzo 压缩文件的，需要去扩展 lzo 的 DeprecatedLzoTextInputFormat 类，但是如何扩展，没有找到合适方法。&lt;/p&gt;

&lt;p&gt;所以，在自定义分隔符的情况下，想支持 lzo 压缩文件，需要另外想办法。例如，使用 &lt;code&gt;SERDE&lt;/code&gt; 的方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create external table IF NOT EXISTS  test(
id string,
name string
)partitioned by (day string) 
ROW FORMAT  
SERDE &#39;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&#39;  
WITH SERDEPROPERTIES  
( &#39;input.regex&#39; = &#39;([^ ]*)\\|\\|\\|([^ ]*)&#39;,  
&#39;output.format.string&#39; = &#39;%1$s %2$s&#39;) 
STORED AS INPUTFORMAT  
  &#39;com.hadoop.mapred.DeprecatedLzoTextInputFormat&#39;  
OUTPUTFORMAT  
  &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;
LOCATION &#39;/log/dw_srclog/test&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要想使用SERDE，必须添加 hive-contrib-XXXX.jar 到 classpath，在 hive-env.sh 中添加下面代码;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export HIVE_AUX_JARS_PATH=/usr/lib/hive/lib/hive-contrib-0.10.0-cdh4.7.0.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用 SERDE  时，字段类型只能为 string。&lt;/li&gt;
  &lt;li&gt;这种方式建表，flume 可以将日志存储为 lzo 并且 hive 能够识别出数据，但是 impala 中却不支持 &lt;code&gt;SERDE&lt;/code&gt; 的语法，故只能放弃该方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后，只能放弃 lzo 压缩文件的想法，改为不做压缩。flume 中 HdfsSink 配置参数 hdfs.fileType 目前只有三种可选值：CompressedStream&lt;br /&gt;
、DataStream、SequenceFile，为了保持向后兼容便于扩展，这里使用了 DataStream 的方式，不做数据压缩。&lt;/p&gt;

&lt;h2 id=&quot;update&quot;&gt;Update&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;最后又经过测试，发现 impala 不支持 hive 的自定义文件格式，详细说明请参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/ciiu_langref_unsupported.html?scroll=langref_unsupported&quot;&gt;SQL Differences Between Impala and Hive&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;日志采集&lt;/h1&gt;

&lt;p&gt;使用 flume 来采集日志，只需要在应用程序服务器上安装一个 agent 就可以监听文件或者目录的改变来搜集日志，但是实际情况你不一定有权限访问应用服务器，更多的方式是应用服务器将日志推送到一个中央的日志集中存储服务器。你只有权限去从该服务器收集数据，并且该服务器对外提供 ftp 的接口供你访问。&lt;/p&gt;

&lt;p&gt;日志采集有 pull 和 push 的两种方式，关于两种方式的一些说明，可以参考这篇文章：&lt;a href=&quot;http://sdjcw.iteye.com/blog/1814703&quot;&gt;大规模日志收集处理项目的技术总结&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;对于当前情况而言，只能从 ftp 服务器轮询文件然后下载文件到本地，最后再将其导入到 hive 中去。以前，使用 kettle 做过这种事情，现在为了简单只是写了个 python 脚本来做这件事情，一个示例代码，请参考 &lt;a href=&quot;https://gist.github.com/javachen/6f7d14aae138c7a284e6#file-fetch-py&quot;&gt;https://gist.github.com/javachen/6f7d14aae138c7a284e6#file-fetch-py&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;该脚本会再 crontab 中每隔5分钟执行一次。&lt;/p&gt;

&lt;p&gt;执行该脚本会往 mongodb 中记录一些状态信息，并往 logs 目录以天为单位记录日志。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;暂时没有使用 flume 的原因：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对 flume 的测试于调研程度还不够&lt;/li&gt;
  &lt;li&gt;flume 中无法对数据去重&lt;/li&gt;
  &lt;li&gt;只能停止 flume 进程，才可以升级 flume，这样会丢失数据&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;等日志采集实时性要求变高，以及对 flume 的熟悉程度变深之后，会考虑使用 flume。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/07/25/collect-log-to-hive.html</link>
      <guid>http://blog.javachen.com/2014/07/25/collect-log-to-hive.html</guid>
      <pubDate>2014-07-25T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Flume-ng的原理和使用</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 介绍&lt;/h1&gt;

&lt;p&gt;Flume NG是Cloudera提供的一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中心化数据存储系统中。由原来的Flume OG到现在的Flume NG，进行了架构重构，并且现在NG版本完全不兼容原来的OG版本。经过架构重构后，Flume NG更像是一个轻量的小工具，非常简单，容易适应各种方式日志收集，并支持failover和负载均衡。&lt;/p&gt;

&lt;p&gt;Flume 使用 java 编写，其需要运行在 Java1.6 或更高版本之上。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;官方网站：&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;用户文档：&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;http://flume.apache.org/FlumeUserGuide.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;开发文档：&lt;a href=&quot;http://flume.apache.org/FlumeDeveloperGuide.html&quot;&gt;http://flume.apache.org/FlumeDeveloperGuide.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 架构&lt;/h1&gt;

&lt;p&gt;Flume的架构主要有一下几个核心概念：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event：一个数据单元，带有一个可选的消息头&lt;/li&gt;
  &lt;li&gt;Flow：Event从源点到达目的点的迁移的抽象&lt;/li&gt;
  &lt;li&gt;Client：操作位于源点处的Event，将其发送到Flume Agent&lt;/li&gt;
  &lt;li&gt;Agent：一个独立的Flume进程，包含组件Source、Channel、Sink&lt;/li&gt;
  &lt;li&gt;Source：用来消费传递到该组件的Event&lt;/li&gt;
  &lt;li&gt;Channel：中转Event的一个临时存储，保存有Source组件传递过来的Event&lt;/li&gt;
  &lt;li&gt;Sink：从Channel中读取并移除Event，将Event传递到Flow Pipeline中的下一个Agent（如果有的话）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2.1 数据流&lt;/h2&gt;

&lt;p&gt;Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。&lt;/p&gt;

&lt;p&gt;Flume 传输的数据的基本单位是 Event，如果是文本文件，通常是一行记录，这也是事务的基本单位。Event 从 Source，流向 Channel，再到 Sink，本身为一个 byte 数组，并可携带 headers 信息。Event 代表着一个数据流的最小完整单元，从外部数据源来，向外部的目的地去。&lt;/p&gt;

&lt;p&gt;Flume 运行的核心是 Agent。它是一个完整的数据收集工具，含有三个核心组件，分别是 source、channel、sink。通过这些组件，Event 可以从一个地方流向另一个地方，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/flume-ng-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;source 可以接收外部源发送过来的数据。不同的 source，可以接受不同的数据格式。比如有目录池(spooling directory)数据源，可以监控指定文件夹中的新文件变化，如果目录中有文件产生，就会立刻读取其内容。&lt;/li&gt;
  &lt;li&gt;channel 是一个存储地，接收 source 的输出，直到有 sink 消费掉 channel 中的数据。channel 中的数据直到进入到下一个channel中或者进入终端才会被删除。当 sink 写入失败后，可以自动重启，不会造成数据丢失，因此很可靠。&lt;/li&gt;
  &lt;li&gt;sink 会消费 channel 中的数据，然后送给外部源或者其他 source。如数据可以写入到 HDFS 或者 HBase 中。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;2.2 核心组件&lt;/h2&gt;

&lt;h3 id=&quot;source&quot;&gt;2.2.1 source&lt;/h3&gt;

&lt;p&gt;Client端操作消费数据的来源，Flume 支持 Avro，log4j，syslog 和 http post(body为json格式)。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。也可以 写一个 Source，以 IPC 或 RPC 的方式接入自己的应用，Avro和 Thrift 都可以(分别有 NettyAvroRpcClient 和 ThriftRpcClient 实现了 RpcClient接口)，其中 Avro 是默认的 RPC 协议。具体代码级别的 Client 端数据接入，可以参考官方手册。&lt;/p&gt;

&lt;p&gt;对现有程序改动最小的使用方式是使用是直接读取程序原来记录的日志文件，基本可以实现无缝接入，不需要对现有程序进行任何改动。 &lt;br /&gt;
对于直接读取文件 Source,有两种方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ExecSource: 以运行 Linux 命令的方式，持续的输出最新的数据，如 &lt;code&gt;tail -F 文件名&lt;/code&gt; 指令，在这种方式下，取的文件名必须是指定的。 ExecSource 可以实现对日志的实时收集，但是存在Flume不运行或者指令执行出错时，将无法收集到日志数据，无法保证日志数据的完整性。&lt;/li&gt;
  &lt;li&gt;SpoolSource: 监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点：拷贝到 spool 目录下的文件不可以再打开编辑；spool 目录下不可包含相应的子目录。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SpoolSource 虽然无法实现实时的收集数据，但是可以使用以分钟的方式分割文件，趋近于实时。&lt;/p&gt;

&lt;p&gt;如果应用无法实现以分钟切割日志文件的话， 可以两种收集方式结合使用。 在实际使用的过程中，可以结合 log4j 使用，使用 log4j的时候，将 log4j 的文件分割机制设为1分钟一次，将文件拷贝到spool的监控目录。&lt;/p&gt;

&lt;p&gt;log4j 有一个 TimeRolling 的插件，可以把 log4j 分割文件到 spool 目录。基本实现了实时的监控。Flume 在传完文件之后，将会修改文件的后缀，变为 .COMPLETED（后缀也可以在配置文件中灵活指定）。&lt;/p&gt;

&lt;p&gt;Flume Source 支持的类型：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Source类型&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Avro Source&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;支持Avro协议（实际上是Avro RPC），内置支持&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Thrift Source&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;支持Thrift协议，内置支持&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Exec Source&lt;/td&gt;
      &lt;td&gt;基于Unix的command在标准输出上生产数据&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;JMS Source&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;从JMS系统（消息、主题）中读取数据，ActiveMQ已经测试过&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Spooling Directory Source&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;监控指定目录内数据变更&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Twitter 1% firehose Source&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;通过API持续下载Twitter数据，试验性质&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Netcat Source&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;监控某个端口，将流经端口的每一个文本行数据作为Event输入&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Sequence Generator Source&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;序列生成器数据源，生产序列数据&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Syslog Sources&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;读取syslog数据，产生Event，支持UDP和TCP两种协议&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HTTP Source&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;基于HTTP POST或GET方式的数据源，支持JSON、BLOB表示形式&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Legacy Sources&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;兼容老的Flume OG中Source（0.9.x版本）&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;channel&quot;&gt;2.2.2 Channel&lt;/h3&gt;

&lt;p&gt;当前有几个 channel 可供选择，分别是 Memory Channel, JDBC Channel , File Channel，Psuedo Transaction Channel。比较常见的是前三种 channel。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MemoryChannel 可以实现高速的吞吐，但是无法保证数据的完整性。&lt;/li&gt;
  &lt;li&gt;MemoryRecoverChannel 在官方文档的建议上已经建义使用FileChannel来替换。&lt;/li&gt;
  &lt;li&gt;FileChannel保证数据的完整性与一致性。在具体配置FileChannel时，建议FileChannel设置的目录和程序日志文件保存的目录设成不同的磁盘，以便提高效率。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;File Channel 是一个持久化的隧道（channel），它持久化所有的事件，并将其存储到磁盘中。因此，即使 Java 虚拟机当掉，或者操作系统崩溃或重启，再或者事件没有在管道中成功地传递到下一个代理（agent），这一切都不会造成数据丢失。Memory Channel 是一个不稳定的隧道，其原因是由于它在内存中存储所有事件。如果 java 进程死掉，任何存储在内存的事件将会丢失。另外，内存的空间收到 RAM大小的限制,而 File Channel 这方面是它的优势，只要磁盘空间足够，它就可以将所有事件数据存储到磁盘上。&lt;/p&gt;

&lt;p&gt;Flume Channel 支持的类型：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Channel类型&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Memory Channel&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Event数据存储在内存中&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;JDBC Channel&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Event数据存储在持久化存储中，当前Flume Channel内置支持Derby&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;File Channel&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Event数据存储在磁盘文件中&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Spillable Memory Channel&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Event数据存储在内存中和磁盘上，当内存队列满了，会持久化到磁盘文件（当前试验性的，不建议生产环境使用）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Pseudo Transaction Channel&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;测试用途&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Custom Channel&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;自定义Channel实现&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;sink&quot;&gt;2.2.3 sink&lt;/h3&gt;

&lt;p&gt;Sink在设置存储数据时，可以向文件系统、数据库、hadoop存数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。&lt;/p&gt;

&lt;p&gt;Flume Sink支持的类型&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Sink类型&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HDFS Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据写入HDFS&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Logger Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据写入日志文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Avro Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据被转换成Avro Event，然后发送到配置的RPC端口上&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Thrift Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据被转换成Thrift Event，然后发送到配置的RPC端口上&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;IRC Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据在IRC上进行回放&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;File Roll Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;存储数据到本地文件系统&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Null Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;丢弃到所有数据&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HBase Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据写入HBase数据库&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Morphline Solr Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据发送到Solr搜索服务器（集群）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ElasticSearch Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据发送到Elastic Search搜索服务器（集群）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Kite Dataset Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;写数据到Kite Dataset，试验性质的&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Custom Sink&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;自定义Sink实现&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;更多sink的内容可以参考&lt;a href=&quot;http://flume.apache.org/FlumeDeveloperGuide.html#sink&quot;&gt;官方手册&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;2.3 可靠性&lt;/h2&gt;

&lt;p&gt;Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。&lt;/p&gt;

&lt;p&gt;Flume 使用事务性的方式保证传送Event整个过程的可靠性。Sink 必须在 Event 被存入 Channel 后，或者，已经被传达到下一站agent里，又或者，已经被存入外部数据目的地之后，才能把 Event 从 Channel 中 remove 掉。这样数据流里的 event 无论是在一个 agent 里还是多个 agent 之间流转，都能保证可靠，因为以上的事务保证了 event 会被成功存储起来。而 Channel 的多种实现在可恢复性上有不同的保证。也保证了 event 不同程度的可靠性。比如 Flume 支持在本地保存一份文件 channel 作为备份，而memory channel 将 event 存在内存 queue 里，速度快，但丢失的话无法恢复。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;2.4 可恢复性&lt;/h2&gt;

&lt;h1 id=&quot;section-6&quot;&gt;3. 使用场景&lt;/h1&gt;

&lt;p&gt;下面，根据官网文档，我们展示几种Flow Pipeline，各自适应于什么样的应用场景：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多个 agent 顺序连接：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/flume-multiseq-agents.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent的数量，因为数据流经的路径变长了，如果不考虑failover的话，出现故障将影响整个Flow上的Agent收集服务。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多个Agent的数据汇聚到同一个Agent:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/flume-join-agent.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种情况应用的场景比较多，比如要收集Web网站的用户行为日志，Web网站为了可用性使用的负载均衡的集群模式，每个节点都产生用户行为日志，可以为每个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多路（Multiplexing）Agent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/flume-multiplexing-agent.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种模式，有两种方式，一种是用来复制（Replication），另一种是用来分流（Multiplexing）。Replication方式，可以将最前端的数据源复制多份，分别传递到多个channel中，每个channel接收到的数据都是相同的。&lt;/p&gt;

&lt;p&gt;配置格式示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;# List the sources, sinks and channels for the agent
&amp;lt;Agent&amp;gt;.sources = &amp;lt;Source1&amp;gt;
&amp;lt;Agent&amp;gt;.sinks = &amp;lt;Sink1&amp;gt; &amp;lt;Sink2&amp;gt;
&amp;lt;Agent&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;

# set list of channels for source (separated by space)
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;

# set channel for sinks
&amp;lt;Agent&amp;gt;.sinks.&amp;lt;Sink1&amp;gt;.channel = &amp;lt;Channel1&amp;gt;
&amp;lt;Agent&amp;gt;.sinks.&amp;lt;Sink2&amp;gt;.channel = &amp;lt;Channel2&amp;gt;

&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.type = replicating
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面指定了selector的type的值为replication，其他的配置没有指定，使用的Replication方式，Source1会将数据分别存储到Channel1和Channel2，这两个channel里面存储的数据是相同的，然后数据被传递到Sink1和Sink2。&lt;/p&gt;

&lt;p&gt;Multiplexing方式，selector可以根据header的值来确定数据传递到哪一个channel，配置格式，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;# Mapping for multiplexing selector
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.type = multiplexing
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.header = &amp;lt;someHeader&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value1&amp;gt; = &amp;lt;Channel1&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value2&amp;gt; = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value3&amp;gt; = &amp;lt;Channel2&amp;gt;
#...

&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.default = &amp;lt;Channel2&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面selector的type的值为multiplexing，同时配置selector的header信息，还配置了多个selector的mapping的值，即header的值：如果header的值为Value1、Value2，数据从Source1路由到Channel1；如果header的值为Value2、Value3，数据从Source1路由到Channel2。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;实现load balance功能&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/flume-load-balance-agents.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Load balancing Sink Processor能够实现load balance功能，上图Agent1是一个路由节点，负责将Channel暂存的Event均衡到对应的多个Sink组件上，而每个Sink组件分别连接到一个独立的Agent上，示例配置，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2 k3
a1.sinkgroups.g1.processor.type = load_balance
a1.sinkgroups.g1.processor.backoff = true
a1.sinkgroups.g1.processor.selector = round_robin
a1.sinkgroups.g1.processor.selector.maxTimeOut=10000
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;实现failover能&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Failover Sink Processor能够实现failover功能，具体流程类似load balance，但是内部处理机制与load balance完全不同：Failover Sink Processor维护一个优先级Sink组件列表，只要有一个Sink组件可用，Event就被传递到下一个组件。如果一个Sink能够成功处理Event，则会加入到一个Pool中，否则会被移出Pool并计算失败次数，设置一个惩罚因子，示例配置如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;a1.sinkgroups = g1
a1.sinkgroups.g1.sinks = k1 k2 k3
a1.sinkgroups.g1.processor.type = failover
a1.sinkgroups.g1.processor.priority.k1 = 5
a1.sinkgroups.g1.processor.priority.k2 = 7
a1.sinkgroups.g1.processor.priority.k3 = 6
a1.sinkgroups.g1.processor.maxpenalty = 20000
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-7&quot;&gt;4. 安装和使用&lt;/h1&gt;

&lt;p&gt;Flume 的 rpm 安装方式很简单，这里不做说明。&lt;/p&gt;

&lt;h2 id=&quot;avro-&quot;&gt;示例1： avro 数据源&lt;/h2&gt;

&lt;p&gt;安装成功之后，在 /etc/flume/conf 目录创建f1.conf 文件，内容如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;agent-1.channels.ch-1.type = memory

agent-1.sources.avro-source1.channels = ch-1
agent-1.sources.avro-source1.type = avro
agent-1.sources.avro-source1.bind = 0.0.0.0
agent-1.sources.avro-source1.port = 41414
agent-1.sources.avro-source1.threads = 5

agent-1.sinks.log-sink1.channel = ch-1
agent-1.sinks.log-sink1.type = logger

agent-1.channels = ch-1
agent-1.sources = avro-source1
agent-1.sinks = log-sink1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 avro-source 配置说明，请参考 &lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html#avro-source&quot;&gt;avro-source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;接下来启动 agent：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/f1.conf -Dflume.root.logger=DEBUG,console -n agent-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;-n&lt;/code&gt; 指定agent名称&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-c&lt;/code&gt; 指定配置文件目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-f&lt;/code&gt; 指定配置文件&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-Dflume.root.logger=DEBUG,console&lt;/code&gt; 设置日志等级&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面可以启动一个 avro-client 客户端生产数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ flume-ng avro-client -c /etc/flume-ng/conf -H localhost -p 41414 -F /etc/passwd -Dflume.root.logger=DEBUG,console
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spooldir-&quot;&gt;示例2：spooldir 数据源&lt;/h2&gt;

&lt;p&gt;在 /etc/flume/conf 目录创建 f2.conf 文件，内容如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;agent-1.channels = ch-1
agent-1.sources = src-1

agent-1.channels.ch-1.type = memory

agent-1.sources.src-1.type = spooldir
agent-1.sources.src-1.channels = ch-1
agent-1.sources.src-1.spoolDir = /root/log
agent-1.sources.src-1.fileHeader = true

agent-1.sinks.log-sink1.channel = ch-1
agent-1.sinks.log-sink1.type = logger

agent-1.sinks = log-sink1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 Spooling Directory Source 配置说明，请参考 &lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html#spooling-directory-source&quot;&gt;Spooling Directory Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;接下来启动 agent：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/f2.conf -Dflume.root.logger=DEBUG,console -n agent-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，手动拷贝一个文件到 /root/log 目录，观察日志输出以及/root/log 目录下的变化。&lt;/p&gt;

&lt;h2 id=&quot;spooldir--hdfs&quot;&gt;示例3：spooldir 数据源，写入 hdfs&lt;/h2&gt;

&lt;p&gt;在 /etc/flume/conf 目录创建 f3.conf 文件，内容如下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;agent-1.channels.ch-1.type = file
agent-1.channels.ch-1.checkpointDir= /root/checkpoint
agent-1.channels.ch-1.dataDirs= /root/data

agent-1.sources.src-1.type = spooldir
agent-1.sources.src-1.channels = ch-1
agent-1.sources.src-1.spoolDir = /root/log
agent-1.sources.src-1.deletePolicy= never
agent-1.sources.src-1.fileHeader = true

agent-1.sources.src-1.interceptors =i1
agent-1.sources.src-1.interceptors.i1.type = timestamp

agent-1.sinks.sink_hdfs.channel = ch-1
agent-1.sinks.sink_hdfs.type = hdfs
agent-1.sinks.sink_hdfs.hdfs.path = hdfs://cdh1:8020/user/root/events/%Y-%m-%d
agent-1.sinks.sink_hdfs.hdfs.filePrefix = logs
agent-1.sinks.sink_hdfs.hdfs.inUsePrefix = .
agent-1.sinks.sink_hdfs.hdfs.rollInterval = 30
agent-1.sinks.sink_hdfs.hdfs.rollSize = 0
agent-1.sinks.sink_hdfs.hdfs.rollCount = 0
agent-1.sinks.sink_hdfs.hdfs.batchSize = 1000
agent-1.sinks.sink_hdfs.hdfs.writeFormat = text
agent-1.sinks.sink_hdfs.hdfs.fileType = DataStream
#agent-1.sinks.sink_hdfs.hdfs.fileType = CompressedStream
#agent-1.sinks.sink_hdfs.hdfs.codeC = lzop

agent-1.channels = ch-1
agent-1.sources = src-1
agent-1.sinks = sink_hdfs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 HDFS Sink配置说明，请参考 &lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html#hdfs-sink&quot;&gt;HDFS Sink&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;通过 interceptors 往 header 里添加 timestamp，这样做，可以在 hdfs.path 引用系统内部的时间变量或者主机的 hostname。&lt;/li&gt;
  &lt;li&gt;通过设置 &lt;code&gt;hdfs.inUsePrefix&lt;/code&gt;，例如设置为 &lt;code&gt;.&lt;/code&gt;时，hdfs 会把该文件当做隐藏文件，以避免在 mr 过程中读到这些临时文件，引起一些错误&lt;/li&gt;
  &lt;li&gt;如果使用 lzo 压缩，则需要手动创建 lzo 索引，可以通过修改 HdfsSink 的代码，通过代码创建索引&lt;/li&gt;
  &lt;li&gt;FileChannel 的目录最好是和 spooldir 的数据目录处于不同磁盘。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;spooldir--hbase&quot;&gt;示例4：spooldir 数据源，写入 HBase&lt;/h2&gt;

&lt;p&gt;关于 HBase Sink 配置说明，请参考 &lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html#hbasesink&quot;&gt;HBase Sink&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-8&quot;&gt;5. 开发相关&lt;/h1&gt;

&lt;h2 id=&quot;section-9&quot;&gt;5.1 编译源代码&lt;/h2&gt;

&lt;p&gt;从 github 下载源代码并编译：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone git@github.com:cloudera/flume-ng.git -b cdh4-1.4.0_4.7.0
$ cd flume-ng
$ mvn install -DskipTests -Phadoop-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果提示找不到 hadoop-test 的 jar 包，则修改 pom.xml 中的版本，如改为 &lt;code&gt;2.0.0-mr1-cdh4.7.0&lt;/code&gt;，具体版本视你使用的分支版本而定，我这里是 cdh4.7.0。&lt;/p&gt;

&lt;p&gt;如果提示找不到 uanodeset-parser 的 jarb，则在 pom.xml 中添加下面仓库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;repository&amp;gt;
  &amp;lt;id&amp;gt;tempo-db&amp;lt;/id&amp;gt;
  &amp;lt;url&amp;gt;http://maven.tempo-db.com/artifactory/list/twitter/
  &amp;lt;/url&amp;gt;
  &amp;lt;snapshots&amp;gt;
    &amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt;
  &amp;lt;/snapshots&amp;gt;
&amp;lt;/repository&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-10&quot;&gt;6. 最佳实践&lt;/h1&gt;

&lt;p&gt;参考&lt;a href=&quot;http://tech.meituan.com/mt-log-system-arch.html&quot;&gt;基于Flume的美团日志收集系统(一)架构和设计&lt;/a&gt;，列出一些最佳实践：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模块命名规则：所有的 Source 以 src 开头，所有的 Channel 以 ch 开头，所有的 Sink 以 sink 开头；&lt;/li&gt;
  &lt;li&gt;模块之间内部通信统一使用 Avro 接口；&lt;/li&gt;
  &lt;li&gt;将日志采集系统系统分为三层：Agent 层，Collector 层和 Store 层，其中 Agent 层每个机器部署一个进程，负责对单机的日志收集工作；Collector 层部署在中心服务器上，负责接收Agent层发送的日志，并且将日志根据路由规则写到相应的 Store 层中；Store 层负责提供永久或者临时的日志存储服务，或者将日志流导向其它服务器。&lt;/li&gt;
  &lt;li&gt;扩展 MemoryChannel 和 FileChannel ，提供 DualChannel 的实现，以提供高吞吐和大缓存&lt;/li&gt;
  &lt;li&gt;监控 collector HdfsSink写数据到 hdfs 的速度、FileChannel 中拥堵的 events 数量，以及写 hdfs 状态（查看是否有 .tmp 文件生成）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;美团对 flume 的改进代码见 github：&lt;a href=&quot;https://github.com/javachen/mt-flume&quot;&gt;https://github.com/javachen/mt-flume&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-11&quot;&gt;7. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;Flume User Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/flume/entry/flume_ng_architecture&quot;&gt;Apache Flume - Architecture of Flume NG&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://shiyanjun.cn/archives/915.html&quot;&gt;Flume(NG)架构设计要点及配置实践&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tech.meituan.com/mt-log-system-arch.html&quot;&gt;基于Flume的美团日志收集系统(一)架构和设计&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tech.meituan.com/mt-log-system-optimization.html&quot;&gt;基于Flume的美团日志收集系统(二)架构和设计&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/07/22/flume-ng.html</link>
      <guid>http://blog.javachen.com/2014/07/22/flume-ng.html</guid>
      <pubDate>2014-07-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>CDH中配置HDFS HA</title>
      <description>&lt;p&gt;最近又安装 hadoop 集群， 故尝试了一下配置 HDFS 的 HA，CDH4支持&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_hag_hdfs_ha_intro.html#topic_2_1_3_unique_1__section_ptk_fh5_mj_unique_1&quot;&gt;Quorum-based Storage&lt;/a&gt;和&lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_hag_hdfs_ha_intro.html#topic_2_1_3_unique_1__section_czt_fh5_mj_unique_1&quot;&gt;shared storage using NFS&lt;/a&gt;两种HA方案，而CDH5只支持第一种方案，即 QJM 的 HA 方案。&lt;/p&gt;

&lt;p&gt;关于 hadoop 集群的安装部署过程你可以参考 &lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt; 或者 &lt;a href=&quot;/2014/07/17/manual-install-cdh-hadoop.html&quot;&gt;手动安装 hadoop 集群的过程&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;集群规划&lt;/h2&gt;

&lt;p&gt;我一共安装了三个节点的集群，对于 HA 方案来说，三个节点准备安装如下服务：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cdh1：hadoop-hdfs-namenode(primary) 、hadoop-hdfs-journalnode、hadoop-hdfs-zkfc&lt;/li&gt;
  &lt;li&gt;cdh2：hadoop-hdfs-namenode(standby)、hadoop-hdfs-journalnode、hadoop-hdfs-zkfc&lt;/li&gt;
  &lt;li&gt;cdh3: hadoop-hdfs-journalnode&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据上面规划，在对应节点上安装相应的服务。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;安装步骤&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;停掉集群&lt;/h3&gt;

&lt;p&gt;停掉集群上所有服务。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/cmd.sh &#39; for x in `ls /etc/init.d/|grep spark` ; do service $x stop ; done&#39;
​$ sh /opt/cmd.sh &#39; for x in `ls /etc/init.d/|grep impala` ; do service $x stop ; done&#39;
$ sh /opt/cmd.sh &#39; for x in `ls /etc/init.d/|grep hive` ; do service $x stop ; done&#39;
$ sh /opt/cmd.sh &#39; for x in `ls /etc/init.d/|grep hbase` ; do service $x stop ; done&#39;
$ sh /opt/cmd.sh &#39; for x in `ls /etc/init.d/|grep hadoop` ; do service $x stop ; done&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cmd.sh代码内容见&lt;a href=&quot;/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop.html&quot;&gt;Hadoop集群部署权限总结&lt;/a&gt;一文中的/opt/shell/cmd.sh。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;停止客户端程序&lt;/h3&gt;

&lt;p&gt;停止服务集群的所有客户端程序，包括定时任务。&lt;/p&gt;

&lt;h3 id=&quot;hdfs-&quot;&gt;备份 hdfs 元数据&lt;/h3&gt;

&lt;p&gt;a，查找本地配置的文件目录（属性名为 dfs.name.dir 或者 dfs.namenode.name.dir或者hadoop.tmp.dir ）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;grep -C1 hadoop.tmp.dir /etc/hadoop/conf/hdfs-site.xml

#或者
grep -C1 dfs.namenode.name.dir /etc/hadoop/conf/hdfs-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过上面的命令，可以看到类似以下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;/data/dfs/nn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b，对hdfs数据进行备份&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd /data/dfs/nn
tar -cvf /root/nn_backup_data.tar .
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-4&quot;&gt;安装服务&lt;/h3&gt;

&lt;p&gt;在 cdh1、cdh2、cdh3 上安装 hadoop-hdfs-journalnode&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &#39;yum install hadoop-hdfs-journalnode -y &#39;
$ ssh cdh2 &#39;yum install hadoop-hdfs-journalnode -y &#39;
$ ssh cdh3 &#39;yum install hadoop-hdfs-journalnode -y &#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh1、cdh2 上安装 hadoop-hdfs-zkfc：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ssh cdh1 &quot;yum install hadoop-hdfs-zkfc -y &quot;
ssh cdh2 &quot;yum install hadoop-hdfs-zkfc -y &quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-5&quot;&gt;修改配置文件&lt;/h3&gt;

&lt;p&gt;修改/etc/hadoop/conf/core-site.xml，做如下修改：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;hdfs://mycluster:8020&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;ha.zookeeper.quorum&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;cdh1:21088,cdh2:21088,cdh3:21088&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改/etc/hadoop/conf/hdfs-site.xml，删掉一些原来的 namenode 配置，增加如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;!--  hadoop  HA --&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.nameservices&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;mycluster&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.ha.namenodes.mycluster&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;nn1,nn2&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.namenode.rpc-address.mycluster.nn1&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;cdh1:8020&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.namenode.rpc-address.mycluster.nn2&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;cdh2:8020&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.namenode.http-address.mycluster.nn1&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;cdh1:50070&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.namenode.http-address.mycluster.nn2&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;cdh2:50070&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.namenode.shared.edits.dir&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;qjournal://cdh1:8485,cdh2:8485,cdh3:8485/mycluster&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.journalnode.edits.dir&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;/data/dfs/jn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.client.failover.proxy.provider.mycluster&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.ha.fencing.methods&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;sshfence(hdfs)&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.ha.fencing.ssh.private-key-files&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;/var/lib/hadoop-hdfs/.ssh/id_rsa&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.ha.automatic-failover.enabled&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-6&quot;&gt;同步配置文件&lt;/h3&gt;

&lt;p&gt;将配置文件同步到集群其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/syn.sh /etc/hadoop/conf /etc/hadoop/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在journalnode的三个节点上创建目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &#39;mkdir -p /data/dfs/jn ; chown -R hdfs:hdfs /data/dfs/jn&#39;
$ ssh cdh2 &#39;mkdir -p /data/dfs/jn ; chown -R hdfs:hdfs /data/dfs/jn&#39;
$ ssh cdh3 &#39;mkdir -p /data/dfs/jn ; chown -R hdfs:hdfs /data/dfs/jn&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-7&quot;&gt;配置无密码登陆&lt;/h3&gt;

&lt;p&gt;在两个NN上配置hdfs用户间无密码登陆：&lt;/p&gt;

&lt;p&gt;对于 cdh1：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ passwd hdfs
$ su - hdfs
$ ssh-keygen
$ ssh-copy-id  cdh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 cdh2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ passwd hdfs
$ su - hdfs
$ ssh-keygen
$ ssh-copy-id   cdh1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;journalnode&quot;&gt;启动journalnode&lt;/h3&gt;

&lt;p&gt;启动cdh1、cdh2、cdh3上的 hadoop-hdfs-journalnode 服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &#39;service hadoop-hdfs-journalnode start&#39;
$ ssh cdh2 &#39;service hadoop-hdfs-journalnode start&#39;
$ ssh cdh3 &#39;service hadoop-hdfs-journalnode start&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-8&quot;&gt;初始化共享存储&lt;/h3&gt;

&lt;p&gt;在namenode上初始化共享存储，如果没有格式化，则先格式化：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hdfs namenode -initializeSharedEdits
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动NameNode：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service hadoop-hdfs-namenode start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;standby-namenode&quot;&gt;同步 Standby NameNode&lt;/h3&gt;

&lt;p&gt;cdh2作为 Standby NameNode，在该节点上先安装namenode服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hadoop-hdfs-namenode -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop namenode -bootstrapStandby
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是使用了kerberos，则先获取hdfs的ticket再执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEM.COM
$ hadoop namenode -bootstrapStandby
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，启动 Standby NameNode：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service hadoop-hdfs-namenode start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-9&quot;&gt;配置自动切换&lt;/h3&gt;

&lt;p&gt;在两个NameNode上，即cdh1和cdh2，安装hadoop-hdfs-zkfc&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &#39;yum install hadoop-hdfs-zkfc -y&#39;
$ ssh cdh2 &#39;yum install hadoop-hdfs-zkfc -y&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在任意一个NameNode上下面命令，其会创建一个znode用于自动故障转移。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hdfs zkfc -formatZK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你想对zookeeper的访问进行加密，则请参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_hag_hdfs_ha_enabling.html&quot;&gt;Enabling HDFS HA&lt;/a&gt; 中 Securing access to ZooKeeper 这一节内容。&lt;/p&gt;

&lt;p&gt;然后再两个 NameNode 节点上启动zkfc：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ssh cdh1 &quot;service hadoop-hdfs-zkfc start&quot;
$ ssh cdh2 &quot;service hadoop-hdfs-zkfc start&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-10&quot;&gt;测试&lt;/h3&gt;

&lt;p&gt;分别访问 http://cdh1:50070/ 和 http://cdh2:50070/ 查看谁是 active namenode，谁是 standyby namenode。&lt;/p&gt;

&lt;p&gt;查看某Namenode的状态：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#查看cdh1状态
$ sudo -u hdfs hdfs haadmin -getServiceState nn1
active

#查看cdh2状态
$ sudo -u hdfs hdfs haadmin -getServiceState nn2
standby
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行手动切换：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hdfs haadmin -failover nn1 nn2
Failover to NameNode at cdh2/192.168.56.122:8020 successful
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次访问 http://cdh1:50070/ 和 http://cdh2:50070/ 查看谁是 active namenode，谁是 standyby namenode。&lt;/p&gt;

&lt;h2 id=&quot;hbase-ha&quot;&gt;配置HBase HA&lt;/h2&gt;

&lt;p&gt;先停掉 hbase，然后修改/etc/hbase/conf/hbase-site.xml，做如下修改：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;!-- Configure HBase to use the HA NameNode nameservice --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://mycluster:8020/hbase&amp;lt;/value&amp;gt;       
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 zookeeper 节点上运行/usr/lib/zookeeper/bin/zkCli.sh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ls /hbase/splitlogs
$ rmr /hbase/splitlogs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后启动 hbase 服务。&lt;/p&gt;

&lt;h2 id=&quot;hive-ha&quot;&gt;配置 Hive HA&lt;/h2&gt;

&lt;p&gt;运行下面命令将hive的metastore的root地址的HDFS nameservice。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /usr/lib/hive/bin/metatool -listFSRoot 
Initializing HiveMetaTool..
Listing FS Roots..
hdfs://cdh1:8020/user/hive/warehouse  

$ /usr/lib/hive/bin/metatool -updateLocation hdfs://mycluster hdfs://cdh1 -tablePropKey avro.schema.url 
-serdePropKey schema.url  

$ metatool -listFSRoot 
Listing FS Roots..
Initializing HiveMetaTool..
hdfs://mycluster:8020/user/hive/warehouse
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;impala&quot;&gt;配置 Impala&lt;/h2&gt;

&lt;p&gt;不需要做什么修改，但是一定要记住 core-site.xml 中 &lt;code&gt;fs.defaultFS&lt;/code&gt; 参数值要带上端口号，在CDH中为 8020。&lt;/p&gt;

&lt;h2 id=&quot;yarn&quot;&gt;配置 YARN&lt;/h2&gt;

&lt;p&gt;暂时未使用，详细说明请参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_hag_mrv1_ha_config.html&quot;&gt;MapReduce (MRv1) and YARN (MRv2) High Availability&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;hue&quot;&gt;配置 Hue&lt;/h2&gt;

&lt;p&gt;暂时未使用，详细说明请参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_hag_hue_ha.html&quot;&gt;Hue High Availability&lt;/a&gt; 。&lt;/p&gt;

&lt;h2 id=&quot;llama&quot;&gt;配置  Llama&lt;/h2&gt;

&lt;p&gt;暂时未使用，详细说明请参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_hag_llama_ha.html&quot;&gt;Llama High Availability&lt;/a&gt; 。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/07/18/install-hdfs-ha-in-cdh.html</link>
      <guid>http://blog.javachen.com/2014/07/18/install-hdfs-ha-in-cdh.html</guid>
      <pubDate>2014-07-18T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>手动安装Hadoop集群的过程</title>
      <description>&lt;p&gt;最近又安装 Hadoop 集群，由于一些原因，没有使用 Hadoop 管理工具或者自动化安装脚本来安装集群，而是手动一步步的来安装，本篇文章主要是记录我手动安装 Hadoop 集群的过程，给大家做个参考。&lt;/p&gt;

&lt;p&gt;这里所说的手动安装，是指一步步的通过脚本来安装集群，并不是使用一键安装脚本或者一些管理界面来安装。&lt;/p&gt;

&lt;p&gt;开始之前，还是说明一下环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs6.4&lt;/li&gt;
  &lt;li&gt;CDH版本：4.7.0&lt;/li&gt;
  &lt;li&gt;节点数：4个&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在开始之前，你可以看看我以前写的一篇文章 &lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;，因为有些细节已经为什么这样做我不会在这篇文章中讲述。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;一些准备工作&lt;/h2&gt;

&lt;p&gt;在开始前，先选择一个节点为管理节点或者说是 NameNode 节点，其他节点为普通节点。&lt;/p&gt;

&lt;p&gt;安装的过程中，是使用 root 用户来运行脚本。&lt;/p&gt;

&lt;p&gt;为了部署方便，我会创建三个批量执行脚本，存放目录为/opt，一个脚本用于批量执行，文件名称为 cmd.sh，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for node in 1 2 3;do
	echo &quot;========$node========&quot;
	ssh 192.168.56.12$node $1
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外一个文件用于批量拷贝，文件名称为 syn.sh，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for node in 1 2 3;do
	echo &quot;========$node========&quot;
	scp -r $1 192.168.56.12$node:$2
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第三个文件用于批量管理 hadoop 服务，文件名称为 cluster.sh，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for node in 1 2 3;do
	ssh 192.168.56.12$node &#39;for src in `ls /etc/init.d|grep &#39;$1&#39;`;do service $src &#39;$2&#39;; done&#39;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，以上三个脚本需要你从当前管理节点配置无密码登陆到所有节点上。&lt;/p&gt;

&lt;p&gt;配置无密码登陆之后，需要在每台机器上安装 jdk 并设置环境变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/cmd.sh &#39;
$ yum remove jdk* -y
$ yum install jdk -y
$ rm -rf /usr/bin/java
$ ln -s /usr/java/default/bin/java /usr/bin/java
$ echo &quot;export JAVA_HOME=/usr/java/default&quot; &amp;gt;&amp;gt;/root/.bashrc
$ echo &quot;export PATH=\$JAVA_HOME/bin:\$PATH&quot; &amp;gt;&amp;gt; /root/.bashrc
$ source /root/.bashrc
&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hosts-&quot;&gt;配置 hosts 文件&lt;/h2&gt;

&lt;p&gt;在该节点上配置 hosts 文件，我安装的集群节点如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121 cdh1
192.168.56.122 cdh2
192.168.56.123 cdh3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将该文件同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/syn.sh /etc/hosts  /etc/hosts
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hadoop-hdfs&quot;&gt;安装 hadoop-hdfs&lt;/h2&gt;

&lt;p&gt;首先，在所有节点上安装一些基本的必须的依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/cmd.sh &#39;yum install hadoop hadoop-hdfs yarn hadoop-mapreduce hive hbase zookeeper hbase&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上只是安装一些基本依赖，并不会在/etc/init.d/下生成一些服务，而会在/etc/目录下创建一些 conf 目录，这样方便修改配置文件并执行批量同步。&lt;/p&gt;

&lt;p&gt;然后，按照你的集群规划，在每个节点上仅仅安装其需要的服务，例如在 cdh1上安装 NameNode，而在其他节点上安装 DataNode。&lt;/p&gt;

&lt;p&gt;接下来在管理节点上修改配置文件（可以参考 &lt;a href=&quot;https://github.com/javachen/hadoop-install/tree/master/shell/edh/template/hadoop&quot;&gt;https://github.com/javachen/hadoop-install/tree/master/shell/edh/template/hadoop&lt;/a&gt;），然后做同步：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/syn.sh /etc/hadoop/conf  /etc/hadoop/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建本地目录。NameNode 的数据目录，我定义在/data/dfs/nn；DataNode 的在/data/dfs/dn，当然还有 yarn 的目录。&lt;/p&gt;

&lt;p&gt;批量创建目录命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/cmd.sh &#39;
$ mkdir -p /data/dfs/nn /data/dfs/dn  /data/yarn/local   /var/log/hadoop-yarn;
$ chown -R hdfs:hdfs /data/dfs;
$ chown -R yarn:yarn /data/yarn/local;
$ chown -R yarn:yarn /var/log/hadoop-yarn;
$ mkdir -p /var/run/hadoop-hdfs
&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，就是格式化 NameNode：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop namenode -format
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动 hadoop-hdfs 相关的服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/cluster.sh hadoop-hdfs start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看状态：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/cluster.sh hadoop-hdfs status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在无法直接服务 web 界面的情况下，可以通过下面命令来检查每个节点是否启动成功：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop dfsadmin -report
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 /tmp 临时目录，并设置权限为 1777：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir /tmp
$ sudo -u hdfs hadoop fs -chmod -R 1777 /tmp
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;yarn&quot;&gt;安装 yarn&lt;/h2&gt;

&lt;p&gt;在 NN 节点上安装 hadoop-yarn-resourcemanager 和 hadoop-mapredice-history，其他节点安装 hadoop-yarn-nodemanager，修改配置文件。&lt;/p&gt;

&lt;p&gt;在 hdfs 上创建目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir -p /yarn/apps
$ sudo -u hdfs hadoop fs -chown yarn:mapred /yarn/apps
$ sudo -u hdfs hadoop fs -chmod -R 1777 /yarn/apps
$ sudo -u hdfs hadoop fs -mkdir -p /user/history
$ sudo -u hdfs hadoop fs -chmod -R 1777 /user/history
$ sudo -u hdfs hadoop fs -chown mapred:hadoop /user/history
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证 HDFS 结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -ls -R /
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你将会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drwxrwxrwt   - hdfs hadoop          0 2014-07-16 11:02 /tmp
drwxr-xr-x   - hdfs hadoop          0 2014-07-16 11:20 /user
drwxrwxrwt   - mapred hadoop          0 2014-07-16 11:20 /user/history
drwxr-xr-x   - hdfs   hadoop          0 2014-07-16 11:20 /yarn
drwxr-xr-x   - yarn   mapred          0 2014-07-16 11:20 /yarn/apps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为每个 MapReduce 用户创建主目录，比如说 hive 用户或者当前用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir /user/$USER
$ sudo -u hdfs hadoop fs -chown $USER /user/$USER
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动 mapred-historyserver :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ etc/init.d/hadoop-mapreduce-historyserver start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个节点启动 YARN :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/cluster.sh hadoop-yarn start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查yarn是否启动成功：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh /opt/cluster.sh hadoop-yarn status
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;其他&lt;/h2&gt;

&lt;p&gt;其他服务均可以参考此方法来简化安装，这里不做详述。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2014/07/17/manual-install-cdh-hadoop.html</link>
      <guid>http://blog.javachen.com/2014/07/17/manual-install-cdh-hadoop.html</guid>
      <pubDate>2014-07-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Spark集群安装和使用</title>
      <description>&lt;p&gt;本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。&lt;/p&gt;

&lt;p&gt;安装环境如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.5&lt;/li&gt;
  &lt;li&gt;Hadoop 版本：&lt;code&gt;cdh-5.4.0&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Spark 版本：&lt;code&gt;cdh5-1.3.0_5.4.0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于 yum 源的配置以及 Hadoop 集群的安装，请参考 &lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装&lt;/h1&gt;

&lt;p&gt;首先查看 Spark 相关的包有哪些：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum list |grep spark
spark-core.noarch                  1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
spark-history-server.noarch        1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
spark-master.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
spark-python.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
spark-worker.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
hue-spark.x86_64                   3.7.0+cdh5.4.0+1145-1.cdh5.4.0.p0.58.el6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上包作用如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;spark-core&lt;/code&gt;: spark 核心功能&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark-worker&lt;/code&gt;: spark-worker 初始化脚本&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark-master&lt;/code&gt;: spark-master 初始化脚本&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark-python&lt;/code&gt;: spark 的 Python 客户端&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hue-spark&lt;/code&gt;: spark 和 hue 集成包&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;spark-history-server&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在已经存在的 Hadoop 集群中，选择一个节点来安装 Spark Master，其余节点安装 Spark worker ，例如：在 cdh1 上安装 master，在 cdh1、cdh2、cdh3 上安装 worker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 在 cdh1 节点上运行
$ sudo yum install spark-core spark-master spark-worker spark-python spark-history-server -y

# 在 cdh1、cdh2、cdh3 上运行
$ sudo yum install spark-core spark-worker spark-python -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装成功后，我的集群各节点部署如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cdh1节点:  spark-master、spark-worker、spark-history-server
cdh2节点:  spark-worker 
cdh3节点:  spark-worker 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 配置&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2.1 修改配置文件&lt;/h2&gt;

&lt;p&gt;设置环境变量，在 &lt;code&gt;.bashrc&lt;/code&gt; 或者 &lt;code&gt;/etc/profile&lt;/code&gt; 中加入下面一行，并使其生效：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;export SPARK_HOME=/usr/lib/spark
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以修改配置文件 &lt;code&gt;/etc/spark/conf/spark-env.sh&lt;/code&gt;，其内容如下，你可以根据需要做一些修改，例如，修改 master 的主机名称为cdh1。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 设置 master 主机名称
export STANDALONE_SPARK_MASTER_HOST=cdh1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置 shuffle 和 RDD 数据存储路径，该值默认为&lt;code&gt;/tmp&lt;/code&gt;。使用默认值，可能会出现&lt;code&gt;No space left on device&lt;/code&gt;的异常，建议修改为空间较大的分区中的一个目录。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export SPARK_LOCAL_DIRS=/data/spark
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你和我一样使用的是虚拟机运行 spark，则你可能需要修改 spark 进程使用的 jvm 大小（关于 jvm 大小设置的相关逻辑见 &lt;code&gt;/usr/lib/spark/bin/spark-class&lt;/code&gt;）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export SPARK_DAEMON_MEMORY=256m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多spark相关的配置参数，请参考 &lt;a href=&quot;https://spark.apache.org/docs/latest/configuration.html&quot;&gt;Spark Configuration&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;spark-history-server&quot;&gt;2.2 配置 Spark History Server&lt;/h2&gt;

&lt;p&gt;在运行Spark应用程序的时候，driver会提供一个webUI给出应用程序的运行信息，但是该webUI随着应用程序的完成而关闭端口，也就是说，Spark应用程序运行完后，将无法查看应用程序的历史记录。Spark history server就是为了应对这种情况而产生的，通过配置，Spark应用程序在运行完应用程序之后，将应用程序的运行信息写入指定目录，而Spark history server可以将这些运行信息装载并以web的方式供用户浏览。&lt;/p&gt;

&lt;p&gt;创建 &lt;code&gt;/etc/spark/conf/spark-defaults.conf&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp /etc/spark/conf/spark-defaults.conf.template /etc/spark/conf/spark-defaults.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加下面配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;spark.master=spark://cdh1:7077
spark.eventLog.dir=/user/spark/applicationHistory
spark.eventLog.enabled=true
spark.yarn.historyServer.address=cdh1:18082
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你是在hdfs上运行Spark，则执行下面命令创建&lt;code&gt;/user/spark/applicationHistory&lt;/code&gt;目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir /user/spark
$ sudo -u hdfs hadoop fs -mkdir /user/spark/applicationHistory
$ sudo -u hdfs hadoop fs -chown -R spark:spark /user/spark
$ sudo -u hdfs hadoop fs -chmod 1777 /user/spark/applicationHistory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置 &lt;code&gt;spark.history.fs.logDirectory&lt;/code&gt; 参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export SPARK_HISTORY_OPTS=&quot;$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=/tmp/spark -Dspark.history.ui.port=18082&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 /tmp/spark 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p /tmp/spark
$ chown spark:spark /tmp/spark
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果集群配置了 kerberos ，则添加下面配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;HOSTNAME=`hostname -f`
export SPARK_HISTORY_OPTS=&quot;$SPARK_HISTORY_OPTS -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=spark/${HOSTNAME}@LASHOU.COM -Dspark.history.kerberos.keytab=/etc/spark/conf/spark.keytab -Dspark.history.ui.acls.enable=true&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hive&quot;&gt;2.3 和Hive集成&lt;/h2&gt;

&lt;p&gt;Spark和hive集成，最好是将hive的配置文件链接到Spark的配置文件目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;2.4 同步配置文件&lt;/h2&gt;

&lt;p&gt;修改完 cdh1 节点上的配置文件之后，需要同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;scp -r /etc/spark/conf  cdh2:/etc/spark
scp -r /etc/spark/conf  cdh3:/etc/spark
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;3. 启动和停止&lt;/h1&gt;

&lt;h2 id=&quot;section-5&quot;&gt;3.1 使用系统服务管理集群&lt;/h2&gt;

&lt;p&gt;启动脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 在 cdh1 节点上运行
$ sudo service spark-master start

# 在 cdh1 节点上运行，如果 hadoop 集群配置了 kerberos，则运行之前需要先获取 spark 用户的凭证
# kinit -k -t /etc/spark/conf/spark.keytab spark/cdh1@JAVACHEN.COM
$ sudo service spark-history-server start

# 在cdh2、cdh3 节点上运行
$ sudo service spark-worker start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo service spark-master stop
$ sudo service spark-worker stop
$ sudo service spark-history-server stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，你还可以设置开机启动：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo chkconfig spark-master on
$ sudo chkconfig spark-worker on
$ sudo chkconfig spark-history-server on
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spark-&quot;&gt;3.2 使用 Spark 自带脚本管理集群&lt;/h2&gt;

&lt;p&gt;另外，你也可以使用 Spark 自带的脚本来启动和停止，这些脚本在 &lt;code&gt;/usr/lib/spark/sbin&lt;/code&gt; 目录下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ls /usr/lib/spark/sbin
slaves.sh        spark-daemons.sh  start-master.sh  stop-all.sh
spark-config.sh  spark-executor    start-slave.sh   stop-master.sh
spark-daemon.sh  start-all.sh      start-slaves.sh  stop-slaves.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在master节点修改 &lt;code&gt;/etc/spark/conf/slaves&lt;/code&gt; 文件添加worker节点的主机名称，并且还需要在master和worker节点之间配置无密码登陆。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# A Spark Worker will be started on each of the machines listed below.
cdh2
cdh3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，你也可以通过下面脚本启动 master 和 worker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /usr/lib/spark/sbin
$ ./start-master.sh
$ ./start-slaves.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，你也可以通过&lt;code&gt;spark-class&lt;/code&gt;脚本来启动，例如，下面脚本以standalone模式启动worker：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./bin/spark-class org.apache.spark.deploy.worker.Worker spark://cdh1:18080
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;web&quot;&gt;3.3 访问web界面&lt;/h2&gt;

&lt;p&gt;你可以通过 &lt;a href=&quot;http://cdh1:18080/&quot;&gt;http://cdh1:18080/&lt;/a&gt; 访问 spark master 的 web 界面。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/spark/spark-master-web-ui.jpg&quot; alt=&quot;spark-master-web-ui&quot; /&gt;&lt;/p&gt;

&lt;p&gt;访问Spark History Server页面：http://cdh1:18082/。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/spark/spark-hs-web-ui.jpg&quot; alt=&quot;spark-hs-web-ui&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注意：我这里使用的是CDH版本的 Spark，Spark master UI的端口为&lt;code&gt;18080&lt;/code&gt;，不是 Apache Spark 的 &lt;code&gt;8080&lt;/code&gt; 端口。CDH发行版中Spark使用的端口列表如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;7077&lt;/code&gt; – Default Master RPC port&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;7078&lt;/code&gt; – Default Worker RPC port&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;18080&lt;/code&gt; – Default Master web UI port&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;18081&lt;/code&gt; – Default Worker web UI port&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;18080&lt;/code&gt; – Default HistoryServer web UI port&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-6&quot;&gt;4. 测试&lt;/h1&gt;

&lt;p&gt;Spark可以以&lt;a href=&quot;/2015/03/30/spark-test-in-local-mode.html&quot;&gt;本地模式运行&lt;/a&gt;，也支持三种集群管理模式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/spark-standalone.html&quot;&gt;Standalone&lt;/a&gt;  – Spark原生的资源管理，由Master负责资源的分配。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/running-on-mesos.html&quot;&gt;Apache Mesos&lt;/a&gt;  – 运行在Mesos之上，由Mesos进行资源调度&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/running-on-yarn.html&quot;&gt;Hadoop YARN&lt;/a&gt; –  运行在Yarn之上，由Yarn进行资源调度。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外 Spark 的 &lt;a href=&quot;https://spark.apache.org/docs/latest/ec2-scripts.html&quot;&gt;EC2 launch scripts&lt;/a&gt; 可以帮助你容易地在Amazon EC2上启动standalone cluster.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;在集群不是特别大，并且没有 mapReduce 和 Spark 同时运行的需求的情况下，用 Standalone 模式效率最高。&lt;/li&gt;
    &lt;li&gt;Spark可以在应用间（通过集群管理器）和应用中（如果一个 SparkContext 中有多项计算任务）进行资源调度。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;standalone-&quot;&gt;4.1 Standalone 模式&lt;/h2&gt;

&lt;p&gt;该模式中，资源调度是Spark框架自己实现的，其节点类型分为Master和Worker节点，其中Driver节点运行在Master节点中，并且有常驻内存的Master进程守护，Worker节点上常驻Worker守护进程，负责与Master通信。&lt;/p&gt;

&lt;p&gt;Standalone 模式是Master-Slaves架构的集群模式，Master存在着单点故障问题，目前，Spark提供了两种解决办法：基于文件系统的故障恢复模式，基于Zookeeper的HA方式。&lt;/p&gt;

&lt;p&gt;Standalone 模式需要在每一个节点部署Spark应用，并按照实际情况配置故障恢复模式。&lt;/p&gt;

&lt;p&gt;你可以使用交互式命令spark-shell、pyspark或者&lt;a href=&quot;https://spark.apache.org/docs/latest/submitting-applications.html&quot;&gt;spark-submit script&lt;/a&gt;连接到集群，下面以wordcount程序为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-shell --master spark://cdh1:7077
scala&amp;gt; val file = sc.textFile(&quot;hdfs://cdh1:8020/tmp/test.txt&quot;)
scala&amp;gt; val counts = file.flatMap(line =&amp;gt; line.split(&quot; &quot;)).map(word =&amp;gt; (word, 1)).reduceByKey(_ + _)
scala&amp;gt; counts.count()
scala&amp;gt; counts.saveAsTextFile(&quot;hdfs://cdh1:8020/tmp/output&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果运行成功，可以打开浏览器访问 http://cdh1:4040 查看应用运行情况。&lt;/p&gt;

&lt;p&gt;运行过程中，可能会出现下面的异常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;14/10/24 14:51:40 WARN hdfs.BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
14/10/24 14:51:40 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1738)
	at java.lang.Runtime.loadLibrary0(Runtime.java:823)
	at java.lang.System.loadLibrary(System.java:1028)
	at com.hadoop.compression.lzo.GPLNativeCodeLoader.&amp;lt;clinit&amp;gt;(GPLNativeCodeLoader.java:32)
	at com.hadoop.compression.lzo.LzoCodec.&amp;lt;clinit&amp;gt;(LzoCodec.java:71)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:249)
	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1836)
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1801)
	at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决方法可以参考 &lt;a href=&quot;http://blog.csdn.net/pelick/article/details/11599391&quot;&gt;Spark连接Hadoop读取HDFS问题小结&lt;/a&gt; 这篇文章，执行以下命令，然后重启服务即可：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp /usr/lib/hadoop/lib/native/libgplcompression.so $JAVA_HOME/jre/lib/amd64/
cp /usr/lib/hadoop/lib/native/libhadoop.so $JAVA_HOME/jre/lib/amd64/
cp /usr/lib/hadoop/lib/native/libsnappy.so $JAVA_HOME/jre/lib/amd64/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 spark-submit 以 Standalone 模式运行 SparkPi 程序的命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit --class org.apache.spark.examples.SparkPi  --master spark://cdh1:7077 /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;需要说明的是&lt;/strong&gt;：&lt;code&gt;Standalone mode does not support talking to a kerberized HDFS&lt;/code&gt;，如果你以 &lt;code&gt;spark-shell --master spark://cdh1:7077&lt;/code&gt; 方式访问安装有 kerberos 的 HDFS 集群上访问数据时，会出现下面异常:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;15/04/02 11:58:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, bj03-bi-pro-hdpnamenn): java.io.IOException: Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]; Host Details : local host is: &quot;cdh1/192.168.56.121&quot;; destination host is: &quot;192.168.56.121&quot;:8020;
        org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        org.apache.hadoop.ipc.Client.call(Client.java:1415)
        org.apache.hadoop.ipc.Client.call(Client.java:1364)
        org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        com.sun.proxy.$Proxy17.getBlockLocations(Unknown Source)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spark-on-mesos-&quot;&gt;4.2 Spark On Mesos 模式&lt;/h2&gt;

&lt;p&gt;参考 &lt;a href=&quot;http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/&quot;&gt;http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;spark-on-yarn-&quot;&gt;4.3 Spark on Yarn 模式&lt;/h2&gt;

&lt;p&gt;Spark on Yarn 模式同样也支持两种在 Yarn 上启动 Spark 的方式，一种是 cluster 模式，Spark driver 在 Yarn 的 application master 进程中运行，客户端在应用初始化完成之后就会退出；一种是 client 模式，Spark driver 运行在客户端进程中。Spark on Yarn 模式是可以访问配置有 kerberos 的 HDFS 文件的。&lt;/p&gt;

&lt;p&gt;CDH Spark中，以 cluster 模式启动，命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit --class path.to.your.Class --deploy-mode cluster --master yarn [options] &amp;lt;app jar&amp;gt; [app options]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CDH Spark中，以 client 模式启动，命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit --class path.to.your.Class --deploy-mode client --master yarn [options] &amp;lt;app jar&amp;gt; [app options]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以SparkPi程序为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ spark-submit --class org.apache.spark.examples.SparkPi \
    --deploy-mode cluster  \
    --master yarn  \
    --num-executors 3 \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar \
    10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，运行在 YARN 集群之上的时候，可以手动把 spark-assembly 相关的 jar 包拷贝到 hdfs 上去，然后设置 &lt;code&gt;SPARK_JAR&lt;/code&gt; 环境变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hdfs dfs -mkdir -p /user/spark/share/lib
$ hdfs dfs -put $SPARK_HOME/lib/spark-assembly.jar  /user/spark/share/lib/spark-assembly.jar

$ SPARK_JAR=hdfs://&amp;lt;nn&amp;gt;:&amp;lt;port&amp;gt;/user/spark/share/lib/spark-assembly.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;spark-sql&quot;&gt;5. Spark-SQL&lt;/h1&gt;

&lt;p&gt;Spark 安装包中包括了 Spark-SQL ，运行 spark-sql 命令，在 cdh5.2 中会出现下面异常：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /usr/lib/spark/bin
$ ./spark-sql
java.lang.ClassNotFoundException: org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:247)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:319)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Failed to load Spark SQL CLI main class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.
You need to build Spark with -Phive.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh5.4 中会出现下面异常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.cli.CliDriver
  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
  ... 18 more
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上可以知道  Spark-SQL 编译时没有集成 Hive，故需要重新编译 spark 源代码。&lt;/p&gt;

&lt;h2 id=&quot;spark-sql-1&quot;&gt;编译 Spark-SQL&lt;/h2&gt;

&lt;p&gt;以下内容参考 &lt;a href=&quot;/2015/04/28/compile-cdh-spark-source-code.html&quot;&gt;编译Spark源代码&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;下载cdh5-1.3.0_5.4.0分支的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone git@github.com:cloudera/spark.git
$ cd spark
$ git checkout -b origin/cdh5-1.3.0_5.4.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用maven 编译，先修改根目录下的 pom.xml，添加一行 &lt;code&gt;&amp;lt;module&amp;gt;sql/hive-thriftserver&amp;lt;/module&amp;gt;&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;modules&amp;gt;
    &amp;lt;module&amp;gt;core&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;bagel&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;graphx&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;mllib&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;tools&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;streaming&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;sql/catalyst&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;sql/core&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;sql/hive&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;sql/hive-thriftserver&amp;lt;/module&amp;gt; &amp;lt;!--添加的一行--&amp;gt;
    &amp;lt;module&amp;gt;repl&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;assembly&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/twitter&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/kafka&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/flume&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/flume-sink&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/zeromq&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;external/mqtt&amp;lt;/module&amp;gt;
    &amp;lt;module&amp;gt;examples&amp;lt;/module&amp;gt;
  &amp;lt;/modules&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot;
$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive -Phive-thriftserver -DskipTests clean package
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果编译成功之后， 会在 assembly/target/scala-2.10 目录下生成：spark-assembly-1.3.0-cdh5.4.0.jar，在 examples/target/scala-2.10 目录下生成：spark-examples-1.3.0-cdh5.4.0.jar，然后将 spark-assembly-1.3.0-cdh5.4.0.jar 拷贝到 /usr/lib/spark/lib 目录，然后再来运行 spark-sql。&lt;/p&gt;

&lt;p&gt;但是，经测试 cdh5.4.0 版本中的 spark 的 sql/hive-thriftserver 模块存在编译错误，最后无法编译成功，故需要等到 cloudera 官方更新源代码或者等待下一个 cdh 版本集成 spark-sql。&lt;/p&gt;

&lt;p&gt;虽然 spark-sql 命令用不了，但是我们可以在 spark-shell 中使用 SQLContext 来运行 sql 语句，限于篇幅，这里不做介绍，你可以参考 &lt;a href=&quot;http://www.infoobjects.com/spark-sql-schemardd-programmatically-specifying-schema/&quot;&gt;http://www.infoobjects.com/spark-sql-schemardd-programmatically-specifying-schema/&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-7&quot;&gt;6. 总结&lt;/h1&gt;

&lt;p&gt;本文主要介绍了 CDH5 集群中 Spark 的安装过程以及三种集群运行模式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Standalone – &lt;code&gt;spark-shell --master spark://host:port&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Apache Mesos – &lt;code&gt;spark-shell --master mesos://host:port&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Hadoop YARN – &lt;code&gt;spark-shell --master yarn&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果以本地模式运行，则为 &lt;code&gt;spark-shell --master local&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;关于 Spark 的更多介绍可以参考官网或者一些&lt;a href=&quot;http://colobu.com/tags/Spark/&quot;&gt;中文翻译的文章&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-8&quot;&gt;7. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/spark-standalone.html&quot;&gt;Spark Standalone Mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/pelick/article/details/11599391&quot;&gt;Spark连接Hadoop读取HDFS问题小结&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/&quot;&gt;Apache Spark探秘：三种分布式部署方式比较&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/07/01/spark-install-and-usage.html</link>
      <guid>http://blog.javachen.com/2014/07/01/spark-install-and-usage.html</guid>
      <pubDate>2014-07-01T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase中的一些注意事项</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 安装集群前&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;配置SSH无密码登陆&lt;/li&gt;
  &lt;li&gt;DNS。HBase使用本地 hostname 才获得IP地址，正反向的DNS都是可以的。你还可以设置 &lt;code&gt;hbase.regionserver.dns.interface&lt;/code&gt; 来指定主接口，设置 &lt;code&gt;hbase.regionserver.dns.nameserver&lt;/code&gt; 来指定nameserver，而不使用系统带的&lt;/li&gt;
  &lt;li&gt;安装NTP服务，并配置和检查crontab是否生效&lt;/li&gt;
  &lt;li&gt;操作系统调优，包括最大文件句柄，nproc hard 和 soft limits等等&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;conf/hdfs-site.xml&lt;/code&gt;里面的 &lt;code&gt;dfs.datanode.max.xcievers&lt;/code&gt; 参数，至少要有4096&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hdfs&quot;&gt;2. HDFS客户端配置&lt;/h1&gt;

&lt;p&gt;如果你希望Hadoop集群上做HDFS 客户端配置 ，例如你的HDFS客户端的配置和服务端的不一样。按照如下的方法配置，HBase就能看到你的配置信息:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在hbase-env.sh里将 &lt;code&gt;HBASE_CLASSPATH&lt;/code&gt; 环境变量加上 &lt;code&gt;HADOOP_CONF_DIR&lt;/code&gt; 。&lt;/li&gt;
  &lt;li&gt;在&lt;code&gt;${HBASE_HOME}/conf&lt;/code&gt; 下面加一个 hdfs-site.xml (或者 hadoop-site.xml) ，最好是软连接&lt;/li&gt;
  &lt;li&gt;如果你的HDFS客户端的配置不多的话，你可以把这些加到 hbase-site.xml上面.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如HDFS的配置 &lt;code&gt;dfs.replication&lt;/code&gt; 你希望复制5份，而不是默认的3份。如果你不照上面的做的话，Hbase只会复制3份。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;3. 一些配置参数&lt;/h1&gt;

&lt;p&gt;以下参数来自apache的hbase版本，如果你使用的其他厂商的hbase，有可能默认值不一样。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;zookeeper.session.timeout&lt;/code&gt;：这个默认值是3分钟。这意味着一旦一个server宕掉了，Master至少需要3分钟才能察觉到宕机，开始恢复。你可能希望将这个超时调短，这样Master就能更快的察觉到了。在你调这个值之前，你需要确认你的JVM的GC参数，否则一个长时间的GC操作就可能导致超时。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.regionserver.handler.count&lt;/code&gt;：这个设置决定了处理用户请求的线程数量。默认是10，这个值设的比较小，主要是为了预防用户用一个比较大的写缓冲，然后还有很多客户端并发，这样region servers会垮掉。有经验的做法是，当请求内容很大(上MB，如大puts, 使用缓存的scans)的时候，把这个值放低。请求内容较小的时候(gets, 小puts, ICVs, deletes)，把这个值放大。把这个值放大的危险之处在于，把所有的Put操作缓冲意味着对内存有很大的压力，甚至会导致OutOfMemory.一个运行在内存不足的机器的RegionServer会频繁的触发GC操作，渐渐就能感受到停顿。一段时间后，集群也会受到影响，因为所有的指向这个region的请求都会变慢。这样就会拖累集群，加剧了这个问题。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.client.keyvalue.maxsize&lt;/code&gt;：一个KeyValue实例的最大size。如果设置为0或者更小，就会禁用这个检查。默认10MB。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.regionserver.lease.period&lt;/code&gt;：户端租用HRegion server 期限，即超时阀值。单位是毫秒。默认情况下，客户端必须在这个时间内发一条信息，否则视为死掉。默认值为60000。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.regionserver.msginterval&lt;/code&gt;：RegionServer 发消息给 Master 时间间隔，单位是毫秒，默认: 3000&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.regionserver.optionallogflushinterval&lt;/code&gt;：将Hlog同步到HDFS的间隔。如果Hlog没有积累到一定的数量，到了时间，也会触发同步。默认是1秒，单位毫秒。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.regionserver.logroll.period&lt;/code&gt;：提交commit log的间隔，不管有没有写足够的值。默认: 3600000&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.regionserver.thread.splitcompactcheckfrequency&lt;/code&gt;：region server 多久执行一次split/compaction 检查。默认: 20000&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.balancer.period&lt;/code&gt;：Master执行region balancer的间隔。默认: 300000&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.hregion.memstore.block.multiplier&lt;/code&gt;：如果memstore有&lt;code&gt;hbase.hregion.memstore.block.multiplier&lt;/code&gt;倍数的&lt;code&gt;hbase.hregion.flush.size&lt;/code&gt;的大小，就会阻塞update操作。这是为了预防在update高峰期会导致的失控。如果不设上界，flush的时候会花很长的时间来合并或者分割，最坏的情况就是引发out of memory异常。默认: 2&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.hstore.compactionThreshold&lt;/code&gt;：当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，把这HStoreFiles写成一个。这个值越大，需要合并的时间就越长。默认: 3&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.hstore.blockingStoreFiles&lt;/code&gt;：当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，update会阻塞直到合并完成，直到超过了&lt;code&gt;hbase.hstore.blockingWaitTime&lt;/code&gt;的值。默认: 7&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;shell-&quot;&gt;4. Shell 技巧&lt;/h1&gt;

&lt;h2 id=&quot;irbrc&quot;&gt;irbrc&lt;/h2&gt;

&lt;p&gt;可以在你自己的Home目录下创建一个.irbrc文件，在这个文件里加入自定义的命令。有一个有用的命令就是记录命令历史，这样你就可以把你的命令保存起来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ more .irbrc
require &#39;irb/ext/save-history&#39;
IRB.conf[:SAVE_HISTORY] = 100
IRB.conf[:HISTORY_FILE] = &quot;#{ENV[&#39;HOME&#39;]}/.irb-save-history&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;shell-debug-&quot;&gt;Shell 切换成debug 模式&lt;/h2&gt;

&lt;p&gt;你可以将shell切换成debug模式。这样可以看到更多的信息。例如可以看到命令异常的stack trace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hbase&amp;gt; debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;想要在shell中看到 DEBUG 级别的 logging ，可以在启动的时候加上 &lt;code&gt;-d&lt;/code&gt; 参数.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./bin/hbase shell -d
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hbase--mapreduce&quot;&gt;5. HBase 和 MapReduce&lt;/h1&gt;

&lt;p&gt;当 MapReduce job的HBase table 使用TableInputFormat为数据源格式的时候,他的splitter会给这个table的每个region一个map。因此，如果一个table有100个region，就有100个map-tasks，不论需要scan多少个column families 。&lt;/p&gt;

&lt;p&gt;通常建议关掉针对HBase的MapReduce job的&lt;code&gt;预测执行&lt;/code&gt;(speculative execution)功能。这个功能也可以用每个Job的配置来完成。对于整个集群，使用预测执行意味着双倍的运算量。这可不是你所希望的。&lt;/p&gt;

&lt;h1 id=&quot;hbase--schema-&quot;&gt;6.HBase 的 Schema 设计&lt;/h1&gt;

&lt;p&gt;flush和compaction操作是针对一个Region。&lt;/p&gt;

&lt;p&gt;Compaction操作现在是根据一个column family下的全部文件的数量触发的，而不是根据文件大小触发的。当很多的column families在flush和compaction时,会造成很多没用的I/O负载(要想解决这个问题，需要将flush和compaction操作只针对一个column family)&lt;/p&gt;

&lt;p&gt;行的版本的数量是HColumnDescriptor设置的，每个column family可以单独设置，默认是3。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;7. 性能调优&lt;/h1&gt;

&lt;p&gt;1、长时间GC停顿&lt;/p&gt;

&lt;p&gt;Hbase中常见的两种stop-the-world的GC操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一种是CMS失败的模式&lt;/li&gt;
  &lt;li&gt;另一种是老一代的堆碎片导致的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要想定位第一种，只要将CMS执行的时间提前就可以了，加入 &lt;code&gt;-XX:CMSInitiatingOccupancyFraction&lt;/code&gt; 参数，把值调低。可以先从60%和70%开始(这个值调的越低，触发的GC次数就越多，消耗的CPU时间就越长)。要想定位第二种错误，Todd加入了一个实验性的功能，将你的Configuration中的 &lt;code&gt;hbase.hregion.memstore.mslab.enabled&lt;/code&gt; 设置为true。&lt;/p&gt;

&lt;p&gt;2、使用压缩&lt;/p&gt;

&lt;p&gt;3、设置合理的版本&lt;/p&gt;

&lt;p&gt;4、控制split和compaction&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;8. 需要理解一些过程&lt;/h1&gt;

&lt;h2 id=&quot;split&quot;&gt;8.1 什么时候做split？&lt;/h2&gt;

&lt;p&gt;答：根据拆分策略算法来定，具体过程见：&lt;a href=&quot;/2014/01/16/hbase-region-split-policy.html&quot;&gt;HBase笔记：Region拆分策略&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;compaction&quot;&gt;8.2 什么时候做compaction？&lt;/h2&gt;

&lt;p&gt;答：当有3个小文件时候，会进行合并小文件&lt;/p&gt;

&lt;h2 id=&quot;memstoreflush&quot;&gt;8.3 memstore什么时候flush，什么时候阻塞写？&lt;/h2&gt;

&lt;p&gt;答：memstore满了64M就会flush，当memstore大小达到128M时候，聚会阻塞update，进行flush。&lt;/p&gt;

&lt;h2 id=&quot;hlog&quot;&gt;8.4 HLog什么时候会阻塞写？&lt;/h2&gt;
</description>
      <link>http://blog.javachen.com/2014/06/26/some-tips-about-hbase.html</link>
      <guid>http://blog.javachen.com/2014/06/26/some-tips-about-hbase.html</guid>
      <pubDate>2014-06-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>MapReduce任务参数调优</title>
      <description>&lt;p&gt;本文主要记录Hadoop 2.x版本中MapReduce参数调优，不涉及Yarn的调优。&lt;/p&gt;

&lt;p&gt;Hadoop的默认配置文件（以cdh5.0.1为例）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.3.0-cdh5.0.1/hadoop-project-dist/hadoop-common/core-default.xml&quot;&gt;core-default.xml&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.3.0-cdh5.0.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml&quot;&gt;hdfs-default.xml&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.3.0-cdh5.0.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml&quot;&gt;mapred-default.xml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;说明：&lt;/p&gt;

  &lt;p&gt;在hadoop2中有些参数名称过时了，例如原来的&lt;code&gt;mapred.reduce.tasks&lt;/code&gt;改名为&lt;code&gt;mapreduce.job.reduces&lt;/code&gt;了，当然，这两个参数你都可以使用，只是第一个参数过时了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 操作系统调优&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;增大打开文件数据和网络连接上限，调整内核参数&lt;code&gt;net.core.somaxconn&lt;/code&gt;，提高读写速度和网络带宽使用率&lt;/li&gt;
  &lt;li&gt;适当调整&lt;code&gt;epoll的文件描述符&lt;/code&gt;上限，提高Hadoop RPC并发&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;关闭swap&lt;/code&gt;。如果进程内存不足，系统会将内存中的部分数据暂时写入磁盘，当需要时再将磁盘上的数据动态换置到内存中，这样会降低进程执行效率&lt;/li&gt;
  &lt;li&gt;增加&lt;code&gt;预读缓存区&lt;/code&gt;大小。预读可以减少磁盘寻道次数和I/O等待时间&lt;/li&gt;
  &lt;li&gt;设置&lt;code&gt;openfile&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hdfs&quot;&gt;2. Hdfs参数调优&lt;/h1&gt;

&lt;h2 id=&quot;core-defaultxml&quot;&gt;2.1 core-default.xml：&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;hadoop.tmp.dir&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值： /tmp&lt;/li&gt;
  &lt;li&gt;说明： 尽量手动配置这个选项，否则的话都默认存在了里系统的默认临时文件/tmp里。并且手动配置的时候，如果服务器是多磁盘的，每个磁盘都设置一个临时文件目录，这样便于mapreduce或者hdfs等使用的时候提高磁盘IO效率。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;fs.trash.interval&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值： 0&lt;/li&gt;
  &lt;li&gt;说明： 这个是开启hdfs文件删除自动转移到垃圾箱的选项，值为垃圾箱文件清除时间。一般开启这个会比较好，以防错误删除重要文件。单位是分钟。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;io.file.buffer.size&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：4096&lt;/li&gt;
  &lt;li&gt;说明：SequenceFiles在读写中可以使用的缓存大小，可减少 I/O 次数。在大型的 Hadoop cluster，建议可设定为 65536 到 131072。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hdfs-defaultxml&quot;&gt;2.2 hdfs-default.xml：&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;dfs.blocksize&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：134217728&lt;/li&gt;
  &lt;li&gt;说明： 这个就是hdfs里一个文件块的大小了，CDH5中默认128M。太大的话会有较少map同时计算，太小的话也浪费可用map个数资源，而且文件太小namenode就浪费内存多。根据需要进行设置。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;dfs.namenode.handler.count&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：10&lt;/li&gt;
  &lt;li&gt;说明：设定 namenode server threads 的数量，这些 threads 會用 RPC 跟其他的 datanodes 沟通。当 datanodes 数量太多时会发現很容易出現 RPC timeout，解決方法是提升网络速度或提高这个值，但要注意的是 thread 数量多也表示 namenode 消耗的内存也随着增加&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;mapreduce&quot;&gt;3. MapReduce参数调优&lt;/h1&gt;

&lt;p&gt;包括以下节点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;合理设置槽位数目&lt;/li&gt;
  &lt;li&gt;调整心跳配置&lt;/li&gt;
  &lt;li&gt;磁盘块配置&lt;/li&gt;
  &lt;li&gt;设置RPC和线程数目&lt;/li&gt;
  &lt;li&gt;启用批量任务调度&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mapred-defaultxml&quot;&gt;3.1 mapred-default.xml：&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;mapred.reduce.tasks&lt;/code&gt;（&lt;code&gt;mapreduce.job.reduces&lt;/code&gt;）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：1&lt;/li&gt;
  &lt;li&gt;说明：默认启动的reduce数。通过该参数可以手动修改reduce的个数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.task.io.sort.factor&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：10&lt;/li&gt;
  &lt;li&gt;说明：Reduce Task中合并小文件时，一次合并的文件数据，每次合并的时候选择最小的前10进行合并。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.task.io.sort.mb&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：100&lt;/li&gt;
  &lt;li&gt;说明： Map Task缓冲区所占内存大小。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapred.child.java.opts&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：-Xmx200m&lt;/li&gt;
  &lt;li&gt;说明：jvm启动的子线程可以使用的最大内存。建议值&lt;code&gt;-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.jobtracker.handler.count&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：10&lt;/li&gt;
  &lt;li&gt;说明：JobTracker可以启动的线程数，一般为tasktracker节点的4%。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：5&lt;/li&gt;
  &lt;li&gt;说明：reuduce shuffle阶段并行传输数据的数量。这里改为10。集群大可以增大。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.tasktracker.http.threads&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：40&lt;/li&gt;
  &lt;li&gt;说明：map和reduce是通过http进行数据传输的，这个是设置传输的并行线程数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.map.output.compress&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值：false&lt;/li&gt;
  &lt;li&gt;说明： map输出是否进行压缩，如果压缩就会多耗cpu，但是减少传输时间，如果不压缩，就需要较多的传输带宽。配合mapreduce.map.output.compress.codec使用，默认是org.apache.hadoop.io.compress.DefaultCodec，可以根据需要设定数据压缩方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.reduce.shuffle.merge.percent&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值： 0.66&lt;/li&gt;
  &lt;li&gt;说明：reduce归并接收map的输出数据可占用的内存配置百分比。类似mapreduce.reduce.shuffle.input.buffer.percen属性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.reduce.shuffle.memory.limit.percent&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值： 0.25&lt;/li&gt;
  &lt;li&gt;说明：一个单一的shuffle的最大内存使用限制。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.jobtracker.handler.count&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值： 10&lt;/li&gt;
  &lt;li&gt;说明：可并发处理来自tasktracker的RPC请求数，默认值10。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapred.job.reuse.jvm.num.tasks&lt;/code&gt;（&lt;code&gt;mapreduce.job.jvm.numtasks&lt;/code&gt;）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值： 1&lt;/li&gt;
  &lt;li&gt;说明：一个jvm可连续启动多个同类型任务，默认值1，若为-1表示不受限制。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;mapreduce.tasktracker.tasks.reduce.maximum&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认值： 2&lt;/li&gt;
  &lt;li&gt;说明：一个tasktracker并发执行的reduce数，建议为cpu核数&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;4. 系统优化&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;4.1 避免排序&lt;/h2&gt;

&lt;p&gt;对于一些不需要排序的应用，比如hash join或者limit n，可以将排序变为可选环节，这样可以带来一些好处：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在Map Collect阶段，不再需要同时比较partition和key，只需要比较partition，并可以使用更快的计数排序（O(n)）代替快速排序（O(NlgN)）&lt;/li&gt;
  &lt;li&gt;在Map Combine阶段，不再需要进行归并排序，只需要按照字节合并数据块即可。&lt;/li&gt;
  &lt;li&gt;去掉排序之后，Shuffle和Reduce可同时进行，这样就消除了Reduce Task的屏障（所有数据拷贝完成之后才能执行reduce()函数）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shuffle&quot;&gt;4.2 Shuffle阶段内部优化&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Map端–用Netty代替Jetty&lt;/li&gt;
  &lt;li&gt;Reduce端–批拷贝&lt;/li&gt;
  &lt;li&gt;将Shuffle阶段从Reduce Task中独立出来&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;section-3&quot;&gt;5. 总结&lt;/h1&gt;

&lt;p&gt;在运行mapreduce任务中，经常调整的参数有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;mapred.reduce.tasks&lt;/code&gt;：手动设置reduce个数&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.map.output.compress&lt;/code&gt;：map输出结果是否压缩&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.map.output.compress.codec&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.output.fileoutputformat.compress&lt;/code&gt;：job输出结果是否压缩&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.output.fileoutputformat.compress.type&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/06/24/tuning-in-mapreduce.html</link>
      <guid>http://blog.javachen.com/2014/06/24/tuning-in-mapreduce.html</guid>
      <pubDate>2014-06-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>MapReduce任务运行过程</title>
      <description>&lt;p&gt;下图是MapReduce任务运行过程的一个图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://zhaomingtai.u.qiniudn.com/mapredurce1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Map-Reduce的处理过程主要涉及以下四个部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;客户端Client：用于提交Map-reduce任务job&lt;/li&gt;
  &lt;li&gt;JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker&lt;/li&gt;
  &lt;li&gt;TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker&lt;/li&gt;
  &lt;li&gt;HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上图中主要包括以下过程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提交作业&lt;/li&gt;
  &lt;li&gt;作业初始化&lt;/li&gt;
  &lt;li&gt;任务分配&lt;/li&gt;
  &lt;li&gt;执行任务&lt;/li&gt;
  &lt;li&gt;进度和状态更新&lt;/li&gt;
  &lt;li&gt;完成作业&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 提交作业&lt;/h1&gt;

&lt;h2 id=&quot;shell&quot;&gt;运行Shell命令&lt;/h2&gt;

&lt;p&gt;使用hadoop提供的命令行或者通过编程接口提交任务，命令行方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ HADOOP_HOME/bin/hadoop jar job.jar \  
    -D mapred.job.name=&quot;task-job&quot; \  
    -D mapred.reduce.tasks=3 \  
    -files=blacklist.txt,whitelist.xml \  
    -libjars=aaa.jar \  
    -archives=bbb.zip \  
    -input /test/input \  
    -output /test/output 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当用户按上述命令格式提交作业后，命令行脚本会调用JobClient.runJob()方法提交作业&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;作业文件上传&lt;/h2&gt;

&lt;p&gt;JobClient将作业提交到JobTracker节点上之前，需要作业写初始化工作。初始化工作由 &lt;code&gt;JobClient.submitJobInternal(job)&lt;/code&gt; 实现，这些初始化包括获取作业的jobId、创建HDFS目录、上传作业以及生成所有的InputSplit分片的相关信息等。&lt;/p&gt;

&lt;p&gt;MapReduce的作业文件的上传和下载都是由DistributedCache透明完成的，它是Hadoop专门开发的数据分发工具。&lt;/p&gt;

&lt;p&gt;JobClient上传文件时可以修改文件副本数（通过参数 &lt;code&gt;mapred.submit.replication&lt;/code&gt; 指定，默认值为10），这样的话可以分摊负载以避免产生访问热点。&lt;/p&gt;

&lt;h2 id=&quot;inputsplit&quot;&gt;产生InputSplit文件&lt;/h2&gt;

&lt;p&gt;作业提交后，JobClient会调用InputFormat的getSplits()方法生成相关的split分片信息，该信息包括InputSplit元数据信息和原始的InputSplit信息，其中元数据信息被JobTracker使用，第二部分在Map Task初始化时由Mapper使用来获取自己要处理的数据，这两部分数据被保存到job.split文件和job.splitmetainfo文件中。&lt;/p&gt;

&lt;h2 id=&quot;jobtracker&quot;&gt;作业提交到JobTracker&lt;/h2&gt;

&lt;p&gt;调用JobTracker的submitJob()方法将作业提交。在这一阶段会依次进行如下操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）、为作业创建JobInProgress对象。JobTracker会为用户提交的每一个作业创建一个JobInProgress对象，这个对象维护了作业的运行时信息，主要用于跟踪正在运行的作业的状态和进度；&lt;/li&gt;
  &lt;li&gt;2）、检查用户是否具有指定队列的作业提交权限。Hadoop以队列为单位来管理作业和资源，每个队列分配有一定亮的资源，管理严可以为每个队列指定哪些用户有权限提交作业；&lt;/li&gt;
  &lt;li&gt;3）、检查作业配置的内存使用量是否合理。用户在提交作业时，可已分别通过参数 &lt;code&gt;mapred.job.map.memory.mb&lt;/code&gt; 和&lt;code&gt;mapred.job.reduce.memory.mb&lt;/code&gt; 指定Map Task和Reduce Task的内存使用量，而管理员可以给集群中的Map Task和Reduce Task分别设置中的内存使用量，一旦用户配置的内存使用量超过总的内存限制，作业就会提交失败；&lt;/li&gt;
  &lt;li&gt;4）、JobTracker收到提交的作业后，并不会马上对其进行初始化，而是会交给TaskScheduler调度器，由他按照一定的策略对作业做初始化操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JobTracker采用了观察者模式将“提交新作业”这一事件告诉TaskScheduler&lt;/p&gt;

&lt;p&gt;提交任务后，runJob每隔一秒钟轮询一次job的进度，将进度返回到命令行，直到任务运行完毕。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;2. 作业初始化&lt;/h1&gt;

&lt;p&gt;调度器调用JobTracker.initJob()方法来对新作业做初始化的。Hadoop将每个作业分节成4中类型的任务：Setup Task，Map Task，Reduce Task和Cleanup Task，它们的运行时信息由TaskInProgress维护，因此，从某个方面将，创建这些任务就是创建TaskInProgress对象。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Setup Task&lt;/code&gt;。作业初始化标志性任务，它进行一些很简单的作业初始化工作。该类型任务又分为Map Setup Task和Reduce Setup Task两种，并且只能运行一次。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Map Task&lt;/code&gt;。Map阶段的数据处理任务。 其数目及对应的处理数据分片由应用程序中的InputFormat中间确定。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Reduce Task&lt;/code&gt;。Reduce阶段的处理数据的任务。其数目可以由用户通过参数 &lt;code&gt;mapred.reduce.tasks&lt;/code&gt; 指定。Hadoop刚开始的时候只会调度Map Task任务，直到Map Task完成数目达到由参数 &lt;code&gt;mapred.reduce.slowstart.completed.maps&lt;/code&gt;指定的百分比（默认值为0.05，即百分之5）后，才开始调度Reduce Task。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Cleanup Task&lt;/code&gt;。作业结束的标志性任务，主要是做一些作业清理的工作，比如删除作业在运行中产生的一些零食目录和数据等信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;说明：可以通过参数 &lt;code&gt;mapred.committer.job.setup.cleanup.needed&lt;/code&gt; 配置是否为作业创建Setup/Cleanup Task，以避免他们拖慢作业执行进度且降低作业的可靠性。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;3. 任务分配&lt;/h1&gt;

&lt;p&gt;Tasktracker 和 JobTracker 通过心跳通信分配一个任务&lt;/p&gt;

&lt;p&gt;TaskTracker 定期发送心跳，告知 JobTracker, tasktracker 是否还存活，并充当两者之间的消息通道。&lt;/p&gt;

&lt;p&gt;TaskTracker 主动向 JobTracker 询问是否有作业。若自己有空闲的 solt,就可在心跳阶段得到 JobTracker 发送过来的 Map 任务或 Reduce 任务。对于 map 任务和 task 任务，TaskTracker 有固定数量的任务槽，准确数量由 tasktracker 核的个数核内存的大小来确定。默认调度器在处理 reduce 任务槽之前，会填充满空闲的 map 任务槽，因此，如果 tasktracker 至少有一个空闲的 map 任务槽，tasktracker 会为它选择一个 map 任务，否则选择一个 reduce 任务。选择 map 任务时，jobTracker 会考虑数据本地化（任务运行在输入分片所在的节点），而 reduce 任务不考虑数据本地化。任务还可能是机架本地化。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;4. 执行任务&lt;/h1&gt;

&lt;p&gt;tasktracker 执行任务大致步骤：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;被分配到一个任务后，从共享文件中把作业的jar复制到本地，并将程序执行需要的全部文件（配置信息、数据分片）复制到本地&lt;/li&gt;
  &lt;li&gt;为任务新建一个本地工作目录&lt;/li&gt;
  &lt;li&gt;内部类TaskRunner实例启动一个新的jvm运行任务&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-5&quot;&gt;5. 进度和状态更新&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;状态包括：作业或认为的状态（成功，失败，运行中）、map 和 reduce 的进度、作业计数器的值、状态消息或描述&lt;/li&gt;
  &lt;li&gt;task 运行时，将自己的状态发送给 TaskTracker,由 TaskTracker 心跳机制向 JobTracker 汇报&lt;/li&gt;
  &lt;li&gt;状态进度由计数器实现&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://zhaomingtai.u.qiniudn.com/updateStatusMapredurce.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;6. 完成作业&lt;/h1&gt;

&lt;p&gt;当JobTracker获得最后一个task的运行成功的报告后，将job得状态改为成功。&lt;/p&gt;

&lt;p&gt;当JobClient从JobTracker轮询的时候，发现此job已经成功结束，则向用户打印消息，从runJob函数中返回。&lt;/p&gt;

&lt;h1 id=&quot;section-7&quot;&gt;7. 总结&lt;/h1&gt;

&lt;p&gt;以上过程通过时序图来表达过程如下：&lt;/p&gt;

&lt;h1 id=&quot;section-8&quot;&gt;8. 参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://kangfoo.u.qiniudn.com/article/2014/03/hadoop-mapreduce--gong-zuo-ji-zhi/&quot;&gt;Hadoop MapReduce 工作机制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/06/24/the-running-process-of-mapreduce-job.html</link>
      <guid>http://blog.javachen.com/2014/06/24/the-running-process-of-mapreduce-job.html</guid>
      <pubDate>2014-06-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase和Cassandra比较</title>
      <description>&lt;p&gt;HBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Google File System一样，HBase构建于和Google File System类似的Hadoop HDFS之上。&lt;/p&gt;

&lt;p&gt;Cassandra可以看作是Amazon Dynamo的开源实现。和Dynamo不同之处在于，Cassandra结合了Google   Bigtable的ColumnFamily的数据模型。可以简单地认为，Cassandra是一个P2P的，高可靠性并具有丰富的数据模型的分布式文件系统。&lt;/p&gt;

&lt;h1 id=&quot;hbase-vs-cassandra&quot;&gt;HBase vs Cassandra&lt;/h1&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;HBase&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Cassandra&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;语言&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Java&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Java&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;出发点&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;BigTable&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;BigTable and Dynamo&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;License&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Apache&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Apache&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Protocol&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HTTP/REST (also Thrift)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Custom, binary (Thrift)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据分布&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;表划分为多个region存在不同region server上&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;改进的一致性哈希（虚拟节点）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;存储目标&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;大文件&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;小文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一致性&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;强一致性&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;最终一致性，Quorum NRW策略&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;架构&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;master/slave&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;p2p&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;高可用性&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;NameNode是HDFS的单点故障点&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;P2P和去中心化设计，不会出现单点故障&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;伸缩性&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Region Server扩容，通过将自身发布到Master，Master均匀分布Region&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;扩容需在Hash Ring上多个节点间调整数据分布&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;读写性能&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据读写定位可能要通过最多6次的网络RPC，性能较低。&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据读写定位非常快&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据冲突处理&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;乐观并发控制（optimistic concurrency control）&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;向量时钟&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;临时故障处理&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Region Server宕机，重做HLog&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;数据回传机制：某节点宕机，hash到该节点的新数据自动路由到下一节点做 hinted handoff，源节点恢复后，推送回源节点。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;永久故障恢复&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Region Server恢复，master重新给其分配region&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Merkle 哈希树，通过Gossip协议同步Merkle Tree，维护集群节点间的数据一致性&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;成员通信及错误检测&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Zookeeper&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;基于Gossip&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CAP&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1，强一致性，0数据丢失。2，可用性低。3，扩容方便。&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1，弱一致性，数据可能丢失。2，可用性高。3，扩容方便。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;facebookcassandra&quot;&gt;facebook为什么放弃Cassandra？&lt;/h1&gt;

&lt;p&gt;参考：&lt;a href=&quot;http://www.zhihu.com/question/19593207&quot;&gt;http://www.zhihu.com/question/19593207&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Facebook开发Cassandra初衷是用于Inbox Search，但是后来的Message System则使用了HBase，Facebook对此给出的解释是Cassandra的&lt;code&gt;最终一致性模型&lt;/code&gt;不适合Message System，HBase具有更简单的一致性模型，当然还有其他的原因。HBase更加的成熟，成功的案例也比较多等等。Twitter和Digg都曾经很高调的选用Cassandra，但是最后也都放弃了，当然Twitter还有部分项目也还在使用Cassandra，但是主要的Tweet已经不是了。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
      <link>http://blog.javachen.com/2014/06/24/hbase-vs-cassandra.html</link>
      <guid>http://blog.javachen.com/2014/06/24/hbase-vs-cassandra.html</guid>
      <pubDate>2014-06-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive中的排序语法</title>
      <description>&lt;h1 id=&quot;order-by&quot;&gt;ORDER BY&lt;/h1&gt;

&lt;p&gt;hive中的ORDER BY语句和关系数据库中的sql语法相似。他会对查询结果做全局排序，这意味着所有的数据会传送到一个Reduce任务上，这样会导致在大数量的情况下，花费大量时间。&lt;/p&gt;

&lt;p&gt;与数据库中 ORDER BY 的区别在于在&lt;code&gt;hive.mapred.mode = strict&lt;/code&gt;模式下，必须指定 limit 否则执行会报错。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; set hive.mapred.mode=strict;
hive&amp;gt; select * from test order by id;
FAILED: SemanticException 1:28 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token &#39;id&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive&amp;gt; set hive.mapred.mode=unstrict;
hive&amp;gt; select * from test order BY id ;
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 1.88 sec   HDFS Read: 305 HDFS Write: 32 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 880 msec
OK
1	a
1	a
2	b
2	b
3	c
3	c
4	d
4	d
Time taken: 24.609 seconds, Fetched: 8 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的日志可以看到：启动了一个reduce进行全局排序。&lt;/p&gt;

&lt;h1 id=&quot;sort-by&quot;&gt;SORT BY&lt;/h1&gt;

&lt;p&gt;SORT BY不是全局排序，其在数据进入reducer前完成排序，因此在有多个reduce任务情况下，SORT BY只能保证每个reduce的输出有序，而不能保证全局有序。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：SORT BY 不受 &lt;code&gt;hive.mapred.mode&lt;/code&gt; 参数的影响&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;你可以通过设置&lt;code&gt;mapred.reduce.tasks&lt;/code&gt;的值来控制reduce的数，然后对reduce输出的结果做二次排序。&lt;/p&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; set mapred.reduce.tasks=3;
hive&amp;gt; select * from test sort BY id ; 
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 3   Cumulative CPU: 4.48 sec   HDFS Read: 305 HDFS Write: 32 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 480 msec
OK
1	a
2	b
3	c
4	d
2	b
3	c
4	d
1	a
Time taken: 29.574 seconds, Fetched: 8 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的日志可以看到：启动了三个reduce分别排序，最后的结果不是有序的。&lt;/p&gt;

&lt;h1 id=&quot;distribute-by-with-sort-by&quot;&gt;DISTRIBUTE BY with SORT BY&lt;/h1&gt;

&lt;p&gt;DISTRIBUTE BY能够控制map的输出在reduce中如何划分。其可以按照指定的字段对数据进行划分到不同的输出reduce/文件中。&lt;/p&gt;

&lt;p&gt;DISTRIBUTE BY和GROUP BY有点类似，DISTRIBUTE BY控制reduce如何处理数据，而SORT BY控制reduce中的数据如何排序。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：hive要求DISTRIBUTE BY语句出现在SORT BY语句之前。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select * from test distribute BY id sort by id asc;  
Job 0: Map: 1  Reduce: 3   Cumulative CPU: 4.24 sec   HDFS Read: 305 HDFS Write: 32 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 240 msec
OK
3	c
3	c
1	a
1	a
4	d
4	d
2	b
2	b
Time taken: 29.89 seconds, Fetched: 8 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的日志可以看到：启动了三个reduce分别排序，最后的结果不是有序的。&lt;/p&gt;

&lt;h1 id=&quot;cluster-by&quot;&gt;CLUSTER BY来代替&lt;/h1&gt;

&lt;p&gt;当DISTRIBUTE BY的字段和SORT BY的字段相同时，可以用CLUSTER BY来代替 DISTRIBUTE BY with SORT BY。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：CLUSTER BY不能添加desc或者asc。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select * from test cluster by id asc;              
FAILED: ParseException line 1:33 extraneous input &#39;asc&#39; expecting EOF near &#39;&amp;lt;EOF&amp;gt;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select * from test cluster by id ;
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 3   Cumulative CPU: 4.58 sec   HDFS Read: 305 HDFS Write: 32 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 580 msec
OK
3	c
3	c
1	a
1	a
4	d
4	d
2	b
2	b
Time taken: 30.646 seconds, Fetched: 8 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的日志可以看到：启动了三个reduce分别排序，最后的结果不是有序的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;怎样让最后的结果是有序的呢？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;可以这样做：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select a.* from (select * from test cluster by id ) a order by a.id ;
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 3   Cumulative CPU: 4.5 sec   HDFS Read: 305 HDFS Write: 448 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 1.96 sec   HDFS Read: 1232 HDFS Write: 32 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 460 msec
OK
1	a
1	a
2	b
2	b
3	c
3	c
4	d
4	d
Time taken: 118.261 seconds, Fetched: 8 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;总结&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ORDER BY是全局排序，但在数据量大的情况下，花费时间会很长&lt;/li&gt;
  &lt;li&gt;SORT BY是将reduce的单个输出进行排序，不能保证全局有序&lt;/li&gt;
  &lt;li&gt;DISTRIBUTE BY可以按指定字段将数据划分到不同的reduce中&lt;/li&gt;
  &lt;li&gt;当DISTRIBUTE BY的字段和SORT BY的字段相同时，可以用CLUSTER BY来代替 DISTRIBUTE BY with SORT BY。&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/06/22/sort-in-hive-query.html</link>
      <guid>http://blog.javachen.com/2014/06/22/sort-in-hive-query.html</guid>
      <pubDate>2014-06-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Lucene介绍</title>
      <description>&lt;h1 id=&quot;lucene&quot;&gt;1. Lucene是什么&lt;/h1&gt;

&lt;p&gt;Lucene 是一个开源的、成熟的全文索引与信息检索(IR)库，采用Java实现。信息检索式指文档搜索、文档内信息搜索或者文档相关的元数据搜索等操作。Lucene是apache软件基金会项目组的一个子项目，是一个开放源代码的全文检索引擎工具包，即它不是一个完整的搜索应用程序，而是为你的应用程序提供索引和搜索功能。&lt;/p&gt;

&lt;p&gt;Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。&lt;code&gt;全文检索&lt;/code&gt;（Full-Text Retrieval）是指以文本作为检索对象，找出含有指定词汇的文本。全面、准确和快速是衡量全文检索系统的关键指标。&lt;/p&gt;

&lt;p&gt;官方网站：&lt;a href=&quot;http://lucene.apache.org/&quot;&gt;http://lucene.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;2. 历史&lt;/h1&gt;

&lt;p&gt;Lucene最初是由Doug Cutting开发的，在SourceForge的网站上提供下载。在2001年9月做为高质量的开源Java产品加入到Apache软件基金会的 Jakarta家族中。随着每个版本的发布，这个项目得到明显的增强，也吸引了更多的用户和开发人员&lt;/p&gt;

&lt;h1 id=&quot;lucene-1&quot;&gt;3. Lucene能做什么&lt;/h1&gt;

&lt;p&gt;Lucene使你可以为你的应用程序添加索引和搜索能力。Lucene可以索引并能使得可以转换成文本格式的任何数据能够被搜索。&lt;/p&gt;

&lt;p&gt;同样，利用Lucene你可以索引存放于数据库中的数据，提供给用户很多数据库没有提供的全文搜索的能力。&lt;/p&gt;

&lt;p&gt;Lucene作为一个全文检索引擎，其具有如下突出的优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;索引文件格式独立于应用平台。Lucene定义了一套以8位字节为基础的索引文件格式，使得兼容系统或者不同平台的应用能够共享建立的索引文件。&lt;/li&gt;
  &lt;li&gt;在传统全文检索引擎的倒排索引的基础上，实现了分块索引，能够针对新的文件建立小文件索引，提升索引速度。然后通过与原有索引的合并，达到优化的目的。&lt;/li&gt;
  &lt;li&gt;优秀的面向对象的系统架构，使得对于Lucene扩展的学习难度降低，方便扩充新功能。&lt;/li&gt;
  &lt;li&gt;设计了独立于语言和文件格式的文本分析接口，索引器通过接受Token流完成索引文件的创立，用户扩展新的语言和文件格式，只需要实现文本分析的接口。&lt;/li&gt;
  &lt;li&gt;已经默认实现了一套强大的查询引擎，用户无需自己编写代码即使系统可获得强大的查询能力，Lucene的查询实现中默认实现了布尔操作、模糊查询、分组查询等等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;lucene-&quot;&gt;4. Lucene 原理&lt;/h1&gt;

&lt;p&gt;Lucene的检索算法属于索引检索，即用空间来换取时间，对需要检索的文件、字符流进行全文索引，在检索的时候对索引进行快速的检索，得到检索位置，这个位置记录检索词出现的文件路径或者某个关键词。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;5. 索引组件&lt;/h1&gt;

&lt;p&gt;为了快速搜索大量的文本，你必须首先简历针对文本索引，将文本内容转换成能够进行快速搜索的格式，从而消除慢速顺序扫描处理所带来的影响。这个过程就叫做&lt;code&gt;索引操作&lt;/code&gt;，它的输出就叫做&lt;code&gt;索引&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Lucene 采用的是一种称为&lt;code&gt;反向索引&lt;/code&gt;（inverted index）的机制。反向索引就是说我们维护了一个词/短语表，对于这个表中的每个词/短语，都有一个链表描述了有哪些文档包含了这个词/短语。这样在用户输入查询条件的时候，就能非常快的得到搜索结果。&lt;/p&gt;

&lt;p&gt;整个索引过程包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;获取内容&lt;/li&gt;
  &lt;li&gt;建立文档&lt;/li&gt;
  &lt;li&gt;文档分析&lt;/li&gt;
  &lt;li&gt;文档索引&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;6. 搜索组件&lt;/h1&gt;

&lt;p&gt;搜索是在一个索引中查找单词来找出它们所出现的文档的过程。搜索质量主要由&lt;code&gt;查准率&lt;/code&gt;和&lt;code&gt;查全率&lt;/code&gt;来衡量。查全率用来衡量搜索系统朝赵相关文档的能力；而查准率用来衡量搜索系统过滤费相关文档的能力。&lt;/p&gt;

&lt;p&gt;搜索引擎的典型组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;用户搜索界面&lt;/li&gt;
  &lt;li&gt;建立查询&lt;/li&gt;
  &lt;li&gt;搜索查询&lt;/li&gt;
  &lt;li&gt;展现结果&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;7. 一些概念&lt;/h1&gt;

&lt;h2 id=&quot;analyzer&quot;&gt;7.1 Analyzer&lt;/h2&gt;

&lt;p&gt;Analyzer是分析器，它的作用是把一个字符串按某种规则划分成一个个词语，并去除其中的无效词语，这里说的无效词语是指英文中的“of”、“the”，中文中的“的”、“地”等词语，这些词语在文章中大量出现，但是本身不包含什么关键信息，去掉有利于缩小索引文件、提高效率、提高命中率。&lt;/p&gt;

&lt;p&gt;分词的规则千变万化，但目的只有一个：按语义划分。这点在英文中比较容易实现，因为英文本身就是以单词为单位的，已经用空格分开；而中文则必须以某种方法将连成一片的句子划分成一个个词语。&lt;/p&gt;

&lt;h2 id=&quot;document&quot;&gt;7.2 Document&lt;/h2&gt;

&lt;p&gt;用户提供的源是一条条记录，它们可以是文本文件、字符串或者数据库表的一条记录等等。一条记录经过索引之后，就是以一个Document的形式存储在索引文件中的。用户进行搜索，也是以Document列表的形式返回。&lt;/p&gt;

&lt;h2 id=&quot;field&quot;&gt;7.3 Field&lt;/h2&gt;

&lt;p&gt;一个Document可以包含多个信息域，例如一篇文章可以包含“标题”、“正文”、“最后修改时间”等信息域，这些信息域就是通过Field在Document中存储的。&lt;/p&gt;

&lt;p&gt;Field有两个属性可选：存储和索引。通过存储属性你可以控制是否对这个Field进行存储；通过索引属性你可以控制是否对该Field进行索引。这看起来似乎有些废话，事实上对这两个属性的正确组合很重要，。&lt;/p&gt;

&lt;p&gt;面举例说明：还是以刚才的文章为例子，我们需要对标题和正文进行全文搜索，所以我们要把索引属性设置为真，同时我们希望能直接从搜索结果中提取文章标题，所以我们把标题域的存储属性设置为真，但是由于正文域太大了，我们为了缩小索引文件大小，将正文域的存储属性设置为假，当需要时再直接读取文件；我们只是希望能从搜索解果中提取最后修改时间，不需要对它进行搜索，所以我们把最后修改时间域的存储属性设置为真，索引属性设置为假。上面的三个域涵盖了两个属性的三种组合，还有一种全为假的没有用到，事实上Field不允许你那么设置，因为既不存储又不索引的域是没有意义的。&lt;/p&gt;

&lt;h2 id=&quot;term&quot;&gt;7.4 Term&lt;/h2&gt;

&lt;p&gt;term是搜索的最小单位，它表示文档的一个词语，term由两部分组成：它表示的词语和这个词语所出现的field。&lt;/p&gt;

&lt;h2 id=&quot;tocken&quot;&gt;7.5 Tocken&lt;/h2&gt;
&lt;p&gt;tocken是term的一次出现，它包含trem文本和相应的起止偏移，以及一个类型字符串。一句话中可以出现多次相同的词语，它们都用同一个term表示，但是用不同的tocken，每个tocken标记该词语出现的地方。&lt;/p&gt;

&lt;h2 id=&quot;segment&quot;&gt;7.6 Segment&lt;/h2&gt;

&lt;p&gt;添加索引时并不是每个document都马上添加到同一个索引文件，它们首先被写入到不同的小文件，然后再合并成一个大索引文件，这里每个小文件都是一个segment。&lt;/p&gt;

&lt;h1 id=&quot;lucene-2&quot;&gt;8. Lucene示例&lt;/h1&gt;

&lt;p&gt;Github上的一些Lucene例子：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yusuke/lucene-examples&quot;&gt;https://github.com/yusuke/lucene-examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yozhao/lucene-examples&quot;&gt;https://github.com/yozhao/lucene-examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/06/21/the-introduction-of-lucene.html</link>
      <guid>http://blog.javachen.com/2014/06/21/the-introduction-of-lucene.html</guid>
      <pubDate>2014-06-21T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Storm集群安装部署步骤</title>
      <description>&lt;p&gt;开始学习Storm，本文主要记录Storm集群安装部署步骤，不包括对Storm的介绍。&lt;/p&gt;

&lt;p&gt;安装storm集群，需要依赖以下组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Zookeeper&lt;/li&gt;
  &lt;li&gt;Python&lt;/li&gt;
  &lt;li&gt;Zeromq&lt;/li&gt;
  &lt;li&gt;Storm&lt;/li&gt;
  &lt;li&gt;JDK&lt;/li&gt;
  &lt;li&gt;JZMQ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;故安装过程根据上面的组件分为以下几步：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;安装JDK&lt;/li&gt;
  &lt;li&gt;安装Zookeeper集群&lt;/li&gt;
  &lt;li&gt;安装Python及依赖&lt;/li&gt;
  &lt;li&gt;安装Storm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外，操作系统环境为：Centos6.4，安装用户为：root。&lt;/p&gt;

&lt;h1 id=&quot;jdk&quot;&gt;1. 安装JDK&lt;/h1&gt;

&lt;p&gt;安装jdk有很多方法，可以参考文博客&lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;中的jdk安装步骤，需要说明的是下面的zookeeper集群安装方法也可以参考此文。&lt;/p&gt;

&lt;p&gt;不管你用什么方法，最后需要配置JAVA_HOME并检测当前jdk版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -version
java version &quot;1.6.0_31&quot;
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;zookeeper&quot;&gt;2. 安装Zookeeper集群&lt;/h1&gt;

&lt;p&gt;可以参考文博客&lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;中的Zookeeper集群安装步骤。&lt;/p&gt;

&lt;h1 id=&quot;python&quot;&gt;3. 安装Python及依赖&lt;/h1&gt;

&lt;p&gt;一般操作系统上都安装了Python，查看当前Python版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python -V
Python 2.6.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;zeromq&quot;&gt;3.1 下载Zeromq&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://download.zeromq.org/zeromq-4.0.4.tar.gz
$ tar zxvf zeromq-4.0.4.tar.gz
$ ./configure
$ make &amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;jzmq&quot;&gt;3.2 安装Jzmq&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone git://github.com/nathanmarz/jzmq.git
$ cd jzmq
$ ./autogen.sh
$ ./configure
$ make &amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;storm&quot;&gt;4. 安装Storm&lt;/h1&gt;

&lt;p&gt;下载稳定版本的storm，然后解压将其拷贝到/usr/lib/storm目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://github.com/downloads/nathanmarz/storm/storm-0.8.1.zip
$ unzip storm-0.8.1.zip 
$ mv storm-0.8.1 /usr/lib/storm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，配置环境变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export STORM_HOME=/usr/lib/storm
export PATH=$PATH:$STORM_HOME/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;建立storm存储目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir /tmp/storm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置文件/usr/lib/storm/conf/storm.yaml，修改为如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt; storm.zookeeper.servers:
     - &quot;cdh1&quot;
     - &quot;cdh2&quot;
     - &quot;cdh3&quot;
 ui.port: 8081
 nimbus.host: &quot;cdh2&quot;
 storm.local.dir: &quot;/tmp/storm&quot;
 supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中，配置参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;storm.zookeeper.servers&lt;/code&gt;：Storm集群使用的Zookeeper集群地址，如果Zookeeper集群使用的不是默认端口，那么还需要&lt;code&gt;storm.zookeeper.port&lt;/code&gt;选项&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ui.port&lt;/code&gt;：Storm UI的服务端口&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;storm.local.dir&lt;/code&gt;：Nimbus和Supervisor进程用于存储少量状态，如jars、confs等的本地磁盘目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;java.library.path&lt;/code&gt;: Storm使用的本地库（ZMQ和JZMQ）加载路径，默认为”/usr/local/lib:/opt/local/lib:/usr/lib”，一般来说ZMQ和JZMQ默认安装在&lt;code&gt;/usr/local/lib&lt;/code&gt;下，因此不需要配置即可。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;nimbus.host&lt;/code&gt;: Storm集群Nimbus机器地址&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;supervisor.slots.ports&lt;/code&gt;: 对于每个Supervisor工作节点，需要配置该工作节点可以运行的worker数量。每个worker占用一个单独的端口用于接收消息，该配置选项即用于定义哪些端口是可被worker使用的。默认情况下，每个节点上可运行4个workers，分别在6700、6701、6702和6703端口&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多配置参数，请参考&lt;a href=&quot;http://www.alidata.org/archives/2118&quot;&gt;Storm配置项详解&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;最后，启动Storm各个后台进程：&lt;/p&gt;

&lt;p&gt;主控节点上启动nimbus：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ storm nimbus &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Storm各个工作节点上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ storm supervisor &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Storm主控节点上启动ui：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ storm ui &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，你可以访问&lt;a href=&quot;http://cdh2:8081/&quot;&gt;http://cdh2:8081/&lt;/a&gt;查看集群的worker资源使用情况、Topologies的运行状态等信息。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/06/19/how-to-install-and-deploy-a-storm-cluster.html</link>
      <guid>http://blog.javachen.com/2014/06/19/how-to-install-and-deploy-a-storm-cluster.html</guid>
      <pubDate>2014-06-19T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Effective Java 笔记</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;创建和销毁对象&lt;/h1&gt;

&lt;h2 id=&quot;no1-&quot;&gt;NO.1 考虑用静态工厂方法代替构造函数&lt;/h2&gt;

&lt;p&gt;静态工厂方法好处：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、构造函数有命名的限制，而静态方法有自己的名字，更加易于理解。&lt;/li&gt;
  &lt;li&gt;2、静态工厂方法在每次调用的时候不要求创建一个新的对象。这种做法对于一个要频繁创建相同对象的程序来说，可以极大的提高性能。它使得一个类可以保证是一个singleton；他使非可变类可以保证“不会有两个相等的实例存在”。&lt;/li&gt;
  &lt;li&gt;3、静态工厂方法在选择返回类型时有更大的灵活性。使用静态工厂方法，可以通过调用方法时使用不同的参数创建不同类的实例，还可以创建非公有类的对象，这就封装了类的实现细节。&lt;/li&gt;
  &lt;li&gt;4、在创建参数化类型实例的时候，他们使代码变的更加简洁。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static Boolean valueOf(boolean f){
	return b ? Boolean.TRUE : Booleab.FALSE;
}

Map&amp;lt;String,List&amp;lt;String&amp;gt;&amp;gt; m=HashMap.newInstance();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;静态工厂方法坏处：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、如果一个类是通过静态工厂方法来取得实例的，并且该类的构造函数都不是公有的或者保护的，那该类就不可能有子类（被继承），子类的构造函数需要首先调用父类的构造函数，因为父类的构造函数是private的，所以即使我们假设继承成功的话，那么子类也根本没有权限去调用父类的私有构造函数，所以是无法被继承的。&lt;/li&gt;
  &lt;li&gt;2、毕竟通过构造函数创建实例还是SUN公司所提倡的，静态工厂方法跟其他的静态方法区别不大，这样创建的实例谁又知道这个静态方法是创建实例呢？弥补的办法就是：静态工厂方法名字使用valueOf、of、getInstance、newInstance、getType、newType。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;no2-&quot;&gt;NO.2 遇到多个构造器参数时要考虑用构建器&lt;/h2&gt;

&lt;p&gt;当有多个构造方法，一般式参数大于4个的时候，建议使用Builder模式。&lt;/p&gt;

&lt;h2 id=&quot;no3-singleton&quot;&gt;NO.3 用私有构造器或者枚举类型强化Singleton属性&lt;/h2&gt;

&lt;p&gt;用单元素的枚举来实现单例模式。&lt;/p&gt;

&lt;h2 id=&quot;no4-&quot;&gt;NO.4 通过私有的构造函数强化不可实例化的能力&lt;/h2&gt;

&lt;p&gt;在面向对象程序设计中，假如存在太多只有静态属性和静态方法的类；那么，面向对象的思想可能在这会损失殆尽。&lt;/p&gt;

&lt;p&gt;但是，并不能说面向对象的程序中就不应该出现只有静态属性和静态方法的类，相反，有时候我们还必须写这样的类作为工具类。这样的类怎么实现呢？有人可能会把该类定义成抽象类(Abstract class)，的确，抽象类是不可以实例化的，但是别忘了还有继承，继承了抽象类的子类在实例化时候，默认是会先调用父类无参数的构造函数的（super();），这时候，父类不是也被实例化了嘛？&lt;/p&gt;

&lt;p&gt;其实我们可以这样做，把该类的构造函数定义为私有的（private），而类的内部又不调用该构造函数的话，就成功了。这样带来的后果就是该类成了 final的，不可能再被任何类继承了，要被继承，得提供一个公有（public）的或者保护（protect）的构造函数，这样才能被子类调用。&lt;/p&gt;

&lt;h2 id=&quot;no5-&quot;&gt;NO.5 避免创建重复的对象&lt;/h2&gt;

&lt;p&gt;如果一个对象是不可变的，那么他总是可以被重用的，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//不推荐，&quot;test&quot;本来就是一个String实例，如果此方法在一个循环中或者被频繁的调用，将会严重影响性能
String s = new String(&quot;test&quot;);
//推荐方式
String s = &quot;test&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于提供静态方法和构造函数的非可变类，推荐使用静态方法，这样可以避免重复创建对象，如：Boolean.vauleOf(String)方法优于构造函数Boolean(String)&lt;/p&gt;

&lt;p&gt;类初始化的顺序：&lt;/p&gt;

&lt;p&gt;先初始化父类的静态代码 —&amp;gt; 初始化子类的静态代码 —&amp;gt; 初始化父类的非静态代码 —&amp;gt; 初始化父类构造函数 —&amp;gt; 初始化子类非静态代码 —&amp;gt;初始化子类构造函数。&lt;/p&gt;

&lt;h2 id=&quot;no6-&quot;&gt;NO.6 消除过期的对象引用&lt;/h2&gt;

&lt;p&gt;内存泄漏问题：如果一个对象的引用被无意识的保留起来，那么垃圾回收机制是不会去处理这个对象，而且也不会去处理被这个对象引用的其它对象。&lt;br /&gt;
比如堆栈的弹出栈元素方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public Object pop(){

   if(size == 0){
       throw new EmptyStackException();
   }

	Object result = elements[--size];
	//自减后把原来的引用置为null
	elements[size] = null;
	return result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内存泄露常出现在：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;过期对象&lt;/li&gt;
  &lt;li&gt;缓存，由于缓存没有及时清除无用的条目而出现，可以使用weakHashMap来避免这种情况&lt;/li&gt;
  &lt;li&gt;监听器和其他回调&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;清理过期对象引用的好处是：如果在以后又被使用到该引用，最会抛下NullPointException而不是让程序继续错误的运行下去，尽可能早的监测出程序中的错误总是有好处的。&lt;/p&gt;

&lt;p&gt;方法：重新使用这个已经指向一个对象的引用，或结束其生命周期。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;对所有对象都通用的方法&lt;/h1&gt;

&lt;h2 id=&quot;equals&quot;&gt;equals方法&lt;/h2&gt;

&lt;p&gt;(1) equals方法一般用于“值类”的情形，比如Integer,Date目的是为了比较两个指向值对象的引用的时候，希望&lt;br /&gt;
它们的逻辑是否相等而不是它们是否指向同一个对象。&lt;/p&gt;

&lt;p&gt;约定：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a 自反性 对任意的对象必须和它自身相等。对值引用x x.equals(x) 一定返回true&lt;/li&gt;
  &lt;li&gt;b 对称性 对任意的值引用x,y,如果x.equals(y) 一定有y.equals(x)&lt;/li&gt;
  &lt;li&gt;c 传递性 对任意的值引用x,y,z，如果x.equals(y),y.equals(z) 一定有x.equals(z)&lt;/li&gt;
  &lt;li&gt;d 一致性 对于任何非null的引用x和y，只要equals的比较操作在对象中所用的信息没有被修改，多次调用x.equals(y)就会一致地返回true&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;p&gt;要想在扩展一个可实例化的类的同时，即要保证增加新的特性，又要保证equals约定，建议复合优于继承原则。若类和类是 a kind of 关系则用继承，若类和类是 a part of 关系则用组合(复合)&lt;/p&gt;

&lt;h2 id=&quot;hashcode&quot;&gt;hashCode&lt;/h2&gt;

&lt;p&gt;相等的对象必须要有相等的散列码，如果违反这个约定可能导致这个类无法与某些散列值得集合结合在一起使用，所以在改写了equals方法的同时一定要重写hashCode方法以保证一致性。&lt;/p&gt;

&lt;h2 id=&quot;tostring&quot;&gt;toString&lt;/h2&gt;

&lt;p&gt;toString返回值中包含所有信息&lt;/p&gt;

&lt;h2 id=&quot;clone&quot;&gt;clone&lt;/h2&gt;

&lt;h2 id=&quot;comparable&quot;&gt;Comparable&lt;/h2&gt;

&lt;h1 id=&quot;section-2&quot;&gt;类和接口&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;使类和成员的可访问能力尽量的小&lt;/li&gt;
  &lt;li&gt;支持非可变性&lt;/li&gt;
  &lt;li&gt;复合优于继承&lt;/li&gt;
  &lt;li&gt;接口优于抽象&lt;/li&gt;
  &lt;li&gt;优先考虑静态成员类&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;方法&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;(1) 检查参数的有效性&lt;/li&gt;
  &lt;li&gt;(2) 需要使用保护性拷贝&lt;/li&gt;
  &lt;li&gt;(3) 方法设计的一些原则&lt;br /&gt;
 	- a、避免长的参数列表，尤其是参数相同的参数列表。&lt;br /&gt;
 	- b、对参数类型使用接口，而不是接口的实现类。&lt;br /&gt;
 	- c、谨慎使用重载。&lt;br /&gt;
 	- d、返回0程度的数组而不是null。&lt;/li&gt;
  &lt;li&gt;(4) 为所有导出的api方法编写注释&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;重载(overloaded method) 选择的是静态的。选择的依据是参数类型&lt;/p&gt;

&lt;p&gt;重写(oveeridden method) 选择的依据是被调用方法所在对象的运行时的类型。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;通用设计方法&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;(1) 将局部变量的作用域最小化&lt;/li&gt;
  &lt;li&gt;(2) foreach优于传统的for循环。有三种情况无法使用foreach循环：过滤、转换、平行迭代&lt;/li&gt;
  &lt;li&gt;(3) 了解和使用类库&lt;/li&gt;
  &lt;li&gt;(4) 如果要得到精确结果，最好是用BigDecimal 而不使用fload或double&lt;/li&gt;
  &lt;li&gt;(5) 对数量大的字符串连接使用StringBuffer而不是String前者速度快。&lt;/li&gt;
  &lt;li&gt;(6) 基本类型优先于装箱类型&lt;/li&gt;
  &lt;li&gt;(7) 当心字符串连接性能&lt;/li&gt;
  &lt;li&gt;(8) 通过接口引用对象&lt;/li&gt;
  &lt;li&gt;(9) 接口优先于反射机制&lt;/li&gt;
  &lt;li&gt;(10) 谨慎使用本地方法&lt;/li&gt;
  &lt;li&gt;(11) 谨慎进行优化&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/06/17/note-about-effective-java.html</link>
      <guid>http://blog.javachen.com/2014/06/17/note-about-effective-java.html</guid>
      <pubDate>2014-06-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase源码分析：HTable put过程</title>
      <description>&lt;p&gt;HBase版本：0.94.15-cdh4.7.0&lt;/p&gt;

&lt;p&gt;在 HBase中，大部分的操作都是在RegionServer完成的，Client端想要插入、删除、查询数据都需要先找到相应的 RegionServer。什么叫相应的RegionServer？就是管理你要操作的那个Region的RegionServer。Client本身并 不知道哪个RegionServer管理哪个Region，那么它是如何找到相应的RegionServer的？本文就是在研究源码的基础上了解这个过程。&lt;/p&gt;

&lt;p&gt;首先来看看写过程的序列图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/hbase-htable-put-sequence.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;客户端代码&lt;/h1&gt;

&lt;p&gt;1、put方法&lt;/p&gt;

&lt;p&gt;HTable的put有两个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public void put(final Put put) throws IOException {
	doPut(put);
	if (autoFlush) {
	  flushCommits();
	}
}

public void put(final List&amp;lt;Put&amp;gt; puts) throws IOException {
	for (Put put : puts) {
	  doPut(put);
	}
	if (autoFlush) {
	  flushCommits();
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面代码可以看出：你既可以一次put一行记录也可以一次put多行记录，两个方法内部都会调用doPut方法，最后再来根据autoFlush（默认为true）判断是否需要flushCommits，在autoFlush为false的时候，如果当前容量超过了缓冲区大小（默认值为：2097152=2M），也会调用flushCommits方法。也就是说，在自动提交情况下，你可以手动控制通过一次put多条记录（这时候缓冲区不会满），然后将这些记录flush，以提高写操作tps。&lt;/p&gt;

&lt;p&gt;doPut代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;private void doPut(Put put) throws IOException{
	validatePut(put); //验证Put有效，主要是判断kv的长度
	writeBuffer.add(put); //写入缓存
	currentWriteBufferSize += put.heapSize(); //计算缓存容量 
	if (currentWriteBufferSize &amp;gt; writeBufferSize) {
	  flushCommits();  //如果超过缓存容量，则调用flushCommits()
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、flushCommits方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public void flushCommits() throws IOException {
    try {
      Object[] results = new Object[writeBuffer.size()];
      try {
        //调用HConnection来提交Put
        this.connection.processBatch(writeBuffer, tableName, pool, results);
      } catch (InterruptedException e) {
        throw new IOException(e);
      } finally {
        // mutate list so that it is empty for complete success, or contains
        // only failed records results are returned in the same order as the
        // requests in list walk the list backwards, so we can remove from list
        // without impacting the indexes of earlier members
        for (int i = results.length - 1; i&amp;gt;=0; i--) {
          if (results[i] instanceof Result) {
            // successful Puts are removed from the list here.
            writeBuffer.remove(i);
          }
        }
      }
    } finally {
      if (clearBufferOnFail) {
        writeBuffer.clear();
        currentWriteBufferSize = 0;
      } else {
        // the write buffer was adjusted by processBatchOfPuts
        currentWriteBufferSize = 0;
        //currentWriteBufferSize又重新计算了一遍，看来一批提交不一定会全部提交完 
        for (Put aPut : writeBuffer) {
          currentWriteBufferSize += aPut.heapSize();
        }
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其核心是调用this.connection的processBatch方法，其参数有：writeBuffer、tableName、pool、results&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;writeBuffer，缓冲区，带提交的数据&lt;/li&gt;
  &lt;li&gt;tableName，表名&lt;/li&gt;
  &lt;li&gt;pool，ExecutorService类，可以通过HTable构造方法传入一个参数来初始化（例如：HConnectionManager的&lt;code&gt;getTable(byte[] tableName, ExecutorService pool)&lt;/code&gt;方法），也可以内部初始化。内部初始化时，其最大线程数由&lt;code&gt;hbase.htable.threads.max&lt;/code&gt;设置，keepAliveTime由&lt;code&gt;hbase.htable.threads.keepalivetime&lt;/code&gt;设置，默认为60秒&lt;/li&gt;
  &lt;li&gt;results，保存运行结果&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在默认情况下，connection由如下方式初始化：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; this.connection = HConnectionManager.getConnection(conf); //HConnection的实现类为HConnectionImplementation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、ConnectionImplementation的processBatch方法&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;   public void processBatch(List&amp;lt;? extends Row&amp;gt; list,
        final byte[] tableName,
        ExecutorService pool,
        Object[] results) throws IOException, InterruptedException {
      // This belongs in HTable!!! Not in here.  St.Ack

      // results must be the same size as list
      if (results.length != list.size()) {
        throw new IllegalArgumentException(&quot;argument results must be the same size as argument list&quot;);
      }

      processBatchCallback(list, tableName, pool, results, null);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后是调用的processBatchCallback方法，第五个参数为空，即没有回调方法。&lt;/p&gt;

&lt;p&gt;processBatchCallback方法内部可以失败后进行重试，重试次数为&lt;code&gt;hbase.client.retries.number&lt;/code&gt;控制，默认为10，每一次重试直接都会休眠一下，每次休眠时间为:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;pause * HConstants.RETRY_BACKOFF[ntries]+(long)(normalPause * RANDOM.nextFloat() * 0.01f);
//RETRY_BACKOFF[] = { 1, 1, 1, 2, 2, 4, 4, 8, 16, 32, 64 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pause通过&lt;code&gt;hbase.client.pause&lt;/code&gt;设置，默认值为1000，即1秒；ntries为当前重复次数&lt;/p&gt;

&lt;p&gt;接下来，第一步，遍历List&amp;lt;? extends Row&amp;gt;，获取每一个行对应HRegion所在位置，并且按regionName对这些待put的行进行分组。&lt;/p&gt;

&lt;p&gt;第二步，发送异步请求到服务端。&lt;/p&gt;

&lt;p&gt;第三步，接收异步请求的结果，收集成功的和失败的，做好重试准备&lt;/p&gt;

&lt;p&gt;第四步，对于失败的，进行重试。&lt;/p&gt;

&lt;p&gt;达到重试次数之后，对运行结果判断是否有异常，如果有则抛出RetriesExhaustedWithDetailsException异常。&lt;/p&gt;

&lt;p&gt;由以上四步可以看出，重点在于第一、二步。&lt;/p&gt;

&lt;p&gt;第一步查找HRegion所在位置过程关键在&lt;code&gt;private HRegionLocation locateRegion(final byte [] tableName,final byte [] row, boolean useCache)&lt;/code&gt;方法中，并且为递归方法，过程如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;调用locateRegionInMeta方法到.META.表中查找tableName的row所对应的HRegion所在位置，先从本地缓存查找，如果没有，则进行下一步；&lt;/li&gt;
  &lt;li&gt;调用locateRegionInMeta方法到-ROOT-表中查找.META.所对应的HRegion所在位置，先从本地缓存查找，如果没有，则进行下一步&lt;/li&gt;
  &lt;li&gt;通过rootRegionTracker（即从zk上）获取RootRegionServer地址，即找到-ROOT-表所在的RegionServer地址，然后获取到.META.所在位置，最后在获取.META.表上所有HRegion，并将其加入到本地缓存。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过示例描述如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;获取 Table2，RowKey为RK10000的RegionServer

=&amp;gt; 获取.META.，RowKey为Table2,RK10000, 99999999999999 的RegionServer
   
=&amp;gt; 获取-ROOT-，RowKey为.META.,Table2,RK10000,99999999999999,99999999999999的RegionServer
   
=&amp;gt; 获取-ROOT-的RegionServer
   
=&amp;gt; 从ZooKeeper得到-ROOT-的RegionServer
   
=&amp;gt; 从-ROOT-表中查到RowKey最接近（小于） .META.,Table2,RK10000,99999999999999,99999999999999 的一条Row，并得到.META.的RegionServer  

=&amp;gt; 从.META.表中查到RowKey最接近（小于）Table2,RK10000,99999999999999 的一条Row，并得到Table2的K10000的Row对应的HRegionLocation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当我们创建一个表时，不管是否预建分区，该表创建之后，在.META.上会有一条记录的。&lt;/li&gt;
  &lt;li&gt;在客户端第一次连接服务端时，会两次查询缓存并没有查到结果，最后在通过&lt;code&gt;-ROOT-&lt;/code&gt;–&amp;gt;&lt;code&gt;.META.&lt;/code&gt;–&amp;gt;HRegion找到对应的HRegion所在位置。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第二步中，先是创建到RegionServer的连接，后是调用RegionServer上的multi方法，显然这是远程调用的过程。第二步中提交的任务通过下面代码创建：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;private &amp;lt;R&amp;gt; Callable&amp;lt;MultiResponse&amp;gt; createCallable(final HRegionLocation loc,
        final MultiAction&amp;lt;R&amp;gt; multi, final byte [] tableName) {
  // TODO: This does not belong in here!!! St.Ack  HConnections should
  // not be dealing in Callables; Callables have HConnections, not other
  // way around.
  final HConnection connection = this;
  return new Callable&amp;lt;MultiResponse&amp;gt;() {
   public MultiResponse call() throws IOException {
     ServerCallable&amp;lt;MultiResponse&amp;gt; callable =
       new ServerCallable&amp;lt;MultiResponse&amp;gt;(connection, tableName, null) {
         public MultiResponse call() throws IOException {
           return server.multi(multi);
         }
         @Override
         public void connect(boolean reload) throws IOException {
           server = connection.getHRegionConnection(loc.getHostname(), loc.getPort());
         }
       };
     return callable.withoutRetries();
   }
 };
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面代码可以看到，通过&lt;code&gt;connection.getHRegionConnection(loc.getHostname(), loc.getPort())&lt;/code&gt;创建一个HRegionInterface的实现类即HRegionServer，方法内使用了代理的方式创建对象。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;server = HBaseRPC.waitForProxy(this.rpcEngine,
  serverInterfaceClass, HRegionInterface.VERSION,
  address, this.conf,
  this.maxRPCAttempts, this.rpcTimeout, this.rpcTimeout);
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;服务端&lt;/h1&gt;

&lt;p&gt;上面客户端调用过程分析完毕，继续跟RegionServer服务端的处理。&lt;/p&gt;

&lt;h3 id=&quot;hregionservermulti&quot;&gt;HRegionServer的multi方法&lt;/h3&gt;

&lt;p&gt;对于客户端写操作，最终会调用HRegionServer的multi方法。&lt;/p&gt;

&lt;p&gt;因为传递到RegionServer都是按regionName分组的，故最后的操作实际上都是调用的HRegion对象的方法。&lt;/p&gt;

&lt;p&gt;该方法主要就是遍历multi并对actionsForRegion按rowid进行排序，然后分类别对action进行处理，Put和Delete操作会放到一起然后调用batchMutate方法批量提交：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;OperationStatus[] codes =region.batchMutate(mutationsWithLocks.toArray(new Pair[]{}));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于Get，会调用get方法；&lt;/li&gt;
  &lt;li&gt;对于Exec，会调用execCoprocessor方法；&lt;/li&gt;
  &lt;li&gt;对于Increment，会调用increment方法；&lt;/li&gt;
  &lt;li&gt;对于Append，会调用append方法；&lt;/li&gt;
  &lt;li&gt;对于RowMutations，会调用mutateRow方法；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于Put和Delete操作（保存在mutations中），在处理之前，先通过cacheFlusher检查memstore大小吃否超过限定值，如果是，则进行flush。&lt;/p&gt;

&lt;p&gt;接下来遍历mutations，为每个Mutation添加一个锁lock，然后再调用region的batchMutate方法。&lt;/p&gt;

&lt;h3 id=&quot;hregionbatchmutate&quot;&gt;HRegion的batchMutate&lt;/h3&gt;

&lt;p&gt;batchMutate方法内部，依次一个个处理：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;先检查是否只读&lt;/li&gt;
  &lt;li&gt;检查当前资源是否支持update操作，会比较memstoreSize和blockingMemStoreSize大小，然后会阻塞线程&lt;/li&gt;
  &lt;li&gt;调用startRegionOperation，给lock.readLock()加锁&lt;/li&gt;
  &lt;li&gt;调用doPreMutationHook执行协作器里的一些方法&lt;/li&gt;
  &lt;li&gt;计算其待添加的大小&lt;/li&gt;
  &lt;li&gt;计算加入memstore之后的memstore大小&lt;/li&gt;
  &lt;li&gt;写完之后，释放lock.readLock()锁&lt;/li&gt;
  &lt;li&gt;判断是否需要flush memstore，如果需要，则调用requestFlush方法，其内部实际是通过RegionServerServices中的FlushRequester（其实现类为MemStoreFlusher）来执行flush操作&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;memstoreflusher-flush&quot;&gt;MemStoreFlusher flush过程&lt;/h3&gt;

&lt;p&gt;HRegion中的requestFlush方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;private void requestFlush() {
    if (this.rsServices == null) {
      return;
    }
    synchronized (writestate) {
      if (this.writestate.isFlushRequested()) {
        return;
      }
      writestate.flushRequested = true;
    }
    // Make request outside of synchronize block; HBASE-818.
    this.rsServices.getFlushRequester().requestFlush(this);
    if (LOG.isDebugEnabled()) {
      LOG.debug(&quot;Flush requested on &quot; + this);
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面this.rsServices.getFlushRequester()其实际上返回的是MemStoreFlusher类。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/hbase-MemStoreFlusher-class.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MemStoreFlusher内部有一个队列和一个Map：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//保存待flush的对象
private final BlockingQueue&amp;lt;FlushQueueEntry&amp;gt; flushQueue =
    new DelayQueue&amp;lt;FlushQueueEntry&amp;gt;();
//记录队列中存在哪些Region
private final Map&amp;lt;HRegion, FlushRegionEntry&amp;gt; regionsInQueue =
    new HashMap&amp;lt;HRegion, FlushRegionEntry&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MemStoreFlusher构造方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;初始化threadWakeFrequency，该值由hbase.server.thread.wakefrequency设置，默认为10 * 1000&lt;/li&gt;
  &lt;li&gt;初始化globalMemStoreLimit，该值为最大堆内存乘以hbase.regionserver.global.memstore.upperLimit的值，hbase.regionserver.global.memstore.upperLimit参数默认值为0.4&lt;/li&gt;
  &lt;li&gt;初始化globalMemStoreLimitLowMark，该值为最大堆内存乘以hbase.regionserver.global.memstore.lowerLimit的值，hbase.regionserver.global.memstore.lowerLimit参数默认值为0.35&lt;/li&gt;
  &lt;li&gt;初始化blockingWaitTime，该值由hbase.hstore.blockingWaitTime设置，默认为90000&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MemStoreFlusher实现了Runnable接口，在RegionServer启动过程中会启动一个线程，其run方法逻辑如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;只要RegionServer一直在运行，该线程就不会停止运行&lt;/li&gt;
  &lt;li&gt;每隔threadWakeFrequency时间从flushQueue中取出一个对象&lt;/li&gt;
  &lt;li&gt;如果取出的对象为空或者WakeupFlushThread，则判断：如果当前RegionServer的总大小大于globalMemStoreLimit值，则找到没有太多storefiles（只个数小于hbase.hstore.blockingStoreFiles的，该参数默认值为7）的最大的region和不管有多少storefiles的最大region，比较两个大小找出最大的一个，然后flush该region，并休眠1秒；最后在唤醒flush线程
    &lt;ul&gt;
      &lt;li&gt;先flush region上的memstore，这部分代码通过HRegion的internalFlushcache方法来完成，其内部使用了mvcc&lt;/li&gt;
      &lt;li&gt;判断是否该拆分，如果是则拆分&lt;br /&gt;
 	- 判断是否该压缩合并，如果是则合并&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如果如果取出的对象为FlushRegionEntry，则flush该对象。
    &lt;ul&gt;
      &lt;li&gt;如果当前region不是meta region并且当前region的storefiles数大于&lt;code&gt;hbase.hstore.blockingStoreFiles&lt;/code&gt;，先判断是否要拆分，然后再判断是否需要合并小文件。这个过程会阻塞blockingWaitTime值定义的时间。&lt;br /&gt;
 	- 否则， 直接flush该region上的memstore（调用HRegion的internalFlushcache方法），然后再判断是否需要拆分和合并&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;最后总结一下，HRegionServer作用如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使得被它管理的一系列HRegion能够被客户端来使用，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。&lt;/li&gt;
  &lt;li&gt;主要负责响应用户I/O请求，向HDFS文件系统中读写数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnblogs.com/cnblogs_com/chenli0513/image0030.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;HRegion定位过程：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;client -&amp;gt; zookeeper -&amp;gt; -ROOT- -&amp;gt; .META -&amp;gt; HRegion地址 -&amp;gt; HRegionServer-&amp;gt; HRegion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个过程中客户端先通过zk找到Root表所在的RegionServer（通过zk上的/hbase/root-region-server节点获取），然后找到Meta表对应的HRegion地址，最后在Meta表里找到目标表所在的HRegion地址，这个过程客户端并没有和HMaster进行交互。&lt;/p&gt;

&lt;p&gt;Client端并不会每次数据操作都做这整个路由过程，因为HRegion的相关信息会缓存到本地，当有变化时，通过zk监听器能够及时感知。&lt;/p&gt;

&lt;p&gt;数据写入过程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;client先根据rowkey找到对应的region和regionserver&lt;/li&gt;
  &lt;li&gt;client想regionserver提交写请求&lt;/li&gt;
  &lt;li&gt;region找到目标region&lt;/li&gt;
  &lt;li&gt;region检查数据是否与scheam一致&lt;/li&gt;
  &lt;li&gt;如果客户端没有指定版本，则获取当前系统时间作为数据版本&lt;/li&gt;
  &lt;li&gt;将更新写入wal log&lt;/li&gt;
  &lt;li&gt;将更新写入memstore&lt;/li&gt;
  &lt;li&gt;判断memstore是否需要flush为store文件&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/06/13/hbase-code-about-htable-put.html</link>
      <guid>http://blog.javachen.com/2014/06/13/hbase-code-about-htable-put.html</guid>
      <pubDate>2014-06-13T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive Over HBase的介绍</title>
      <description>&lt;p&gt;Hive Over HBase是基于Hive的HQL查询引擎支持对hbase表提供及时查询的功能，它并不是将hql语句翻译成mapreduce来运行，其响应时间在秒级别。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;特性&lt;/h1&gt;

&lt;p&gt;支持的字段类型：&lt;/p&gt;

&lt;p&gt;boolean, tinyint, smallint, int, bigint, float, double, string, struct&lt;br /&gt;
(当hbase中的rowkey字段为struct类型，请将子字段定义为string类型，同时指定表的collection items terminated分隔字符以及各字段的长度参数:hbase.rowkey.column.length)&lt;/p&gt;

&lt;p&gt;支持的sql语法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;where子句&lt;/li&gt;
  &lt;li&gt;group by，having子句&lt;/li&gt;
  &lt;li&gt;聚合函数: count, max, min, sum, avg&lt;/li&gt;
  &lt;li&gt;order by with limit(top N)&lt;/li&gt;
  &lt;li&gt;limit 子句&lt;/li&gt;
  &lt;li&gt;explain&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支持的运算&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;关系操作：&amp;gt;, &amp;gt;=, &amp;lt;=, &amp;lt;, =&lt;/li&gt;
  &lt;li&gt;算术操作：+, - , * , / , %&lt;/li&gt;
  &lt;li&gt;逻辑操作：and, or, not&lt;/li&gt;
  &lt;li&gt;字符串操作函数: substring, concat&lt;/li&gt;
  &lt;li&gt;Distinct : 支持&lt;code&gt;select distinct &amp;lt;col-list&amp;gt; from &amp;lt;tab&amp;gt; where &amp;lt;expr&amp;gt;, select aggr-fun(distinct &amp;lt;col_list&amp;gt;) from &amp;lt;tab&amp;gt; where &amp;lt;expr&amp;gt;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Like: 通配符’_’, ’%’&lt;/li&gt;
  &lt;li&gt;Case when子句&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不支持:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sub-query&lt;/li&gt;
  &lt;li&gt;Join&lt;/li&gt;
  &lt;li&gt;Union&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;原理&lt;/h1&gt;

&lt;p&gt;扩展HBase客户端代码，实现&lt;a href=&quot;/2014/06/12/hbase-aggregate-client.html&quot;&gt;简单聚合计算&lt;/a&gt;，基于协作器实现分组计算的功能，并且修改hive的查询引擎，将HQL语句转换成HBase的Task，然后调用HBase中的api实现对HQL语句的解析。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;源码&lt;/h1&gt;

&lt;p&gt;暂时不公开。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/06/12/intro-of-hive-over-hbase.html</link>
      <guid>http://blog.javachen.com/2014/06/12/intro-of-hive-over-hbase.html</guid>
      <pubDate>2014-06-12T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase客户端实现并行扫描</title>
      <description>&lt;p&gt;HBase中有一个类可以实现客户端扫描数据，叫做ClientScanner，该类不是并行的，有没有办法实现一个并行的扫描类，加快扫描速度呢？&lt;/p&gt;

&lt;p&gt;如果是一个Scan，我们可以根据startkey和stopkey将其拆分为多个子Scan，然后让这些Scan并行的去查询数据，然后分别返回执行结果。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;实现方式&lt;/h1&gt;

&lt;p&gt;说明：我使用的HBase版本为：cdh4-0.94.15_4.7.0。&lt;/p&gt;

&lt;p&gt;在org.apache.hadoop.hbase.client创建ParallelClientScanner类，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package org.apache.hadoop.hbase.client;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Queue;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HConstants;
import org.apache.hadoop.hbase.Stoppable;
import org.apache.hadoop.hbase.util.Bytes;

public class ParallelClientScanner extends AbstractClientScanner {
  private static final int SCANNER_WAIT_TIME = 1000;
  private final HTableInterface table;
  private final int resultQueueSize;
  private final int threadCount;
  private List&amp;lt;Scan&amp;gt; scans;
  private Iterator&amp;lt;Scan&amp;gt; iter;
  private ParallelScannerWorkerThread currThread;
  private Queue&amp;lt;ParallelScannerWorkerThread&amp;gt; nextThreads;
  private boolean closed;

  public ParallelClientScanner(Configuration conf, Scan scan,
      byte[] tableName, List&amp;lt;byte[]&amp;gt; splitKeys) throws IOException {
    this(new HTable(conf, tableName), scan, splitKeys);
  }

  public ParallelClientScanner(HTableInterface table, Scan scan,
      List&amp;lt;byte[]&amp;gt; splitKeys) throws IOException {
    this.table = table;
    this.resultQueueSize = table.getConfiguration().getInt(
      &quot;hbase.parallel.scanner.queue.size&quot;, 1000);

    this.threadCount = (table.getConfiguration().getInt(
      &quot;hbase.parallel.scanner.thread.count&quot;, 10) - 1);

    this.scans = new ArrayList&amp;lt;Scan&amp;gt;();
    byte[] stopRow = scan.getStopRow();
    byte[] lastSplitKey = scan.getStartRow();
    Scan subScan = null;
    for (Iterator&amp;lt;byte[]&amp;gt; it = splitKeys.iterator(); it.hasNext();) {
      byte[] splitKey = (byte[]) it.next();
      if ((Bytes.compareTo(splitKey, lastSplitKey) &amp;lt;= 0)
          || ((Bytes.compareTo(splitKey, stopRow) &amp;gt;= 0) &amp;amp;&amp;amp; (!Bytes
              .equals(stopRow, HConstants.EMPTY_END_ROW)))
          || (Bytes.equals(splitKey, HConstants.EMPTY_END_ROW))) {
        continue;
      }

      subScan = new Scan(scan);
      subScan.setStartRow(lastSplitKey);
      subScan.setStopRow(splitKey);
      subScan.setParallel(false);
      this.scans.add(subScan);
      lastSplitKey = splitKey;
    }
    subScan = new Scan(scan);
    subScan.setStartRow(lastSplitKey);
    subScan.setParallel(false);
    this.scans.add(subScan);

    this.nextThreads = new LinkedList&amp;lt;ParallelScannerWorkerThread&amp;gt;();
    this.closed = true;
    initialize();
  }

  public ParallelClientScanner(Configuration conf, List&amp;lt;Scan&amp;gt; scans,
      byte[] tableName) throws IOException {
    this(new HTable(conf, tableName), scans);
  }

  public ParallelClientScanner(HTableInterface table, List&amp;lt;Scan&amp;gt; scanList)
      throws IOException {
    if (null == scanList) {
      throw new IOException(&quot;ScanList cannot be null&quot;);
    }
    sort(scanList);

    this.table = table;
    this.resultQueueSize = table.getConfiguration().getInt(
      &quot;hbase.parallel.scanner.queue.size&quot;, 1000);

    this.threadCount = (table.getConfiguration().getInt(
      &quot;hbase.parallel.scanner.thread.count&quot;, 10) - 1);

    this.scans = new ArrayList&amp;lt;Scan&amp;gt;();
    Scan subScan = null;
    for (int i = 0; i &amp;lt; scanList.size(); i++) {
      subScan = new Scan((Scan) scanList.get(i));
      subScan.setParallel(false);
      this.scans.add(subScan);
    }

    this.nextThreads = new LinkedList&amp;lt;ParallelScannerWorkerThread&amp;gt;();
    this.closed = true;
    initialize();
  }

  protected void initialize() throws IOException {
    try {
      this.iter = this.scans.iterator();
      if (this.iter.hasNext()) {
        this.currThread = new ParallelScannerWorkerThread(
            (Scan) this.iter.next());
        this.currThread.start();
      }
      while ((this.iter.hasNext())
          &amp;amp;&amp;amp; (this.nextThreads.size() &amp;lt; this.threadCount)) {
        ParallelScannerWorkerThread worker = new ParallelScannerWorkerThread(
            (Scan) this.iter.next());

        this.nextThreads.offer(worker);
        worker.start();
      }
      this.closed = false;
    } catch (IOException e) {
      close();
      throw e;
    }
  }

  public void close() {
    this.closed = true;
    if (this.currThread != null) {
      this.currThread.stop(&quot;Closed by user.&quot;);
    }
    for (ParallelScannerWorkerThread worker : this.nextThreads)
      worker.stop(&quot;Closed by user.&quot;);
  }

  public Result next() throws IOException {
    try {
      return nextInternal();
    } catch (IOException e) {
      close();
      throw e;
    }
  }

  private Result nextInternal() throws IOException {
    if ((this.closed) || (this.currThread == null)) {
      return null;
    }

    Result next = this.currThread.next();
    if (next != null) {
      return next;
    }

    if (this.currThread.isError()) {
      Exception ex = this.currThread.getException();
      throw ((ex instanceof IOException) ? (IOException) ex
          : new IOException(ex));
    }

    while ((next == null) &amp;amp;&amp;amp; (this.currThread != null)) {
      if (!this.currThread.isStopped()) {
        this.currThread.stop(&quot;Scanner complete.&quot;);
      }
      this.currThread = ((ParallelScannerWorkerThread) this.nextThreads
          .poll());
      if (this.iter.hasNext()) {
        ParallelScannerWorkerThread worker = new ParallelScannerWorkerThread(
            (Scan) this.iter.next());

        this.nextThreads.offer(worker);
        worker.start();
      }
      if (this.currThread != null) {
        next = this.currThread.next();
      }
    }

    return next;
  }

  public Result[] next(int nbRows) throws IOException {
    ArrayList&amp;lt;Result&amp;gt; resultSets = new ArrayList&amp;lt;Result&amp;gt;(nbRows);
    for (int i = 0; i &amp;lt; nbRows; i++) {
      Result next = next();
      if (next == null) break;
      resultSets.add(next);
    }

    return (Result[]) resultSets.toArray(new Result[resultSets.size()]);
  }

  private void sort(List&amp;lt;Scan&amp;gt; scanList) throws IOException {
    Collections.sort(scanList, new ScanComparator());
    for (int i = 1; i &amp;lt; scanList.size(); i++) {
      byte[] currentStartRow = ((Scan) scanList.get(i)).getStartRow();
      byte[] lastStopRow = ((Scan) scanList.get(i - 1)).getStopRow();

      if ((lastStopRow == null) || (lastStopRow.length == 0)) throw new IOException(
          &quot;Scan has overlap, last scan&#39;s stopRow is null&quot;);
      if ((currentStartRow == null) || (currentStartRow.length == 0)) {
        throw new IOException(
            &quot;Scan has overlap, current scan&#39;s startRow is null&quot;);
      }
      if (0 &amp;gt;= Bytes.compareTo(lastStopRow, 0, lastStopRow.length,
        currentStartRow, 0, currentStartRow.length)) continue;
      throw new IOException(
          &quot;Scan has overlap, current scan&#39;s startRow is smaller than last scan&#39;s stop row&quot;);
    }
  }

  private static class ScanComparator implements Comparator&amp;lt;Scan&amp;gt; {
    public int compare(Scan o1, Scan o2) {
      if ((o1.getStartRow() == null) || (o1.getStartRow().length == 0)) return -1;
      if ((o2.getStartRow() == null) || (o2.getStartRow().length == 0)) {
        return 1;
      }
      byte[] o1StartRow = o1.getStartRow();
      byte[] o2StartRow = o2.getStartRow();
      return Bytes.compareTo(o1StartRow, 0, o1StartRow.length,
        o2StartRow, 0, o2StartRow.length);
    }
  }

  private class ParallelScannerWorkerThread extends Thread implements
      Stoppable {
    private ResultScanner scanner;
    private BlockingQueue&amp;lt;Result&amp;gt; results;
    private Object empty;
    private AtomicBoolean stopped;
    private Exception exception;

    protected ParallelScannerWorkerThread(Scan scan) throws IOException {
      this.scanner = ParallelClientScanner.this.table.getScanner(scan);
      this.results = new ArrayBlockingQueue&amp;lt;Result&amp;gt;(
          ParallelClientScanner.this.resultQueueSize);
      this.empty = new Object();
      this.stopped = new AtomicBoolean(false);
    }

    public Result next() throws IOException {
      Result r = (Result) this.results.poll();
      while ((!this.stopped.get()) &amp;amp;&amp;amp; (r == null)) {
        try {
          synchronized (this.empty) {
            this.empty.wait();
          }
          r = (Result) this.results.poll();
        } catch (InterruptedException e) {
          throw ((IOException) (IOException) new IOException()
              .initCause(e));
        }
      }

      return r;
    }

    public void stop(String why) {
      this.stopped.compareAndSet(false, true);
    }

    public boolean isStopped() {
      return this.stopped.get();
    }

    public boolean isError() {
      return this.exception != null;
    }

    public Exception getException() {
      return this.exception;
    }

    public void run() {
      try {
        Result r = this.scanner.next();
        while (r != null) {
          boolean added = false;
          while (!added) {
            added = this.results.offer(r, SCANNER_WAIT_TIME,
              TimeUnit.MILLISECONDS);
          }
          synchronized (this.empty) {
            this.empty.notify();
          }
          r = this.scanner.next();
        }
      } catch (IOException ioe) {
        this.exception = ioe;
      } catch (InterruptedException ite) {
        this.exception = ite;
      } finally {
        this.scanner.close();
        this.stopped.compareAndSet(false, true);
        synchronized (this.empty) {
          this.empty.notify();
        }
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，需要对Scan做些修改：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;作为一个新特性，你需要修改SCAN_VERSION值&lt;/li&gt;
  &lt;li&gt;增加parallel属性，用于判断是否需要并行扫描&lt;/li&gt;
  &lt;li&gt;你需要修改序列化和反序列化方法，加上parallel的值&lt;/li&gt;
  &lt;li&gt;修改HTable中原来的&lt;code&gt;public ResultScanner getScanner(final Scan scan)&lt;/code&gt;方法&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改SCAN_VERSION版本值为3，原来值为2：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;private static final byte SCAN_VERSION = (byte)3;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在合适地方添加一个parallel属性和get/set方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;this.parallel = scan.isParallel();

private boolean isParallel() {
	return parallel;
}

public void setParallel(boolean parallel) {
	this.parallel = parallel;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在public Scan(Scan scan)构造方法中初始化parallel属性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public Scan(Scan scan) throws IOException {
    startRow = scan.getStartRow();
    stopRow  = scan.getStopRow();
    maxVersions = scan.getMaxVersions();
    batch = scan.getBatch();
    caching = scan.getCaching();
    cacheBlocks = scan.getCacheBlocks();
    filter = scan.getFilter(); // clone?
    this.parallel = scan.isParallel(); 		// 初始化parallel属性
    TimeRange ctr = scan.getTimeRange();
    tr = new TimeRange(ctr.getMin(), ctr.getMax());
    Map&amp;lt;byte[], NavigableSet&amp;lt;byte[]&amp;gt;&amp;gt; fams = scan.getFamilyMap();
    for (Map.Entry&amp;lt;byte[],NavigableSet&amp;lt;byte[]&amp;gt;&amp;gt; entry : fams.entrySet()) {
      byte [] fam = entry.getKey();
      NavigableSet&amp;lt;byte[]&amp;gt; cols = entry.getValue();
      if (cols != null &amp;amp;&amp;amp; cols.size() &amp;gt; 0) {
        for (byte[] col : cols) {
          addColumn(fam, col);
        }
      } else {
        addFamily(fam);
      }
    }
    for (Map.Entry&amp;lt;String, byte[]&amp;gt; attr : scan.getAttributesMap().entrySet()) {
      setAttribute(attr.getKey(), attr.getValue());
    }
  }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;public Map&amp;lt;String, Object&amp;gt; toMap(int maxCols)&lt;/code&gt;中添加上parallel属性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;map.put(&quot;parallel&quot;, Boolean.valueOf(this.parallel));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改readFields方法，在最后添加代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;if (version &amp;gt; 2) {
    this.parallel = in.readBoolean();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改write方法，在最后添加代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;out.writeBoolean(this.parallel);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改HTable中原来的&lt;code&gt;public ResultScanner getScanner(final Scan scan)&lt;/code&gt;方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public ResultScanner getScanner(final Scan scan) throws IOException {
	if (scan.getCaching() &amp;lt;= 0) {
		scan.setCaching(getScannerCaching());
	}
	return scan.getParallel() ? new ParallelClientScanner(this, scan, getStartKeysInRange(scan.getStartRow(), scan.getStopRow())) : new ClientScanner(getConfiguration(), scan, getTableName(), this.connection);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，如果你想再HBase shell中使用该特性，你要做如下修改：&lt;/p&gt;

&lt;p&gt;修改src/main/ruby/hbase.rb，在&lt;code&gt;SELECT = &quot;SELECT&quot;&lt;/code&gt;下面添加代码一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;COLUMN_INTERPRETER=&quot;COLUMN_INTERPRETER&quot;
KEY = &quot;KEY&quot;
SELECT = &quot;SELECT&quot;
PARALLEL = &quot;PARALLEL&quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改src/main/ruby/hbase/table.rb，在&lt;code&gt;raw = args[&quot;RAW&quot;] || false&lt;/code&gt;下面添加一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;parallel = args[&quot;PARALLEL&quot;] || false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在scan.setRaw(raw)下面添加一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;scan.setMaxVersions(versions) if versions &amp;gt; 1
scan.setTimeRange(timerange[0], timerange[1]) if timerange
scan.setRaw(raw)
scan.setParallel(parallel)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改完之后，编译源代码，就可以进行测试了。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;测试&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;通过hbase shell测试&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;运行hbase shell进行测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hbase(main):003:0&amp;gt; scan &#39;t&#39;,{PARALLEL=&amp;gt;true}
ROW                                      COLUMN+CELL                                                                                                        
 1                                       column=f:id, timestamp=1382528597662, value=1
 2                                       column=f:id, timestamp=1382528594343, value=2
 3                                       column=f:id, timestamp=1382528478893, value=3
 4                                       column=f:id, timestamp=1382528483161, value=4
4 row(s) in 0.0240 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;通过hbase client测试&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;创建ParallelScannerTest.java类并添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;
import org.apache.hadoop.hbase.util.Bytes;

public class ParallelScannerTest {
  public static void main(String[] args) throws Exception {
    Configuration conf = HBaseConfiguration.create();
    HTable table = new HTable(conf, &quot;t&quot;);
    String startKey = &quot;1&quot;;
    String stopKey = &quot;3&quot;;
    Scan scan = new Scan(Bytes.toBytes(startKey), Bytes.toBytes(stopKey));
    int count = 0;
    
    scan.setParallel(true);

    ResultScanner scanner = table.getScanner(scan);
    Result r = scanner.next();
    while (r != null) {
      count++;
      r = scanner.next();
    }
    System.out.println(&quot;++ Scanning finished with count : &quot; + count + &quot; ++&quot;);
    scanner.close();
    table.close();
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行该类并查看输出结果。&lt;/p&gt;

&lt;p&gt;你可以在hbase-site.xml中修改线程数大小和队列大小，下面两个参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hbase.parallel.scanner.queue.size&lt;/li&gt;
  &lt;li&gt;hbase.parallel.scanner.thread.count&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上源代码及所做的修改我已提交到我github仓库上hbase的cdh4-0.94.15_4.7.0分支，见提交日志&lt;a href=&quot;https://github.com/javachen/hbase/commit/66268c4179f674a7d64f36c91dd4070f1195d169&quot;&gt;add ParallelClientScanner&lt;/a&gt;。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/06/12/hbase-parallel-client-scanner.html</link>
      <guid>http://blog.javachen.com/2014/06/12/hbase-parallel-client-scanner.html</guid>
      <pubDate>2014-06-12T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase实现简单聚合计算</title>
      <description>&lt;p&gt;本文主要记录如何通过打补丁的方式将“hbase中实现简单聚合计算”的特性引入hbase源代码中，并介绍通过命令行和java代码的使用方法。&lt;/p&gt;

&lt;p&gt;支持的简单聚合计算，包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rowcount&lt;/li&gt;
  &lt;li&gt;min&lt;/li&gt;
  &lt;li&gt;max&lt;/li&gt;
  &lt;li&gt;sum&lt;/li&gt;
  &lt;li&gt;std&lt;/li&gt;
  &lt;li&gt;avg&lt;/li&gt;
  &lt;li&gt;median&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1、 下载并编译hbase源代码&lt;/p&gt;

&lt;p&gt;我这里使用的HBase源代码版本是：cdh4-0.94.6_4.3.0，如果你使用其他版本，有可能patch打不上。&lt;/p&gt;

&lt;p&gt;2、 引入patch&lt;/p&gt;

&lt;p&gt;基于提交日志&lt;a href=&quot;https://github.com/javachen/hbase/commit/94e61f28d60cac40f2b499b8530dd1989adf76d3&quot;&gt;add-aggregate-support-in-hbase-shell&lt;/a&gt;生成patch文件，然后打patch，或者也可以使用其他方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git apply add-aggregate-in-hbase-shell.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该patch所做修改包括如下文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;src/main/java/org/apache/hadoop/hbase/client/coprocessor/AbstractDoubleColumnInterpreter.java
src/main/java/org/apache/hadoop/hbase/client/coprocessor/AbstractLongColumnInterpreter.java
src/main/java/org/apache/hadoop/hbase/client/coprocessor/CompositeDoubleStrColumnInterpreter.java
src/main/java/org/apache/hadoop/hbase/client/coprocessor/CompositeLongStrColumnInterpreter.java
src/main/java/org/apache/hadoop/hbase/client/coprocessor/DoubleColumnInterpreter.java
src/main/java/org/apache/hadoop/hbase/client/coprocessor/DoubleStrColumnInterpreter.java
src/main/java/org/apache/hadoop/hbase/client/coprocessor/LongColumnInterpreter.java
src/main/java/org/apache/hadoop/hbase/client/coprocessor/LongStrColumnInterpreter.java
src/main/ruby/hbase.rb
src/main/ruby/hbase/coprocessor.rb
src/main/ruby/hbase/hbase.rb
src/main/ruby/shell.rb
src/main/ruby/shell/commands.rb
src/main/ruby/shell/commands/aggregate.rb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、 编译源代码&lt;/p&gt;

&lt;p&gt;为了使编译花费时间不会太长，请运行如下命令编译代码，你也可以自己修改下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ MAVEN_OPTS=&quot;-Xmx2g&quot; mvn clean install  -DskipTests -Prelease,security -Drat.numUnapprovedLicenses=200 -Dhadoop.profile=2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、测试&lt;/p&gt;

&lt;p&gt;然后将target目录下生成的jar包拷贝到集群中每个hbase节点的/usr/lib/hbase目录。&lt;/p&gt;

&lt;p&gt;修改hbase-site.xml配置文件，添加如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hbase.coprocessor.region.classes&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启hbase服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /etc/init.d/hbase-master restart
$ /etc/init.d/hbase-regionserver restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;a）运行hbase shell进行测试&lt;/p&gt;

&lt;p&gt;首先创建表并插入几条记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create &#39;t&#39;,&#39;f&#39;
 
put &#39;t&#39;,&#39;1&#39;,&#39;f:id&#39;,&#39;1&#39;
put &#39;t&#39;,&#39;2&#39;,&#39;f:id&#39;,&#39;2&#39;
put &#39;t&#39;,&#39;2&#39;,&#39;f:id&#39;,&#39;3&#39;
put &#39;t&#39;,&#39;3&#39;,&#39;f:id&#39;,&#39;4&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在hbase shell命令行中输入agg并按tab键自动提示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hbase(main):004:0&amp;gt; aggregate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;什么参数不输入，提示如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hbase(main):004:0&amp;gt; aggregate
 
ERROR: wrong number of arguments (0 for 2)
 
Here is some help for this command:
Execute a Coprocessor aggregation function; pass aggregation function name, table name, column name, column interpreter and optionally a dictionary of aggregation specifications. Aggregation specifications may include STARTROW, STOPROW or FILTER. For a cross-site big table, if no clusters are specified, all clusters will be counted for aggregation.
Usage: aggregate &#39;subcommand&#39;,&#39;table&#39;,&#39;column&#39;,[{COLUMN_INTERPRETER =&amp;gt; org.apache.hadoop.hbase.client.coprocessor.LongColumnInterpreter.new, STARTROW =&amp;gt; &#39;abc&#39;, STOPROW =&amp;gt; &#39;def&#39;, FILTER =&amp;gt; org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)}]
Available subcommands:
rowcount
min
max
sum
std
avg
median
Available COLUMN_INTERPRETER:
org.apache.hadoop.hbase.client.coprocessor.LongColumnInterpreter.new
org.apache.hadoop.hbase.client.coprocessor.LongStrColumnInterpreter.new
org.apache.hadoop.hbase.client.coprocessor.CompositeLongStrColumnInterpreter.new(&quot;,&quot;, 0)
The default COLUMN_INTERPRETER is org.apache.hadoop.hbase.client.coprocessor.LongStrColumnInterpreter.new.
 
Some examples:
 
hbase&amp;gt; aggregate &#39;min&#39;,&#39;t1&#39;,&#39;f1:c1&#39;
hbase&amp;gt; aggregate &#39;sum&#39;,&#39;t1&#39;,&#39;f1:c1&#39;,&#39;f1:c2&#39;
hbase&amp;gt; aggregate &#39;rowcount&#39;,&#39;t1&#39;,&#39;f1:c1&#39; ,{COLUMN_INTERPRETER =&amp;gt; org.apache.hadoop.hbase.client.coprocessor.CompositeLongStrColumnInterpreter.new(&quot;,&quot;, 0)}
hbase&amp;gt; aggregate &#39;min&#39;,&#39;t1&#39;,&#39;f1:c1&#39;,{STARTROW =&amp;gt; &#39;abc&#39;, STOPROW =&amp;gt; &#39;def&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上可以看到aggregate的帮助说明。&lt;/p&gt;

&lt;p&gt;接下来进行测试，例如求id列的最小值：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hbase(main):005:0&amp;gt; aggregate &#39;min&#39;,&#39;t&#39;,&#39;f:id&#39;
The result of min for table t is 1
0 row(s) in 0.0170 seconds
 
hbase(main):006:0&amp;gt; aggregate &#39;avg&#39;,&#39;t&#39;,&#39;f:id&#39;
The result of avg for table t is 2.5
0 row(s) in 0.0170 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正确输出结果，表明测试成功。&lt;/p&gt;

&lt;p&gt;b）通过hbase client测试&lt;/p&gt;

&lt;p&gt;创建AggregateTest.java类并添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.client.coprocessor.AggregationClient;
import org.apache.hadoop.hbase.client.coprocessor.LongStrColumnInterpreter;
import org.apache.hadoop.hbase.coprocessor.ColumnInterpreter;
import org.apache.hadoop.hbase.util.Bytes;
public class AggregateTest {
  public static void main(String[] args) {
    Configuration conf = HBaseConfiguration.create();
    conf.setInt(&quot;hbase.client.retries.number&quot;, 1);
    conf.setInt(&quot;ipc.client.connect.max.retries&quot;, 1);
     
    byte[] table = Bytes.toBytes(&quot;t&quot;);
    Scan scan = new Scan();
    scan.addColumn(Bytes.toBytes(&quot;f&quot;), Bytes.toBytes(&quot;id&quot;));
    final ColumnInterpreter&amp;lt;Long, Long&amp;gt; columnInterpreter = new LongStrColumnInterpreter();
    try {
      AggregationClient aClient = new AggregationClient(conf);
      Long rowCount = aClient.min(table, columnInterpreter, scan);
      System.out.println(&quot;The result is &quot; + rowCount);
    } catch (Throwable e) {
      e.printStackTrace();
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行该类并查看输出结果。&lt;/p&gt;

&lt;p&gt;以上源代码及所做的修改我已提交到我github仓库上hbase的cdh4-0.94.15_4.7.0分支，见提交日志&lt;a href=&quot;https://github.com/javachen/hbase/commit/94e61f28d60cac40f2b499b8530dd1989adf76d3&quot;&gt;add-aggregate-support-in-hbase-shell&lt;/a&gt;。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/06/12/hbase-aggregate-client.html</link>
      <guid>http://blog.javachen.com/2014/06/12/hbase-aggregate-client.html</guid>
      <pubDate>2014-06-12T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive中数据的加载和导出</title>
      <description>&lt;p&gt;关于 Hive DML 语法，你可以参考 apache 官方文档的说明:&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML&quot;&gt;Hive Data Manipulation Language&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;apache的hive版本现在应该是 0.13.0，而我使用的 hadoop 版本是 CDH5.0.1，其对应的 hive 版本是 0.12.0。故只能参考apache官方文档来看 cdh5.0.1 实现了哪些特性。&lt;/p&gt;

&lt;p&gt;因为 hive 版本会持续升级，故本篇文章不一定会和最新版本保持一致。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 准备测试数据&lt;/h1&gt;

&lt;p&gt;首先创建普通表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create table test(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; STORED AS TEXTFILE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建分区表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;CREATE EXTERNAL TABLE test_p(
id int,
name string 
)
partitioned by (date STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\,&#39; LINES TERMINATED BY &#39;\n&#39; 
STORED AS TEXTFILE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;准备数据文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[/tmp]# cat test.txt 
1,a
2,b
3,c
4,d
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2.加载数据&lt;/h1&gt;

&lt;p&gt;语法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;filepath 可能是：&lt;/li&gt;
  &lt;li&gt;一个相对路径&lt;/li&gt;
  &lt;li&gt;一个绝对路径，例如：&lt;code&gt;/root/project/data1 &lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;一个url地址，可选的可以带上授权信息，例如：&lt;code&gt;hdfs://namenode:9000/user/hive/project/data1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;目标可能是一个表或者分区，如果该表是分区，则必须制定分区列。&lt;/li&gt;
  &lt;li&gt;filepath 可以是一个文件也可以是目录&lt;/li&gt;
  &lt;li&gt;如果指定了 &lt;code&gt;LOCAL&lt;/code&gt;，则：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;load&lt;/code&gt; 命令会在本地查找 filepath。如果 filepath 是相对路径，则相对于当前路径，也可以指定一个 url 或者本地文件，例如：&lt;code&gt;file:///user/hive/project/data1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;如果没有指定 &lt;code&gt;LOCAL&lt;/code&gt; ，则hive会使用全路径的url，url 中如果没有制定 schema，则默认使用 &lt;code&gt;fs.default.name&lt;/code&gt;的值；如果该路径不是绝对路径，则会相对于 &lt;code&gt;/user/&amp;lt;username&amp;gt; &lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;如果使用 &lt;code&gt;OVERWRITE&lt;/code&gt; ，则会删除原来的数据，然后导入新的数据，否则，就是追加数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要注意的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;filepath&lt;/code&gt; 中不能包括子目录&lt;/li&gt;
  &lt;li&gt;如果没有指定 &lt;code&gt;LOCAL&lt;/code&gt;，则 &lt;code&gt;filepath&lt;/code&gt; 指向目标表或者分区所在的文件系统。&lt;/li&gt;
  &lt;li&gt;如果需要压缩，则参考 &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/CompressedStorage&quot;&gt; CompressedStorage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2.1 测试&lt;/h2&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.1.1 加载本地文件&lt;/h3&gt;

&lt;p&gt;a) 加载到普通表中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; load data local inpath &#39;/tmp/test.txt&#39; into table test;                              
Copying data from file:/tmp/test.txt
Copying file: file:/tmp/test.txt
Loading data to table default.test
Table default.test stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16, raw_data_size: 0]
OK
Time taken: 0.572 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看hdfs上的数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hadoop fs -ls /user/hive/warehouse/test
Found 1 items
-rwxrwxrwt   3 hive hadoop         16 2014-06-09 18:36 /user/hive/warehouse/test/test.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看表中数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select * from test;
OK
1	a
2	b
3	c
4	d
Time taken: 0.562 seconds, Fetched: 4 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b) 加载文件到分区表&lt;/p&gt;

&lt;p&gt;通常是直接使用 load 命令加载：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;LOAD DATA LOCAL INPATH &quot;/tmp/test.txt&quot; INTO TABLE test_p PARTITION (date=20140722)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：如果没有加上 &lt;code&gt;overwrite&lt;/code&gt; 关键字，则加载相同文件最后会存在多个文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;还有一种方法是：创建分区目录，手动上传文件，最后再添加新的分区，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoop fs -mkdir  /user/hive/warehouse/test/date=20140320
ALTER TABLE test_p ADD IF NOT EXISTS PARTITION (date=20140320);

hive hadoop fs -rm /user/hive/warehouse/test/date=20140320/test.txt
hadoop fs -put /tmp/test.txt  /user/hive/warehouse/test/date=20140320
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样，你也可以查看 hdfs 和表中的数据。&lt;/p&gt;

&lt;h3 id=&quot;hdfs&quot;&gt;2.1.2 加载hdfs上的文件&lt;/h3&gt;

&lt;p&gt;拷贝 test.txt 为test_1.txt 并将其上传到 &lt;code&gt;/user/hive/warehouse&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp test.txt test_1.txt
$ sudo -u hive hadoop fs -put test_1.txt /user/hive/warehouse
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将 &lt;code&gt;/user/hive/warehouse/test_1.txt&lt;/code&gt; 导入到test表中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; load data inpath &#39;/user/hive/warehouse/test_1.txt&#39; into table test; 
Loading data to table default.test
Table default.test stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16, raw_data_size: 0]
OK
Time taken: 2.941 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看hdfs上的数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hadoop fs -ls /user/hive/warehouse/test
Found 2 items
-rwxr-xr-x   3 hive hadoop         16 2014-06-09 18:48 /user/hive/warehouse/test/test.txt
-rwxr-xr-x   3 hive hadoop         16 2014-06-09 18:45 /user/hive/warehouse/test/test_1.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看表中数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select * from test;                                                 
OK
1	a
2	b
3	c
4	d
1	a
2	b
3	c
4	d
Time taken: 0.302 seconds, Fetched: 8 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;3. 插入数据&lt;/h1&gt;

&lt;p&gt;标准语法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;

INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;扩展语法（多个insert）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;FROM from_statement
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1
[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2]
[INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...;

FROM from_statement
INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1
[INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2]
[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] ...;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;扩展语法（动态分区insert）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement;

INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;INSERT OVERWRITE 会覆盖存在的数据&lt;/li&gt;
  &lt;li&gt;输出的格式和序列化类取决于表的元数据&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-1180&quot;&gt;hive 0.13.0&lt;/a&gt;之后，select语句可以使用 CTEs 表达式，语法请参考 &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-SelectSyntax&quot;&gt; SELECT syntax&lt;/a&gt;，示例见 &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Common+Table+Expression#CommonTableExpression-CTEinViews,CTAS,andInsertStatements&quot;&gt;Common Table Expression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Dynamic Partition Inserts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;dynamic partition inserts在hive 0.6.0中引入。相关的配置参数有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive.exec.dynamic.partition
hive.exec.dynamic.partition.mode
hive.exec.max.dynamic.partitions.pernode
hive.exec.max.dynamic.partitions
hive.exec.max.created.files
hive.error.on.empty.partition
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;FROM page_view_stg pvs
INSERT OVERWRITE TABLE page_view PARTITION(dt=&#39;2008-06-08&#39;, country)
       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.cnt
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-5&quot;&gt;4. 导出数据&lt;/h1&gt;

&lt;p&gt;标准语法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;INSERT OVERWRITE [LOCAL] DIRECTORY directory1
  [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0)
  SELECT ... FROM ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;扩展语法（多个insert）：&lt;/p&gt;

&lt;p&gt;``````&lt;br /&gt;
FROM from_statement&lt;br /&gt;
INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1&lt;br /&gt;
[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] …&lt;br /&gt;
```&lt;/p&gt;

&lt;p&gt;row_format相关语法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]
        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
        [NULL DEFINED AS char](Note: Only available starting with Hive 0.13)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Directory 可以是一个全路径的 url。&lt;/li&gt;
  &lt;li&gt;如果指定 &lt;code&gt;LOCAL&lt;/code&gt;，则会将数据写到本地文件系统。&lt;/li&gt;
  &lt;li&gt;输出的数据序列化为 text 格式，分隔符为 &lt;code&gt;^A&lt;/code&gt;，行于行之间通过换行符连接。如果存在不是基本类型的列，则这些列将被序列化为 JSON 格式。&lt;/li&gt;
  &lt;li&gt;在 Hive 0.11.0 可以输出字段的分隔符，之前版本的默认为 &lt;code&gt;^A&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-6&quot;&gt;4.1 测试;&lt;/h2&gt;

&lt;h3 id=&quot;section-7&quot;&gt;4.1.1 导出到本地文件系统&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;hive&amp;gt; insert overwrite local directory &#39;/tmp/test&#39; select * from test;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there&#39;s no reduce operator
Starting Job = job_1402248601715_0016, Tracking URL = http://cdh1:8088/proxy/application_1402248601715_0016/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1402248601715_0016
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2014-06-09 19:25:12,896 Stage-1 map = 0%,  reduce = 0%
2014-06-09 19:25:20,380 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
2014-06-09 19:25:21,433 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
MapReduce Total cumulative CPU time: 990 msec
Ended Job = job_1402248601715_0016
Copying data to local directory /tmp/test
Copying data to local directory /tmp/test
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.99 sec   HDFS Read: 305 HDFS Write: 32 SUCCESS
Total MapReduce CPU Time Spent: 990 msec
OK
Time taken: 18.438 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;导出后的数据预览如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[/tmp]# vim test/000000_0 
1^Aa
2^Ab
3^Ac
4^Ad
1^Aa
2^Ab
3^Ac
4^Ad
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到数据中的列与列之间的分隔符是&lt;code&gt;^A&lt;/code&gt;(ascii码是&lt;code&gt;\00001&lt;/code&gt;)，如果想修改分隔符，可以做如下修改：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; insert overwrite local directory &#39;/tmp/test&#39; row format delimited fields terminated by &#39;,&#39; select * from test;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来查看数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim test/000000_3 
1,a
2,b
3,c
4,d
1,a
2,b
3,c
4,d
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;hdfs-&quot;&gt;4.1.2 导出到 HDFS 中&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;hive&amp;gt; insert overwrite  directory &#39;/user/hive/tmp&#39; select * from test;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;p&gt;和导出文件到本地文件系统的HQL少一个local，数据的存放路径不一样了。&lt;/p&gt;

&lt;h3 id=&quot;hive&quot;&gt;4.1.3 导出到Hive的另一个表中&lt;/h3&gt;

&lt;p&gt;在实际情况中，表的输出结果可能太多，不适于显示在控制台上，这时候，将Hive的查询输出结果直接存在一个新的表中是非常方便的，我们称这种情况为CTAS（ &lt;code&gt;create table .. as select&lt;/code&gt;）如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive&amp;gt; create table test2 as select * from test;
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/06/09/hive-data-manipulation-language.html</link>
      <guid>http://blog.javachen.com/2014/06/09/hive-data-manipulation-language.html</guid>
      <pubDate>2014-06-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive中的FetchTask任务</title>
      <description>&lt;p&gt;Hive中有各种各样的Task任务，其中FetchTask算是最简单的一种了。FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。当你执行简单的&lt;code&gt;select * with limit&lt;/code&gt;语句的时候，其不会运行mapreduce任务。&lt;/p&gt;

&lt;p&gt;例如，运行下面语句不会出现mapreduce任务（说明：t表有一个字段，id为int类型，该表没有数据）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select * from t limit 1;            
OK
Time taken: 2.466 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;去掉limit语句，再执行一次，结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select * from t ;       
OK
Time taken: 0.097 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从结果来看，这种查询应该是有个默认的limit限制吧。&lt;/p&gt;

&lt;p&gt;如果修改查询语句，只查询某一些列呢？&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; select id from t ;                 
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there is no reduce operator
Starting Job = job_1402248601715_0004, Tracking URL = http://cdh1:8088/proxy/application_1402248601715_0004/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1402248601715_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2014-06-09 11:12:54,817 Stage-1 map = 0%,  reduce = 0%
2014-06-09 11:13:15,790 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.96 sec
2014-06-09 11:13:16,982 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.96 sec
MapReduce Total cumulative CPU time: 2 seconds 960 msec
Ended Job = job_1402248601715_0004
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 2.96 sec   HDFS Read: 257 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 960 msec
OK
Time taken: 51.496 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看上面运行日志，可以看到该次查询启动了mapreduce任务，mapper数为1，没有reducer任务。有没有一种方法，让上面语句也不允许mapreduce任务呢？&lt;/p&gt;

&lt;p&gt;答案是肯定的！这就要用到 &lt;code&gt;hive.fetch.task.conversion&lt;/code&gt; 参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.fetch.task.conversion&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;minimal&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;
    Some select queries can be converted to single FETCH task 
    minimizing latency.Currently the query should be single 
    sourced not having any subquery and should not have
    any aggregations or distincts (which incurrs RS), 
    lateral views and joins.
    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
    2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)
  &amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该参数默认值为minimal，表示运行“select * ”并带有limit查询时候，会将其转换为FetchTask；如果参数值为more，则select某一些列并带有limit条件时，也会将其转换为FetchTask任务。当然，还有前天条件：单一数据源，即输入来源一个表或者分区；没有子查询；没有聚合运算和distinct；不能用于视图和join。&lt;/p&gt;

&lt;p&gt;测试一下，先讲其参数值设为more，再运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; set hive.fetch.task.conversion=more;
hive&amp;gt; select id from t limit 1;           
OK
Time taken: 0.242 seconds
hive&amp;gt; select id from t ;                  
OK
Time taken: 0.496 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，在hive源码中搜索一下&lt;code&gt;hive.fetch.task.conversion&lt;/code&gt;，可以找到下面代码(来自SimpleFetchOptimizer类):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// returns non-null FetchTask instance when succeeded
  @SuppressWarnings(&quot;unchecked&quot;)
  private FetchTask optimize(ParseContext pctx, String alias, TableScanOperator source)
      throws HiveException {
    String mode = HiveConf.getVar(
        pctx.getConf(), HiveConf.ConfVars.HIVEFETCHTASKCONVERSION);

    boolean aggressive = &quot;more&quot;.equals(mode);
    FetchData fetch = checkTree(aggressive, pctx, alias, source);
    if (fetch != null) {
      int limit = pctx.getQB().getParseInfo().getOuterQueryLimit();
      FetchWork fetchWork = fetch.convertToWork();
      FetchTask fetchTask = (FetchTask) TaskFactory.get(fetchWork, pctx.getConf());
      fetchWork.setSink(fetch.completed(pctx, fetchWork));
      fetchWork.setSource(source);
      fetchWork.setLimit(limit);
      return fetchTask;
    }
    return null;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从源码中，简单分析可以知道，hive优化器在做FetchTask优化的时候，如果&lt;code&gt;hive.fetch.task.conversion&lt;/code&gt;为more，则会做一些优化。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2014/06/09/fetchtask-in-hive.html</link>
      <guid>http://blog.javachen.com/2014/06/09/fetchtask-in-hive.html</guid>
      <pubDate>2014-06-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Scrapy爬取知乎网站</title>
      <description>&lt;p&gt;本文主要记录使用使用 Scrapy 登录并爬取知乎网站的思路。Scrapy的相关介绍请参考 &lt;a href=&quot;/2014/05/24/using-scrapy-to-cralw-data.html&quot;&gt;使用Scrapy抓取数据&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;相关代码，见 &lt;a href=&quot;https://github.com/javachen/scrapy-zhihu-github&quot;&gt;https://github.com/javachen/scrapy-zhihu-github&lt;/a&gt; ，在阅读这部分代码之前，请先了解 Scrapy 的一些基本用法。&lt;/p&gt;

&lt;h1 id=&quot;cookie&quot;&gt;使用cookie模拟登陆&lt;/h1&gt;

&lt;p&gt;关于 cookie 的介绍和如何使用 python 实现模拟登陆，请参考&lt;a href=&quot;http://blog.csdn.net/figo829/article/details/18728381&quot;&gt;python爬虫实践之模拟登录&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;从这篇文章你可以学习到如何获取一个网站的 cookie 信息。下面所讲述的方法就是使用 cookie 来模拟登陆知乎网站并爬取用户信息。&lt;/p&gt;

&lt;p&gt;一个模拟登陆知乎网站的示例代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -*- coding:utf-8 -*-

from scrapy.selector import Selector
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.http import Request,FormRequest

from zhihu.settings import *

class ZhihuLoginSpider(CrawlSpider):
    name = &#39;zhihulogin1&#39;
    allowed_domains = [&#39;zhihu.com&#39;]
    start_urls = [&#39;http://www.zhihu.com/lookup/class/&#39;]

    rules = (
        Rule(SgmlLinkExtractor(allow=r&#39;search/&#39;)),
        Rule(SgmlLinkExtractor(allow=r&#39;&#39;)),
    )

    def __init__(self):
        self.headers =HEADER
        self.cookies =COOKIES

    def start_requests(self):
        for i, url in enumerate(self.start_urls):
            yield FormRequest(url, meta = {&#39;cookiejar&#39;: i}, \
                              headers = self.headers, \
                              cookies =self.cookies,
                              callback = self.parse_item)#jump to login page

    def parse_item(self, response):
        selector = Selector(response)

        urls = []
        for ele in selector.xpath(&#39;//ul/li[@class=&quot;suggest-item&quot;]/div/a/@href&#39;).extract():
           urls.append(ele)
        print urls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面是一个简单的示例，重写了 &lt;code&gt;start_requests&lt;/code&gt; 方法，针对 &lt;code&gt;start_urls&lt;/code&gt; 中的每一个url，这里为 &lt;a href=&quot;http://www.zhihu.com/lookup/class/&quot;&gt;http://www.zhihu.com/lookup/class/&lt;/a&gt;，重新创建 FormRequest 请求该 url，并设置 headers 和 cookies 两个参数，这样可以通过 cookies 伪造登陆。&lt;/p&gt;

&lt;p&gt;FormRequest 请求中有一个回调函数 parse_item 用于解析页面内容。&lt;/p&gt;

&lt;p&gt;HEADER 和 COOKIES 在 settings.py 中定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HEADER={
    &quot;Host&quot;: &quot;www.zhihu.com&quot;,
    &quot;Connection&quot;: &quot;keep-alive&quot;,
    &quot;Cache-Control&quot;: &quot;max-age=0&quot;,
    &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&quot;,
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36&quot;,
    &quot;Referer&quot;: &quot;http://www.zhihu.com/people/raymond-wang&quot;,
    &quot;Accept-Encoding&quot;: &quot;gzip,deflate,sdch&quot;,
    &quot;Accept-Language&quot;: &quot;zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2&quot;,
    }

COOKIES={
    &#39;checkcode&#39;:r&#39;&quot;$2a$10$9FVE.1nXJKq/F.nH62OhCevrCqs4skby2bC4IO6VPJITlc7Sh.NZa&quot;&#39;,
    &#39;c_c&#39;:r&#39;a153f80493f411e3801452540a3121f7&#39;,
    &#39;_ga&#39;:r&#39;GA1.2.1063404131.1384259893&#39;,
    &#39;zata&#39;:r&#39;zhihu.com.021715f934634a988abbd3f1f7f31f37.470330&#39;,
    &#39;q_c1&#39;:r&#39;59c45c60a48d4a5f9a12a52028a9aee7|1400081868000|1400081868000&#39;,
    &#39;_xsrf&#39;:r&#39;2a7cf7208bf24dbda3f70d953e948135&#39;,
    &#39;q_c0&#39;:r&#39;&quot;NmE0NzBjZTdmZGI4Yzg3ZWE0NjhkNjkwZGNiZTNiN2F8V2FhRTQ1QklrRjNjNGhMdQ==|1400082425|a801fc83ab07cb92236a75c87de58dcf3fa15cff&quot;&#39;,
    &#39;__utma&#39;:r&#39;51854390.1063404131.1384259893.1400518549.1400522270.5&#39;,
    &#39;__utmb&#39;:r&#39;51854390.4.10.1400522270&#39;,
    &#39;__utmc&#39;:r&#39;51854390&#39;,
    &#39;__utmz&#39;:r&#39;51854390.1400513283.3.3.utmcsr=zhihu.com|utmccn=(referral)|utmcmd=referral|utmcct=/people/hallson&#39;,
    &#39;__utmv&#39;:r&#39;51854390.100-1|2=registration_date=20121016=1^3=entry_date=20121016=1&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两个参数你都可以通过浏览器的一些开发工具查看到，特别是 COOKIES 中的信息。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;通过账号登陆&lt;/h1&gt;

&lt;p&gt;使用账户和密码进行登陆代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -*- coding:utf-8 -*-
from scrapy.selector import Selector
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.http import Request,FormRequest

import sys

reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

host=&#39;http://www.zhihu.com&#39;

class ZhihuUserSpider(CrawlSpider):
    name = &#39;zhihu_user&#39;
    allowed_domains = [&#39;zhihu.com&#39;]
    start_urls = [&quot;http://www.zhihu.com/lookup/people&quot;,]

    #使用rule时候，不要定义parse方法
    rules = (
        Rule(SgmlLinkExtractor(allow=(&quot;/lookup/class/[^/]+/?$&quot;, )), follow=True,callback=&#39;parse_item&#39;),
        Rule(SgmlLinkExtractor(allow=(&quot;/lookup/class/$&quot;, )), follow=True,callback=&#39;parse_item&#39;),
        Rule(SgmlLinkExtractor(allow=(&quot;/lookup/people&quot;, )),  callback=&#39;parse_item&#39;),
   )

    def __init__(self,  *a,  **kwargs):
        super(ZhihuLoginSpider, self).__init__(*a, **kwargs)

    def start_requests(self):
        return [FormRequest(
            &quot;http://www.zhihu.com/login&quot;,
            formdata = {&#39;email&#39;:&#39;XXXXXX&#39;,
                        &#39;password&#39;:&#39;XXXXXX&#39;
            },
            callback = self.after_login
        )]

    def after_login(self, response):
        for url in self.start_urls:
            yield self.make_requests_from_url(url)

    def parse_item(self, response):
        selector = Selector(response)
        for link in selector.xpath(&#39;//div[@id=&quot;suggest-list-wrap&quot;]/ul/li/div/a/@href&#39;).extract():
            #link  ===&amp;gt; /people/javachen
            yield Request(host+link+&quot;/about&quot;, callback=self.parse_user)

    def parse_user(self, response):
        selector = Selector(response)
        user = ZhihuUserItem()
        user[&#39;_id&#39;]=user[&#39;username&#39;]=response.url.split(&#39;/&#39;)[-2]
        user[&#39;url&#39;]= response.url
        user[&#39;nickname&#39;] = &#39;&#39;.join(selector.xpath(&quot;//div[@class=&#39;title-section ellipsis&#39;]/a[@class=&#39;name&#39;]/text()&quot;).extract())
        user[&#39;location&#39;] = &#39;&#39;.join(selector.xpath(&quot;//span[@class=&#39;location item&#39;]/@title&quot;).extract())
        user[&#39;industry&#39;] = &#39;&#39;.join(selector.xpath(&quot;//span[@class=&#39;business item&#39;]/@title&quot;).extract())
        user[&#39;sex&#39;] = &#39;&#39;.join(selector.xpath(&#39;//div[@class=&quot;item editable-group&quot;]/span/span[@class=&quot;item&quot;]/i/@class&#39;).extract()).replace(&quot;zg-icon gender &quot;,&quot;&quot;)
        user[&#39;description&#39;] = &#39;&#39;.join(selector.xpath(&quot;//span[@class=&#39;description unfold-item&#39;]/span/text()&quot;).extract()).strip().replace(&quot;\n&quot;,&#39;&#39;)
        user[&#39;view_num&#39;] = &#39;&#39;.join(selector.xpath(&quot;//span[@class=&#39;zg-gray-normal&#39;]/strong/text()&quot;).extract())
        user[&#39;update_time&#39;] = str(datetime.now())
        #抓取用户信息，此处省略代码
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该代码逻辑如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;重写 &lt;code&gt;start_requests&lt;/code&gt; 方法，通过设置 FormRequest 的 formdata 参数，这里是 email 和 password，然后提交请求到 &lt;code&gt;http://www.zhihu.com/login&lt;/code&gt;进行登陆，如果登陆成功之后，调用 &lt;code&gt;after_login&lt;/code&gt; 回调方法。&lt;/li&gt;
  &lt;li&gt;在 &lt;code&gt;after_login&lt;/code&gt; 方法中，一个个访问 &lt;code&gt;start_urls&lt;/code&gt; 中的 url&lt;/li&gt;
  &lt;li&gt;rules 中定义了一些正则匹配的 url 所对应的回调函数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 &lt;code&gt;parse_user&lt;/code&gt; 方法里，你可以通过 xpath 获取到用户的相关信息，也可以去获取关注和粉丝列表的数据。&lt;/p&gt;

&lt;p&gt;例如，先获取到用户的关注数 &lt;code&gt;followee_num&lt;/code&gt;，就可以通过下面一段代码去获取该用户所有的关注列表。代码如下&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;_xsrf = &#39;&#39;.join(selector.xpath(&#39;//input[@name=&quot;_xsrf&quot;]/@value&#39;).extract())
hash_id = &#39;&#39;.join(selector.xpath(&#39;//div[@class=&quot;zm-profile-header-op-btns clearfix&quot;]/button/@data-id&#39;).extract())

num = int(followee_num) if followee_num else 0
page_num = num/20
page_num += 1 if num%20 else 0
for i in xrange(page_num):
    params = json.dumps({&quot;hash_id&quot;:hash_id,&quot;order_by&quot;:&quot;created&quot;,&quot;offset&quot;:i*20})
    payload = {&quot;method&quot;:&quot;next&quot;, &quot;params&quot;: params, &quot;_xsrf&quot;:_xsrf}
    yield Request(&quot;http://www.zhihu.com/node/ProfileFolloweesListV2?&quot;+urlencode(payload), callback=self.parse_follow_url)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，你需要增加一个处理关注列表的回调方法 &lt;code&gt;parse_follow_url&lt;/code&gt;，这部分代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def parse_follow_url(self, response):
        selector = Selector(response)

        for link in selector.xpath(&#39;//div[@class=&quot;zm-list-content-medium&quot;]/h2/a/@href&#39;).extract():
            #link  ===&amp;gt; http://www.zhihu.com/people/peng-leslie-97
            username_tmp = link.split(&#39;/&#39;)[-1]
            if username_tmp in self.user_names:
                print &#39;GET:&#39; + &#39;%s&#39; % username_tmp
                continue

            yield Request(link+&quot;/about&quot;, callback=self.parse_user)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取粉丝列表的代码和上面代码类似。&lt;/p&gt;

&lt;p&gt;有了用户数据之后，你可以再编写一个爬虫根据用户去爬取问题和答案了，这部分代码略去，详细内容请参考 &lt;a href=&quot;https://github.com/javachen/scrapy-zhihu-github&quot;&gt;https://github.com/javachen/scrapy-zhihu-github&lt;/a&gt;。其中，还有抓取 github 用户等的相关代码。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;其他一些技巧&lt;/h1&gt;

&lt;p&gt;在使用 xpath 过程中，你可以下载浏览器插件 &lt;a href=&quot;https://chrome.google.com/webstore/detail/hgimnogjllphhhkhlmebbmlgjoejdpjl&quot;&gt;XPath Helper&lt;/a&gt;来快速定位元素并获取到 xpath 表达式，关于该插件用法，请自行 google 之。&lt;/p&gt;

&lt;p&gt;由于隐私设置的缘故，有些用户可能没有显示一些数据，故针对某些用户 xpath 表达式可能会抛出一些异常，如下面代码获取用户的名称：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;user[&#39;nickname&#39;] = selector.xpath(&quot;//div[@class=&#39;title-section ellipsis&#39;]/a[@class=&#39;name&#39;]/text()&quot;).extract()[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以将上面代码修改如下，以避免出现一个异常：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;user[&#39;nickname&#39;] = &#39;&#39;.join(selector.xpath(&quot;//div[@class=&#39;title-section ellipsis&#39;]/a[@class=&#39;name&#39;]/text()&quot;).extract())
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/06/08/using-scrapy-to-cralw-zhihu.html</link>
      <guid>http://blog.javachen.com/2014/06/08/using-scrapy-to-cralw-zhihu.html</guid>
      <pubDate>2014-06-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>MongoDB介绍</title>
      <description>&lt;p&gt;MongoDB 是一个开源的，高性能，无模式（或者说是模式自由），使用 C++ 语言编写的面向文档的数据库。正因为 MongoDB 是面向文档的，所以它可以管理类似 JSON 的文档集合。又因为数据可以被嵌套到复杂的体系中并保持可以查询可索引，这样一来，应用程序便可以以一种更加自然的方式来为数据建模。&lt;/p&gt;

&lt;p&gt;官方网站：&lt;a href=&quot;http://www.mongodb.org/&quot;&gt;http://www.mongodb.org/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;mongodb&quot;&gt;MongoDB介绍&lt;/h1&gt;

&lt;p&gt;所谓“面向集合”（Collenction-Orented），意思是数据被分组存储在数据集中，被称为一个集合（Collenction)。每个集合在数据库中都有一个唯一的标识名，并且可以包含无限数目的文档。集合的概念类似关系型数据库（RDBMS）里的表（table），不同的是它不需要定义任何模式（schema)。&lt;/p&gt;

&lt;p&gt;模式自由（schema-free)，意味着对于存储在mongodb数据库中的文件，我们不需要知道它的任何结构定义。如果需要的话，你完全可以把不同结构的文件存储在同一个数据库里。&lt;/p&gt;

&lt;p&gt;存储在集合中的文档，被存储为键-值对的形式。键用于唯一标识一个文档，为字符串类型，而值则可以是各中复杂的文件类型。我们称这种存储形式为BSON（Binary Serialized dOcument Format）。&lt;/p&gt;

&lt;p&gt;MongoDB 把数据存储在文件中（默认路径为：&lt;code&gt;/data/db&lt;/code&gt;），为提高效率使用内存映射文件进行管理。MongoDB的主要目标是在键/值存储方式（提供了高性能和高度伸缩性）以及传统的 RDBMS 系统（丰富的功能）架起一座桥梁，集两者的优势于一身。&lt;/p&gt;

&lt;p&gt;根据官方网站的描述，Mongo 适合用于以下场景：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;网站数据：Mongo非常适合实时的插入，更新与查询，并具备网站实时数据存储所需的复制及高度伸缩性。&lt;/li&gt;
  &lt;li&gt;缓存：由于性能很高，Mongo也适合作为信息基础设施的缓存层。在系统重启之后，由Mongo搭建的持久化缓存层可以避免下层的数据源过载。&lt;/li&gt;
  &lt;li&gt;大尺寸，低价值的数据：使用传统的关系型数据库存储一些数据时可能会比较昂贵，在此之前，很多时候程序员往往会选择传统的文件进行存储。&lt;/li&gt;
  &lt;li&gt;高伸缩性的场景：Mongo非常适合由数十或数百台服务器组成的数据库。Mongo的路线图中已经包含对MapReduce引擎的内置支持。&lt;/li&gt;
  &lt;li&gt;用于对象及JSON数据的存储：Mongo的BSON数据格式非常适合文档化格式的存储及查询。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;自然，MongoDB 的使用也会有一些限制，例如它不适合：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;高度事务性的系统：例如银行或会计系统。传统的关系型数据库目前还是更适用于需要大量原子性复杂事务的应用程序。&lt;/li&gt;
  &lt;li&gt;传统的商业智能应用：针对特定问题的BI数据库会对产生高度优化的查询方式。对于此类应用，数据仓库可能是更合适的选择。&lt;/li&gt;
  &lt;li&gt;需要SQL的问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;mongodb-1&quot;&gt;MongoDB的安装&lt;/h1&gt;

&lt;p&gt;mongodb的官网就有下载，根据系统windows还是linux还是别的下载32位还是64位的，然后解压安装。&lt;/p&gt;

&lt;p&gt;如果你使用的时mac系统，你可以通过 brewhome 来安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew install mongodb
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;mongodb-2&quot;&gt;MongoDB的管理&lt;/h1&gt;

&lt;p&gt;解压完之后进入bin目录，里面都是一些可执行文件，mongo，mongod，mongodump，mongoexport等。&lt;/p&gt;

&lt;p&gt;1).启动和停止MongoDB&lt;/p&gt;

&lt;p&gt;通过执行mongod来启动MongoDB服务器:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mongod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mongod有很多的配置启动选项的，可以通过&lt;code&gt;mongod --help&lt;/code&gt;来查看，其中有一些主要的选项：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;--dbpath&lt;/code&gt;：指定数据目录，默认是&lt;code&gt;/data/db/&lt;/code&gt;。每个mongod进程都需要独立的目录，启动mongod时就会在数据目录中创建mongod.lock文件，防止其他mongod进程使用该数据目录。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;：指定服务器监听的端口，默认是&lt;code&gt;27017&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--fork&lt;/code&gt;：以守护进程的方式运行MongoDB。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--logpath&lt;/code&gt;：指定日志输出路径，如果不指定则会在终端输出。每次启动都会覆盖原来的日志，如果不想覆盖就要用&lt;code&gt;--logappend&lt;/code&gt;选项。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--config&lt;/code&gt;：指定配置文件，加载命令行未指定的各种选项。我们可以讲我们需要用到的选项写在某个文件中，然后通过该选项来指定这个文件就不必每次启动mongod时都要写。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2).数据备份&lt;/p&gt;

&lt;p&gt;a. 数据文件备份&lt;/p&gt;

&lt;p&gt;最简单的备份就是数据文件的备份，就是直接赋值data/db这个目录，因为我们前面已经指定了数据目录就是data/db，那么MongoDB多有的数据都在这里，但是有个问题就是最新的数据还在缓存中，没用同步到磁盘，可以先停止shutdownServer()再备份。但是这样会影响MongoDB正常的工作。&lt;/p&gt;

&lt;p&gt;b. mongodump和mongorestore&lt;/p&gt;

&lt;p&gt;bin 中还有 mongodump 和 mongorestore 两个可执行文件，这个是对MongoDB的某个数据库进行备份，可以在MongoDB正在运行时进行备份，比如备份test数据库，然后将备份的数据库文件再倒入别的MongoDB服务器上。这种备份的方式备份的不是最新数据，只是已经写入MongoDB中的数据，可能还有的数据在缓存中没有写入，那么这部分数据就是备份不到的。mongodump和mongorestore也可以通过&lt;code&gt;--help&lt;/code&gt;查询所有选项。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mongodump -d text -o test_db
connected to: 127.0.0.1
2014-06-06T12:54:59.741+0800 DATABASE: text	 to 	test_db/text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;-d&lt;/code&gt;是指定数据库，&lt;code&gt;-o&lt;/code&gt;是输出备份文件，上面将 test 数据库备份为 test_db 文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mongorestore --port 27017 -d temple --drop test_db/test/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里将上面备份出来的 test 数据库现在重新导入到 temple 数据库，&lt;code&gt;--drop&lt;/code&gt;代表如果有了 temple 数据库则将其中的所有集合删除，不指定就会和原来 temple 中的集合合并。&lt;/p&gt;

&lt;p&gt;c. mongoexport和mongoimport&lt;/p&gt;

&lt;p&gt;备份某个数据库中的某个表，同样可以通过&lt;code&gt;--help&lt;/code&gt;来查看所有的选项，当然mongoexport也是可以不统计的备份，但是却不一定是最新数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mongoexport --port 27017 -d zhihu -c zh_user -o zh_user 
$ mongoimport --port 9352 -d temple -c user zh_user
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;-c&lt;/code&gt;表示 collection 集合，上面将 zhihu 库中的 zh_user 集合备份出来为 zh_user 文件，然后再导入到 temple 库中的user集合。&lt;/p&gt;

&lt;p&gt;d. sync和锁&lt;/p&gt;

&lt;p&gt;mongodump和mongoexport对都可以不停MongoDB服务器来进行备份，但是却失去了获取实时数据的能力，而fsync命令也能在不停MongoDB的情况下备份实时数据，它的实现其实就是上锁，阻止对数据库的写入操作，然后将缓冲区的数据写入磁盘，备份后再解锁。在这个期间，任何对数据库的写入操作都会阻塞，直到锁被释放。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.runCommand({&quot;fsync&quot; : 1, &quot;lock&quot; : 1})
{
        &quot;info&quot; : &quot;now locked against writes, use db.fsyncUnlock() to unlock&quot;,
        &quot;seeAlso&quot; : &quot;http://dochub.mongodb.org/core/fsynccommand&quot;,
        &quot;ok&quot; : 1
}
&amp;gt;
&amp;gt;在这之间进行备份，执行任何insert操作都会阻塞
&amp;gt;
&amp;gt; db.fsyncUnlock()
{ &quot;ok&quot; : 1, &quot;info&quot; : &quot;unlock completed&quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;数据类型&lt;/h1&gt;

&lt;p&gt;MongoDB的文件存储格式为BSON,同JSON一样支持往其它文档对象和数组中再插入文档对象和数组，同时扩展了JSON的数据类型.与数据库打交道的那些应用。例如，JSON没有日期类型，这会使得处理本来简单的日期问题变得非常繁琐。只有一种数字类型，没法区分浮点数和整数，更不能区分32位和64位数字。也没有办法表示其他常用类型，如正则表达式或函数。&lt;/p&gt;

&lt;p&gt;下面是MongoDB的支持的数据类型：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;null    null用于表示空值或者不存在的字段。 &lt;code&gt;{&quot;x&quot;:null}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;布尔   布尔类型有两个值’true’和’false1’。 &lt;code&gt;{&quot;X&quot;:true}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;32位整数  类型不可用。JavaScript仅支持64位浮点数，所以32位整数会被自动转换。&lt;/li&gt;
  &lt;li&gt;64位整数  不支持这个类型。shell会使用一个特殊的内嵌文档来显示64位整数。&lt;/li&gt;
  &lt;li&gt;64位浮点数  shell中的数字都是这种类型。下面的表示都是浮点数： &lt;code&gt;{&quot;X&quot; : 3.1415926} {&quot;X&quot; : 3}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;字符串   UTF-8字符串都可表示为字符串类型的数据： &lt;code&gt;{&quot;x&quot; : &quot;foobar&quot;}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;符号  不支持这种类型。shell将数据库里的符号类型转换成字符串。&lt;/li&gt;
  &lt;li&gt;对象id  对象id是文档的12字节的唯一ID：&lt;code&gt;{&quot;_id&quot;:ObjectId() }&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;日期  日期类型存储的是从标准纪元开始的毫秒数。不存储时区： &lt;code&gt;{&quot;X&quot; : new Date()}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;正则表达式  文档中可以包含正则表达式，采用JavaScript的正则表达式语法：&lt;code&gt;{&quot;x&quot;: /foobar/i}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;代码  文档中还可以包含JavaScript代码：&lt;code&gt;{&quot;x&quot;: function() { /* …… */ }}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;二进制数据  二进制数据可以由任意字节的串组成。不过shell中无法使用。&lt;/li&gt;
  &lt;li&gt;最大值  BSON包括一个特殊类型，表示可能的最大值。shell中没有这个类型。&lt;/li&gt;
  &lt;li&gt;最小值  BSON包括一个特殊类型，表示可能的最小值。shell中没有这个类型。&lt;/li&gt;
  &lt;li&gt;未定义  文档中也可以使用未定义类型：&lt;code&gt;{&quot;x&quot;:undefined}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;数组  值的集合或者列表可以表示成数组：&lt;code&gt;{&quot;x&quot; : [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]}&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;内嵌文档  文档可以包含别的文档，也可以作为值嵌入到父文档中，数据可以组织得更自然些，不用非得存成扁平结构的：{“x” : {“food” ： “noodle”}}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JavaScript中只有一种“数字”类型。因为MongoDB中有3种数字类型（32位整数、64位整数和64位浮点数），shell必须绕过JavaScript的限制。默认情况下，shell中的数字都被MongoDB当做是双精度数。这意味着如果你从数据库中获得的是一个32位整数，修改文档后，将文档存回数据库的时候，这个整数也被转换成了浮点数，即便保持这个整数原封不动也会这样的。所以明智的做法是尽量不要在shell下覆盖整个文档。&lt;/p&gt;

&lt;p&gt;由于MongoDB中不同文档的同一个key的value数据类型可以不同，当我们根据某个key查询的时候会发生不同数据类型之间的比较。所以MongoDB内部设定了数据类型间的大小，顺序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;最小值&amp;lt;null&amp;lt;数字(32位整数、63位整形、64位浮点数)&amp;lt;字符串&amp;lt;对象/文档&amp;lt;数组&amp;lt;二进制数据&amp;lt;对象ID&amp;lt;布尔型&amp;lt;日期型&amp;lt;时间戳&amp;lt;正则&amp;lt;最大值
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://segmentfault.com/a/1190000000368210&quot;&gt;MongoDB 入门须知&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://www.2cto.com/database/201406/306599.html&quot;&gt;MongoDB入门学习(一)：MongoDB的安装和管理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/06/06/about-mongodb.html</link>
      <guid>http://blog.javachen.com/2014/06/06/about-mongodb.html</guid>
      <pubDate>2014-06-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>不用Cloudera Manager安装Cloudera Search</title>
      <description>&lt;p&gt;Cloudera Search 用来在 hadoop 基础上建立索引和全文检索，本文主要记录如何安装 CLoudera Search 的过程，其中也包括如何安装和启动 Zookeeper、Solr、MapReduce等工具和服务。&lt;/p&gt;

&lt;h1 id=&quot;cloudera-search&quot;&gt;Cloudera Search介绍&lt;/h1&gt;

&lt;p&gt;Cloudera Search 核心部件包括 Hadoop 和 Solr，后者建立在 Lucene 之上；而 Hadoop 也正是在06年正式成为 Lucene 的一个子项目而发展起来的。&lt;/p&gt;

&lt;p&gt;通过 Tika, Cloudera Search 支持大量的被广泛使用的文件格式；除此之外，Cloudera Search 还支持很多其他在Hadoop应用中常用的数据，譬如 Avro, SequenceFile, 日志文件等。&lt;/p&gt;

&lt;p&gt;用来建立索引和全文检索的数据可以是来自于 HDFS，譬如日志文件，Hive 或者 HBase 的表等等（通过集成 NGData 的 Lily 项目，对 HBasae 的支持工作也在进行中）。或者通过结合使用 Flume 采集于外部数据源，通过一个新支持的 Flume Sink 直接写到索引库里；同时还可以充分利用 Flume 来对要建立索引的数据进行各种预处理，譬如转换，提取创建元数据等。&lt;/p&gt;

&lt;p&gt;建立的索引存储于 HDFS。这给搜索带来了易于扩展，冗余和容错的好处。此外，我们还可以运行 MapReduce 来对我们所需要检索的数据进行索引，提供给 Solr。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;环境&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：&lt;code&gt;CentOs6.5&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Hadoop：&lt;code&gt;cdh5.3.0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hadoop&quot;&gt;安装 Hadoop集群&lt;/h1&gt;

&lt;p&gt;这里使用参考 &lt;a href=&quot;/2013/06/24/install-cdh-by-cloudera-manager.html&quot;&gt;通过Cloudera Manager安装CDH&lt;/a&gt;一文搭建的集群，其中也包括了一个三节点的 ZooKeeper 集群。该集群包括三个节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	192.168.56.121        cdh1     NameNode、Hive、ResourceManager、HBase
	192.168.56.122        cdh2     DataNode、SSNameNode、NodeManager、HBase
	192.168.56.123        cdh3     DataNode、HBase、NodeManager
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;zookeeper&quot;&gt;安装 ZooKeeper&lt;/h1&gt;

&lt;p&gt;Zookeeper 的安装过程，请参考 &lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;solr&quot;&gt;安装 Solr&lt;/h1&gt;

&lt;p&gt;Zookeeper 启动之后，需要安装 Solr，关于 Solr 的安装，可以参考 &lt;a href=&quot;/2014/03/10/how-to-install-solrcloud.html&quot;&gt;Apache SolrCloud安装&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在三个节点上安装 solr-server：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum install solr-server solr solr-doc -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装 Spark Indexer：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo yum install solr-crunch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装 MapReduce Tools：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo yum install solr-mapreduce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装 Lily HBase Indexer：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo yum install hbase-solr-indexer hbase-solr-doc
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：Lily HBase Indexer和 cdh5 工作的时候，你需要在运行 MapReduce 任务之前运行下面命令：&lt;br /&gt;
&lt;code&gt;export HADOOP_CLASSPATH=&amp;lt;Path to hbase-protocol-**.jar&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;solr-1&quot;&gt;配置 Solr&lt;/h1&gt;

&lt;p&gt;修改 solr 配置文件 &lt;code&gt;/etc/default/solr&lt;/code&gt; 中 ZooKeeper 连接地址：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SOLR_ZK_ENSEMBLE=cdh1:2181,cdh2:2181,cdh3:2181/solr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 solr 配置文件 &lt;code&gt;/etc/default/solr&lt;/code&gt; 中 HDFS 连接地址：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SOLR_HDFS_HOME=hdfs://cdh1:8020/solr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置 HDFS 配置文件目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;SOLR_HDFS_CONFIG=/etc/hadoop/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你配置了 Kerberos，则在 kerberos 服务器上为每个安装 solr 的节点先生成 solr 的凭证：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;kadmin:  addprinc -randkey solr/cdh1@JAVACHEN.COM
kadmin:  xst -norandkey -k solr.keytab solr/cdh1

kadmin:  addprinc -randkey solr/cdh2@JAVACHEN.COM
kadmin:  xst -norandkey -k solr.keytab solr/cdh2

kadmin:  addprinc -randkey solr/cdh3@JAVACHEN.COM
kadmin:  xst -norandkey -k solr.keytab solr/cdh3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，将 solr.keytab 拷贝到 /etc/solr/conf：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo mv solr.keytab /etc/solr/conf/

$ sudo chown solr:hadoop /etc/solr/conf/solr.keytab
$ sudo chmod 400 /etc/solr/conf/solr.keytab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，修改每个安装了 solr 节点的 /etc/default/solr，例如在 cdh1节点上修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;SOLR_KERBEROS_ENABLED=true
SOLR_KERBEROS_KEYTAB=/etc/solr/conf/solr.keytab
SOLR_KERBEROS_PRINCIPAL=solr/cdh1@JAVACHEN.COM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 HDFS 中创建 &lt;code&gt;/solr&lt;/code&gt; 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo -u hdfs hadoop fs -mkdir /solr
$ sudo -u hdfs hadoop fs -chown solr /solr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果开启了 kerberos，则先获取 hdfs 服务的凭证在运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.com

$ hadoop fs -mkdir /solr
$ hadoop fs -chown solr /solr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集成 Sentry，取消 /etc/default/solr 下面注释：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;# SOLR_AUTHORIZATION_SENTRY_SITE=/etc/solr/conf/sentry-site.xml
# SOLR_AUTHORIZATION_SUPERUSER=solr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集成 Hue，取消 /etc/default/solr 下面注释：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;# SOLR_SECURITY_ALLOWED_PROXYUSERS=hue
# SOLR_SECURITY_PROXYUSER_hue_HOSTS=*
# SOLR_SECURITY_PROXYUSER_hue_GROUPS=*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化 ZooKeeper Namespace：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ solrctl init
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：你可以添加 &lt;code&gt;--force&lt;/code&gt; 参数强制清空 ZooKeeper 数据，清空之前，请先停止 ZooKeeper 集群。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;solr-2&quot;&gt;启动 Solr&lt;/h1&gt;

&lt;p&gt;在每一个安装了 Solr server 的节点上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo service solr-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过下面命令查看 Solr 是否启动成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ jps -lm
28053 org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer
31710 org.apache.zookeeper.server.quorum.QuorumPeerMain /etc/zookeeper/conf/zoo.cfg
14479 org.apache.catalina.startup.Bootstrap start
29994 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
29739 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
27298 org.apache.catalina.startup.Bootstrap httpfs start
30123 org.apache.hadoop.hdfs.server.namenode.NameNode
21761 sun.tools.jps.Jps -lm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面用到了 solrctl 命令，该命令用来管理 SolrCloud 的部署和配置，其语法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;solrctl [options] command [command-arg] [command [command-arg]] ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可选参数有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;--solr&lt;/code&gt;：指定 SolrCloud 的 web API，如果在 SolrCloud 集群之外的节点运行命令，就需要指定该参数。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--zk&lt;/code&gt;：指定 zk 集群地址。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--help&lt;/code&gt;：打印帮助信息。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--quiet&lt;/code&gt;：静默模式运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;command 命令有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;init [--force]&lt;/code&gt;：初始化配置。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;instancedir&lt;/code&gt;：维护实体目录。可选的参数有：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;--generate path&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--create name path&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--update name path&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--get name path&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--delete name&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--list&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;collection&lt;/code&gt;：维护 collections。可选的参数有：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;--create name -s &amp;lt;numShards&amp;gt; [-c &amp;lt;collection.configName&amp;gt;] [-r &amp;lt;replicationFactor&amp;gt;] [-m &amp;lt;maxShardsPerNode&amp;gt;] [-n &amp;lt;createNodeSet&amp;gt;]]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--delete name&lt;/code&gt;: Deletes a collection.&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--reload name&lt;/code&gt;: Reloads a collection.&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--stat name&lt;/code&gt;: Outputs SolrCloud specific run-time information for a collection.&lt;/li&gt;
      &lt;li&gt;``–list`: Lists all collections registered in SolrCloud.&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--deletedocs name&lt;/code&gt;: Purges all indexed documents from a collection.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;core&lt;/code&gt;：维护 cores。可选的参数有：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;--create name [-p name=value]...]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--reload name&lt;/code&gt;: Reloads a core.&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--unload name&lt;/code&gt;: Unloads a core.&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--status name&lt;/code&gt;: Prints status of a core.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cluster&lt;/code&gt;：维护集群配置信息。可选的参数有：
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;--get-solrxml file&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;--put-solrxml file&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;solr-&quot;&gt;创建 Solr 运行时配置&lt;/h1&gt;

&lt;p&gt;在一个节点上（例如 cdh1）生成配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ solrctl instancedir --generate $HOME/solr_configs
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：你可以在 &lt;code&gt;/var/lib/solr&lt;/code&gt; 创建目录，维护配置文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;执行完之后，你可以修改 $HOME/solr_configs/conf 目录下的配置文件，其目录下文件如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ll ~/solr_configs/conf/
total 348
-rw-r--r-- 1 root root  1092 Jun  2 23:10 admin-extra.html
-rw-r--r-- 1 root root   953 Jun  2 23:10 admin-extra.menu-bottom.html
-rw-r--r-- 1 root root   951 Jun  2 23:10 admin-extra.menu-top.html
-rw-r--r-- 1 root root  4041 Jun  2 23:10 currency.xml
-rw-r--r-- 1 root root  1386 Jun  2 23:10 elevate.xml
drwxr-xr-x 2 root root  4096 Jun  2 23:10 lang
-rw-r--r-- 1 root root 82327 Jun  2 23:10 mapping-FoldToASCII.txt
-rw-r--r-- 1 root root  3114 Jun  2 23:10 mapping-ISOLatin1Accent.txt
-rw-r--r-- 1 root root   894 Jun  2 23:10 protwords.txt
-rw-r--r-- 1 root root 59635 Jun  2 23:10 schema.xml
-rw-r--r-- 1 root root   921 Jun  2 23:10 scripts.conf
-rw-r--r-- 1 root root 72219 Jun  2 23:10 solrconfig.xml
-rw-r--r-- 1 root root 73608 Jun  2 23:10 solrconfig.xml.secure
-rw-r--r-- 1 root root    16 Jun  2 23:10 spellings.txt
-rw-r--r-- 1 root root   795 Jun  2 23:10 stopwords.txt
-rw-r--r-- 1 root root  1148 Jun  2 23:10 synonyms.txt
-rw-r--r-- 1 root root  1469 Jun  2 23:10 update-script.js
drwxr-xr-x 2 root root  4096 Jun  2 23:10 velocity
drwxr-xr-x 2 root root  4096 Jun  2 23:10 xslt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 collection1 实例并将配置文件上传到 zookeeper：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ solrctl instancedir --create collection1 $HOME/solr_configs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以通过下面命令查看上传的 instance：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ solrctl instancedir --list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上传到 zookeeper 之后，其他节点就可以从上面下载配置文件。&lt;/p&gt;

&lt;p&gt;接下来，还是在 cdh1 节点上创建 collection，因为我的 SolrCloud 集群有三个节点，故这里分片数设为3，并设置副本为1，如果有更多节点，可以将副本设置为更大的一个数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ solrctl collection --create collection1 -s 3 -r 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行成功之后，你可以通过 &lt;a href=&quot;http://cdh1:8983/solr/#/~cloud&quot;&gt;http://cdh1:8983/solr/#/~cloud&lt;/a&gt;、&lt;a href=&quot;http://cdh2:8983/solr/#/~cloud&quot;&gt;http://cdh2:8983/solr/#/~cloud&lt;/a&gt;、&lt;a href=&quot;http://cdh3:8983/solr/#/~cloud&quot;&gt;http://cdh3:8983/solr/#/~cloud&lt;/a&gt; 查看创建的分片。从网页上可以看到，已经自动创建了三个分片，并且三个分片分布在3个节点之上。&lt;/p&gt;

&lt;h1 id=&quot;hbase-solr-indexer&quot;&gt;配置 hbase-solr-indexer&lt;/h1&gt;

&lt;p&gt;1）开启 HBase replication&lt;/p&gt;

&lt;p&gt;Lily HBase Indexer 的实现依赖于 HBase的replication，故需要开启复制。将 &lt;code&gt;/usr/share/doc/hbase-solr-doc*/demo/hbase-site.xml&lt;/code&gt;文件的内容拷贝到 &lt;code&gt;hbase-site.xml&lt;/code&gt;，&lt;strong&gt;注意&lt;/strong&gt;：删除掉 &lt;code&gt;replication.replicationsource.implementation&lt;/code&gt; 参数配置。&lt;/p&gt;

&lt;p&gt;2）将 hbase-solr-indexer 服务指向 hbase 集群&lt;/p&gt;

&lt;p&gt;修改 &lt;code&gt;/etc/hbase-solr/conf/hbase-indexer-site.xml&lt;/code&gt;，添加如下参数，其值和 &lt;code&gt;hbase-site.xml&lt;/code&gt; 中的 &lt;code&gt;hbase.zookeeper.quorum&lt;/code&gt; 属性值保持一致（注意添加上端口）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
   &amp;lt;name&amp;gt;hbaseindexer.zookeeper.connectstring&amp;lt;/name&amp;gt;
   &amp;lt;value&amp;gt;cdh1:2181,cdh2:2181,cdh3:2181&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后再重启服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service hbase-solr-indexer restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;本文内容主要介绍了如何不使用 Cloudera Manager 来安装 Cloudera Search，下篇文章将介绍如何使用 Cloudera Search。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://blog.sina.com.cn/s/blog_9bf980ad010182ph.html&quot;&gt;Cloudera Search: 轻松实现Hadoop全文检索 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Search/latest/Cloudera-Search-User-Guide/Cloudera-Search-User-Guide.html&quot;&gt;Cloudera-Search-User-Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/06/03/install_cloudera_search_without_cm.html</link>
      <guid>http://blog.javachen.com/2014/06/03/install_cloudera_search_without_cm.html</guid>
      <pubDate>2014-06-03T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>关于CAP理论的一些笔记</title>
      <description>&lt;h1 id=&quot;cap&quot;&gt;CAP的概念&lt;/h1&gt;

&lt;p&gt;2000年，Eric Brewer 教授在 ACM 分布式计算年会上指出了著名的 CAP 理论：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;分布式系统不可能同时满足一致性(C: Consistency)，可用性(A: Availability)和分区容错性(P: Tolerance of network Partition)这三个需求。大约两年后，Seth Gilbert 和 Nancy lynch 两人证明了CAP理论的正确性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;三者的含义如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Consistency&lt;/code&gt;：一致性，一个服务是一致的完整操作或完全不操作（A service that is consistent operates fully or not at all，精确起见列出原文），也有人将其简称为数据一致性，任何一个读操作总是能读取到之前完成的写操作结果，也就是在分布式环境中，多点的数据是一致的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Availability&lt;/code&gt;：可用性，每一个操作总是能够在确定的时间内返回，也就是系统随时都是可用的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Tolerance of network Partition&lt;/code&gt;：分区容忍性，节点 crash 或者网络分片都不应该导致一个分布式系统停止服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于 CAP 理论的历史和介绍可以参考 &lt;a href=&quot;http://www.julianbrowne.com/article/viewer/brewers-cap-theorem&quot;&gt;Brewer’s CAP Theorem&lt;/a&gt; 和 &lt;a href=&quot;http://duanple.blog.163.com/blog/static/7097176720114733655390/&quot;&gt;中文翻译&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;cap-1&quot;&gt;基本CAP的证明思路&lt;/h1&gt;

&lt;p&gt;CAP 的证明基于 &lt;code&gt;异步网络&lt;/code&gt;，异步网络也是反映了真实网络中情况的模型。&lt;/p&gt;

&lt;p&gt;真实的网络系统中，&lt;strong&gt;节点之间不可能保持同步，即便是时钟也不可能保持同步，所有的节点依靠获得的消息来进行本地计算和通讯&lt;/strong&gt;。这个概念其实是相当强的，&lt;strong&gt;意味着任何超时判断也是不可能的&lt;/strong&gt;，因为没有共同的时间标准。之后我们会扩展 CAP 的证明到弱一点的异步网络中，这个网络中时钟不完全一致，但是时钟运行的步调是一致的，这种系统是允许节点做超时判断的。&lt;/p&gt;

&lt;p&gt;CAP 的证明很简单，假设两个节点集{G1, G2}，由于网络分片导致 G1 和 G2 之间所有的通讯都断开了，如果在 G1 中写，在 G2 中读刚写的数据， G2 中返回的值不可能是 G1 中的写值。由于 &lt;code&gt;A&lt;/code&gt; 的要求，G2 一定要返回这次读请求，由于 &lt;code&gt;P&lt;/code&gt; 的存在，导致 &lt;code&gt;C&lt;/code&gt;一定是不可满足的。&lt;/p&gt;

&lt;p&gt;为什么不能完全保证这个三点了，个人觉得主要是因为一旦进行分区了，就说明了必须节点之间必须进行通信，&lt;strong&gt;涉及到通信，就无法确保在有限的时间内完成指定的行为&lt;/strong&gt;，如果要求两个操作之间要完整的进行，肯定会存在某一个时刻只完成一部分的业务操作，在通信完成的这一段时间内，数据就是不一致性的。如果要求保证一致性，那么就必须在通信完成这一段时间内保护数据，使得任何访问这些数据的操作不可用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如果想保证一致性和可用性，那么数据就不能够分区&lt;/strong&gt;。一个简单的理解就是所有的数据就必须存放在一个数据库里面，不能进行数据库拆分。这个对于大数据量，高并发的互联网应用来说，是不可接受的。&lt;/p&gt;

&lt;p&gt;这里引用 &lt;a href=&quot;http://duanple.blog.163.com/blog/static/7097176720114733655390/&quot;&gt;Brewer’s CAP Theorem&lt;/a&gt; 中的图和文字来说明。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.julianbrowne.com//attachments/brewers-cap-theorem/images/intro.png&quot; alt=&quot;intro&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图显示了网络中的两个节点 N1，N2，他们共享同一数据 V，其值为 V0。N1 上有一个算法 A，我们可以认为 A 是安全、无 bug、可预测和可靠的。N2 上有一个类似的算法 B。在这个例子中，A 写入 V 的新值而 B 读取 V 的值。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.julianbrowne.com//attachments/brewers-cap-theorem/images/scenario-1.png&quot; alt=&quot;scenario-1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;正常情况过程如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1) A 写入新的 V 值，我们称作 v1。&lt;/li&gt;
  &lt;li&gt;2) N1 发送信息给 N2，更新 V 的值。&lt;/li&gt;
  &lt;li&gt;3) 现在 B 读取的 V 值将会是 V1。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://www.julianbrowne.com//attachments/brewers-cap-theorem/images/scenario-2.png&quot; alt=&quot;scenario-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果网络断开，意味着从 N1 无法发送信息到 N2，那么在第3步的时候，N2 就会包含一个步一致的 V 值。&lt;/p&gt;

&lt;p&gt;即使将其扩展到几百个事务中，这也会成为一个大问题。如果 M 是一个异步消息，那么 N1 无法知道 N2 是否收到了消息。即使 M 是保证能发送的，N1 也无法知道是否消息由于分区事件的发生而延迟，或 N2 上的其他故障而延迟。即使将 M 作为同步消息也不能解决问题，因为那将会使得 N1 上 A 的写操作和 N1 到 N2 的更新事件成为一个原子操作，而这将导致同样的等待问题。&lt;strong&gt;Gilbert 和Lynch已经证明，使用其他的变种方式，即使是部分同步模型（每个节点上使用安排好的时钟）也无法保证原子性。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;因此，CAP 告诉我们，如果想让 A 和 B 是高可用的（例如，以最小的延迟提供服务）并且想让所有的 N1 到 Nn（n的值可以是数百甚至是上千）的节点能够冗余网络的分区（丢失信息，无法传递信息，硬件无法提供服务，处理失败），那么有时我们就得面临这样的情况：某些节点认为 V 的值是 V0 而其他节点会认为 V 的值是 V1。&lt;/p&gt;

&lt;p&gt;让我们从事务的角度分析一下。下面的图中 a 是整个过程，要具有一致性的话需要等待 a1 进行 write，然后同步到 a2，然后 a2 再进行 write，只有整个事务完成以后，a2 才能够进行 read。但是这样的话使得整个系统的可用性下降，a2 一直阻塞在那里等待 a1 同步到 a2。这个时候如果对一致性要求不高的话，a2 可以不等待 a1 数据对于 a2 的写同步，直接读取，这样虽然此时的读写不具有一致性，但是在后面可以通过异步的方式使得 a1 和 a2 的数据最终一致，达到 &lt;code&gt;最终一致性&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.julianbrowne.com//attachments/brewers-cap-theorem/images/tx-view.png&quot; alt=&quot;tx-view&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;base&quot;&gt;BASE理论&lt;/h1&gt;

&lt;p&gt;BASE 理论是 CAP 理论结合实际的产物。 BASE(Basically Available, Soft-state,Eventuallyconsistent)英文中有碱的意思，这个正好和 ACID 的酸的意义相对，很有意思。BASE 恰好和 ACID 是相对的，BASE 要求牺牲高一致性，获得可用性或可靠性。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;Basically Availble&lt;/code&gt;： 基本可用(支持分区失败)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Soft-state&lt;/code&gt;：软状态/柔性事务(无状态连接，支持异步)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Eventual Consistency&lt;/code&gt;： 最终一致性(不要求高一致性，只要求最终能够一致)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BASE 理论的核心是：&lt;strong&gt;牺牲高一致性，获得可用性或可靠性&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&quot;cap-2&quot;&gt;CAP选择&lt;/h1&gt;

&lt;p&gt;当处理 CAP 的问题时，你会有几个选择。最明显的是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;放弃 Tolerance of network Partition&lt;/code&gt;。如果你想避免分区问题发生，你就必须要阻止其发生。一种做法是将所有的东西（与事务相关的）都放到一台机器或者一个机架上。这样还是有可能部分失败，但你不太可能碰到由分区问题带来的负面效果。当然，这个选择会严重影响系统的扩展。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;放弃 Availability&lt;/code&gt;。相对于放弃 Tolerance of network Partition 来说，其反面就是放弃 Availability。一旦遇到分区事件，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务。在多个节点上控制这一点会相当复杂，而且恢复的节点需要处理逻辑，以便平滑地返回服务状态。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;放弃 Consistency&lt;/code&gt;。放弃一致性，你的系统可能返回不太精确的数据，但系统将会变得“最终一致”，即使是网络发生分区的时候也是如此。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是一个使用 CAP 理论的生态系统的分布图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://hi.csdn.net/attachment/201109/6/0_1315316512jhTH.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;任何架构师在设计分布式的系统的时候，都必须在这三者之间进行取舍。首先就是是否选择分区，由于在一个数据分区内，根据数据库的ACID特性，是可以保证一致性的，不会存在可用性和一致性的问题，唯一需要考虑的就是性能问题。对于可用性和一致性，大多数应用就必须保证可用性，毕竟是互联网应用，牺牲了可用性，相当于间接的影响了用户体验，而唯一可以考虑就是一致性了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section&quot;&gt;牺牲一致性&lt;/h2&gt;

&lt;p&gt;对于牺牲一致性的情况最多的就是缓存和数据库的数据同步问题，我们把缓存看做一个数据分区节点，数据库看作另外一个节点，这两个节点之间的数据在任何时刻都无法保证一致性的。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;异常错误检测和补偿&lt;/h2&gt;

&lt;p&gt;还有一种牺牲一致性的方法就是通过一种错误补偿机制来进行&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;两阶段提交协议&lt;/h2&gt;

&lt;p&gt;第一阶段和第二阶段之间，数据也可不能是一致性的，也可能出现同样的情况导致异常。&lt;/p&gt;

&lt;h1 id=&quot;cap-3&quot;&gt;CAP的反对声音&lt;/h1&gt;

&lt;p&gt;1，2008年9月CTO of atomikos写了一篇文章“A CAP Solution (Proving Brewer Wrong)”，试图达到CAP都得的效果。&lt;/p&gt;

&lt;p&gt;这篇文章的核心内容就是放松Gilbert和Lynch证明中的限制：“系统必须同时达到CAP三个属性”，放松到“系统可以不同时达到CAP，而是分时达到”。&lt;/p&gt;

&lt;p&gt;他设计的系统如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1)程序如果能够读取数据库的话读取数据库，如果不能的话可以使用缓存代替。&lt;/li&gt;
  &lt;li&gt;(2)所有的读取操作使用版本号或者其他可以使用乐观锁的机制。&lt;/li&gt;
  &lt;li&gt;(3)客户端的所有更新操作全部放在队列中顺序处理。更新操作中要包括该更新的读取操作时的版本信息。&lt;/li&gt;
  &lt;li&gt;(4)当分区数量足够少的时候，可以处理队列中的更新操作。比较简单的方式是建立一个跨越所有分布式副本的事务，对每个副本进行更新操作（其他方式比如quorum等等也可以）。如果该更新的读取操作时的版本信息不是当前数据库中数据的版本信息，则将失败返回给客户端，否则返回成功。&lt;/li&gt;
  &lt;li&gt;(5)数据库操作结果（确认或者取消）通过异步的方式发送到客户端，可以通过邮件，消息队列或者其他异步方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;该系统符合CAP如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;符合C（高一致性）：读取的数据都是基于快照的，而且错误的更新操作不会执行。&lt;/li&gt;
  &lt;li&gt;符合A（高可用性）：读取和更新都会返回数据。&lt;/li&gt;
  &lt;li&gt;符合P（高分区容错性）：允许网络或者节点出错。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;该设计是符合BASE理论的。&lt;/p&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1) 读数据可能会不一致，因为之前的写还在排队&lt;/li&gt;
  &lt;li&gt;2) partition必须在有限的时间内解决&lt;/li&gt;
  &lt;li&gt;3) update操作必须在所有的节点上保持同样的顺序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2， 2011年11月Twitter的首席工程师Nathan Marz写了一篇文章，描述了他是如何试图打败CAP定理的： How to beat the CAP theorem&lt;/p&gt;

&lt;p&gt;本文中，作者还是非常尊重CAP定律，并表示不是要“击败”CAP，而是尝试对数据存储系统进行重新设计，以可控的复杂度来实现CAP。&lt;/p&gt;

&lt;p&gt;Marz认为一个分布式系统面临CAP难题的两大问题就是：在数据库中如何使用不断变化的数据，如何使用算法来更新数据库中的数据。Marz提出了几个由于云计算的兴起而改变的传统概念：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1) 数据不存在update，只存在append操作。这样就把对数据的处理由CRUD变为CR&lt;/li&gt;
  &lt;li&gt;2) 所有的数据的操作就只剩下Create和Read。把Read作为一个Query来处理，而一个Query就是一个对整个数据集执行一个函数操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这样的模型下，我们使用最终一致性的模型来处理数据，可以保证在P的情况下保证A。而所有的不一致性都可以通过重复进行Query去除掉。Martz认为就是因为要不断的更新数据库中的数据，再加上CAP，才导致那些即便使用最终一致性的系统也会变得无比复杂，需要用到向量时钟、读修复这种技术，而如果系统中不存在会改变的数据，所有的更新都作为创建新数据的方式存在，读数据转化为一次请求，这样就可以避免最终一致性的复杂性，转而拥抱CAP。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://duanple.blog.163.com/blog/static/7097176720114733655390/&quot;&gt;Brewer’s CAP Theorem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://www.cnblogs.com/mmjx/archive/2011/12/19/2290540.html&quot;&gt;CAP理论&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://blog.csdn.net/godfrey90/article/details/6754884&quot;&gt;NoSQL学习笔记(二)之CAP理论&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/05/30/note-about-brewers-cap-theorem.html</link>
      <guid>http://blog.javachen.com/2014/05/30/note-about-brewers-cap-theorem.html</guid>
      <pubDate>2014-05-30T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Scrapy抓取数据</title>
      <description>&lt;p&gt;Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;官方主页： &lt;a href=&quot;http://www.scrapy.org/&quot;&gt;http://www.scrapy.org/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;中文文档：&lt;a href=&quot;http://scrapy-chs.readthedocs.org/zh_CN/latest/index.html&quot;&gt;Scrapy 0.22 文档&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;GitHub项目主页：&lt;a href=&quot;https://github.com/scrapy/scrapy&quot;&gt;https://github.com/scrapy/scrapy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scrapy 使用了 Twisted 异步网络库来处理网络通讯。整体架构大致如下（注：图片来自互联网）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://blog.pluskid.org/wp-content/uploads/2009/08/scrapy_architecture.png&quot; alt=&quot;scrapy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scrapy主要包括了以下组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;引擎，用来处理整个系统的数据流处理，触发事务。&lt;/li&gt;
  &lt;li&gt;调度器，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。&lt;/li&gt;
  &lt;li&gt;下载器，用于下载网页内容，并将网页内容返回给蜘蛛。&lt;/li&gt;
  &lt;li&gt;蜘蛛，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。&lt;/li&gt;
  &lt;li&gt;项目管道，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。&lt;/li&gt;
  &lt;li&gt;下载器中间件，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。&lt;/li&gt;
  &lt;li&gt;蜘蛛中间件，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。&lt;/li&gt;
  &lt;li&gt;调度中间件，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装&lt;/h1&gt;

&lt;h2 id=&quot;python&quot;&gt;安装 python&lt;/h2&gt;

&lt;p&gt;Scrapy 目前最新版本为0.22.2，该版本需要 python 2.7，故需要先安装 python 2.7。这里我使用 centos 服务器来做测试，因为系统自带了 python ，需要先检查 python 版本。&lt;/p&gt;

&lt;p&gt;查看python版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python -V
Python 2.6.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;升级版本到2.7：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz
$ tar xf Python-2.7.6.tar.xz
$ cd Python-2.7.6
$ ./configure --prefix=/usr/local --enable-unicode=ucs4 --enable-shared LDFLAGS=&quot;-Wl,-rpath /usr/local/lib&quot;
$ make &amp;amp;&amp;amp; make altinstall
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;建立软连接，使系统默认的 python指向 python2.7&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mv /usr/bin/python /usr/bin/python2.6.6 
$ ln -s /usr/local/bin/python2.7 /usr/bin/python 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次查看python版本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python -V
Python 2.7.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;安装&lt;/h2&gt;

&lt;p&gt;这里使用 wget 的方式来安装 &lt;a href=&quot;http://pypi.python.org/pypi/setuptools&quot;&gt;setuptools&lt;/a&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://bootstrap.pypa.io/ez_setup.py -O - | python
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;zopeinterface&quot;&gt;安装 zope.interface&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ easy_install zope.interface
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;twisted&quot;&gt;安装 twisted&lt;/h2&gt;

&lt;p&gt;Scrapy 使用了 Twisted 异步网络库来处理网络通讯，故需要安装 twisted。&lt;/p&gt;

&lt;p&gt;安装 twisted 前，需要先安装 gcc：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install gcc -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，再通过 easy_install 安装 twisted：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ easy_install twisted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果出现下面错误：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ easy_install twisted
Searching for twisted
Reading https://pypi.python.org/simple/twisted/
Best match: Twisted 14.0.0
Downloading https://pypi.python.org/packages/source/T/Twisted/Twisted-14.0.0.tar.bz2#md5=9625c094e0a18da77faa4627b98c9815
Processing Twisted-14.0.0.tar.bz2
Writing /tmp/easy_install-kYHKjn/Twisted-14.0.0/setup.cfg
Running Twisted-14.0.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-kYHKjn/Twisted-14.0.0/egg-dist-tmp-vu1n6Y
twisted/runner/portmap.c:10:20: error: Python.h: No such file or directory
twisted/runner/portmap.c:14: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘*’ token
twisted/runner/portmap.c:31: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘*’ token
twisted/runner/portmap.c:45: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘PortmapMethods’
twisted/runner/portmap.c: In function ‘initportmap’:
twisted/runner/portmap.c:55: warning: implicit declaration of function ‘Py_InitModule’
twisted/runner/portmap.c:55: error: ‘PortmapMethods’ undeclared (first use in this function)
twisted/runner/portmap.c:55: error: (Each undeclared identifier is reported only once
twisted/runner/portmap.c:55: error: for each function it appears in.)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请安装 python-devel 然后再次运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install python-devel -y
$ easy_install twisted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果出现下面异常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;error: Not a recognized archive type: /tmp/easy_install-tVwC5O/Twisted-14.0.0.tar.bz2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请手动下载然后安装，下载地址在&lt;a href=&quot;http://pypi.python.org/pypi/Twisted&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://pypi.python.org/packages/source/T/Twisted/Twisted-14.0.0.tar.bz2#md5=9625c094e0a18da77faa4627b98c9815
$ tar -vxjf Twisted-14.0.0.tar.bz2
$ cd Twisted-14.0.0
$ python setup.py install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;pyopenssl&quot;&gt;安装 pyOpenSSL&lt;/h2&gt;

&lt;p&gt;先安装一些依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install libffi libffi-devel openssl-devel -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后，再通过 easy_install 安装 pyOpenSSL：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ easy_install pyOpenSSL
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;scrapy&quot;&gt;安装 Scrapy&lt;/h2&gt;

&lt;p&gt;先安装一些依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install libxml2 libxslt libxslt-devel -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后再来安装 Scrapy ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ easy_install scrapy
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;scrapy-1&quot;&gt;2. 使用 Scrapy&lt;/h1&gt;

&lt;p&gt;在安装成功之后，你可以了解一些 Scrapy 的基本概念和使用方法，并学习 Scrapy 项目的例子 dirbot 。&lt;/p&gt;

&lt;p&gt;Dirbot 项目位于 &lt;a href=&quot;https://github.com/scrapy/dirbot&quot;&gt;https://github.com/scrapy/dirbot&lt;/a&gt;，该项目包含一个 README 文件，它详细描述了项目的内容。如果你熟悉 git，你可以 checkout 它的源代码。或者你可以通过点击 Downloads 下载 tarball 或 zip 格式的文件。&lt;/p&gt;

&lt;p&gt;下面以该例子来描述如何使用 Scrapy 创建一个爬虫项目。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;新建工程&lt;/h2&gt;

&lt;p&gt;在抓取之前，你需要新建一个 Scrapy 工程。进入一个你想用来保存代码的目录，然后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scrapy startproject tutorial
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个命令会在当前目录下创建一个新目录 tutorial，它的结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── scrapy.cfg
└── tutorial
    ├── __init__.py
    ├── items.py
    ├── pipelines.py
    ├── settings.py
    └── spiders
        └── __init__.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些文件主要是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;scrapy.cfg: 项目配置文件&lt;/li&gt;
  &lt;li&gt;tutorial/: 项目python模块, 呆会代码将从这里导入&lt;/li&gt;
  &lt;li&gt;tutorial/items.py: 项目items文件&lt;/li&gt;
  &lt;li&gt;tutorial/pipelines.py: 项目管道文件&lt;/li&gt;
  &lt;li&gt;tutorial/settings.py: 项目配置文件&lt;/li&gt;
  &lt;li&gt;tutorial/spiders: 放置spider的目录&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;item&quot;&gt;定义Item&lt;/h2&gt;

&lt;p&gt;Items是将要装载抓取的数据的容器，它工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误。&lt;/p&gt;

&lt;p&gt;它通过创建一个 &lt;code&gt;scrapy.item.Item&lt;/code&gt; 类来声明，定义它的属性为 &lt;code&gt;scrpy.item.Field&lt;/code&gt; 对象，就像是一个对象关系映射(ORM). &lt;br /&gt;
我们通过将需要的item模型化，来控制从 dmoz.org 获得的站点数据，比如我们要获得站点的名字，url 和网站描述，我们定义这三种属性的域。要做到这点，我们编辑在 tutorial 目录下的 items.py 文件，我们的 Item 类将会是这样&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from scrapy.item import Item, Field 
class DmozItem(Item):
    title = Field()
    link = Field()
    desc = Field()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;刚开始看起来可能会有些困惑，但是定义这些 item 能让你用其他 Scrapy 组件的时候知道你的 items 到底是什么。&lt;/p&gt;

&lt;h2 id=&quot;spider&quot;&gt;编写爬虫(Spider)&lt;/h2&gt;

&lt;p&gt;Spider 是用户编写的类，用于从一个域（或域组）中抓取信息。们定义了用于下载的URL的初步列表，如何跟踪链接，以及如何来解析这些网页的内容用于提取items。&lt;/p&gt;

&lt;p&gt;要建立一个 Spider，你可以为 &lt;code&gt;scrapy.spider.BaseSpider&lt;/code&gt; 创建一个子类，并确定三个主要的、强制的属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;name&lt;/code&gt;：爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;start_urls&lt;/code&gt;：爬虫开始爬的一个 URL 列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些 URLS 开始。其他子 URL 将会从这些起始 URL 中继承性生成。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;parse()&lt;/code&gt;：爬虫的方法，调用时候传入从每一个 URL 传回的 Response 对象作为参数，response 将会是 parse 方法的唯一的一个参数,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个方法负责解析返回的数据、匹配抓取的数据(解析为 item )并跟踪更多的 URL。&lt;/p&gt;

&lt;p&gt;在 tutorial/spiders 目录下创建 DmozSpider.py&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from scrapy.spider import BaseSpider

class DmozSpider(BaseSpider):
    name = &quot;dmoz&quot;
    allowed_domains = [&quot;dmoz.org&quot;]
    start_urls = [
        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;,
        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;
    ]

    def parse(self, response):
        filename = response.url.split(&quot;/&quot;)[-2]
        open(filename, &#39;wb&#39;).write(response.body)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;运行项目&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scrapy crawl dmoz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令从 dmoz.org 域启动爬虫，第三个参数为 DmozSpider.py 中的 name 属性值。&lt;/p&gt;

&lt;h2 id=&quot;xpath&quot;&gt;xpath选择器&lt;/h2&gt;

&lt;p&gt;Scrapy 使用一种叫做 XPath selectors 的机制，它基于 XPath 表达式。如果你想了解更多selectors和其他机制你可以查阅&lt;a href=&quot;http://doc.scrapy.org/topics/selectors.html#topics-selectors&quot;&gt;资料&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;这是一些XPath表达式的例子和他们的含义：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;/html/head/title&lt;/code&gt;: 选择HTML文档 &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; 元素下面的 &lt;code&gt;&amp;lt;title&amp;gt;&lt;/code&gt; 标签。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/html/head/title/text()&lt;/code&gt;: 选择前面提到的&lt;code&gt; &amp;lt;title&amp;gt;&lt;/code&gt; 元素下面的文本内容&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;//td&lt;/code&gt;: 选择所有 &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt; 元素&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;//div[@class=&quot;mine&quot;]&lt;/code&gt;: 选择所有包含 &lt;code&gt;class=&quot;mine&quot;&lt;/code&gt; 属性的div 标签元素&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这只是几个使用 XPath 的简单例子，但是实际上 XPath 非常强大。如果你想了解更多 XPATH 的内容，我们向你推荐这个 &lt;a href=&quot;http://www.w3schools.com/XPath/default.asp&quot;&gt;XPath 教程&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;为了方便使用 XPaths，Scrapy 提供 Selector 类， 有三种方法&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;xpath()&lt;/code&gt;：返回selectors列表, 每一个select表示一个xpath参数表达式选择的节点.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;extract()&lt;/code&gt;：返回一个unicode字符串，该字符串为XPath选择器返回的数据&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;re()&lt;/code&gt;： 返回unicode字符串列表，字符串作为参数由正则表达式提取出来&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;css()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;提取数据&lt;/h2&gt;

&lt;p&gt;我们可以通过如下命令选择每个在网站中的 &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; 元素:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;sel.xpath(&#39;//ul/li&#39;) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后是网站描述:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;sel.xpath(&#39;//ul/li/text()&#39;).extract()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;网站标题:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;sel.xpath(&#39;//ul/li/a/text()&#39;).extract()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;网站链接:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;sel.xpath(&#39;//ul/li/a/@href&#39;).extract()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如前所述，每个 &lt;code&gt;xpath()&lt;/code&gt; 调用返回一个 selectors 列表，所以我们可以结合 &lt;code&gt;xpath()&lt;/code&gt; 去挖掘更深的节点。我们将会用到这些特性，所以:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;sites = sel.xpath(&#39;//ul/li&#39;)
for site in sites:
    title = site.xpath(&#39;a/text()&#39;).extract()
    link = site.xpath(&#39;a/@href&#39;).extract()
    desc = site.xpath(&#39;text()&#39;).extract()
    print title, link, desc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;item-1&quot;&gt;使用Item&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;scrapy.item.Item&lt;/code&gt; 的调用接口类似于 python 的 dict ，Item 包含多个 &lt;code&gt;scrapy.item.Field&lt;/code&gt;。这跟 django 的 Model 与&lt;/p&gt;

&lt;p&gt;Item 通常是在 Spider 的 parse 方法里使用，它用来保存解析到的数据。&lt;/p&gt;

&lt;p&gt;最后修改爬虫类，使用 Item 来保存数据，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from scrapy.spider import Spider
from scrapy.selector import Selector

from dirbot.items import Website


class DmozSpider(Spider):
    name = &quot;dmoz&quot;
    allowed_domains = [&quot;dmoz.org&quot;]
    start_urls = [
        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;,
        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;,
    ]

    def parse(self, response):
        &quot;&quot;&quot;
        The lines below is a spider contract. For more info see:
        http://doc.scrapy.org/en/latest/topics/contracts.html

        @url http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/
        @scrapes name
        &quot;&quot;&quot;
        sel = Selector(response)
        sites = sel.xpath(&#39;//ul[@class=&quot;directory-url&quot;]/li&#39;)
        items = []

        for site in sites:
            item = Website()
            item[&#39;name&#39;] = site.xpath(&#39;a/text()&#39;).extract()
            item[&#39;url&#39;] = site.xpath(&#39;a/@href&#39;).extract()
            item[&#39;description&#39;] = site.xpath(&#39;text()&#39;).re(&#39;-\s([^\n]*?)\\n&#39;)
            items.append(item)

        return items
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，可以再次运行该项目查看运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scrapy crawl dmoz
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;item-pipeline&quot;&gt;使用Item Pipeline&lt;/h2&gt;

&lt;p&gt;在 settings.py 中设置 &lt;code&gt;ITEM_PIPELINES&lt;/code&gt;，其默认为&lt;code&gt;[]&lt;/code&gt;，与 django 的 &lt;code&gt;MIDDLEWARE_CLASSES&lt;/code&gt; 等相似。&lt;br /&gt;
从 Spider 的 parse 返回的 Item 数据将依次被 &lt;code&gt;ITEM_PIPELINES&lt;/code&gt; 列表中的 Pipeline 类处理。&lt;/p&gt;

&lt;p&gt;一个 Item Pipeline 类必须实现以下方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;process_item(item, spider)&lt;/code&gt; 为每个 item pipeline 组件调用，并且需要返回一个 &lt;code&gt;scrapy.item.Item&lt;/code&gt; 实例对象或者抛出一个 &lt;code&gt;scrapy.exceptions.DropItem&lt;/code&gt; 异常。当抛出异常后该 item 将不会被之后的 pipeline 处理。参数:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;item (Item object)&lt;/code&gt; – 由 parse 方法返回的 Item 对象&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;spider (BaseSpider object)&lt;/code&gt; – 抓取到这个 Item 对象对应的爬虫对象&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;也可额外的实现以下两个方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;open_spider(spider)&lt;/code&gt; 当爬虫打开之后被调用。参数: &lt;code&gt;spider (BaseSpider object)&lt;/code&gt; – 已经运行的爬虫&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;close_spider(spider)&lt;/code&gt; 当爬虫关闭之后被调用。参数: &lt;code&gt;spider (BaseSpider object)&lt;/code&gt; – 已经关闭的爬虫&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;保存抓取的数据&lt;/h2&gt;

&lt;p&gt;保存信息的最简单的方法是通过 &lt;a href=&quot;http://doc.scrapy.org/en/0.22/topics/feed-exports.html#topics-feed-exports&quot;&gt;Feed exports&lt;/a&gt;，命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scrapy crawl dmoz -o items.json -t json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了 json 格式之外，还支持 JSON lines、CSV、XML格式，你也可以通过接口扩展一些格式。&lt;/p&gt;

&lt;p&gt;对于小项目用这种方法也足够了。如果是比较复杂的数据的话可能就需要编写一个 Item Pipeline 进行处理了。&lt;/p&gt;

&lt;p&gt;所有抓取的 items 将以 JSON 格式被保存在新生成的 items.json 文件中&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;上面描述了如何创建一个爬虫项目的过程，你可以参照上面过程联系一遍。作为学习的例子，你还可以参考这篇文章：&lt;a href=&quot;http://wsky.org/archives/191.html&quot;&gt;scrapy 中文教程（爬cnbeta实例）&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;这篇文章中的爬虫类代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.selector import Selector
 
from cnbeta.items import CnbetaItem
 
class CBSpider(CrawlSpider):
    name = &#39;cnbeta&#39;
    allowed_domains = [&#39;cnbeta.com&#39;]
    start_urls = [&#39;http://www.cnbeta.com&#39;]
 
    rules = (
        Rule(SgmlLinkExtractor(allow=(&#39;/articles/.*\.htm&#39;, )),
             callback=&#39;parse_page&#39;, follow=True),
    )
 
    def parse_page(self, response):
        item = CnbetaItem()
        sel = Selector(response)
        item[&#39;title&#39;] = sel.xpath(&#39;//title/text()&#39;).extract()
        item[&#39;url&#39;] = response.url
        return item
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要说明的是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;该爬虫类继承的是 &lt;code&gt;CrawlSpider&lt;/code&gt; 类，并且定义规则，rules指定了含有 &lt;code&gt;/articles/.*\.htm&lt;/code&gt; 的链接都会被匹配。&lt;/li&gt;
  &lt;li&gt;该类并没有实现parse方法，并且规则中定义了回调函数 &lt;code&gt;parse_page&lt;/code&gt;，你可以参考更多资料了解 CrawlSpider 的用法&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-7&quot;&gt;3. 学习资料&lt;/h1&gt;

&lt;p&gt;接触 Scrapy，是因为想爬取一些知乎的数据，最开始的时候搜索了一些相关的资料和别人的实现方式。&lt;/p&gt;

&lt;p&gt;Github 上已经有人或多或少的实现了对知乎数据的爬取，我搜索到的有以下几个仓库：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/KeithYue/Zhihu_Spider&quot;&gt;https://github.com/KeithYue/Zhihu_Spider&lt;/a&gt; 实现先通过用户名和密码登陆再爬取数据，代码见 &lt;a href=&quot;https://github.com/KeithYue/Zhihu_Spider/blob/master/zhihu/zhihu/spiders/zhihu_spider.py&quot;&gt;zhihu_spider.py&lt;/a&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/immzz/zhihu-scrapy&quot;&gt;https://github.com/immzz/zhihu-scrapy&lt;/a&gt; 使用 selenium 下载和执行 javascript 代码。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tangerinewhite32/zhihu-stat-py&quot;&gt;https://github.com/tangerinewhite32/zhihu-stat-py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Zcc/zhihu&quot;&gt;https://github.com/Zcc/zhihu&lt;/a&gt; 主要是爬指定话题的topanswers，还有用户个人资料，添加了登录代码。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pelick/VerticleSearchEngine&quot;&gt;https://github.com/pelick/VerticleSearchEngine&lt;/a&gt; 基于爬取的学术资源，提供搜索、推荐、可视化、分享四块。使用了 Scrapy、MongoDB、Apache Lucene/Solr、Apache Tika等技术。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/geekan/scrapy-examples&quot;&gt;https://github.com/geekan/scrapy-examples&lt;/a&gt; scrapy的一些例子，包括获取豆瓣数据、linkedin、腾讯招聘数据等例子。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/owengbs/deeplearning&quot;&gt;https://github.com/owengbs/deeplearning&lt;/a&gt; 实现分页获取话题。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gnemoug/distribute_crawler&quot;&gt;https://github.com/gnemoug/distribute_crawler&lt;/a&gt; 使用scrapy、redis、mongodb、graphite实现的一个分布式网络爬虫,底层存储mongodb集群,分布式使用redis实现,爬虫状态显示使用graphite实现&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/weizetao/spider-roach&quot;&gt;https://github.com/weizetao/spider-roach&lt;/a&gt; 一个分布式定向抓取集群的简单实现。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/scrapinghub/portia&quot;&gt;https://github.com/scrapinghub/portia&lt;/a&gt; 这是一个可视化爬虫，基于Scrapy。它提供了可视化操作的Web页面，你只需点击页面上你要抽取的数据就行&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/binux/pyspider&quot;&gt;https://github.com/binux/pyspider&lt;/a&gt; 你如果不喜欢 Scrapy，可以试试 pyspider ，他让你在 WEB 界面编写调试脚本，监控执行状态，查看历史和结果 ，你可以在线试下 demo：&lt;a href=&quot;http://demo.pyspider.org/&quot;&gt;Dashboard - pyspider&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其他资料：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.52ml.net/tags/Scrapy&quot;&gt;http://www.52ml.net/tags/Scrapy&lt;/a&gt; 收集了很多关于 Scrapy 的文章，&lt;strong&gt;推荐阅读&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://zihaolucky.github.io/using-python-to-build-zhihu-cralwer/&quot;&gt;用Python Requests抓取知乎用户信息&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.it165.net/pro/html/201405/13112.html&quot;&gt;使用scrapy框架爬取自己的博文&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.windwild.net/2013/03/scrapy002/&quot;&gt;Scrapy 深入一点点&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kankanews.com/ICkengine/archives/94817.shtml&quot;&gt;使用python，scrapy写（定制）爬虫的经验，资料，杂。&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.pluskid.org/?p=366&amp;amp;cpage=1&quot;&gt;Scrapy 轻松定制网络爬虫&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://my.oschina.net/chengye/blog/124162&quot;&gt;在scrapy中怎么让Spider自动去抓取豆瓣小组页面&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;scrapy 和 javascript 交互例子：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.xuebuyuan.com/2017949.html&quot;&gt;用scrapy框架爬取js交互式表格数据&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wsky.org/archives/211.html&quot;&gt;scrapy + selenium 解析javascript 实例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有一些待整理的知识点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;如何先登陆再爬数据&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;如何使用规则做过滤&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;如何递归爬取数据&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;scrapy的参数设置和优化&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;如何实现分布式爬取&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-8&quot;&gt;4. 总结&lt;/h1&gt;

&lt;p&gt;以上就是最近几天学习 Scrapy 的一个笔记和知识整理，参考了一些网上的文章才写成此文，对此表示感谢，也希望这篇文章能够对你有所帮助。如果你有什么想法，欢迎留言；如果喜欢此文，请帮忙分享，谢谢!&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/05/24/using-scrapy-to-cralw-data.html</link>
      <guid>http://blog.javachen.com/2014/05/24/using-scrapy-to-cralw-data.html</guid>
      <pubDate>2014-05-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Nutch介绍及使用</title>
      <description>&lt;h1 id=&quot;nutch&quot;&gt;1. Nutch介绍&lt;/h1&gt;

&lt;p&gt;Nutch是一个开源的网络爬虫项目，更具体些是一个爬虫软件，可以直接用于抓取网页内容。&lt;/p&gt;

&lt;p&gt;现在Nutch分为两个版本，1.x和2.x。1.x最新版本为1.7，2.x最新版本为2.2.1。两个版本的主要区别在于底层的存储不同。&lt;/p&gt;

&lt;p&gt;1.x版本是基于Hadoop架构的，底层存储使用的是HDFS，而2.x通过使用Apache Gora，使得Nutch可以访问HBase、Accumulo、Cassandra、MySQL、DataFileAvroStore、AvroStore等NoSQL。&lt;/p&gt;

&lt;h1 id=&quot;nutch-1&quot;&gt;2. 编译Nutch&lt;/h1&gt;

&lt;p&gt;Nutch1.x从1.7版本开始不再提供完整的部署文件，只提供源代码文件及相关的build.xml文件,这就要求用户自己编译Nutch，而整个Nutch2.x版本都不提供编译完成的文件，所以想要学习Nutch2.2.1的功能，就必须自己手动编译文件。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;2.1 下载解压&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://archive.apache.org/dist/nutch/2.2.1/apache-nutch-2.2.1-src.tar.gz
$ tar zxf apache-nutch-2.2.1-src.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2.2 编译&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd apache-nutch-2.2.1
$ ant
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有可能你会得到如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Trying to override old definition of task javac
  [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.

ivy-probe-antlib:

ivy-download:
  [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;下载sonar-ant-task-2.1.jar，将其拷贝到apache-nutch-2.2.1目录下面&lt;/li&gt;
  &lt;li&gt;修改build.xml，引入上面添加的jar包：&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;!-- Define the Sonar task if this hasn&#39;t been done in a common script --&amp;gt;
&amp;lt;taskdef uri=&quot;antlib:org.sonar.ant&quot; resource=&quot;org/sonar/ant/antlib.xml&quot;&amp;gt;
	&amp;lt;classpath path=&quot;${ant.library.dir}&quot; /&amp;gt;
	&amp;lt;classpath path=&quot;${mysql.library.dir}&quot; /&amp;gt;
	&amp;lt;classpath&amp;gt;&amp;lt;fileset dir=&quot;.&quot; includes=&quot;sonar*.jar&quot; /&amp;gt;&amp;lt;/classpath&amp;gt;
&amp;lt;/taskdef&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nutch使用ivy进行构建，故编译需要很长时间，如果编译时间过长，建议修改maven仓库地址，修改方法：&lt;/p&gt;

&lt;p&gt;通过用&lt;code&gt;http://mirrors.ibiblio.org/maven2/&lt;/code&gt;替换&lt;code&gt;ivy/下ivysettings.xml&lt;/code&gt;中的&lt;code&gt;http://repo1.maven.org/maven2/&lt;/code&gt;来解决。代码位置为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property name=&quot;repo.maven.org&quot; value=&quot;http://repo1.maven.org/maven2/&quot; override=&quot;false&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译之后的目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  apache-nutch-2.2.1  tree -L 1
.
├── CHANGES.txt
├── LICENSE.txt
├── NOTICE.txt
├── README.txt
├── build
├── build.xml
├── conf
├── default.properties
├── docs
├── ivy
├── lib
├── runtime
├── sonar-ant-task-2.1.jar
└── src

7 directories, 7 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到编译之后多了两个目录：build和runtime&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 修改配置文件&lt;/h1&gt;

&lt;p&gt;由于Nutch2.x版本存储采用Gora访问Cassandra、HBase、Accumulo、Avro等，需要在该文件中制定Gora属性，比如指定默认的存储方式&lt;code&gt;gora.datastore.default= org.apache.gora.hbase.store.HBaseStore&lt;/code&gt;，该属性的值可以在nutch-default.xml中查找&lt;code&gt;storage.data.store.class&lt;/code&gt;属性取得，在不做gora.properties文件修改的情况下，存储类为&lt;code&gt;org.apache.gora.memory.store.MemStore&lt;/code&gt;，该类将数据存储在内存中，仅用于测试目的。&lt;/p&gt;

&lt;p&gt;这里，将其存储方式改为HBase,请参考 &lt;a href=&quot;http://wiki.apache.org/nutch/Nutch2Tutorial&quot;&gt;http://wiki.apache.org/nutch/Nutch2Tutorial&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;修改 &lt;code&gt;conf/nutch-site.xml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;storage.data.store.class&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;org.apache.gora.hbase.store.HBaseStore&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;Default class for storing data&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 &lt;code&gt;ivy/ivy.xml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;!-- Uncomment this to use HBase as Gora backend. --&amp;gt;
&amp;lt;dependency org=&quot;org.apache.gora&quot; name=&quot;gora-hbase&quot; rev=&quot;0.3&quot; conf=&quot;*-&amp;gt;default&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 &lt;code&gt;conf/gora.properties&lt;/code&gt;，确保HBaseStore被设置为默认的存储，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gora.datastore.default=org.apache.gora.hbase.store.HBaseStore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为这里用到了HBase，故还需要一个HBase环境，你可以使用Standalone模式搭建一个HBase环境，请参考 &lt;a href=&quot;http://hbase.apache.org/book/quickstart.html&quot;&gt;HBase Quick Start&lt;/a&gt;。需要说明的时，&lt;strong&gt;目前HBase的版本要求为 hbase-0.90.4。&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;solr&quot;&gt;4. 集成Solr&lt;/h1&gt;

&lt;p&gt;由于建索引的时候需要使用Solr，因此我们需要安装并启动一个Solr服务器。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4.1 下载，解压&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://mirrors.cnnic.cn/apache/lucene/solr/4.8.0/solr-4.8.0.tgz 
$ tar -zxf solr-4.8.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;solr-1&quot;&gt;4.2 运行Solr&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd solr-4.8.0/example
$ java -jar start.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证是否启动成功&lt;/p&gt;

&lt;p&gt;用浏览器打开 &lt;a href=&quot;http://localhost:8983/solr/admin/&quot;&gt;http://localhost:8983/solr/admin/&lt;/a&gt;，如果能看到页面，说明启动成功。&lt;/p&gt;

&lt;h1 id=&quot;solr-2&quot;&gt;4.3 修改Solr配置文件&lt;/h1&gt;

&lt;p&gt;将&lt;code&gt;apache-nutch-2.2.1/conf/schema-solr4.xml&lt;/code&gt;拷贝到&lt;code&gt;solr-4.8.0/solr/collection1/conf/schema.xml&lt;/code&gt;，并在&lt;code&gt;&amp;lt;fields&amp;gt;...&amp;lt;/fields&amp;gt;&lt;/code&gt;最后添加一行:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;field name=&quot;_version_&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;false&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启Solr，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Ctrl+C to stop Solr
$ java -jar start.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5. 抓取数据&lt;/h1&gt;

&lt;p&gt;编译后的脚本在 runtime/local/bin 目录下，可以运行命令查看使用方法：&lt;/p&gt;

&lt;p&gt;crawl命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd runtime/local/bin 
$ ./crawl 
Missing seedDir : crawl &amp;lt;seedDir&amp;gt; &amp;lt;crawlID&amp;gt; &amp;lt;solrURL&amp;gt; &amp;lt;numberOfRounds&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nutch命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./nutch 
Usage: nutch COMMAND
where COMMAND is one of:
 inject		inject new urls into the database
 hostinject     creates or updates an existing host table from a text file
 generate 	generate new batches to fetch from crawl db
 fetch 		fetch URLs marked during generate
 parse 		parse URLs marked during fetch
 updatedb 	update web table after parsing
 updatehostdb   update host table after parsing
 readdb 	read/dump records from page database
 readhostdb     display entries from the hostDB
 elasticindex   run the elasticsearch indexer
 solrindex 	run the solr indexer on parsed batches
 solrdedup 	remove duplicates from solr
 parsechecker   check the parser for a given url
 indexchecker   check the indexing filters for a given url
 plugin 	load a plugin and run one of its classes main()
 nutchserver    run a (local) Nutch server on a user defined port
 junit         	runs the given JUnit test
 or
 CLASSNAME 	run the class named CLASSNAME
Most commands print help when invoked w/o parameters.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来可以抓取网页了。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;6. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/skywalker_only/article/category/1842591&quot;&gt;Nutch-2.2.1学习&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cn.soulmachine.me/blog/20140201/&quot;&gt;Nutch 快速入门(Nutch 2.2.1)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/05/20/nutch-intro.html</link>
      <guid>http://blog.javachen.com/2014/05/20/nutch-intro.html</guid>
      <pubDate>2014-05-20T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Python开发框架Flask</title>
      <description>&lt;h1 id=&quot;flask&quot;&gt;1. Flask介绍&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://flask.pocoo.org/&quot;&gt;Flask&lt;/a&gt; 是一个基于Python的微型的web开发框架。虽然Flask是微框架，不过我们并不需要像别的微框架建议的那样把所有代码都写到单文件中。毕竟微框架真正的含义是简单和短小。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://flask.pocoo.org/static/logo.png&quot; alt=&quot;flask-logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于Flask值得知道的一些事：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flask由Armin Ronacher于2010年创建。&lt;/li&gt;
  &lt;li&gt;Flask的灵感来自Sinatra。（Sinatra是一个极力避免小题大作的创建web应用的Ruby框架。）&lt;/li&gt;
  &lt;li&gt;Flask 依赖两个外部库： &lt;a href=&quot;http://jinja.pocoo.org/2/&quot;&gt;Jinja2&lt;/a&gt; 模板引擎和 &lt;a href=&quot;http://werkzeug.pocoo.org/&quot;&gt;Werkzeug&lt;/a&gt; WSGI 工具集。&lt;/li&gt;
  &lt;li&gt;Flask遵循“约定优于配置”以及合理的默认值原则。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;默认情况下，Flask 不包含数据库抽象层、表单验证或是任何其它现有库可以胜任的东西。作为替代的是，Flask 支持扩展来给应用添加这些功能，如同是在 Flask 自身中实现。众多的扩展提供了数据库集成、表单验证、上传处理、多种开放认证技术等功能。&lt;/p&gt;

&lt;p&gt;Flask 数目众多的配置选项在初始状况下都有一个明智的默认值，并遵循一些惯例。 例如，按照惯例，模板和静态文件存储在应用的 Python 源代码树下的子目录中，名称分别为 templates 和 static 。虽然可以更改这个配置，但你通常不必这么做， 尤其是在刚接触 Flask 的时候。&lt;/p&gt;

&lt;h1 id=&quot;flask-1&quot;&gt;2. Flask安装&lt;/h1&gt;

&lt;p&gt;你首先需要 Python 2.6 或更高的版本，所以请确认有一个最新的 Python 2.x 安装。&lt;/p&gt;

&lt;h2 id=&quot;virtualenv&quot;&gt;virtualenv&lt;/h2&gt;

&lt;p&gt;virtualenv 允许多个版本的 Python 同时存在，对应不同的项目。 它实际上并没有安装独立的 Python 副本，但是它确实提供了一种巧妙的方式来让各项目环境保持独立。&lt;/p&gt;

&lt;p&gt;如果你在 Mac OS X 或 Linux下，下面两条命令可能会适用:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo easy_install virtualenv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或更好的:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install virtualenv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述的命令会在你的系统中安装 virtualenv。它甚至可能会存在于包管理器中，如果你使用 Ubuntu ，可以尝试:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install python-virtualenv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在你只需要键入以下的命令来激活 virtualenv 中的 Flask:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip install Flask
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;全局安装&lt;/h2&gt;

&lt;p&gt;这样也是可以的，只需要以 root 权限运行 pip:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install Flask
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;flask-2&quot;&gt;3. Flask入门&lt;/h1&gt;

&lt;p&gt;一个最小的 Flask 应用看起来是这样:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from flask import Flask
app = Flask(__name__)

@app.route(&#39;/&#39;)
def hello_world():
    return &#39;Hello World!&#39;

if __name__ == &#39;__main__&#39;:
    app.run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把它保存为 hello.py（或是类似的），然后用 Python 解释器来运行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python hello.py
 * Running on http://127.0.0.1:5000/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在访问&lt;code&gt;http://127.0.0.1:5000/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;我们来解释一下上面的代码吧：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一行导入了Flask类，以便创建一个Flask应用的实例。&lt;/li&gt;
  &lt;li&gt;接下来一行我们创建了一个Flask类的实例。这是一个WSGI应用实例。WSGI是”Web服务器网关接口”Web Service Gateway Interface）的缩写，同时也是架设web项目的Python标准。这一行要告诉Flask到哪里去找应用所需的静态资源和模板。在我们的例子中，我们传递了name，让Flask在当前模块内定位资源。&lt;/li&gt;
  &lt;li&gt;接着我们定义了一些关于&lt;code&gt;/&lt;/code&gt;的路由。第一个路由是为根路径&lt;code&gt;/&lt;/code&gt;准备的，第二个则对应于类似&lt;code&gt;/shekhar&lt;/code&gt;、&lt;code&gt;/abc&lt;/code&gt;之类的路径。对于&lt;code&gt;/&lt;/code&gt;路由，我们将初始的name设定为Guest。如果用户访问 &lt;code&gt;http://localhost:5000/&lt;/code&gt; ，那么他会看到&lt;code&gt;Hello Guest&lt;/code&gt;。如果用户访问 &lt;code&gt;http://localhost:5000/shekhar&lt;/code&gt; ，那么他会看到 &lt;code&gt;Hello shekhar&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;最后我们用 run() 函数来让应用运行在本地服务器上。 其中 &lt;code&gt;if __name__ == &#39;__main__&#39;&lt;/code&gt;: 确保服务器只会在该脚本被 Python 解释器直接执行的时候才会运行，而不是作为模块导入的时候。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果你禁用了 debug 或信任你所在网络的用户，你可以简单修改调用 run() 的方法使你的服务器公开可用，如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.run(host=&#39;0.0.0.0&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这会让操作系统监听所有公开的IP。&lt;/p&gt;

&lt;p&gt;有两种途径来启用调试模式。一种是在应用对象上设置:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.debug = True
app.run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另一种是作为 run 方法的一个参数传入:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.run(debug=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;4. 总结&lt;/h1&gt;

&lt;p&gt;本文简单介绍了Flask框架的安装和使用，如果你想要深入研究 Flask 的话，可以查看 &lt;a href=&quot;http://www.pythondoc.com/flask/api.html#api&quot;&gt;API&lt;/a&gt;。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/05/11/flask-intro.html</link>
      <guid>http://blog.javachen.com/2014/05/11/flask-intro.html</guid>
      <pubDate>2014-05-11T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Bower介绍</title>
      <description>&lt;h1 id=&quot;bower&quot;&gt;1. bower介绍&lt;/h1&gt;

&lt;p&gt;Bower 是用于 web 前端开发的包管理器。对于前端包管理方面的问题，它提供了一套通用、客观的解决方案。它通过一个 API 暴露包之间的依赖模型，这样更利于使用更合适的构建工具。bower 没有系统级的依赖，在不同 app 之间也不互相依赖，依赖树是扁平的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://sfault-image.b0.upaiyun.com/bc/b4/bcb41307d0c6b3f16013c8abf865fe85&quot; alt=&quot;bower-logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bower 运行在 Git 之上，它将所有包都视作一个黑盒子。任何类型的资源文件都可以打包为一个模块，并且可以使用任何规范（例如：AMD、CommonJS 等）。&lt;/p&gt;

&lt;p&gt;包管理工具一般有以下的功能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;注册机制&lt;/code&gt;：每个包需要确定一个唯一的 ID 使得搜索和下载的时候能够正确匹配，所以包管理工具需要维护注册信息，可以依赖其他平台。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;文件存储&lt;/code&gt;：确定文件存放的位置，下载的时候可以找到，当然这个地址在网络上是可访问的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;上传下载&lt;/code&gt;：这是工具的主要功能，能提高包使用的便利性。比如想用 jquery 只需要 install 一下就可以了，不用到处找下载。上传并不是必备的，根据文件存储的位置而定，但需要有一定的机制保障。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;依赖分析&lt;/code&gt;：这也是包管理工具主要解决的问题之一，既然包之间是有联系的，那么下载的时候就需要处理他们之间的依赖。下载一个包的时候也需要下载依赖的包。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;功能介绍，摘自文章：&lt;a href=&quot;http://chuo.me/2013/02/twitter-bower.html&quot;&gt;http://chuo.me/2013/02/twitter-bower.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;2. 安装&lt;/h1&gt;

&lt;p&gt;bower 插件是通过 npm, node.js 包管理器安装和管理的。&lt;/p&gt;

&lt;p&gt;npm 是 node 程序包管理器。它是捆绑在 node.js 的安装程序上的，所以一旦你已经安装了 node，npm 也就安装好了。&lt;/p&gt;

&lt;p&gt;在 mac上 安装 node.js 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install nodejs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 npm 安装 bower 到全局环境中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install -g bower
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;3. 使用&lt;/h1&gt;

&lt;p&gt;安装之后，可以通过 &lt;code&gt;bower help&lt;/code&gt; 命令可以获取更多帮助信息。了解了这些信息就可以开始了。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;安装包及其依赖的包&lt;/h2&gt;

&lt;p&gt;Bower 提供了几种方式用于安装包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Using the dependencies listed in the current directory&#39;s bower.json
bower install
# Using a local or remote package
bower install &amp;lt;package&amp;gt;
# Using a specific Git-tagged version from a remote package
bower install &amp;lt;package&amp;gt;#&amp;lt;version&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中，&lt;code&gt;&amp;lt;package&amp;gt;&lt;/code&gt; 可以是以下列出的一种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;注册到 bower 中的一个包名, 例如：jquery。&lt;/li&gt;
  &lt;li&gt;一个 Git 仓库地址,例如： git://github.com/someone/some-package.git&lt;/li&gt;
  &lt;li&gt;一个本地的 Git 目录&lt;/li&gt;
  &lt;li&gt;github 的别名,例如：someone/some-package (defaults to GitHub)。&lt;/li&gt;
  &lt;li&gt;一个文件 url，包括 zip 和t ar.gz 文件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举例来看一下来如何使用 bower 安装 jQuery，在你想要安装该包的地方创建一个新的文件夹，键入如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower install jquery
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述命令完成以后，你会在你刚才创建的目录下看到一个 &lt;code&gt;bower_components&lt;/code&gt; 的文件夹，其中目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tree bower_components/
bower_components
└── jquery
    ├── MIT-LICENSE.txt
    ├── bower.json
    ├── dist
    │   ├── jquery.js
    │   ├── jquery.min.js
    │   └── jquery.min.map
    └── src
        ├── ajax
        │   ├── jsonp.js
        │   ├── load.js
        │   ├── parseJSON.js
        │   ├── parseXML.js
        │   ├── script.js
        │   ├── var
        │   │   ├── nonce.js
        │   │   └── rquery.js
        │   └── xhr.js
        ├── ajax.js
        ├── attributes
        │   ├── attr.js
        │   ├── classes.js
        │   ├── prop.js
        │   ├── support.js
        │   └── val.js
        ├── attributes.js
        ├── callbacks.js
        ├── core
        │   ├── access.js
        │   ├── init.js
        │   ├── parseHTML.js
        │   ├── ready.js
        │   └── var
        │       └── rsingleTag.js
        ├── core.js
        ├── css
        │   ├── addGetHookIf.js
        │   ├── curCSS.js
        │   ├── defaultDisplay.js
        │   ├── hiddenVisibleSelectors.js
        │   ├── support.js
        │   ├── swap.js
        │   └── var
        │       ├── cssExpand.js
        │       ├── getStyles.js
        │       ├── isHidden.js
        │       ├── rmargin.js
        │       └── rnumnonpx.js
        ├── css.js
        ├── data
        │   ├── Data.js
        │   ├── accepts.js
        │   └── var
        │       ├── data_priv.js
        │       └── data_user.js
        ├── data.js
        ├── deferred.js
        ├── deprecated.js
        ├── dimensions.js
        ├── effects
        │   ├── Tween.js
        │   └── animatedSelector.js
        ├── effects.js
        ├── event
        │   ├── alias.js
        │   └── support.js
        ├── event.js
        ├── exports
        │   ├── amd.js
        │   └── global.js
        ├── intro.js
        ├── jquery.js
        ├── manipulation
        │   ├── _evalUrl.js
        │   ├── support.js
        │   └── var
        │       └── rcheckableType.js
        ├── manipulation.js
        ├── offset.js
        ├── outro.js
        ├── queue
        │   └── delay.js
        ├── queue.js
        ├── selector-native.js
        ├── selector-sizzle.js
        ├── selector.js
        ├── serialize.js
        ├── sizzle
        │   └── dist
        │       ├── sizzle.js
        │       ├── sizzle.min.js
        │       └── sizzle.min.map
        ├── traversing
        │   ├── findFilter.js
        │   └── var
        │       └── rneedsContext.js
        ├── traversing.js
        ├── var
        │   ├── arr.js
        │   ├── class2type.js
        │   ├── concat.js
        │   ├── hasOwn.js
        │   ├── indexOf.js
        │   ├── pnum.js
        │   ├── push.js
        │   ├── rnotwhite.js
        │   ├── slice.js
        │   ├── strundefined.js
        │   ├── support.js
        │   └── toString.js
        └── wrap.js

23 directories, 88 files
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;包的使用&lt;/h2&gt;

&lt;p&gt;现在就可以在应用程序中使用 jQuery 包了，在 jQuery 里创建一个简单的 html5 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;!doctype html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;Learning bower&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;button&amp;gt;Animate Me!!&amp;lt;/button&amp;gt;
&amp;lt;div style=&quot;background:red;height:100px;width:100px;position:absolute;&quot;&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;script type=&quot;text/javascript&quot; src=&quot;bower_components/jquery/jquery.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
    $(document).ready(function(){
        $(&quot;button&quot;).click(function(){
            $(&quot;div&quot;).animate({left:&#39;250px&#39;});
        });
    });
&amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正如你所看到的，你刚刚引用 jquery.min.js 文件，现阶段完成。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;查看安装的包&lt;/h2&gt;

&lt;p&gt;执行以下命令可以列出所有本地安装的包。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower list
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;查找包&lt;/h2&gt;

&lt;p&gt;查找bower注册的包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower search [&amp;lt;name&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只需执行 &lt;code&gt;bower search&lt;/code&gt; 命令即可列出所有已经注册的包。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;包的信息&lt;/h2&gt;

&lt;p&gt;如果你想看到关于特定的包的信息，可以使用 &lt;code&gt;info&lt;/code&gt; 命令来查看该包的所有信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower info jquery
bower cached        git://github.com/jquery/jquery.git#2.1.1
bower validate      2.1.1 against git://github.com/jquery/jquery.git#*

{
  name: &#39;jquery&#39;,
  version: &#39;2.1.1&#39;,
  main: &#39;dist/jquery.js&#39;,
  license: &#39;MIT&#39;,
  ignore: [
    &#39;**/.*&#39;,
    &#39;build&#39;,
    &#39;speed&#39;,
    &#39;test&#39;,
    &#39;*.md&#39;,
    &#39;AUTHORS.txt&#39;,
    &#39;Gruntfile.js&#39;,
    &#39;package.json&#39;
  ],
  devDependencies: {
    sizzle: &#39;1.10.19&#39;,
    requirejs: &#39;2.1.10&#39;,
    qunit: &#39;1.14.0&#39;,
    sinon: &#39;1.8.1&#39;
  },
  keywords: [
    &#39;jquery&#39;,
    &#39;javascript&#39;,
    &#39;library&#39;
  ],
  homepage: &#39;https://github.com/jquery/jquery&#39;
}

Available versions:
  - 2.1.1
  - 2.1.1-rc2
  - 2.1.1-rc1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-7&quot;&gt;注册包&lt;/h2&gt;

&lt;p&gt;可以注册自己的包，这样其他人也可以使用了:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower register project git://github.com/yourname/project
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-8&quot;&gt;包的卸载&lt;/h2&gt;

&lt;p&gt;卸载包可以使用 uninstall 命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower uninstall jquery
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-9&quot;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;每个包应该有一个配置文件，描述包的信息，jquery 的配置文件为 bower.json。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
  &quot;name&quot;: &quot;jquery&quot;,
  &quot;version&quot;: &quot;2.1.1&quot;,
  &quot;main&quot;: &quot;dist/jquery.js&quot;,
  &quot;license&quot;: &quot;MIT&quot;,
  &quot;ignore&quot;: [
    &quot;**/.*&quot;,
    &quot;build&quot;,
    &quot;speed&quot;,
    &quot;test&quot;,
    &quot;*.md&quot;,
    &quot;AUTHORS.txt&quot;,
    &quot;Gruntfile.js&quot;,
    &quot;package.json&quot;
  ],
  &quot;devDependencies&quot;: {
    &quot;sizzle&quot;: &quot;1.10.19&quot;,
    &quot;requirejs&quot;: &quot;2.1.10&quot;,
    &quot;qunit&quot;: &quot;1.14.0&quot;,
    &quot;sinon&quot;: &quot;1.8.1&quot;
  },
  &quot;keywords&quot;: [
    &quot;jquery&quot;,
    &quot;javascript&quot;,
    &quot;library&quot;
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;name 和 version 描述包的名称和版本，dependencies 描述这个包依赖的其他包。main 指定包中的静态文件，可以为一个数组。license 指定版权协议，ignore 指定忽略哪些文件，devDependencies 指定依赖，keywords 描述该包的关键字。&lt;/p&gt;

&lt;p&gt;除了包的配置文件，Bower 有一个项目的配置文件 .bowerrc ，存在于当期项目目录下，和 bower.json 文件同级，该文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
  &quot;directory&quot;: &quot;src/main/resources/static/libs&quot;,
  &quot;json&quot;: &quot;bower.json&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;directory&lt;/code&gt;：配置下载的依赖存放路径，默认为 bower_components&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;json&lt;/code&gt;：保存项目依赖的文件名称，默认为 bower.json&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多的配置参数，需要查验官方文档，这里暂时不做补充。&lt;/p&gt;

&lt;p&gt;类似的，你可以配置一个全局的配置文件 &lt;code&gt;~/.bowerrc&lt;/code&gt;，位于用户根目录下面。&lt;/p&gt;

&lt;h2 id=&quot;section-10&quot;&gt;项目中使用&lt;/h2&gt;

&lt;p&gt;bower.json 文件的使用可以让包的安装更容易，你可以在应用程序的根目录下创建一个名为 &lt;code&gt;bower.json&lt;/code&gt; 的文件，并定义它的依赖关系。使用 &lt;code&gt;bower init&lt;/code&gt;命令来创建 bower.json 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[?] name: blog
[?] version: 0.0.0
[?] description: 
[?] main file: 
[?] what types of modules does this package expose? 
[?] keywords: 
[?] authors: javachen &amp;lt;june.chan@javachen.com&amp;gt;
[?] license: MIT
[?] homepage: 
[?] would you like to mark this package as private which prevents it from being accidentally published to the registry? No
accidentally published to the registry? (y/N) 
{
  name: &#39;blog&#39;,
  version: &#39;0.0.0&#39;,
  authors: [
    &#39;javachen &amp;lt;june.chan@javachen.com&amp;gt;&#39;
  ],
  license: &#39;MIT&#39;,
  ignore: [
    &#39;**/.*&#39;,
    &#39;node_modules&#39;,
    &#39;bower_components&#39;,
    &#39;test&#39;,
    &#39;tests&#39;
  ],
  dependencies: {
    jquery: &#39;~2.1.1&#39;
  }
}

[?] Looks good? Yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意看，它已经加入了 jQuery 依赖关系。&lt;/p&gt;

&lt;p&gt;现在假设也想用 twitter bootstrap，我们可以用下面的命令安装 twitter bootstrap 并更新 bower.json文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ bower install bootstrap --save
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它会自动安装最新版本的 bootstrap 并更新 bower.json 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;{
  name: &#39;blog&#39;,
  version: &#39;0.0.0&#39;,
  authors: [
    &#39;javachen &amp;lt;june.chan@javachen.com&amp;gt;&#39;
  ],
  license: &#39;MIT&#39;,
  ignore: [
    &#39;**/.*&#39;,
    &#39;node_modules&#39;,
    &#39;bower_components&#39;,
    &#39;test&#39;,
    &#39;tests&#39;
  ],
  dependencies: {
    jquery: &#39;~2.1.1&#39;,
    bootstrap: &#39;~3.0.0&#39;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想查看有哪些包和文件，可执行 &lt;code&gt;bower list --path&lt;/code&gt;。比如安装了 jquery，可以看到以下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
  &quot;jquery&quot;: &quot;bower_components/jquery/dist/jquery.js&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在就可以使用了，在当前目录建一个页面，script 嵌入需要的 js。&lt;/p&gt;

&lt;h1 id=&quot;section-11&quot;&gt;4. 总结&lt;/h1&gt;

&lt;p&gt;Bower 类似 maven 用于管理 javascript 的版本及其依赖，使用非常简单。&lt;/p&gt;

&lt;h1 id=&quot;section-12&quot;&gt;5. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://bower.jsbin.cn/&quot;&gt;用于web前端开发的包管理器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://chuo.me/2013/02/twitter-bower.html&quot;&gt;twitter 的包管理工具 - bower&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://segmentfault.com/a/1190000000349555&quot;&gt;Day 1: Bower —— 管理你的客户端依赖关系&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/05/10/bower-intro.html</link>
      <guid>http://blog.javachen.com/2014/05/10/bower-intro.html</guid>
      <pubDate>2014-05-10T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>All Things Markdown</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;目录&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#Intro&quot;&gt;概述&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Feture&quot;&gt;特点&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Syntax&quot;&gt;语法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Editor&quot;&gt;编辑器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#BrowserPlugin&quot;&gt;浏览器插件&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Implements&quot;&gt;实现版本&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Reference&quot;&gt;参考资料&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;a-idintroa&quot;&gt;&lt;a id=&quot;Intro&quot;&gt;概述&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://zh.wikipedia.org/wiki/Markdown&quot;&gt;Markdown&lt;/a&gt; 是一种轻量级标记语言，创始人为约翰·格鲁伯（John Gruber）和亚伦·斯沃茨（Aaron Swartz）。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML(或者HTML)文档”。这种语言吸收了很多在电子邮件中已有的纯文本标记的特性。&lt;/p&gt;

&lt;h1 id=&quot;a-idfeturea&quot;&gt;&lt;a id=&quot;Feture&quot;&gt;特点&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;html&quot;&gt;兼容 HTML&lt;/h2&gt;

&lt;p&gt;要在Markdown中输写HTML区块元素，比如&lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 等标签，必须在前后加上空行与其它内容区隔开，还要求它们的开始标签与结尾标签不能用制表符或空格来缩进。Markdown 的生成器有足够智能，不会在 HTML 区块标签外加上不必要的 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 标签。&lt;/p&gt;

&lt;p&gt;例子如下，在 Markdown 文件里加上一段 HTML 表格：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;这是一个普通段落。

&amp;lt;table&amp;gt;
    &amp;lt;tr&amp;gt;
        &amp;lt;td&amp;gt;a&amp;lt;/td&amp;gt;
        &amp;lt;td&amp;gt;a&amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr&amp;gt;
        &amp;lt;td&amp;gt;b&amp;lt;/td&amp;gt;
        &amp;lt;td&amp;gt;b&amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
&amp;lt;/table&amp;gt;

这是另一个普通段落。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请注意:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 HTML 区块标签间的 Markdown 格式语法将不会被处理。&lt;/li&gt;
  &lt;li&gt;HTML 的区段（行内）标签如 &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;cite&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;del&amp;gt;&lt;/code&gt; 可以在 Markdown 的段落、列表或是标题里随意使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;特殊字符自动转换&lt;/h2&gt;

&lt;p&gt;Markdown会对一些特殊字符进行转化，如：&lt;code&gt;&amp;amp;&lt;/code&gt;、&lt;code&gt;©&lt;/code&gt;、&lt;code&gt;&amp;lt;&lt;/code&gt;、&lt;code&gt;&amp;gt;&lt;/code&gt;,如果&lt;code&gt;&amp;lt;&lt;/code&gt;、&lt;code&gt;&amp;gt;&lt;/code&gt;用作html标签的界定符，则不会对其转换。&lt;/p&gt;

&lt;p&gt;不过需要注意的是，code 范围内，不论是行内还是区块， &lt;code&gt;&amp;lt;&lt;/code&gt; 和 &lt;code&gt;&amp;amp;&lt;/code&gt; 两个符号都一定会被转换成 HTML 实体。&lt;/p&gt;

&lt;h1 id=&quot;a-idsyntaxa&quot;&gt;&lt;a id=&quot;Syntax&quot;&gt;语法&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;标题&lt;/h2&gt;

&lt;p&gt;Markdown 支持两种标题的语法，类 &lt;a href=&quot;http://docutils.sourceforge.net/mirror/setext.html&quot;&gt;Setext&lt;/a&gt; 和类 &lt;a href=&quot;http://www.aaronsw.com/2002/atx/&quot;&gt;atx&lt;/a&gt; 形式。&lt;/p&gt;

&lt;p&gt;类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# This is an &amp;lt;h1&amp;gt; tag
## This is an &amp;lt;h2&amp;gt; tag
###### This is an &amp;lt;h6&amp;gt; tag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以选择性地「闭合」类 atx 样式的标题，这纯粹只是美观用的，若是觉得这样看起来比较舒适，你就可以在行尾加上 #，而行尾的 # 数量也不用和开头一样（行首的井字符数量决定标题的阶数）&lt;/p&gt;

&lt;p&gt;类 Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This is an H1
=============

This is an H2
-------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;任何数量的 = 和 - 都可以有效果。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;强调&lt;/h2&gt;

&lt;p&gt;Markdown 使用星号（&lt;code&gt;*&lt;/code&gt;）和底线（&lt;code&gt;_&lt;/code&gt;）作为标记强调字词的符号，被 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 包围的字词会被转成用 &lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt; 标签包围，用两个 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;_&lt;/code&gt; 包起来的话，则会被转成 &lt;code&gt;&amp;lt;strong&amp;gt;&lt;/code&gt;，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*This text will be italic*
_This will also be italic_

**This text will be bold**
__This will also be bold__

*You **can** combine them*
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;断行&lt;/h2&gt;

&lt;p&gt;如果你真的想在Markdown中插入换行标签&lt;code&gt;&amp;lt;br/&amp;gt;&lt;/code&gt;，你可以在行尾输入两个或以上的空格，然后回车。 这样插入换行十分麻烦，但是“每个换行都转换为&lt;code&gt;&amp;lt;br/&amp;gt;&lt;/code&gt;”在 Markdown中并不合适，所以只在你确定你需要时手动添加。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;列表&lt;/h2&gt;

&lt;p&gt;Markdown 支持有序列表和无序列表。&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;无序&lt;/h3&gt;

&lt;p&gt;无序列表使用星号、加号或是减号作为列表标记：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*   Red
*   Green
*   Blue
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-7&quot;&gt;有序&lt;/h3&gt;

&lt;p&gt;有序列表则使用数字接着一个英文句点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.  Bird
2.  McHale
3.  Parish
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很重要的一点是，你在列表标记上使用的数字并不会影响输出的 HTML 结果。&lt;/p&gt;

&lt;p&gt;如果你的列表标记写成：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.  Bird
1.  McHale
1.  Parish
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;或甚至是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3. Bird
1. McHale
8. Parish
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你都会得到完全相同的 HTML 输出。&lt;/p&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果列表项目间用空行分开，在输出 HTML 时 Markdown 就会将项目内容用 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 标签包起来&lt;/li&gt;
  &lt;li&gt;列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符&lt;/li&gt;
  &lt;li&gt;如果要放代码区块的话，该区块就需要缩进两次，也就是 8 个空格或是 2 个制表符&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-8&quot;&gt;区块引用&lt;/h2&gt;

&lt;p&gt;Markdown 标记区块引用是使用类似 email 中用 &lt;code&gt;&amp;gt;&lt;/code&gt; 的引用方式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;As Kanye West said:

&amp;gt; We&#39;re living the future so
&amp;gt; the present is our past.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Markdown 也允许你偷懒只在整个段落的第一行最前面加上 &lt;code&gt;&amp;gt;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 &amp;gt; ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; This is the first level of quoting.
&amp;gt;
&amp;gt; &amp;gt; This is nested blockquote.
&amp;gt;
&amp;gt; Back to the first level.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; ## 这是一个标题。
&amp;gt; 
&amp;gt; 1.   这是第一行列表项。
&amp;gt; 2.   这是第二行列表项。
&amp;gt; 
&amp;gt; 给出一些例子代码：
&amp;gt; 
&amp;gt;     return shell_exec(&quot;echo $input | $markdown_script&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-9&quot;&gt;分隔线&lt;/h2&gt;

&lt;p&gt;你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;* * *
***
*****
- - -
---------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-10&quot;&gt;代码区块&lt;/h2&gt;

&lt;p&gt;和程序相关的写作或是标签语言原始码通常会有已经排版好的代码区块，通常这些区块我们并不希望它以一般段落文件的方式去排版，而是照原来的样子显示，Markdown 会用 &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt; 和 &lt;code&gt;&amp;lt;code&amp;gt;&lt;/code&gt; 标签来把代码区块包起来。&lt;/p&gt;

&lt;p&gt;要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;内联代码&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;I think you should use an `&amp;lt;addr&amp;gt;` element here instead.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-12&quot;&gt;链接&lt;/h2&gt;

&lt;p&gt;Markdown 支持两种形式的链接语法： 行内式和参考式两种形式。&lt;/p&gt;

&lt;p&gt;不管是哪一种，链接文字都是用 [方括号] 来标记。&lt;/p&gt;

&lt;p&gt;要建立一个&lt;strong&gt;行内式的链接&lt;/strong&gt;，只要在方块括号后面紧接着圆括号并插入网址链接即可，如果你还想要加上链接的 title 文字，只要在网址后面，用双引号把 title 文字包起来即可，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This is [an example](http://example.com/ &quot;Title&quot;) inline link.
[This link](http://example.net/) has no title attribute.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;参考式的链接&lt;/strong&gt;**是在链接文字的括号后面再接上另一个方括号，而在第二个方括号里面要填入用以辨识链接的标记：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This is [an example][id] reference-style link.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以选择性地在两个方括号中间加上一个空格：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This is [an example] [id] reference-style link.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着，在文件的任意处，你可以把这个标记的链接内容定义出来（&lt;strong&gt;注意去掉冒号前面空格&lt;/strong&gt;）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[id] : http://example.com/  &quot;Optional Title Here&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;引用本地资源，可以使用相对路径：&lt;code&gt;[about me](/about/)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;隐式链接&lt;/strong&gt;标记功能让你可以省略指定链接标记，这种情形下，链接标记会视为等同于链接文字，要用隐式链接标记只要在链接文字后面加上一个空的方括号，如果你要让 “Google” 链接到 google.com，你可以简化成：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Google][]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后定义链接内容（&lt;strong&gt;注意去掉冒号前面空格&lt;/strong&gt;）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Google] : http://google.com/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于链接文字可能包含空白，所以这种简化型的标记内也许包含多个单词：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Visit [Daring Fireball][] for more information.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后接着定义链接（&lt;strong&gt;注意去掉冒号前面空格&lt;/strong&gt;）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Daring Fireball] : http://daringfireball.net/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;链接的定义可以放在文件中的任何一个地方，我比较偏好直接放在链接出现段落的后面，你也可以把它放在文件最后面，就像是注解一样。&lt;/p&gt;

&lt;p&gt;下面是一个参考式链接的范例（&lt;strong&gt;注意去掉冒号前面空格&lt;/strong&gt;）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I get 10 times more traffic from [Google] [1] than from
[Yahoo] [2] or [MSN] [3].

[1] : http://google.com/        &quot;Google&quot;
[2] : http://search.yahoo.com/  &quot;Yahoo Search&quot;
[3] : http://search.msn.com/    &quot;MSN Search&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果改成用链接名称的方式写（&lt;strong&gt;注意去掉冒号前面空格&lt;/strong&gt;）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I get 10 times more traffic from [Google][] than from
[Yahoo][] or [MSN][].

  [google] : http://google.com/        &quot;Google&quot;
  [yahoo] :  http://search.yahoo.com/  &quot;Yahoo Search&quot;
  [msn] :    http://search.msn.com/    &quot;MSN Search&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-13&quot;&gt;图片&lt;/h2&gt;

&lt;p&gt;Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式。&lt;/p&gt;

&lt;p&gt;行内式的图片语法看起来像是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![Alt text](/path/to/img.jpg)

![Alt text](/path/to/img.jpg &quot;Optional title&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;详细叙述如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一个惊叹号 !&lt;/li&gt;
  &lt;li&gt;接着一个方括号，里面放上图片的替代文字&lt;/li&gt;
  &lt;li&gt;接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考式的图片语法则长得像这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![Alt text][id]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;「id」是图片参考的名称，图片参考的定义方式则和连结参考一样（&lt;strong&gt;注意去掉冒号前面空格&lt;/strong&gt;）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[id] : url/to/image  &quot;Optional title attribute&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到目前为止， Markdown 还没有办法指定图片的宽高，如果你需要的话，你可以使用普通的 &lt;img /&gt; 标签。&lt;/p&gt;

&lt;h2 id=&quot;section-14&quot;&gt;自动链接&lt;/h2&gt;

&lt;p&gt;Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用方括号包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;http://example.com/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Markdown 会转为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a href=&quot;http://example.com/&quot;&amp;gt;http://example.com/&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;邮址的自动链接也很类似，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;address@example.com&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Markdown 会转成：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a href=&quot;mailto:address@example.com&quot;&amp;gt;address@example.com&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-15&quot;&gt;反斜杠&lt;/h2&gt;

&lt;p&gt;Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果（但不用 &lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt; 标签），你可以在星号的前面加上反斜杠：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\*literal asterisks\*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\   反斜线
`   反引号
*   星号
_   底线
{}  花括号
[]  方括号
()  括弧
#   井字号
+   加号
-   减号
.   英文句点
!   惊叹号
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-16&quot;&gt;表格&lt;/h2&gt;

&lt;p&gt;使用语法解释引擎 Redcarpet(需要开启&lt;code&gt;tables&lt;/code&gt;选项)，则表格如下定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;|head1 head1 head1|head2 head2 head2|head3 head3 head3|head4 head4 head4|
|---|:---|:---:|---:|
|row1text1|row1text3|row1text3|row1text4|
|row2text1|row2text3|row2text3|row2text4|
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中&lt;code&gt;:&lt;/code&gt;所在位置表示表格的位置对齐&lt;/p&gt;

&lt;p&gt;添加 &lt;code&gt;table thead tobody th tr td&lt;/code&gt; 样式后显示的效果是：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;head1 head1 head1&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;head2 head2 head2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;head3 head3 head3&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;head4 head4 head4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;row1text1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;row1text3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;row1text3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;row1text4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;row2text1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;row2text3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;row2text3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;row2text4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Github中定义表格方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;First Header | Second Header
------------ | -------------
Content from cell 1 | Content from cell 2
Content in the first column | Content in the second column
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示的效果如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;First Header&lt;/th&gt;
      &lt;th&gt;Second Header&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Content from cell 1&lt;/td&gt;
      &lt;td&gt;Content from cell 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Content in the first column&lt;/td&gt;
      &lt;td&gt;Content in the second column&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;a-ideditora&quot;&gt;&lt;a id=&quot;Editor&quot;&gt;在线编辑器&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;作为一种小型标记语言，Markdown很容易阅读，也很容易用普通的文本编辑器编辑。另外也有一些编辑器专为Markdown设计，可以直接预览文档的样式。下面有一些编辑器可供参考：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zybuluo.com/mdeditor&quot;&gt;Cmd Markdown&lt;/a&gt; 支持实时同步预览，区分写作和阅读模式，支持在线存储，分享文稿网址。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dillinger.io/&quot;&gt;Dillinger.io&lt;/a&gt; 一个在线Markdown编辑器，提供实时预览以及到 GitHub 和 Dropbox 的拓展连接。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://notepag.es/&quot;&gt;notepag&lt;/a&gt; 另一个在线Markdown编辑器，支持实时预览，提供临时网址和和密码，可以分享给其他人。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mouapp.com/&quot;&gt;Mou&lt;/a&gt; 一个Mac OS X上的Markdown编辑器。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.markdownpad.com/&quot;&gt;MarkdownPad&lt;/a&gt; a full-featured Markdown editor for Windows.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://code.google.com/p/wmd/&quot;&gt;WMD&lt;/a&gt; a Javascript WYSIWYM editor for Markdown (from AttackLab)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://code.google.com/p/pagedown/&quot;&gt;PageDown&lt;/a&gt; 一个Javascript WYSIWYM Markdown编辑器 (来自 StackOverflow)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ipython.org/notebook&quot;&gt;IPython Notebook&lt;/a&gt; 以ipython为后台，利用浏览器做IDE，支持MarkDown与LaTex公式。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mahua.jser.me/&quot;&gt;http://mahua.jser.me/&lt;/a&gt; 一个在线编辑markdown文档的编辑器&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://markable.in/editor/&quot;&gt;http://markable.in/editor/&lt;/a&gt; A remarkable online markdown editor&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tool.oschina.net/markdown&quot;&gt;OsChina&lt;/a&gt; OsChina在线编辑器&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://logdown.com/&quot;&gt;Logdown&lt;/a&gt; 是台湾一个博客写手和开发者在一个周末和三位朋友在24小时之内做的一个Hackathon 項目。这是一个支持Markdown的博客写作平台。在国际上也引起关注。它的写作界面是单栏宽屏。&lt;/li&gt;
  &lt;li&gt;
    &lt;jianshu.io&gt; 这是一个支持Markdown的中文写作社区。
&lt;/jianshu.io&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[有记&lt;/td&gt;
          &lt;td&gt;noteton](http://noteton.com/) 有记提供基于云笔记服务的博客发布平台。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;a-idbrowserplugina&quot;&gt;&lt;a id=&quot;BrowserPlugin&quot;&gt;浏览器插件&lt;/a&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/oknndfeeopgpibecfjljjfanledpbkog&quot;&gt;MaDe&lt;/a&gt; (Chrome)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://markdown-here.com/&quot;&gt;Markdown Here&lt;/a&gt; (Chrome, Firefox, Safari, and Thunderbird)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/poe-markdown-editor/mpghdlgejmakmgbigejnjnmgdjaddhje&quot;&gt;Poe: Markdown Editor&lt;/a&gt; (Chrome)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/markdown/anjbpnjfkbpkkjpfnaomopbpcldihjlg&quot;&gt;MarkDown&lt;/a&gt; (Chrome)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/stackedit/iiooodelglhkcpgbajoejffhijaclcdg&quot;&gt;StackEdit&lt;/a&gt; (Chrome)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/markdown-preview-plus/febilkbfcbhebfnokafefeacimjdckgl&quot;&gt;扩展程序Markdown Preview Plus&lt;/a&gt; (Chrome)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;a-idimplementsa&quot;&gt;&lt;a id=&quot;Implements&quot;&gt;实现版本&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;由于Markdown的易读易写，很多人用不同的编程语言实现了多个版本的解析器和生成器。下面是一个按编程语言排序的实现列表。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;C
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/vmg/sundown&quot;&gt;Sundown&lt;/a&gt;, 一个用C写的Markdown实现。&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.pell.portland.or.us/~orc/Code/discount/&quot;&gt;Discount&lt;/a&gt;, 一个Markdown标记语言的C语言实现版本。&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jgm/peg-markdown&quot;&gt;peg-markdown&lt;/a&gt;, 一个用C写的，使用了PEG (parsing expression grammar)的Markdown实现。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Java
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://code.google.com/p/markdownj/&quot;&gt;MarkdownJ&lt;/a&gt; the pure Java port of Markdown.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://github.com/sirthias/pegdown&quot;&gt;Pegdown&lt;/a&gt;, a pure-Java Markdown implementation based on a PEG parser&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://markdown.tautua.org/&quot;&gt;MarkdownPapers&lt;/a&gt;, Java implementation based on a JavaCC parser&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://github.com/rjeschke/txtmark/&quot;&gt;Txtmark&lt;/a&gt;, another Markdown implementation written in Java&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Lua
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://luaforge.net/projects/markdown/&quot;&gt;Markdown.lua&lt;/a&gt;, a Markdown implementation in Lua&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://jgm.github.com/lunamark&quot;&gt;Lunamark&lt;/a&gt;, a markdown to HTML and LaTeX converter written in Lua, using a PEG grammar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PHP
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://michelf.com/projects/php-markdown/&quot;&gt;PHP Markdown&lt;/a&gt; and Markdown Extra&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.wolfiezero.com/935/markdown-viewer-for-php/&quot;&gt;Markdown Viewer for PHP&lt;/a&gt;, allows the viewing of a Mardown doc via a local PHP server (a wrapper for PHP Markdown)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;JavaScript
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/amir-hadzic/uedit&quot;&gt;Uedit&lt;/a&gt;, a Javascript “WYSIWYM” &lt;br /&gt;
editor for Markdown&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://strapdownjs.com/&quot;&gt;Strapdown.js&lt;/a&gt; - JavaScript客户端解析markdown内容为html&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/chjj/marked/&quot;&gt;Marked&lt;/a&gt; - Fast Markdown parser in JavaScript&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Python
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://freewisdom.org/projects/python-markdown&quot;&gt;Markdown in Python&lt;/a&gt;, A Python implementation of Markdown&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://misaka.61924.nl/&quot;&gt;Misaka&lt;/a&gt;, a Python binding for Sundown.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ruby
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://deveiate.org/projects/BlueCloth/&quot;&gt;BlueCloth&lt;/a&gt;, an implementation of Markdown in Ruby&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Scala
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://tristanhunt.com/projects/knockoff/&quot;&gt;Knockoff&lt;/a&gt;, a Markdown implementation written in Scala using Parser Combinators&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://henkelmann.eu/projects/actuarius/&quot;&gt;Actuarius&lt;/a&gt;, another Markdown implementation written in Scala using Parser Combinators&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GO
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://github.com/russross/blackfriday&quot;&gt;Blackfriday&lt;/a&gt;, another Markdown implementation written in Go&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;其它
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://code.google.com/p/markdownsharp/&quot;&gt;MarkdownSharp&lt;/a&gt;, a slightly modified C# implementation of the Markdown markup language. Developed and used by Stack Overflow.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://markdownr.com/&quot;&gt;Markdownr.com&lt;/a&gt;, a simple website to preview markdown in real time&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://johnmacfarlane.net/pandoc/&quot;&gt;Pandoc&lt;/a&gt;, a universal document converter written in Haskell&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://sourceforge.net/p/retext/home/ReText/&quot;&gt;ReText&lt;/a&gt;, an implementation for Markdown and reStructuredText.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-17&quot;&gt;其他资料&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yangzhiping.com/tech/r-markdown-knitr.html&quot;&gt;Markdown+R写作&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;a-idreferencea&quot;&gt;&lt;a id=&quot;Reference&quot;&gt;参考资料&lt;/a&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://zh.wikipedia.org/wiki/Markdown&quot;&gt;维基百科 Markdown&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://wowubuntu.com/markdown/&quot;&gt;Markdown 语法说明 (简体中文版)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;https://guides.github.com/features/mastering-markdown&quot;&gt;GitHub Guides:Mastering Markdown&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[4] &lt;a href=&quot;http://daringfireball.net/projects/markdown/syntax&quot;&gt;Daring Fireball&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[5] &lt;a href=&quot;http://joinwee.com/lesson/10/&quot;&gt;Markdown在线写作速成&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/04/24/all-things-markdown.html</link>
      <guid>http://blog.javachen.com/2014/04/24/all-things-markdown.html</guid>
      <pubDate>2014-04-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>重装Mac系统之后</title>
      <description>&lt;p&gt;本文主要记录重装Mac系统之后的一些软件安装和环境变量配置。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;系统偏好配置&lt;/h1&gt;

&lt;p&gt;设置主机名：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo scutil --set HostName june－mac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置鼠标滚轮滑动的方向：系统偏好设置－－&amp;gt;鼠标－－&amp;gt;”滚动方向：自然”前面的勾去掉&lt;/p&gt;

&lt;p&gt;显示/隐藏Mac隐藏文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;defaults write com.apple.finder AppleShowAllFiles -bool true  #显示Mac隐藏文件的命令
defaults write com.apple.finder AppleShowAllFiles -bool false #隐藏Mac隐藏文件的命令
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;触控板&lt;/li&gt;
  &lt;li&gt;光标与点按 &amp;gt; 三指移动 ：这样就可以三指拖动文件了&lt;/li&gt;
  &lt;li&gt;光标与点按 &amp;gt; 轻拍来点按 ：习惯了轻点完成实际按击&lt;/li&gt;
  &lt;li&gt;光标与点按 &amp;gt; 跟踪速度 ：默认的指针滑动速度有点慢，设置成刻度7差不多了&lt;/li&gt;
  &lt;li&gt;键盘&lt;/li&gt;
  &lt;li&gt;快捷键 &amp;gt; 服务 &amp;gt; 新建位于文件夹位置的终端标签：勾选这设置并设置了快捷键（control+cmt+c），以后在Finder中选择一个目录按下快捷键就可以打开终端并来到当前当前目录，功能很实用啊！注意：在Finder中文件列表使用分栏方式显示时快捷键是无效的。&lt;/li&gt;
  &lt;li&gt;网络&lt;br /&gt;
 -高级… &amp;gt; DNS ：公共DNS是必须添加的
    &lt;ul&gt;
      &lt;li&gt;223.6.6.6 阿里提供的&lt;/li&gt;
      &lt;li&gt;8.8.4.4 google提供的&lt;/li&gt;
      &lt;li&gt;114.114.114.114 114服务提供的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apps&quot;&gt;Apps&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;VirtualBox&lt;/li&gt;
  &lt;li&gt;Vagrant&lt;/li&gt;
  &lt;li&gt;Unarchiver: 支持多种格式（包括 windows下的格式）的压缩/解压缩工具&lt;/li&gt;
  &lt;li&gt;OminiFocus ：时间管理工具&lt;/li&gt;
  &lt;li&gt;Mou：Markdown 编辑器，国人出品&lt;/li&gt;
  &lt;li&gt;Dash&lt;/li&gt;
  &lt;li&gt;Xmind&lt;/li&gt;
  &lt;li&gt;Shadowsocks&lt;/li&gt;
  &lt;li&gt;WizNote：为知笔记&lt;/li&gt;
  &lt;li&gt;yEd：画时序图&lt;/li&gt;
  &lt;li&gt;Iterm2&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dreamhead/moco&quot;&gt;Moco&lt;/a&gt;，一个用来模拟服务器的工具。在服务器端没有开发完成时，可以通过配置来搭建一个模拟服务， 这样可以方便客户端的开发。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;homebrew&quot;&gt;Homebrew&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://brew.sh/&quot;&gt;Brew&lt;/a&gt; 是 Mac 下面的包管理工具，通过 Github 托管适合 Mac 的编译配置以及 Patch，可以方便的安装开发工具。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ruby -e &quot;$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过brew安装软件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install git git-flow  curl  wget  putty  tmux ack source-highlight aria2 dos2unix nmap iotop htop  ctags tree openvpn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;紧接着，我们需要做一件事让通过 Homebrew 安装的程序的启动链接 (在 /usr/local/bin中）可以直接运行，无需将完整路径写出。通过以下命令将 /usr/local/bin 添加至 $PATH 环境变量中:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo &#39;export PATH=&quot;/usr/local/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cmd+T 打开一个新的 terminal 标签页，运行以下命令，确保 brew 运行正常。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew doctor
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;使用&lt;/h2&gt;

&lt;p&gt;安装一个包，可以简单的运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install &amp;lt;package_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新 Homebrew 在服务器端上的包目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看你的包是否需要更新：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew outdated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew upgrade &amp;lt;package_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Homebrew 将会把老版本的包缓存下来，以便当你想回滚至旧版本时使用。但这是比较少使用的情况，当你想清理旧版本的包缓存时，可以运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew cleanup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看你安装过的包列表（包括版本号）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew list --versions
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;cask&quot;&gt;Cask&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/phinze/homebrew-cask&quot;&gt;Brew cask&lt;/a&gt; 是类似 Brew 的管理工具， 直接提供 dmg 级别的二进制包，（Brew 是不带源码，只有对应项目所在的 URL）。我们可以通过 Homebrew Cask 优雅、简单、快速的安装和管理 OS X 图形界面程序，比如 Google Chrome 和 Dropbox。&lt;/p&gt;

&lt;p&gt;Brew cask 安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew tap phinze/homebrew-cask
$ brew install brew-cask
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我通过 Brew cask 安装的软件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew cask install google-chrome omnigraffle xtrafinder

$ brew update &amp;amp;&amp;amp; brew upgrade brew-cask &amp;amp;&amp;amp; brew cleanup # 更新
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;相对于 brew cask 的安装方式，本人更倾向于到 App Store 或官方下载 OS X 图形界面程序。主要因为名字不好记忆、偶尔需要手动更新，另外当你使用 Alfred 或 Spotlight ，你将发现将程序安装在 ~/Application 会很方便。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;oh-my-zsh&quot;&gt;oh-my-zsh&lt;/h1&gt;

&lt;p&gt;使用 Homebrew 完成 zsh 和 zsh completions 的安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;brew install zsh zsh-completions
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把默认 Shell 换为 zsh。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ chsh -s /bin/zsh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后用下面的两句（任选其一）可以自动安装 oh-my-zsh：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget --no-check-certificate https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑 ~/.zshrc：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;source ~/.bashrc&#39; &amp;gt;&amp;gt;~/.zshrc
echo &#39;source ~/.bash_profile&#39; &amp;gt;&amp;gt;~/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用文本编辑器或 vi 打开 .zshrc 进行以下编辑:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ZSH_THEME=pygmalion
plugins=(git colored-man colorize github jira vagrant virtualenv pip python brew osx zsh-syntax-highlighting)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 &lt;code&gt;ctrl+r&lt;/code&gt; 查找历史命令，在 &lt;code&gt;~/.zshrc&lt;/code&gt; 中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bindkey &quot;^R&quot; history-incremental-search-backward
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;使用&lt;/h2&gt;

&lt;p&gt;使用上默认加了很多快捷映射，如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;~&lt;/code&gt;: 进入用户根目录，可以少打cd三个字符了&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;l&lt;/code&gt;: 相当于ls -lah&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;..&lt;/code&gt;: 返回上层目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;...&lt;/code&gt;: 返回上上层目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-&lt;/code&gt;: 打开上次所在目录&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的可以查看其&lt;a href=&quot;https://github.com/robbyrussell/oh-my-zsh/blob/master/lib/aliases.zsh&quot;&gt;配置文件&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;git&quot;&gt;Git&lt;/h1&gt;

&lt;p&gt;安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;好的，现在我们来测试一下 gti 是否安装完好：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git --version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行 &lt;code&gt;$ which git&lt;/code&gt; 将会输出 /usr/local/bin/git.&lt;/p&gt;

&lt;p&gt;接着，我们将定义你的 Git 帐号（与你在 GitHub 使用的用户名和邮箱一致）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git config --global user.name &quot;Your Name Here&quot;
$ git config --global user.email &quot;your_email@youremail.com&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些配置信息将会添加进 ~/.gitconfig 文件中.&lt;/p&gt;

&lt;p&gt;我们将推荐使用 HTTPS 方法（另一个是 SSH），将你的代码推送到 Github 上的仓库。如果你不想每次都输入用户名和密码的话，可以按照此 &lt;a href=&quot;https://help.github.com/articles/set-up-git&quot;&gt;描述&lt;/a&gt; 说的那样，运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git config --global credential.helper osxkeychain
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外，如果你打算使用 SSH方式，可以参考此 &lt;a href=&quot;https://help.github.com/articles/generating-ssh-keys&quot;&gt;链接&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;git-ignore&quot;&gt;Git Ignore&lt;/h2&gt;

&lt;p&gt;创建一个新文件 ~/.gitignore ，并将以下内容添加进去，这样全部 git 仓库将会忽略以下内容所提及的文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Folder view configuration files
.DS_Store
Desktop.ini

# Thumbnail cache files
._*
Thumbs.db

# Files that might appear on external disks
.Spotlight-V100
.Trashes

# Compiled Java files
.classpath
.project
.settings
bin
build
target
dependency-reduced-pom.xml
.gradle
README.html
.idea
*.iml

# Compiled Python files
*.pyc

# Compiled C++ files
*.out

# Application specific files
venv
node_modules
.sass-cache
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;vim&quot;&gt;安装Vim插件&lt;/h1&gt;

&lt;p&gt;安装 pathogen：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p ~/.vim/autoload ~/.vim/bundle; \
$ curl -Sso ~/.vim/autoload/pathogen.vim \
    https://raw.github.com/tpope/vim-pathogen/master/autoload/pathogen.vim
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装NERDTree：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd ~/.vim/bundle
$ git clone https://github.com/scrooloose/nerdtree.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多请参考：&lt;a href=&quot;/2014/01/14/vim-config-and-plugins.html&quot;&gt;vim配置和插件管理&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;ruby&quot;&gt;安装Ruby&lt;/h1&gt;

&lt;p&gt;先安装依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install libksba autoconf automake libtool gcc46 libyaml readline
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过rvm安装ruby：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ curl -L get.rvm.io | bash -s stable $ source ~/.bash_profile
$ sed -i -e &#39;s/ftp\.ruby-lang\.org\/pub\/ruby/ruby\.taobao\.org\/mirrors\/ruby/g&#39; ~/.rvm/config/db
$ sudo rvm install 1.9.3 --with-gcc=clang
$ rvm --default 1.9.3
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;jekyll&quot;&gt;安装Jekyll&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo gem install rdoc
$ sudo gem install jekyll redcarpet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置环境变量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo &#39;export PATH=$PATH:$HOME/.rvm/bin&#39; &amp;gt;&amp;gt; ~/.bash_profile
$ echo &#39;[[ -s &quot;$HOME/.rvm/scripts/rvm&quot; ]] &amp;amp;&amp;amp; . &quot;$HOME/.rvm/scripts/rvm&quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;java&quot;&gt;Java开发环境&lt;/h1&gt;

&lt;p&gt;下载 jdk：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;jdk6：&lt;a href=&quot;http://support.apple.com/downloads/DL1572/en_US/JavaForOSX2013-05.dmg&quot;&gt;http://support.apple.com/downloads/DL1572/en_US/JavaForOSX2013-05.dmg&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;jdk7：&lt;a href=&quot;http://download.oracle.com/otn-pub/java/jdk/7u60-b19/jdk-7u60-macosx-x64.dmg?AuthParam=1403450902_0b8ed262d4128ca82031dcbdc2627aaf&quot;&gt;http://download.oracle.com/otn-pub/java/jdk/7u60-b19/jdk-7u60-macosx-x64.dmg?AuthParam=1403450902_0b8ed262d4128ca82031dcbdc2627aaf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;设置 java_home 为 1.7:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export JAVA_HOME=$(/usr/libexec/java_home -v 1.7)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 brew 来安装 ant、maven、ivy、forrest、springboot 等：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install https://raw.github.com/Homebrew/homebrew-versions/master/maven30.rb ant ivy apache-forrest  springboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置 ant、maven 和 ivy 仓库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ rm -rf ~/.ivy2/cache ~/.m2/repository
$ mkdir -p ~/.ivy2 ~/.m2
$ ln -s ~/app/repository/cache/  ~/.ivy2/cache
$ ln -s ~/app/repository/m2/  ~/.m2/repository
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，这里我在 &lt;code&gt;~/app/repository&lt;/code&gt; 有两个目录，cache 用于存放 ivy 下载的文件，m2 用于存放 maven 的仓库。&lt;/p&gt;

&lt;h1 id=&quot;python&quot;&gt;Python开发环境&lt;/h1&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://aaaaaashu.gitbooks.io/mac-dev-setup/content/index.html&quot;&gt;Mac 开发配置手册&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nootn.com/blog/archives/87/&quot;&gt;MacBook Pro 配置&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/04/23/after-reinstall-mac.html</link>
      <guid>http://blog.javachen.com/2014/04/23/after-reinstall-mac.html</guid>
      <pubDate>2014-04-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Ubuntu系统编译Bigtop</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 安装系统依赖&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;系统更新并安装新的包&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt-get update

sudo apt-get install -y cmake git-core git-svn subversion checkinstall build-essential dh-make debhelper ant ant-optional autoconf automake liblzo2-dev libzip-dev sharutils libfuse-dev reprepro libtool libssl-dev asciidoc xmlto ssh curl

sudo apt-get install -y devscripts

sudo apt-get build-dep pkg-config
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sun-jdk-6openjdk-7&quot;&gt;安装Sun JDK 6或OpenJDK 7&lt;/h2&gt;

&lt;p&gt;Sun JDK 6:&lt;/p&gt;

&lt;p&gt;执行以下脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget http://archive.cloudera.com/cm4/ubuntu/precise/amd64/cm/pool/contrib/o/oracle-j2sdk1.6/oracle-j2sdk1.6_1.6.0+update31_amd64.deb
dpkg -i oracle-j2sdk1.6_1.6.0+update31_amd64.deb

sudo rm /usr/lib/jvm/default-java
sudo ln -s /usr/lib/jvm/j2sdk1.6-oracle /usr/lib/jvm/default-java
sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/default-java/bin/java 5
sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/default-java/bin/javac 5
sudo update-alternatives --set java /usr/lib/jvm/default-java/bin/java
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OpenJDK 7:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;OpenJDK 6 fails to build Hadoop because of issue MAPREDUCE-4115 Need to use &lt;a href=&quot;http://www.shinephp.com/install-jdk-7-on-ubuntu/&quot;&gt;OpenJDK 7&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt-get install openjdk-7-jdk
sudo rm /usr/lib/jvm/default-java
sudo ln -s /usr/lib/jvm/java-7-openjdk-amd64 /usr/lib/jvm/default-java
sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/default-java/bin/java 5
sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/default-java/bin/javac 5
sudo update-alternatives --set java /usr/lib/jvm/default-java/bin/java
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;maven-3&quot;&gt;安装Maven 3&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget http://apache.petsads.us/maven/maven-3/3.0.5/binaries/apache-maven-3.0.5-bin.tar.gz
tar -xzvf apache-maven-3.0.5-bin.tar.gz

sudo mkdir /usr/local/maven-3
sudo mv apache-maven-3.0.5 /usr/local/maven-3/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;apache-forrest&quot;&gt;安装Apache Forrest&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd $HOME
wget http://archive.apache.org/dist/forrest/0.9/apache-forrest-0.9.tar.gz
tar -xzvf /home/ubuntu/Downloads/apache-forrest-0.9.tar.gz
# modify certain lines in the forrest-validate xml, otherwise build fails. either sed or nano are fine.
sed -i &#39;s/property name=&quot;forrest.validate.sitemap&quot; value=&quot;${forrest.validate}&quot;/property name=&quot;forrest.validate.sitemap&quot; value=&quot;false&quot;/g&#39; apache-forrest-0.9/main/targets/validate.xml
sed -i &#39;s/property name=&quot;forrest.validate.stylesheets&quot; value=&quot;${forrest.validate}&quot;/property name=&quot;forrest.validate.stylesheets&quot; value=&quot;false&quot;/g&#39; apache-forrest-0.9/main/targets/validate.xml
sed -i &#39;s/property name=&quot;forrest.validate.stylesheets.failonerror&quot; value=&quot;${forrest.validate.failonerror}&quot;/property name=&quot;forrest.validate.stylesheets.failonerror&quot; value=&quot;false&quot;/g&#39; apache-forrest-0.9/main/targets/validate.xml
sed -i &#39;s/property name=&quot;forrest.validate.skins.stylesheets&quot; value=&quot;${forrest.validate.skins}&quot;/property name=&quot;forrest.validate.skins.stylesheets&quot; value=&quot;false&quot;/g&#39; apache-forrest-0.9/main/targets/validate.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;protobuf&quot;&gt;安装protobuf&lt;/h2&gt;

&lt;p&gt;protobuf版本至少需要2.4.0,具体版本视hadoop版本而定，例如&lt;code&gt;hadoop-2.4.0&lt;/code&gt;即需要依赖&lt;code&gt;protobuf-2.5.0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;到 Protocol Buffers 的官网&lt;a href=&quot;https://code.google.com/p/protobuf/&quot;&gt;https://code.google.com/p/protobuf/&lt;/a&gt;下载2.5.0的安装源文件进行安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;tar -zxf protobuf-2.5.0.tar.gz
cd protobuf-2.5.0
./configure --prefix=/usr/local/protobuf
make check
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装完成后，执行 &lt;code&gt;protoc --vresion&lt;/code&gt; 验证是否安装成功。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;2. 设置环境变量&lt;/h1&gt;

&lt;p&gt;创建&lt;code&gt;/etc/profile.d/bigtop.sh&lt;/code&gt;并添加如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export JAVA_HOME=&quot;/usr/lib/jvm/default-java&quot;
export JAVA5_HOME=&quot;/usr/lib/jvm/default-java&quot;
export JVM_ARGS=&quot;-Xmx1024m -XX:MaxPermSize=512m&quot;
export MAVEN_HOME=&quot;/usr/local/maven-3/apache-maven-3.0.5&quot;
export MAVEN_OPTS=&quot;-Xmx1024m -XX:MaxPermSize=512m&quot;
PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:$MAVEN_HOME/bin&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将&lt;code&gt;FORREST_HOME&lt;/code&gt;添加到&lt;code&gt;~/.bashrc&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export FORREST_HOME=&quot;$HOME/apache-forrest-0.9&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;3. 下载并编译源代码&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone git://git.apache.org/bigtop.git # put files under bigtop directory
cd bigtop
# you can also use a different branch, e.g. git checkout branch-0.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了加快编译速度，你可以修改&lt;code&gt;Makefile&lt;/code&gt;文件中的&lt;code&gt;APACHE_MIRROR&lt;/code&gt;和&lt;code&gt;APACHE_ARCHIVE&lt;/code&gt;为国内的速度较快的apache镜像地址，例如：&lt;a href=&quot;http://mirror.bit.edu.cn/apache&quot;&gt;http://mirror.bit.edu.cn/apache&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;编译源代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./check-env.sh # make sure all the required environment variables are set
make realclean
make bigtop-utils-deb # build this project first
make bigtop-jsvc-deb
make bigtop-tomcat-deb
make hadoop-deb # to build just for hadoop first
make deb # build all the rest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译之后deb输出在output目录&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;4. 安装和测试&lt;/h1&gt;

&lt;p&gt;在使用&lt;code&gt;dpkg&lt;/code&gt;命令安装之前，先关掉自动启动服务。使用root用欢创建&lt;code&gt;/usr/sbin/policy-rc.d&lt;/code&gt;，该文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
exit 101
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加执行权限：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chmod +x /usr/sbin/policy-rc.d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装deb文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd output/bigtop-utils
sudo dpkg --install *.deb
cd ..
sudo dpkg --install **/**.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后别忘了删除掉&lt;code&gt;policy-rc.d&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo rm /usr/sbin/policy-rc.d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化hdfs：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo -u hdfs hadoop namenode -format
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo /etc/init.d/hadoop-hdfs-namenode start
sudo /etc/init.d/hadoop-hdfs-datanode start

#sudo /etc/init.d/hadoop-xxxx start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来可以查看日志和web页面是否正常了。访问&lt;a href=&quot;http://localhost:50070/&quot;&gt;http://localhost:50070/&lt;/a&gt;，你就可以看到hadoop-2.3.0的小清新的管理界面了。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;5. 排错&lt;/h1&gt;

&lt;p&gt;1) bigtop-0.7依赖的是&lt;code&gt;protobuf-2.4.0&lt;/code&gt;而不是&lt;code&gt;protobuf-2.5.0&lt;/code&gt;，导致编译过程出现protobuf的版本需要2.5.0的提示，请卸载2.4.0版本重新编译protobuf-2.5.0。&lt;/p&gt;

&lt;p&gt;2) 运行&lt;code&gt;make deb&lt;/code&gt;时出现&lt;code&gt;more change data or trailer&lt;/code&gt;的异常(详细异常信息见下面)，请将操作系统的LANG修改为&lt;code&gt;en_US&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;parsechangelog/debian: warning:     debian/changelog(l4): badly formatted trailer line
LINE:  -- Bigtop &amp;lt;dev@bigtop.apache.org&amp;gt;  四, 17 4月 2014 14:30:17 +0800
parsechangelog/debian: warning:     debian/changelog(l4): found eof where expected more change data or trailer
dpkg-buildpackage: source package zookeeper
dpkg-buildpackage: source version 3.4.5-1
dpkg-buildpackage: error: unable to determine source changed by
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-6&quot;&gt;6. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;https://cwiki.apache.org/confluence/display/BIGTOP/Building+Bigtop+on+Ubuntu&quot;&gt;Building Bigtop on Ubuntu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/04/17/building-bigtop-on-ubuntu.html</link>
      <guid>http://blog.javachen.com/2014/04/17/building-bigtop-on-ubuntu.html</guid>
      <pubDate>2014-04-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Java笔记：Java内存模型</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 基本概念&lt;/h1&gt;

&lt;p&gt;《深入理解Java内存模型》详细讲解了java的内存模型，这里对其中的一些基本概念做个简单的笔记。以下内容摘自 &lt;a href=&quot;http://www.cnblogs.com/skywang12345/p/3447546.html&quot;&gt;《深入理解Java内存模型》读书总结&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;并发&lt;/h2&gt;

&lt;p&gt;定义：即，并发(同时)发生。在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理机上运行，但任一个时刻点上只有一个程序在处理机上运行。&lt;/p&gt;

&lt;p&gt;并发需要处理两个关键问题：&lt;code&gt;线程之间如何通信&lt;/code&gt;及&lt;code&gt;线程之间如何同步&lt;/code&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;通信&lt;/code&gt;：是指线程之间如何交换信息。在命令式编程中，线程之间的通信机制有两种：&lt;code&gt;共享内存&lt;/code&gt;和&lt;code&gt;消息传递&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;同步&lt;/code&gt;：是指程序用于控制不同线程之间操作发生相对顺序的机制。在Java中，可以通过&lt;code&gt;volatile&lt;/code&gt;、&lt;code&gt;synchronized&lt;/code&gt;、&lt;code&gt;锁&lt;/code&gt;等方式实现同步。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;主内存和本地内存&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;主内存&lt;/code&gt;：即 main memory。在java中，实例域、静态域和数组元素是线程之间共享的数据，它们存储在主内存中。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;本地内存&lt;/code&gt;：即 local memory。 局部变量，方法定义参数 和 异常处理器参数是不会在线程之间共享的，它们存储在线程的本地内存中。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;重排序&lt;/h2&gt;

&lt;p&gt;定义：&lt;code&gt;重排序&lt;/code&gt;是指“编译器和处理器”为了提高性能，而在程序执行时会对程序进行的重排序。&lt;/p&gt;

&lt;p&gt;说明：重排序分为“编译器”和“处理器”两个方面，而“处理器”重排序又包括“指令级重排序”和“内存的重排序”。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;关于重排序，我们需要理解它的思想：&lt;br /&gt;
为了提高程序的并发度，从而提高性能！但是对于多线程程序，重排序可能会导致程序执行的结果不是我们需要的结果！因此，就需要我们通过volatile、synchronize、锁等方式实现同步。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-4&quot;&gt;内存屏障&lt;/h2&gt;

&lt;p&gt;定义：包括LoadLoad, LoadStore, StoreLoad, StoreStore共4种内存屏障。内存屏障是与相应的内存重排序相对应的。&lt;/p&gt;

&lt;p&gt;作用：通过内存屏障可以禁止特定类型处理器的重排序，从而让程序按我们预想的流程去执行。&lt;/p&gt;

&lt;h2 id=&quot;happens-before&quot;&gt;happens-before&lt;/h2&gt;

&lt;p&gt;定义：JDK5(JSR-133)提供的概念，用于描述多线程操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在 &lt;code&gt;happens-before&lt;/code&gt; 关系。&lt;/p&gt;

&lt;p&gt;作用：描述多线程操作之间的内存可见性。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;数据依赖性&lt;/h2&gt;

&lt;p&gt;定义：如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在&lt;code&gt;数据依赖性&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;作用：编译器和处理器不会对“存在数据依赖关系的两个操作”执行重排序。&lt;/p&gt;

&lt;h2 id=&quot;as-if-serial&quot;&gt;as-if-serial&lt;/h2&gt;

&lt;p&gt;定义：不管怎么重排序，程序的执行结果不能被改变。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;顺序一致性内存模型&lt;/h2&gt;

&lt;p&gt;定义：它是理想化的内存模型。有以下规则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一个线程中的所有操作必须按照程序的顺序来执行。&lt;/li&gt;
  &lt;li&gt;所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;java&quot;&gt;Java内存模型&lt;/h2&gt;

&lt;p&gt;定义：Java Memory Mode，它是Java线程之间通信的控制机制。&lt;/p&gt;

&lt;p&gt;说明：JMM 对 Java 程序作出保证，如果程序是正确同步的，程序的执行将具有顺序一致性。即，程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;可见性&lt;/h2&gt;

&lt;p&gt;可见性一般用于指不同线程之间的数据是否可见。&lt;/p&gt;

&lt;p&gt;在 java 中， 实例域、静态域和数组元素这些数据是线程之间共享的数据，它们存储在主内存中；主内存中的所有数据对该内存中的线程都是可见的。而局部变量，方法定义参数和异常处理器参数这些数据是不会在线程之间共享的，它们存储在线程的本地内存中；它们对其它线程是不可见的。&lt;/p&gt;

&lt;p&gt;此外，对于主内存中的数据，在本地内存中会对应的创建该数据的副本(相当于缓冲)；这些副本对于其它线程也是不可见的。&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;原子性&lt;/h2&gt;

&lt;p&gt;是指一个操作是按原子的方式执行的。要么该操作不被执行；要么以原子方式执行，即执行过程中不会被其它线程中断。&lt;/p&gt;

&lt;h1 id=&quot;jvm&quot;&gt;2. JVM内存模型&lt;/h1&gt;

&lt;p&gt;虽然平时我们用的大多是 Sun JDK 提供的 JVM，但是 JVM 本身是一个 &lt;a href=&quot;http://java.sun.com/docs/books/jvms/second_edition/html/VMSpecTOC.doc.html&quot;&gt;规范&lt;/a&gt;，所以可以有多种实现，除了 &lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/tech/index-jsp-136373.html&quot;&gt;Hotspot&lt;/a&gt; 外，还有诸如 Oracle 的 &lt;a href=&quot;http://www.oracle.com/technetwork/middleware/jrockit/overview/index.html&quot;&gt;JRockit&lt;/a&gt;、IBM 的 &lt;a href=&quot;http://en.wikipedia.org/wiki/IBM_J9&quot;&gt;J9&lt;/a&gt;也都是非常有名的 JVM。&lt;/p&gt;

&lt;p&gt;Java 虚拟机在执行 Java 程序的过程中会把它所管理的内存划分为若干个不同的数据区域，这些区域都有各自的用途，以及创建和销毁的时间。有的区域随着虚拟机进程的启动就存在了， 有的区域则是依赖用户线程。根据《Java虚拟机规范（第二版）》，Java 虚拟机所管理的内存包含如下图的几个区域。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/Java-Memory.png&quot; alt=&quot;Java-Memory.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由上图可以看出 JVM 组成如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;运行时数据区（内存空间）
    &lt;ul&gt;
      &lt;li&gt;方法区&lt;/li&gt;
      &lt;li&gt;堆&lt;/li&gt;
      &lt;li&gt;虚拟机栈&lt;/li&gt;
      &lt;li&gt;程序计数器&lt;/li&gt;
      &lt;li&gt;本地方法栈&lt;/li&gt;
      &lt;li&gt;直接内存&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;执行引擎&lt;/li&gt;
  &lt;li&gt;本地库接口&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从上图中还可以看出，在内存空间中方法区和堆是所有Java线程共享的，称之为&lt;code&gt;线程共享数据区&lt;/code&gt;，而虚拟机栈、程序计数器、本地方法栈则由每个线程私有，称之为&lt;code&gt;线程隔离数据区&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;关于本地方法：&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;众所周知，Java 语言具有跨平台的特性，这也是由 JVM 来实现的。更准确地说，是 Sun 利用 JVM 在不同平台上的实现帮我们把平台相关性的问题给解决了，这就好比是 HTML 语言可以在不同厂商的浏览器上呈现元素（虽然某些浏览器在对W3C标准的支持上还有一些问题）。同时，Java 语言支持通过 JNI（Java Native Interface）来实现本地方法的调用，但是需要注意到，如果你在 Java 程序用调用了本地方法，那么你的程序就很可能不再具有跨平台性，即本地方法会破坏平台无关性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面分别就线程共享数据区和线程共享数据区进行说明。&lt;/p&gt;

&lt;h2 id=&quot;section-9&quot;&gt;2.1 线程共享数据区&lt;/h2&gt;

&lt;p&gt;所谓线程共享数据区，是指在多线程环境下，该部分区域数据可以被所有线程所共享，主要有方法区和堆。&lt;/p&gt;

&lt;h3 id=&quot;section-10&quot;&gt;方法区&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;方法区&lt;/code&gt;用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等等。方法区中对于每个类存储了以下数据：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;类及其父类的全限定名（java.lang.Object没有父类）&lt;/li&gt;
  &lt;li&gt;类的类型（Class or Interface）&lt;/li&gt;
  &lt;li&gt;访问修饰符（public, abstract, final）&lt;/li&gt;
  &lt;li&gt;实现的接口的全限定名的列表&lt;/li&gt;
  &lt;li&gt;常量池&lt;/li&gt;
  &lt;li&gt;字段信息&lt;/li&gt;
  &lt;li&gt;方法信息&lt;/li&gt;
  &lt;li&gt;静态变量&lt;/li&gt;
  &lt;li&gt;ClassLoader 引用&lt;/li&gt;
  &lt;li&gt;Class 引用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见类的所有信息都存储在方法区中。由于方法区是所有线程共享的，所以必须保证&lt;code&gt;线程安全&lt;/code&gt;，举例来说：如果两个类同时要加载一个尚未被加载的类，那么一个类会请求它的 ClassLoader 去加载需要的类，另一个类只能等待而不会重复加载。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意事项：&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;在 HotSpot 虚拟机中，很多人都把方法区成为&lt;code&gt;永久代&lt;/code&gt;，默认最小值为16MB，最大值为64MB。其实只在 HotSpot 才存在方法区，在其他的虚拟机没有方法区这一个说法的。本文是采用 Hotspot，所以把方法区介绍了。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;如果方法区无法满足内存分配需求时候就会抛出 OutOfMemoryError 异常。&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;section-11&quot;&gt;堆&lt;/h3&gt;

&lt;p&gt;堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例及数组内容，几乎所有的对象实例都在这里分配内存。堆中有指向类数据的指针，该指针指向了方法区中对应的类型信息，堆中还可能存放了指向方法表的指针。堆是所有线程共享的，所以在进行实例化对象等操作时，需要解决同步问题。此外，堆中的实例数据中还包含了对象锁，并且针对不同的垃圾收集策略，可能存放了引用计数或清扫标记等数据。&lt;/p&gt;

&lt;p&gt;在 Java 中，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old )。新生代 ( Young ) 又被划分为三个区域：Eden、From Survivor、To Survivor。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/jvm-heap.png&quot; alt=&quot;jvm-heap.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​从图中可以看出： &lt;code&gt;堆大小 = 新生代 + 老年代&lt;/code&gt;，其中，堆的大小可以通过参数 &lt;code&gt;-Xms&lt;/code&gt;、&lt;code&gt;-Xmx&lt;/code&gt; 来指定。本人使用的是 JDK1.6，以下涉及的 JVM 默认值均以该版本为准。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;默认的，&lt;code&gt;Young : Old = 1 : m&lt;/code&gt; ，该比例值 m 可以通过参数 &lt;code&gt;-XX:NewRatio&lt;/code&gt; 来指定，默认值为2，即新生代 ( Young ) = 1/3 的堆空间大小，老年代 ( Old ) = 2/3 的堆空间大小。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;默认的，&lt;code&gt;Edem : from : to = n : 1 : 1&lt;/code&gt; ，该比例值 n 可以参数 &lt;code&gt;-XX:SurvivorRatio&lt;/code&gt; 来设定，默认值为8 ，即 Eden = 8/10 的新生代空间大小，from = to = 1/10 的新生代空间大小。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;JVM 每次只会使用 Eden 和其中的一块 Survivor 区域来为对象服务，所以无论什么时候，总是有一块 Survivor 区域是空闲着的，因此，新生代实际可用的内存空间为 9/10 ( 即90% )的新生代空间。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据 Java 虚拟机规范的规定，Java 堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出 OutOfMemoryError 异常。&lt;/p&gt;

&lt;h2 id=&quot;section-12&quot;&gt;2.2 线程隔离数据区&lt;/h2&gt;

&lt;p&gt;所谓线程隔离数据区是指在多线程环境下，每个线程所独享的数据区域。主要有程序计数器、Java虚拟机栈、本地方法栈三个数据区。&lt;/p&gt;

&lt;h3 id=&quot;section-13&quot;&gt;程序计数器&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://baike.baidu.com/view/178145.htm?fr=aladdin&quot;&gt;程序计数器&lt;/a&gt; ，计算机处理器中的寄存器，它包含当前正在执行的指令的地址（位置）。当每个指令被获取，程序计数器的存储地址加一。在每个指令被获取之后，程序计数器指向顺序中的下一个指令。当计算机重启或复位时，程序计数器通常恢复到零。&lt;/p&gt;

&lt;p&gt;在Java中程序计数器是一块较小的内存空间，充当当前线程所执行的字节码的行号指示器的角色。&lt;/p&gt;

&lt;p&gt;在多线程环境下，当某个线程失去处理器执行权时，需要记录该线程被切换出去时所执行的程序位置。从而方便该线程被切换回来(重新被处理器处理)时能恢复到当初的执行位置，因此每个线程都需要有一个独立的程序计数器。各个线程的程序计数器互不影响，并且独立存储。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当线程正在执行一个 java 方法时，这个程序计数器记录的时正在执行的虚拟机字节码指令的地址。&lt;/li&gt;
  &lt;li&gt;当线程执行的是 &lt;a href=&quot;http://www.enet.com.cn/article/2007/1029/A20071029886398.shtml&quot;&gt;Native方法&lt;/a&gt;，这个计数器值为空。&lt;/li&gt;
  &lt;li&gt;此内存区域是唯一一个在 java 虚拟机规范中没有规定任何 OutOfMemoryError 情况的区域。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;java-&quot;&gt;Java 虚拟机栈&lt;/h3&gt;

&lt;p&gt;与程序计数器一样，Java 虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。Java 虚拟机栈描述的是 Java 方法执行的内存模型，每个方法在执行的同时都会创建一个&lt;a href=&quot;http://baike.baidu.com/view/8128123.htm?fr=aladdin&quot;&gt;栈帧&lt;/a&gt;用于存储&lt;a href=&quot;http://blog.csdn.net/kevin_luan/article/details/22986081&quot;&gt;局部变量表&lt;/a&gt;、&lt;a href=&quot;http://denverj.iteye.com/blog/1218359&quot;&gt;操作数栈&lt;/a&gt;、&lt;a href=&quot;http://jnn.iteye.com/blog/83105&quot;&gt;动态链接&lt;/a&gt;、方法出口等信息。每个方法从调用直至执行完成的过程，对应着一个栈帧在虚拟机中入栈到进栈的过程。&lt;/p&gt;

&lt;p&gt;在 Hot Spot 虚拟机中，可以使用 &lt;code&gt;-Xss&lt;/code&gt; 参数来设置栈的大小。栈的大小直接决定了函数调用的深度。&lt;/p&gt;

&lt;p&gt;某个线程正在执行的方法被称为该线程的当前方法，当前方法使用的栈帧成为当前帧，当前方法所属的类成为当前类，当前类的常量池成为当前常量池。在线程执行一个方法时，它会跟踪当前类和当前常量池。此外，当虚拟机遇到栈内操作指令时，它对当前帧内数据执行操作。&lt;/p&gt;

&lt;p&gt;它分为三部分：&lt;code&gt;局部变量区&lt;/code&gt;、&lt;code&gt;操作数栈&lt;/code&gt;、&lt;code&gt;帧数据区&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;1、局部变量区&lt;/p&gt;

&lt;p&gt;局部变量区是以字长为单位的数组，在这里，byte、short、char 类型会被转换成 int 类型存储，除了 long 和 double 类型占两个字长以外，其余类型都只占用一个字长。特别地，boolean 类型在编译时会被转换成 int 或 byte 类型，boolean 数组会被当做 byte 类型数组来处理。局部变量区也会包含对象的引用，包括类引用、接口引用以及数组引用。&lt;/p&gt;

&lt;p&gt;局部变量区包含了方法参数和局部变量，此外，实例方法隐含第一个局部变量 this，它指向调用该方法的对象引用。对于对象，局部变量区中永远只有指向堆的引用。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;/p&gt;

  &lt;p&gt;局部变量表中的字可能会影响 GC 回收。如果这个字没有被后续代码复用，那么它所引用的对象不会被 GC 释放，手工对要释放的变量赋值为 null，是一种有效的做法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2、操作数栈&lt;/p&gt;

&lt;p&gt;操作数栈也是以字长为单位的数组，但是正如其名，它只能进行入栈出栈的基本操作。在进行计算时，操作数被弹出栈，计算完毕后再入栈。&lt;/p&gt;

&lt;p&gt;每当线程调用一个Java方法时，虚拟机都会在该线程的Java栈中压入一个新帧。而这个新帧自然就成为了当前帧。在执行这个方法时，它使用这个帧来存储参数、局部变量、中间运算结果等等数据。&lt;/p&gt;

&lt;p&gt;Java 方法可以以两种方式完成。一种通过 return 返回的，称为正常返回；一种是通过抛出异常而异常中止的。不管以哪种方式返回，虚拟机都会将当前帧弹出Java栈然后释放掉，这样上一个方法的帧就成为当前帧了。&lt;/p&gt;

&lt;p&gt;Java 栈上的所有数据都是此线程私有的。任何线程都不能访问另一个线程的栈数据，因此我们不需要考虑多线程情况下栈数据的访问同步问题。当一个线程调用一个方法时，方法的局部变量保存在调用线程 Java 栈的帧中。只有一个线程总是访问哪些局部变量，即调用方法的线程。&lt;/p&gt;

&lt;p&gt;3、帧数据区&lt;/p&gt;

&lt;p&gt;帧数据区的任务主要有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a.记录指向类的常量池的指针，以便于解析。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;b.帮助方法的正常返回，包括恢复调用该方法的栈帧，设置PC寄存器指向调用方法对应的下一条指令，把返回值压入调用栈帧的操作数栈中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;c.记录异常表，发生异常时将控制权交由对应异常的catch子句，如果没有找到对应的catch子句，会恢复调用方法的栈帧并重新抛出异常。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;局部变量区和操作数栈的大小依照具体方法在编译时就已经确定。调用方法时会从方法区中找到对应类的类型信息，从中得到具体方法的局部变量区和操作数栈的大小，依此分配栈帧内存，压入Java栈。&lt;/p&gt;

&lt;p&gt;在 Java 虚拟机规范中，对这个区域规定了&lt;strong&gt;两种异常状况&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果线程请求的栈深度大于虚拟机所允许的深度，将抛出 StackOverflowError 异常；&lt;/li&gt;
  &lt;li&gt;如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出 OutOfMemoryError 异常。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-14&quot;&gt;本地方法栈&lt;/h3&gt;

&lt;p&gt;本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行 Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的 Native 方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如 Sun HotSpot 虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出 StackOverflowError 和 OutOfMemoryError 异常。&lt;/p&gt;

&lt;h2 id=&quot;section-15&quot;&gt;2.3 直接内存&lt;/h2&gt;

&lt;p&gt;直接内存并不是虚拟机运行时数据区的一部分，也不是 Java 虚拟机规范中定义的内存区域。&lt;/p&gt;

&lt;p&gt;JDK1.4 中出现了 NIO，其引入了一种基于通道与缓冲区的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中得 DirectoryByteBuffer 对象作为这块内存的引用进行操作。这样可以避免 Java 堆和 Native 堆之间的来回复制数据。&lt;/p&gt;

&lt;p&gt;当机器直接内存去除 JVM 内存之后的内存不能满足直接内存大小要求其，将会抛出 OutOfMemoryError 异常。&lt;/p&gt;

&lt;h1 id=&quot;section-16&quot;&gt;3. 垃圾回收过程&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/jvm-heap.png&quot; alt=&quot;jvm-heap.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;JVM 采用一种&lt;code&gt;分代回收&lt;/code&gt; (generational collection) 的策略，用较高的频率对年轻的对象进行扫描和回收，这种叫做 &lt;code&gt;minor collection&lt;/code&gt; ，而对老对象的检查回收频率要低很多，称为 &lt;code&gt;major collection&lt;/code&gt;。这样就不需要每次 GC 都将内存中所有对象都检查一遍。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;新生代被划分为三部分，Eden 区和两个大小严格相同的 Survivor 区，其中 Survivor 区间，&lt;code&gt;某一时刻只有其中一个是被使用的，另外一个留做垃圾收集时复制对象用&lt;/code&gt;，在 Young 区间变满的时候，minor GC 就会将存活的对象移到空闲的 Survivor 区间中，根据 JVM 的策略，&lt;code&gt;在经过几次垃圾收集后，仍然存活于 Survivor 的对象将被移动到老年代&lt;/code&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;老年代主要保存生命周期长的对象，一般是一些老的对象，&lt;code&gt;当一些对象在 Young 复制转移一定的次数以后，对象就会被转移到老年区&lt;/code&gt;，一般如果系统中用了 application 级别的缓存，缓存中的对象往往会被转移到这一区间。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Minor collection 的过程就是将 eden 和在用survivor space中的活对象 copy 到空闲survivor space中。所谓 survivor，也就是大部分对象在 eden 出生后，根本活不过一次 GC。对象在新生代里经历了一定次数的 minor collection 后，年纪大了，就会被移到老年代中，称为 tenuring。&lt;/p&gt;

  &lt;p&gt;剩余内存空间不足会触发 GC，如 eden 空间不够了就要进行 minor collection，老年代空间不够要进行 major collection，永久代(Permanent Space)空间不足会引发full GC。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;举例：当一个 URL 被访问时，内存申请过程如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A. JVM 会试图为相关 Java 对象在 Eden 中初始化一块内存区域&lt;/li&gt;
  &lt;li&gt;B. 当 Eden 空间足够时，内存申请结束。否则到下一步&lt;/li&gt;
  &lt;li&gt;C. JVM 试图释放在 Eden 中所有不活跃的对象，释放后若 Eden 空间仍然不足以放入新对象，则试图将部分 Eden 中活跃对象放入 Survivor 区&lt;/li&gt;
  &lt;li&gt;D. Survivor 区被用来作为 Eden 及 Old 的中间交换区域，当 Old 区空间足够时，Survivor 区的对象会被移到 Old 区，否则会被保留在 Survivor区&lt;/li&gt;
  &lt;li&gt;E. 当 Old 区空间不够时，JVM 会在 Old 区进行完全的垃圾收集&lt;/li&gt;
  &lt;li&gt;F. 完全垃圾收集后，若 Survivor 及 Old 区仍然无法存放从 Eden 复制过来的部分对象，导致 JVM 无法在 Eden 区为新对象创建内存区域，则出现 &lt;code&gt;out of memory&lt;/code&gt; 错误&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;HotSpot jvm 都给我们提供了下面参数来对内存进行配置：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;配置总内存&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-Xms&lt;/code&gt; ：指定了 JVM 初始启动以后初始化内存&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;-Xmx&lt;/code&gt;：指定 JVM 堆得最大内存，在JVM启动以后，会分配 &lt;code&gt;-Xmx&lt;/code&gt; 参数指定大小的内存给 JVM，但是不一定全部使用，JVM 会根据 &lt;code&gt;-Xms&lt;/code&gt; 参数来调节真正用于JVM的内存，&lt;code&gt;-Xmx-Xms&lt;/code&gt; 之差就是三个 Virtual 空间的大小&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;配置新生代&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-Xmn&lt;/code&gt;: 参数设置了年轻代的大小&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-XX:SurvivorRatio&lt;/code&gt;: 表示 eden 和一个 surivivor 的比例，缺省值为8&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;-XX:NewSize&lt;/code&gt; 和 &lt;code&gt;-XX:MaxNewSize&lt;/code&gt;：直接指定了年轻代的缺省大小和最大大小&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;配置老年代&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;-XX:NewRatio&lt;/code&gt;: 表示年老年代和新生代内存的比例，缺省值为2&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;配置持久代&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-XX:MaxPermSize&lt;/code&gt;：表示持久代的最大值&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;-XX:PermSize&lt;/code&gt;：设置最小分配空间&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;配置虚拟机栈&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-Xss&lt;/code&gt;：参数来设置栈的大小，默认值为128 kb。栈的大小直接决定了函数调用的深度&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-17&quot;&gt;4. 常见的垃圾收集策略&lt;/h1&gt;

&lt;p&gt;垃圾收集提供了内存管理的机制，使得应用程序不需要在关注内存如何释放，内存用完后，垃圾收集会进行收集，这样就减轻了因为人为的管理内存而造成的错误，比如在 C++ 语言里，出现内存泄露时很常见的。Java 语言是目前使用最多的依赖于垃圾收集器的语言，但是垃圾收集器策略从20世纪60年代就已经流行起来了，比如 Smalltalk,Eiffel 等编程语言也集成了垃圾收集器的机制。&lt;/p&gt;

&lt;p&gt;所有的垃圾收集算法都面临同一个问题，那就是找出应用程序不可到达的内存块，将其释放，这里面得不可到达主要是指应用程序已经没有内存块的引用了，而在 JAVA中，某个对象对应用程序是可到达的是指：这个对象被根（根主要是指类的静态变量，常量或者活跃在所有线程栈的对象的引用）引用或者对象被另一个可到达的对象引用。&lt;/p&gt;

&lt;p&gt;下面我们介绍一下几种常见的垃圾收集策略：&lt;/p&gt;

&lt;h2 id=&quot;reference-counting&quot;&gt;4.1 Reference Counting(引用计数）&lt;/h2&gt;

&lt;p&gt;引用计数是最简单直接的一种方式，这种方式在每一个对象中增加一个引用的计数，这个计数代表当前程序有多少个引用引用了此对象，如果此对象的引用计数变为0，那么此对象就可以作为垃圾收集器的目标对象来收集。&lt;/p&gt;

&lt;p&gt;优点：简单，直接，不需要暂停整个应用&lt;/p&gt;

&lt;p&gt;缺点：需要编译器的配合，编译器要生成特殊的指令来进行引用计数的操作，比如每次将对象赋值给新的引用，或者者对象的引用超出了作用域等。&lt;br /&gt;
不能处理循环引用的问题&lt;/p&gt;

&lt;h2 id=&quot;section-18&quot;&gt;4.2 跟踪收集器&lt;/h2&gt;

&lt;p&gt;跟踪收集器首先要暂停整个应用程序，然后开始从根对象扫描整个堆，判断扫描的对象是否有对象引用。&lt;/p&gt;

&lt;p&gt;如果每次扫描整个堆，那么势必让 GC 的时间变长，从而影响了应用本身的执行。因此在 JVM 里面采用了分代收集，在新生代收集的时候 minor gc 只需要扫描新生代，而不需要扫描老生代。minor gc 怎么判断是否有老生代的对象引用了新生代的对象，JVM 采用了卡片标记的策略，卡片标记将老生代分成了一块一块的，划分以后的每一个块就叫做一个卡片，JVM 采用卡表维护了每一个块的状态，当 JAVA 程序运行的时候，如果发现老生代对象引用或者释放了新生代对象的引用，那么就 JVM 就将卡表的状态设置为脏状态，这样每次 minor gc 的时候就会只扫描被标记为脏状态的卡片，而不需要扫描整个堆。&lt;/p&gt;

&lt;p&gt;上面说了 Jvm 需要判断对象是否有引用存在，而 Java 中的引用又分为了如下几种，不同种类的引用对垃圾收集有不同的影响，下面我们分开描述一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）Strong Reference(强引用)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;强引用是 JAVA 中默认采用的一种方式，我们平时创建的引用都属于强引用。如果一个对象没有强引用，那么对象就会被回收。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public void testStrongReference(){
    Object referent = new Object();
    Object strongReference = referent;
    referent = null;
    System.gc();
    assertNotNull(strongReference);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;2）Soft Reference(软引用)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;软引用的对象在 GC 的时候不会被回收，只有当内存不够用的时候才会真正的回收，因此软引用适合缓存的场合，这样使得缓存中的对象可以尽量的再内存中待长久一点。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Public void testSoftReference(){
    String  str =  &quot;test&quot;;
    SoftReference&amp;lt;String&amp;gt; softreference = new SoftReference&amp;lt;String&amp;gt;(str);
    str=null;
    System.gc();
    assertNotNull(softreference.get());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;3）Weak Reference(弱引用)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;弱引用有利于对象更快的被回收，假如一个对象没有强引用只有弱引用，那么在 GC 后，这个对象肯定会被回收。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Public void testWeakReference(){
    String  str =  &quot;test&quot;;
    WeakReference&amp;lt;String&amp;gt; weakReference = new WeakReference&amp;lt;String&amp;gt;(str);
    str=null;
    System.gc();
    assertNull(weakReference.get());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;4）Phantom reference(幽灵引用)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;幽灵引用说是引用，但是你不能通过幽灵引用来获取对象实例，它主要目的是为了当设置了幽灵引用的对象在被回收的时候可以收到通知。&lt;/p&gt;

&lt;p&gt;跟踪收集器常见的有如下几种：&lt;/p&gt;

&lt;h3 id=&quot;mark-sweep-collector-&quot;&gt;4.2.1 Mark-Sweep Collector(标记-清除收集器）&lt;/h3&gt;

&lt;p&gt;标记清除收集器最早由Lisp的发明人于1960年提出，标记清除收集器停止所有的工作，从根扫描每个活跃的对象，然后标记扫描过的对象，标记完成以后，清除那些没有被标记的对象。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解决循环引用的问题&lt;/li&gt;
  &lt;li&gt;不需要编译器的配合，从而就不执行额外的指令&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每个活跃的对象都要进行扫描，收集暂停的时间比较长。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;copying-collector&quot;&gt;4.2.2 Copying Collector(复制收集器）&lt;/h3&gt;

&lt;p&gt;复制收集器将内存分为两块一样大小空间，某一个时刻，只有一个空间处于活跃的状态，当活跃的空间满的时候，GC就会将活跃的对象复制到未使用的空间中去，原来不活跃的空间就变为了活跃的空间。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;只扫描可以到达的对象，不需要扫描所有的对象，从而减少了应用暂停的时间&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;需要额外的空间消耗，某一个时刻，总是有一块内存处于未使用状态&lt;/li&gt;
  &lt;li&gt;复制对象需要一定的开销&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mark-compact-collector-&quot;&gt;4.2.3 Mark-Compact Collector(标记-整理收集器）&lt;/h3&gt;

&lt;p&gt;标记整理收集器汲取了标记清除和复制收集器的优点，它分两个阶段执行，在第一个阶段，首先扫描所有活跃的对象，并标记所有活跃的对象，第二个阶段首先清除未标记的对象，然后将活跃的的对象复制到堆得底部。&lt;/p&gt;

&lt;p&gt;Mark-compact 策略极大的减少了内存碎片，并且不需要像 Copy Collector 一样需要两倍的空间。&lt;/p&gt;

&lt;h1 id=&quot;hotspot-jvm-&quot;&gt;5. HotSpot JVM 垃圾收集策略&lt;/h1&gt;

&lt;p&gt;GC 的执行时要耗费一定的 CPU 资源和时间的，因此在 JDK1.2 以后，JVM 引入了分代收集的策略，其中对新生代采用 ”Mark-Compact” 策略，而对老生代采用了 “Mark-Sweep” 的策略。其中新生代的垃圾收集器命名为 “minor gc”，老生代的 GC 命名为 ”Full Gc 或者Major GC”。其中用 &lt;code&gt;System.gc()&lt;/code&gt; 强制执行的是 Full GC。&lt;/p&gt;

&lt;p&gt;HotSpot JVM 的垃圾收集器按照并发性可以分为如下三种类型：&lt;/p&gt;

&lt;h2 id=&quot;serial-collector&quot;&gt;5.1 串行收集器（Serial Collector）&lt;/h2&gt;

&lt;p&gt;Serial Collector 是指任何时刻都只有一个线程进行垃圾收集，这种策略有一个名字 &lt;code&gt;stop the whole world&lt;/code&gt;，它需要停止整个应用的执行。这种类型的收集器适合于单CPU的机器。&lt;/p&gt;

&lt;p&gt;Serial Collector 有如下两个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）Serial Copying Collector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此种 GC 用 &lt;code&gt;-XX:UseSerialGC&lt;/code&gt; 选项配置，它只用于新生代对象的收集。&lt;/p&gt;

&lt;p&gt;JDK 1.5.0 以后 &lt;code&gt;-XX:MaxTenuringThreshold&lt;/code&gt; 用来设置对象复制的次数。当 eden 空间不够的时候，GC 会将 eden 的活跃对象和一个名叫 From survivor 空间中尚不够资格放入 Old 代的对象复制到另外一个名字叫 To Survivor 的空间。而此参数就是用来说明到底 From survivor 中的哪些对象不够资格，假如这个参数设置为31，那么也就是说只有对象复制31次以后才算是有资格的对象。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这里需要注意几个个问题：&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;From Survivor 和 To survivor的角色是不断的变化的，同一时间只有一块空间处于使用状态，这个空间就叫做 From Survivor 区，当复制一次后角色就发生了变化。&lt;/li&gt;
    &lt;li&gt;如果复制的过程中发现 To survivor 空间已经满了，那么就直接复制到 old generation。&lt;/li&gt;
    &lt;li&gt;比较大的对象也会直接复制到Old generation，在开发中，我们应该尽量避免这种情况的发生。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;2）Serial Mark-Compact Collector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;串行的标记-整理收集器是 JDK5 update6 之前默认的老生代的垃圾收集器，此收集使得内存碎片最少化，但是它需要暂停的时间比较长&lt;/p&gt;

&lt;h2 id=&quot;parallel-collector&quot;&gt;5.2 并行收集器（Parallel Collector）&lt;/h2&gt;

&lt;p&gt;Parallel Collector 主要是为了应对多 CPU，大数据量的环境。Parallel Collector又可以分为以下三种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）Parallel Copying Collector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此种 GC 用 &lt;code&gt;-XX:UseParNewGC&lt;/code&gt; 参数配置，它主要用于新生代的收集，此 GC 可以配合CMS一起使用，适用于1.4.1以后。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2）Parallel Mark-Compact Collector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此种 GC 用 &lt;code&gt;-XX:UseParallelOldGC&lt;/code&gt; 参数配置，此 GC 主要用于老生代对象的收集。适用于1.6.0以后。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3）Parallel scavenging Collector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此种 GC 用 &lt;code&gt;-XX:UseParallelGC&lt;/code&gt; 参数配置，它是对新生代对象的垃圾收集器，但是它不能和CMS配合使用，它适合于比较大新生代的情况，此收集器起始于 jdk 1.4.0。它比较适合于对吞吐量高于暂停时间的场合。&lt;/p&gt;

&lt;h2 id=&quot;concurrent-collector&quot;&gt;5.3 并发收集器 (Concurrent Collector)&lt;/h2&gt;

&lt;p&gt;Concurrent Collector 通过并行的方式进行垃圾收集，这样就减少了垃圾收集器收集一次的时间，在 HotSpot JVM 中，我们称之为 &lt;code&gt;CMS GC&lt;/code&gt;，这种 GC 在实时性要求高于吞吐量的时候比较有用。此种 GC 可以用参数 &lt;code&gt;-XX:UseConcMarkSweepGC&lt;/code&gt; 配置，此 GC 主要用于老生代和 Perm 代的收集。&lt;/p&gt;

&lt;p&gt;CMS GC有可能出现并发模型失败：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;CMS GC 在运行的时候，用户线程也在运行，当 GC 的速度比新增对象的速度慢的时候，或者说当正在 GC 的时候，老年代的空间不能满足用户线程内存分配的需求的时候，就会出现并发模型失败，出现并发模型失败的时候，JVM 会触发一次 &lt;code&gt;stop-the-world&lt;/code&gt; 的 Full GC 这将导致暂停时间过长。不过 CMS GC 提供了一个参数 &lt;code&gt;-XX:CMSInitiatingOccupancyFraction&lt;/code&gt; 来指定当老年代的空间超过某个值的时候即触发 GC，因此如果此参数设置的过高，可能会导致更多的并发模型失败。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;并发和并行收集器区别：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;并发收集器是指垃圾收集器线程和应用线程可以并发的执行，也就是清除的时候不需要 &lt;code&gt;stop the world&lt;/code&gt;，但是并行收集器指的的是可以多个线程并行的进行垃圾收集，并行收集器还是要暂停应用的&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;hotspot-jvm--1&quot;&gt;6. HotSpot Jvm 垃圾收集器的配置策略&lt;/h1&gt;

&lt;p&gt;下面我们分两种情况来分别描述一下不同情况下的垃圾收集配置策略。&lt;/p&gt;

&lt;h2 id=&quot;section-19&quot;&gt;6.1 吞吐量优先&lt;/h2&gt;

&lt;p&gt;吞吐量是指 GC 的时间与运行总时间的比值，比如系统运行了100 分钟，而 GC 占用了一分钟，那么吞吐量就是 99%，吞吐量优先一般运用于对响应性要求不高的场合，比如 web 应用，因为网络传输本来就有延迟的问题，GC 造成的短暂的暂停使得用户以为是网络阻塞所致。&lt;/p&gt;

&lt;p&gt;吞吐量优先可以通过 &lt;code&gt;-XX:GCTimeRatio&lt;/code&gt; 来指定。当通过 &lt;code&gt;-XX:GCTimeRatio&lt;/code&gt; 不能满足系统的要求以后，我们可以更加细致的来对 JVM 进行调优。&lt;/p&gt;

&lt;p&gt;首先因为要求高吞吐量，这样就需要一个较大的 Young generation，此时就需要引入 &lt;code&gt;Parallel scavenging Collector&lt;/code&gt; ，可以通过参数：&lt;code&gt;-XX:UseParallelGC&lt;/code&gt;来配置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -server -Xms3072m -Xmx3072m -XX:NewSize=2560m -XX:MaxNewSize=2560 -XX:SurvivorRatio=2 -XX:+UseParallelGC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当年轻代使用了 &lt;code&gt;Parallel scavenge collector&lt;/code&gt; 后，老生代就不能使用 &lt;code&gt;CMS GC&lt;/code&gt; 了，在 JDK1.6 之前，此时老生代只能采用串行收集，而 JDK1.6 引入了并行版本的老生代收集器，可以用参数 &lt;code&gt;-XX:UseParallelOldGC&lt;/code&gt; 来配置。&lt;/p&gt;

&lt;p&gt;1.控制并行的线程数&lt;/p&gt;

&lt;p&gt;缺省情况下，&lt;code&gt;Parallel scavenging Collector&lt;/code&gt; 会开启与 cpu 数量相同的线程进行并行的收集，但是也可以调节并行的线程数。假如你想用4个并行的线程去收集 Young generation 的话，那么就可以配置 &lt;code&gt;-XX:ParallelGCThreads=4&lt;/code&gt;，此时JVM的配置参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -server -Xms3072m -Xmx3072m -XX:NewSize=2560m -XX:MaxNewSize=2560 -XX:SurvivorRatio=2 -XX:+UseParallelGC -XX:ParallelGCThreads=4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.自动调节新生代&lt;/p&gt;

&lt;p&gt;在采用了 &lt;code&gt;Parallel scavenge collector&lt;/code&gt; 后，此 GC 会根据运行时的情况自动调节 survivor ratio 来使得性能最优，因此 &lt;code&gt;Parallel scavenge collector&lt;/code&gt; 应该总是开启 &lt;code&gt;-XX:+UseAdaptiveSizePolicy&lt;/code&gt; 参数。此时JVM的参数配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -server -Xms3072m -Xmx3072m -XX:+UseParallelGC -XX:ParallelGCThreads=4 -XX:+UseAdaptiveSizePolicy
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-20&quot;&gt;6.2 响应时间优先&lt;/h2&gt;

&lt;p&gt;响应时间优先是指 GC 每次运行的时间不能太久，这种情况一般使用与对及时性要求很高的系统，比如股票系统等。&lt;/p&gt;

&lt;p&gt;响应时间优先可以通过参数 &lt;code&gt;-XX:MaxGCPauseMillis &lt;/code&gt;来配置，配置以后 JVM 将会自动调节年轻代，老生代的内存分配来满足参数设置。&lt;/p&gt;

&lt;p&gt;在一般情况下，JVM 的默认配置就可以满足要求，只有默认配置不能满足系统的要求时候，才会根据具体的情况来对 JVM 进行性能调优。如果采用默认的配置不能满足系统的要求，那么此时就可以自己动手来调节。此时 Young generation 可以采用 &lt;code&gt;Parallel copying collector&lt;/code&gt;，而 Old generation 则可以采用 &lt;code&gt;Concurrent Collector&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;举个例子来说，以下参数设置了新生代用 Parallel Copying Collector，老生代采用 CMS 收集器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -server -Xms512m -Xmx512m -XX:NewSize=64m -XX:MaxNewSize=64m -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时需要注意两个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.如果没有指定 &lt;code&gt;-XX:+UseParNewGC&lt;/code&gt;，则采用默认的非并行版本的 copy collector&lt;/li&gt;
  &lt;li&gt;2.如果在一个单 CPU 的系统上设置了 &lt;code&gt;-XX:+UseParNewGC&lt;/code&gt;，则默认还是采用缺省的copy collector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1.控制并行的线程数&lt;/p&gt;

&lt;p&gt;默认情况下，Parallel copy collector 启动和 CPU 数量一样的线程，也可以通过参数 &lt;code&gt;-XX:ParallelGCThreads&lt;/code&gt; 来指定，比如你想用 4 个线程去进行并发的复制收集，那么可以改变上述参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -server -Xms512m -Xmx512m -XX:NewSize=64m -XX:MaxNewSize=64m -XX:SurvivorRatio=2 -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.控制并发收集的临界值&lt;/p&gt;

&lt;p&gt;默认情况下，CMS GC在 old generation 空间占用率高于 68% 的时候，就会进行垃圾收集，而如果想控制收集的临界值，可以通过参数：&lt;code&gt;-XX:CMSInitiatingOccupancyFraction&lt;/code&gt; 来控制，比如改变上述的JVM配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -server -Xms512m -Xmx512m -XX:NewSize=64m -XX:MaxNewSize=64m -XX:SurvivorRatio=2 -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:CMSInitiatingOccupancyFraction=35
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外顺便说一个参数：&lt;code&gt;-XX:+PrintCommandLineFlags&lt;/code&gt; 通过此参数可以知道在没有显示指定内存配置和垃圾收集算法的情况下，JVM 采用的默认配置。&lt;/p&gt;

&lt;p&gt;比如我在自己的机器上面通过如下命令 &lt;code&gt;java -XX:+PrintCommandLineFlags -version&lt;/code&gt; 得到的结果如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-XX:InitialHeapSize=1055308032 -XX:MaxHeapSize=16884928512 -XX:ParallelGCThreads=8 -XX:+PrintCommandLineFlags -XX:+UseCompressedOops -XX:+UseParallelGC
java version &quot;1.6.0_45&quot;
Java(TM) SE Runtime Environment (build 1.6.0_45-b06)
Java HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode)
You have new mail in /var/spool/mail/root
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从输出可以清楚的看到JVM通过自己检测硬件配置而给出的缺省配置。&lt;/p&gt;

&lt;h1 id=&quot;section-21&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://imtiger.net/blog/2010/02/21/jvm-memory-and-gc/&quot;&gt;Jvm内存模型以及垃圾收集策略解析系列(一)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://imtiger.net/blog/2010/02/21/jvm-memory-and-gc-2/&quot;&gt;Jvm内存模型以及垃圾收集策略解析系列(二)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ibm.com/developerworks/library/j-jtp10283/index.html?S_TACT=105AGX52&amp;amp;S_CMP=cn-a-j&quot;&gt;Java theory and practice: A brief history of garbage collection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ibm.com/developerworks/library/j-jtp11253/index.html?S_TACT=105AGX52&amp;amp;S_CMP=cn-a-j&quot;&gt;Java theory and practice: Garbage collection in the HotSpot JVM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.oracle.com/poonam/entry/understanding_cms_gc_logs&quot;&gt;Understanding CMS GC Logs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.oracle.com/javase/6/docs/technotes/guides/vm/server-class.html&quot;&gt;Java HotSpot VM Options Server-Class Machine Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/04/09/note-about-jvm-memery-model.html</link>
      <guid>http://blog.javachen.com/2014/04/09/note-about-jvm-memery-model.html</guid>
      <pubDate>2014-04-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>RHEL系统下安装atlassian-jira-5</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;部署环境&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：RHEL 6.4 x86_64&lt;/li&gt;
  &lt;li&gt;Jira版本：&lt;code&gt;atlassian-jira-5.2.11-x64.bin&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;安装路径:&lt;code&gt;/opt/atlassian/jira/&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;数据保存路径：&lt;code&gt;/opt/atlassian/application-data/jira&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;安装用户：jira&lt;/li&gt;
  &lt;li&gt;数据库：postgresql&lt;/li&gt;
  &lt;li&gt;JDK：1.6.0_43&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;jira下载页面：&lt;a href=&quot;https://www.atlassian.com/software/jira/download&quot;&gt;https://www.atlassian.com/software/jira/download&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;安装步骤&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;运行安装文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ . atlassian-jira-5.2.11-x64.bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在安装过程中会出现选项：&lt;/p&gt;

&lt;p&gt;确认安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This will install JIRA 6.2.2 on your computer.
OK [o, Enter], Cancel [c]
o
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;选择安装类型－1默认安装 －2自定义安装 －3升级&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Choose the appropriate installation or upgrade option.
Please choose one of the following:
Express Install (use default settings) [1], Custom Install (recommended for advanced users) [2], Upgrade an existing JIRA installation [3, Enter]
3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;你可以选择2自定义安装路径、启动端口等等。&lt;/p&gt;

&lt;p&gt;接下来选择确认直到安装成功。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;初始化数据库&lt;/h2&gt;

&lt;p&gt;这里我选择PostgreSql数据库，先安装数据库，然后创建用户(jira)和数据库(jira)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ su - postgres
-bash-4.1$ cd
-bash-4.1$ cd bin
-bash-4.1$ ./psql -U postgres
psql (9.0.3)
Type &quot;help&quot; for help.

Cannot read termcap database;
using dumb terminal settings.
postgres=# CREATE USER jira WITH PASSWORD &#39;redhat&#39;;
postgres=# CREATE DATABASE jira owner=jira;
postgres=# GRANT ALL privileges ON DATABASE jira TO jira;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后打开浏览器范围jira页面：&lt;code&gt;http://ip:8080/&lt;/code&gt;，在该页面选择数据库类型，并填写数据库连接信息，测试是否可以ping通，接着运行下一步,在jira官网上注册一个帐号。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;破解&lt;/h2&gt;

&lt;p&gt;破解文件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://download.csdn.net/detail/royalapex/6710573&quot;&gt;atlassian-extras-2.2.2.jar&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://download.csdn.net/detail/royalapex/6710589&quot;&gt;atlassian-extras-2.2.2.crack&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;执行以下命令覆盖原来文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp atlassian-extras-2.2.2.crack /opt/atlassian/jira/atlassian-jira/WEB-INF/classes/
$ \cp atlassian-extras-2.2.2.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查是否可以创建issue，并查看jira版本和过期时间。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;汉化&lt;/h2&gt;

&lt;p&gt;下载JIRA汉化包：&lt;a href=&quot;http://download.csdn.net/detail/royalapex/6711881&quot;&gt;JIRA-5.0-language-pack-zh_CN.jar&lt;/a&gt;,并在jira管理页面将其上传，然后在个人设置页面，可以设置语言为中文。&lt;/p&gt;

&lt;h2 id=&quot;jira&quot;&gt;JIRA使用&lt;/h2&gt;

&lt;p&gt;启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /opt/atlassian/jira/bin
$ sh start-jira.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /opt/atlassian/jira/bin
$ sh stop-jira.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /opt/atlassian/jira/bin
$ tailf catalina.out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;## 修改JVM参数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ vi /opt/atlassian/jira/bin/setenv.sh
JAVA_OPTS=&quot;-Xms1024m -Xmx2048m -XX:MaxPermSize=256m $JAVA_OPTS -Djava.awt.headless=true &quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/04/09/install-jira5-on-rhel-system.html</link>
      <guid>http://blog.javachen.com/2014/04/09/install-jira5-on-rhel-system.html</guid>
      <pubDate>2014-04-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>PostgreSQL测试工具PGbench</title>
      <description>&lt;p&gt;pgbench 是一个简单的给 PostgreSQL 做性能测试的程序。它反复运行同样的 SQL 命令序列，可能是在多个并发数据库会话上头，然后检查平均的事务速度（每秒的事务数 tps）。缺省的时候，pgbench 测试一个（松散的）接近 TPC-B 的情况，每个事务包括五个 SELECT，UPDATE，和 INSERT命令。不过，我们可以很轻松地使用自己的事务脚本文件来实现其它情况。&lt;/p&gt;

&lt;p&gt;典型的输出看上去会是这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;transaction type: TPC-B (sort of)
scaling factor: 10
number of clients: 10
number of transactions per client: 1000
number of transactions actually processed: 10000/10000
tps = 85.184871 (including connections establishing)
tps = 85.296346 (excluding connections establishing)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;头四行只是报告一些最重要的参数设置。跟着的一行报告完成的事务数和期望完成的事务数（后者只是客户端数乘以事务数）；这两个会相等，除非在完成之前运行就失败了。最后两行报告 TPS 速率，分别有计算启动数据库会话时间和不计算启动会话时间的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;使用环境：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在比较新的9.1，9.2，9.3数据库的发行版本中,pgbench是在安装contrib包时直接编译的,可以在postgres的bin目录下找到该命令，如果没有发现该命令可以在安装contrib的目录下找到pgbench的源码文件包，编译一下就可以使用。&lt;/p&gt;

&lt;h1 id=&quot;pgbench&quot;&gt;1. pgbench测试库初始化&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;postgres$ pgbench --help                 # 和postgres其他命令的使用方式一样，--help获取命令使用方式的简单介绍
postgres$ createdb pgbench               # 创建测试库
postgres$ pgbench -i pgbench             # 初始化测试库
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认会在测试库中建4张表&lt;code&gt;pgbench_accounts&lt;/code&gt;，&lt;code&gt;pgbench_branches&lt;/code&gt;，&lt;code&gt;pgbench_history&lt;/code&gt;，&lt;code&gt;pgbench_tellers&lt;/code&gt; 。当然也可以自己建表，自己写测试脚本，这四张表只是默认的测试脚本会用到。&lt;/p&gt;

&lt;p&gt;pgbench在建默认库时 &lt;code&gt;-s&lt;/code&gt; 参数设定测设表的大小，默认参数是1 。&lt;code&gt;pgbench_accounts&lt;/code&gt; 总行数是10W,&lt;code&gt;-s&lt;/code&gt;后面接具体数值如100，则&lt;code&gt;pgbench_accounts&lt;/code&gt;中的测试数据将达到1千万行&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;2. 默认的测试脚本介绍&lt;/h1&gt;

&lt;p&gt;默认的测试脚本可以在官方文档中找到（新的版本中不指定模板就会使用默认模板）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat test.sql
\set nbranches 1 * :scale
\set ntellers 10 * :scale
\set naccounts 100000 * :scale
\setrandom aid 1 :naccounts
\setrandom bid 1 :nbranches
\setrandom tid 1 :ntellers
\setrandom delta -5000 5000
BEGIN;
UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;
SELECT abalance FROM pgbench_accounts WHERE aid = :aid;
UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;
UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;
INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);
END;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;脚本说明：&lt;/p&gt;

&lt;p&gt;可以看到脚本中的一个事物包含了update,select,insert操作，不同的操作起到不同的测试目的&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;（1）UPDATE pgbench_accounts:作为最大的表，起到促发磁盘I/O的作用。&lt;/li&gt;
  &lt;li&gt;（2）SELECT abalance:由于上一条UPDATE语句更新一些信息，存在于缓存内用于回应这个查询。&lt;/li&gt;
  &lt;li&gt;（3）UPDATE pgbench_tellers:职员的数量比账号的数量要少得多，所以这个表也很小，并且极有可能存在于内存中。&lt;/li&gt;
  &lt;li&gt;（4）UPDATE pgbench_branches:作为更小的表，内容被缓存，如果用户的环境是数量较小的数据库和多个客户端时，对其锁操作可能会成为性能的瓶颈。&lt;/li&gt;
  &lt;li&gt;（5）INSERT INTO pgbench_history:history表是个附加表，后续并不会进行更新或查询操作，而且也没有任何索引。相对于UPDATE语句，对其的插入操作对磁盘的写入成本也很小。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;3. 测试结果说明&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;postgres$ pgbench -c 15 -t 300 pgbench -r -f test.sql             #执行命令
starting vacuum...end.
transaction type: Custom query
scaling factor: 1
query mode: simple 
number of clients: 15                                            #-c参数控制并发量
number of threads: 1                                                    
number of transactions per client: 300                           #每个客户端执行事务的数量
number of transactions actually processed: 4500/4500             #总执行量
tps = 453.309203 (including connections establishing)            #tps每秒钟处理的事务数包含网络开销      
tps = 457.358998 (excluding connections establishing)            #不包含网络开销
statement latencies in milliseconds:                             #带-r的效果，每个客户端事务具体的执行时间，单位是毫秒
0.005198 \set nbranches 1 * :scale                               
0.001144 \set ntellers 10 * :scale
0.001088 \set naccounts 100000 * :scale                     
0.001400 \setrandom aid 1 :naccounts
0.000814 \setrandom bid 1 :nbranches
0.000929 \setrandom tid 1 :ntellers
0.000981 \setrandom delta -5000 5000
0.613757 BEGIN;
1.027969 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;
0.754162 SELECT abalance FROM pgbench_accounts WHERE aid = :aid;
14.167980 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;
13.587156 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;
0.582075 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);
1.628262 END;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认的基准测试给出了一个指标TPS，同样的测试参数，tps的值越高，相对来说服务器的性能越好。上面的测试由于数据量的问题，表的内容全部缓存进了内存,磁盘io对上面的结果影响较小。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;4. 自定义测试环境&lt;/h1&gt;

&lt;p&gt;在实际的应用中测试可以自己定义测试环境，模拟生产需求。&lt;/p&gt;

&lt;p&gt;测试举例&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pgbench=# create table pg_test (a1 serial,a2 int,a3 varchar(20),a4 timestamp);                        #创建测试表
postgres$cat pg_test.sql
pgbench=# insert into pg_test(a2,a3,a4) select (random()*(2*10^5)),substr(&#39;abcdefghijklmnopqrstuvwxyz&#39;,1, (random()*26)::integer),now();
                                                                                                      #每个事务插入一条数据 
postgres$pgbench -c 90 -T 10 pgbench -r -f pg_test.sql                                                   #90个并发测试每秒插入的数据量
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试结果截取：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;number of transactions actually processed: 20196             #10秒钟90并发用户共插入20196条数据，每条数据插入费时42ms，平均每秒插入2000条数据 
tps = 1997.514876 (including connections establishing)
tps = 2119.279239 (excluding connections establishing)
statement latencies in milliseconds:
42.217948
 
pgbench=# select count(*) from pg_test;
count 
-------
20196
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;pgbench-1&quot;&gt;5. pgbench在参数调节上的辅助使用&lt;/h1&gt;

&lt;p&gt;简单举例：&lt;code&gt;work_mem&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;postgres=# show work_mem ;                                              #数据库当前的work_mem
work_mem 
----------
1MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查询样本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;postgres$cat select.sql
SELECT customerid FROM customers ORDER BY zip;                          #orders表是一张postgres样例表，样例库全名dellstore2
postgres$pgbench -c 90 -T 5 pgbench -r -f select.sql                    #多用户并发做单表排序操作单个事务执行的时间可能会很大，但是平均事务执行时间和单个用户的执行时间差距没那么明显。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行结果截取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;number of clients: 90
number of threads: 1
duration: 5 s
number of transactions actually processed: 150
tps = 26.593887 (including connections establishing)
tps = 27.972988 (excluding connections establishing)
statement latencies in milliseconds:
3115.754673 SELECT customerid FROM customers ORDER BY zip;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试环境相同调节&lt;code&gt;work_mem&lt;/code&gt;参数为2M试试&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;number of clients: 90
number of threads: 1
duration: 5 s
number of transactions actually processed: 243
tps = 44.553026 (including connections establishing)
tps = 47.027276 (excluding connections establishing)
statement latencies in milliseconds:
1865.636761 SELECT customerid FROM customers ORDER BY zip;             #5s内事务执行的总量明显增加一共做了243次单表排序
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因分析，由于排序操作会关系到&lt;code&gt;work_mem&lt;/code&gt;，排序操作能全在缓存中进行当然速度会明显加快，查看执行计划&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres=# explain analyze SELECT customerid FROM customers ORDER BY zip;
QUERY PLAN 

--------------------------------------------------------------------------------------------
Sort (cost=2116.77..2166.77 rows=20000 width=8) (actual time=42.536..46.117 rows=20000 loo
ps=1)
Sort Key: zip
Sort Method: external sort Disk: 352kB
-&amp;gt; Seq Scan on customers (cost=0.00..688.00 rows=20000 width=8) (actual time=0.013..8.9
42 rows=20000 loops=1)
Total runtime: 48.858 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由上面的执行计划可以看出在&lt;code&gt;work_mem&lt;/code&gt;大小为1M的时候排序一共需要1.352M空间做排序,所以加大&lt;code&gt;work_mem&lt;/code&gt;参数排序速度明显增加。&lt;/p&gt;

&lt;p&gt;这只是个简单的例子，&lt;code&gt;work_mem&lt;/code&gt;的大小调节还有很多其他方面要考虑的，比如在高并发的情况下，需要为每个用户分配同样大小的排序空间，会占用大量的内存空间。参数调节在任何时候保持一个均衡才是应该考虑的。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://www.pgsqldb.com:8079/mwiki/index.php/PGbench&quot;&gt;PGbench&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/04/08/a-benchmark-tool-on-postgresql.html</link>
      <guid>http://blog.javachen.com/2014/04/08/a-benchmark-tool-on-postgresql.html</guid>
      <pubDate>2014-04-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>PostgreSQL监控指标</title>
      <description>&lt;p&gt;数据库版本：9.3.1（不同版本数据库相关表列名可能略有不同）&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;数据库状态信息&lt;/h1&gt;

&lt;p&gt;数据库状态信息主要体现数据库的当前状态&lt;/p&gt;

&lt;p&gt;1.目前客户端的连接数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres=# SELECT count(*) FROM pg_stat_activity WHERE NOT pid=pg_backend_pid();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.连接状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres=# SELECT pid,waiting,current_timestamp - least(query_start,xact_start) AS runtime,substr(query,1,25) AS current_query 
FROM pg_stat_activity WHERE NOT pid=pg_backend_pid();
 pid  | waiting | runtime         | current_query 
------+---------+-----------------+-----------------------
9381  | f       | 00:01:24.425487 | select * from orders;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以查看每个连接的pid，执行的操作，是否发生等待，根据查询,或者事务统计开始时间等等。有多少个链接查询就有多少行 所以可以用order by,limit限制查询行数&lt;/p&gt;

&lt;p&gt;3.数据库占用空间&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres=# select pg_size_pretty(pg_database_size(&#39;postgres&#39;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个数据库数据文件对应存储在以这个数据库PID命名的文件中,通过统计所有以PID命名文件的总大小，也可以得出这个数据库占用的空间。&lt;/p&gt;

&lt;p&gt;表占用的空间使用&lt;code&gt;pg_relation_size()&lt;/code&gt;查询，对应的系统中的文件名和pg_class中的filenode相同，一个热点的表最好一天一统计大小，获得表的一个增长状况，来预测数据库未来可能的增长状况。根据需求提前准备空间应付数据库的增长。&lt;/p&gt;

&lt;p&gt;4.等待事件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres# SELECT bl.pid AS blocked_pid, a.usename AS blocked_user, kl.pid AS blocking_pid, ka.usename AS blocking_user, a.query AS blocked_statement 
FROM pg_locks bl 
JOIN pg_stat_activity a ON a.pid = bl.pid 
JOIN pg_locks kl ON kl.transactionid = bl.transactionid AND kl.pid != bl.pid 
JOIN pg_stat_activity ka ON ka.pid = kl.pid WHERE NOT bl.granted;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据阻塞的语句的会话PID做相应处理&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;数据库统计信息&lt;/h1&gt;

&lt;p&gt;1.sql语句统计&lt;/p&gt;

&lt;p&gt;实现上述统计需要安装一个pg的extension:&lt;code&gt;pg_stat_statements–1.1.sql&lt;/code&gt;，调整postgres.conf:&lt;code&gt;shared_preload_libraries = &#39;pg_stat_statements&#39;&lt;/code&gt;,就可以使用了&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres=# SELECT calls, total_time, rows, 100.0 * shared_blks_hit /nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent,substr(query,1,25)
FROM pg_stat_statements ORDER BY total_time DESC LIMIT 5;
calls  | total_time | rows | hit_percent          | substr 
-------+------------+------+----------------------+---------------------------
1      | 23.104     | 17   | 99.4974874371859296  | SELECT n.nspname as Sche
1      | 21.808     | 2    |                      | select * from pg_stat_sta
2      | 5.391      | 2    |                      | SELECT name FROM (SELECT
3      | 3.307      | 57   | 100.0000000000000000 | SELECT pg_catalog.quote_i
4      | 1.318      | 19   | 100.0000000000000000 | SELECT calls, total_time,
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上述查询是按照查询的执行时间来排序的，可以定位执行比较慢的sql,也可以用来查出常用sql，以及sql共享内存的命中率等信息&lt;/p&gt;

&lt;p&gt;2.表的共享内存的利用情况统计&lt;/p&gt;

&lt;p&gt;实现上述统计需要安装一个pg的extension:&lt;code&gt;pg_buffercache–1.0.sql&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres=# SELECT c.relname, count(*) AS buffers 
FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) 
AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 5;
relname                         | buffers 
--------------------------------+---------
pg_proc                         | 28
pg_attribute                    | 23
pg_operator                     | 14
pg_proc_proname_args_nsp_index  | 10
pg_class                        | 9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表在共享内存中占用的块数，用来查看表是不是在内存中，buffers的单位是数据块，默认8K，如果计算大小等于表的大小，说明全表的数据都在缓存中，这时的查询速度是很快的&lt;/p&gt;

&lt;p&gt;3.对一个表执行操作的统计&lt;/p&gt;

&lt;p&gt;实现统计对一个表操作的偏重，insert,update,delete的比率&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres=# update products set price=11.00 where prod_id=30;
UPDATE 1
postgres=# delete from products where prod_id=30;
DELETE 1
postgres=# SELECT relname,cast(n_tup_ins AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS ins_pct,
cast(n_tup_upd AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS upd_pct, 
cast(n_tup_del AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS del_pct 
FROM pg_stat_user_tables where relname=&#39;products&#39;;
relname   | ins_pct                | upd_pct                | del_pct 
----------+------------------------+------------------------+------------------------
products  | 0.00000000000000000000 | 0.50000000000000000000 | 0.50000000000000000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.表的IO和索引的IO&lt;/p&gt;

&lt;p&gt;表的IO主要涉及查询的时候查询的逻辑块和物理块，归到底也是命中率的问题，当然最简单的思维方式，一张表存在内存中的内容越多，相应的查询速度越快。相关视图：&lt;code&gt;pg_stat_user_tables&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;相对于表的大小来说，索引占用的空间要小的多，所以常用的表，可以让其索引一直存在内存中，很多时候保持索引的一个高命中率是非常必要的。相关视图: &lt;code&gt;pg_stat_user_indexes&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres# SELECT indexrelname,cast(idx_blks_hit as numeric) / (idx_blks_hit + idx_blks_read) AS hit_pct,
idx_blks_hit,idx_blks_read 
FROM pg_statio_user_indexes WHERE (idx_blks_hit + idx_blks_read)&amp;gt;0 ORDER BY hit_pct;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5.buffer background 和 checkpoint&lt;/p&gt;

&lt;p&gt;涉及检查点写和后台写的比率问题，检查点的集中数据写入会对数据库IO的性能有很大的提升，但相应的需要部分空间存储脏数据，而且一旦数据库崩溃，内存中未被写入磁盘的脏数据越多，数据库恢复时间也就越长，这是一个数据库的平衡问题，相关问题在调优文档中做深入研究。 相关视图：&lt;code&gt;pg_stat_bgwriter&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;postgres=# SELECT
(100 * checkpoints_req) / (checkpoints_timed + checkpoints_req) AS checkpoints_req_pct,
pg_size_pretty(buffers_checkpoint * block_size / (checkpoints_timed + checkpoints_req)) AS avg_checkpoint_write,
pg_size_pretty(block_size * (buffers_checkpoint + buffers_clean + buffers_backend)) AS total_written,
100 * buffers_checkpoint / (buffers_checkpoint + buffers_clean + buffers_backend) AS checkpoint_write_pct,
100 * buffers_backend / (buffers_checkpoint + buffers_clean + buffers_backend) AS backend_write_pct,*
FROM pg_stat_bgwriter,(SELECT cast(current_setting(&#39;block_size&#39;) AS integer) AS block_size) AS bs;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;系统监控信息&lt;/h1&gt;

&lt;p&gt;介绍一些关于数据库需要查看的系统状态信息&lt;/p&gt;

&lt;p&gt;1.数据库基本状态信息&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# ps -ef | grep postgres
# netstat -altunp | grep 5432
# pg_controdata   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pg_controdata命令和psql同在postgres的bin目录下,系统命令下查询到最全面的数据库快照信息&lt;/p&gt;

&lt;p&gt;2.top动态信息配合其他命令使用(截取相关行)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# top -u postgres
Cpu(s): 1.7%us, 1.0%sy, 0.0%ni, 97.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%st
Mem: 2051164k total, 1476536k used, 574628k free, 239624k buffers
Swap: 2097144k total, 0k used, 2097144k free, 768676k cached
PID   USER    PR NI VIRT RES SHR   S %CPU %MEM TIME+ COMMAND 
11172 postgres 20 0 709m 34m 33m   S 0.0  1.7 0:00.79 postgres 
9380  postgres 20 0 167m 5284 2272 S 0.0  0.3 0:00.05 psql 
11178 postgres 20 0 709m 5168 4408 S 0.0  0.3 0:00.01 postgres 
11179 postgres 20 0 709m 4656 3920 S 0.0  0.2 0:00.01 postgres 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# free
total used free shared buffers cached
Mem: 2051164 1476032 575132 0 239624 768676
-/+ buffers/cache: 467732 1583432
Swap: 2097144 0 2097144
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# iotop -u postgres
Total DISK READ: 0.00 B/s | Total DISK WRITE: 0.00 B/s
11175 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: logger process
11181 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: autovacuum launcher process
11178 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: checkpointer process
11180 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: wal writer process
11182 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: stats collector process
11179 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: writer process
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单分析下top命令,用top可以分析出系统的当前总体的负载状况，如上例，总体负载率很低，在io等待率高的时候可以使用iotop来定位io具体的进程，top中的VIRT RES 可以看出进程希望获取的内存，和占用系统内存的数量，可以根据来判定系统内存的分配情况，内存的值也关联到一些参数的设定，如postgres在发生检查点之前checkpointer process进程会消耗比较大的物理内存，直到检查点发生后，占用的内存才会被释放掉，所以在设置检查点时间的时候也要将内存的占用考虑进去。&lt;/p&gt;

&lt;p&gt;free总体的表现内存的使用情况，buffers和cached在应用申请内存的时候会被系统释放掉，所以应用实际可以使用的内存是free的值加上buffers和cached的。&lt;/p&gt;

&lt;p&gt;3.sar做辅助分析&lt;/p&gt;

&lt;p&gt;sar类似于快照的方式来保存系统过去的信息&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# sar
03:40:01 PM CPU %user %nice %system %iowait %steal %idle
03:50:01 PM all 1.56  0.00  0.68    0.10    0.00   97.67
04:00:02 PM all 1.91  0.00  0.63    0.05    0.00   97.41
Average:    all 1.07  0.04  1.95    2.65    0.00   94.29
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# sar -r
12:40:01 PM kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit
12:50:01 PM 567124    1484040   72.35    237596    755720   1426740  34.39
01:10:01 PM 567256    1483908   72.34    237600    755720   1426740  34.39
01:20:01 PM 567132    1484032   72.35    237600    755724   1426740  34.39
Average:    742177    1308987   63.82    195809    669444   1390037  33.51
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些统计信息可以对照性能问题，查看当时的系统状态。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2014/04/07/some-metrics-in-postgresql.html</link>
      <guid>http://blog.javachen.com/2014/04/07/some-metrics-in-postgresql.html</guid>
      <pubDate>2014-04-07T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>RHEL系统安装PostgreSQL</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;环境说明&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;OS：RHEL6.4（x86_64）&lt;/li&gt;
  &lt;li&gt;postgresql版本：PostgreSQL9.2.8&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;安装步骤&lt;/h1&gt;

&lt;h2 id=&quot;postgresql-rpm&quot;&gt;1. 下载所需的PostgreSQL rpm包&lt;/h2&gt;

&lt;p&gt;基础安装：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm&lt;/li&gt;
  &lt;li&gt;postgresql92-9.2.8-1PGDG.rhel6.x86_64.rpm&lt;/li&gt;
  &lt;li&gt;postgresql92-server-9.2.8-1PGDG.rhel6.x86_64.rpm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;扩展安装：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;postgresql92-contrib-9.2.8-1PGDG.rhel6.x86_64.rpm&lt;/li&gt;
  &lt;li&gt;postgresql92-devel-9.2.8-1PGDG.rhel6.x86_64.rpm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下载地址：&lt;a href=&quot;http://yum.postgresql.org/9.2/redhat/rhel-6.4-x86_64/&quot;&gt;http://yum.postgresql.org/9.2/redhat/rhel-6.4-x86_64/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;rpm&quot;&gt;2. 安装基础的rpm包&lt;/h2&gt;

&lt;p&gt;在命令行执行如下命令进行安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rpm -ivh postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm
$ rpm -ivh postgresql92-9.2.8-1PGDG.rhel6.x86_64.rpm
$ rpm -ivh postgresql92-server-9.2.8-1PGDG.rhel6.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;按照上面的顺序安装rpm时，会报与系统的libcrypto.so.10和libssl.so.10依赖错误，错误信息如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rpm -ivh postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm 
warning: postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID 442df0f8: NOKEY
error: Failed dependencies:
 libcrypto.so.10(libcrypto.so.10)(64bit) is needed by postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64
 libssl.so.10(libssl.so.10)(64bit) is needed by postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因此，我们需要对系统的openssl进行升级。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;升级步骤&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先，使用下面的命令卸载系统的openssl：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rpm --nodeps -e openssl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，下载PostgreSQL9.2.8依赖的的openssl10并安装。&lt;/p&gt;

&lt;p&gt;下载地址：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;ftp://ftp.pbone.net/mirror/dl.iuscommunity.org/pub/ius/stable/Redhat/6/x86_64/openssl10-libs-1.0.1e-1.ius.el6.x86_64.rpm&quot;&gt;ftp://ftp.pbone.net/mirror/dl.iuscommunity.org/pub/ius/stable/Redhat/6/x86_64/openssl10-libs-1.0.1e-1.ius.el6.x86_64.rpm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;最后，重新安装PostgreSQL9.2.8的rpm包。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3. 初始化数据到自定义目录&lt;/h2&gt;

&lt;p&gt;创建自定义目录&lt;code&gt;/opt/pg/data&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir /opt/pg_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更改目录所有者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chown postgres:postgres /opt/pg_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用postgres用户初始化数据目录（每次启动数据库的时加&lt;code&gt;-D&lt;/code&gt;参数指定路径，或者修改postgres用户下的&lt;code&gt;$PGDATA&lt;/code&gt;变量为当前数据目录）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/pgsql-9.1/bin/initdb -D /opt/pg_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化数据后，会显示启动数据库的命令。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/04/07/install-postgresql-on-rhel-system.html</link>
      <guid>http://blog.javachen.com/2014/04/07/install-postgresql-on-rhel-system.html</guid>
      <pubDate>2014-04-07T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>RHEL系统安装MySQL主备环境</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;环境准备&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统： rhel6.4&lt;/li&gt;
  &lt;li&gt;数据库： percona 5.6.14&lt;/li&gt;
  &lt;li&gt;使用3306端口保证端口未被占用，selinux关闭状态&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;原理说明&lt;/h1&gt;

&lt;p&gt;mysql的复制（Replication)是一个异步的复制，从一个mysql instance(称之为master)复制到另一个mysql instance(称之为slave).实现整个复制操作主要由三个进程完成的，其中俩个进程在slave(sql进程和io进程），另外一个进程在master（IO进程）上。&lt;/p&gt;

&lt;p&gt;要实施复制，首先要打开master端的binary log(bin-log)功能，否则无法实现。因为整个复制过程实际上就是slave从master端获取该日志然后在自己升上完全顺序的执行日志中所记录的各种操作。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;配置说明&lt;/h1&gt;

&lt;h2 id=&quot;master&quot;&gt;1. 配置master并启动&lt;/h2&gt;

&lt;p&gt;拷贝配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp /usr/share/doc/Percona-Server-server-56-5.6.14/my-default.cnf /etc/my.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑&lt;code&gt;/etc/my.cnf&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mysqld]
explicit_defaults_for_timestamp=true   ##开启查询缓存
# log_bin
log_bin = mysql-bin
server_id = 36
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动mysql服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service mysql start 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;slave&quot;&gt;2. 配置slave并启动&lt;/h2&gt;

&lt;p&gt;拷贝配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp /usr/share/doc/Percona-Server-server-56-5.6.14/my-default.cnf /etc/my.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑&lt;code&gt;/etc/my.cnf&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mysqld]
explicit_defaults_for_timestamp=true 
log_bin = mysql-bin
server_id = 37
relay_log=/var/lib/mysql/mysql-relay-bin ##传送过来的日志存放目录
log_slave_updates=1
read_only=1 ##这个参数只对普通用户生效，超级用户和复制用户无效
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动mysql服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service mysql start 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3. 主从分别授权用户&lt;/h2&gt;

&lt;p&gt;在master,slave分别执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#mysql
mysql-&amp;gt;grant replication slave,replication client on *.* to repl@&#39;%&#39; identified by  &#39;123456&#39;;
mysql-&amp;gt;flush priviledges;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;4. 主库数据备份到从库&lt;/h2&gt;

&lt;p&gt;master上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mysqldump -A &amp;gt;all.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;slave上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mysql &amp;lt;all.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;5. 根据主状态启动从库&lt;/h2&gt;

&lt;p&gt;(1) 查询主库状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql-&amp;gt;show master status \G
 *************************** 1. row ***************************
             File: mysql-bin.000001
         Position: 697
     Binlog_Do_DB: 
 Binlog_Ignore_DB: 
Executed_Gtid_Set: 
1 row in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(2) 从库启动复制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#mysql
mysql-&amp;gt;change master to
        -&amp;gt;master_host=&quot;192.168.0.114&quot;,master_port=3306,master_user=&quot;repl&quot;,master_password=&quot;123456&quot;,master_log_file=&quot;mysql-bin.000001&quot;,master_log_pos=697;
mysql-&amp;gt;start slave;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(3) 从库状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show slave status \G
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 192.168.0.114
                  Master_User: repl
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000001
          Read_Master_Log_Pos: 697
               Relay_Log_File: mysql-relay-bin.000002
                Relay_Log_Pos: 563
        Relay_Master_Log_File: mysql-bin.000001
         ....Exec_Master_Log_Pos: 697
         ....
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;6. 主从用于复制的进程&lt;/h2&gt;

&lt;p&gt;在master上查看进程：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql-&amp;gt;show processlist
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1个如下状态的进程:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Master has sent all binlog to slave; waiting for binlog to be updated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在slave上查看进程：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql-&amp;gt;show processlist
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2个如下状态的进程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Waiting for master to send event
Slave has read all relay log; waiting for the slave I/O thread to update it
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-7&quot;&gt;7. 主从状态监控&lt;/h1&gt;

&lt;p&gt;常规做法是slave上&lt;code&gt;show slave status\G&lt;/code&gt;中的两个变量的差值（&lt;code&gt;Read_Master_Log_Pos&lt;/code&gt;，&lt;code&gt;Exec_Master_Log_Pos&lt;/code&gt;),也可以使用percona提供的工具包&lt;code&gt;pt-heartbeat&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;percona-tookit-pg-heartbeat&quot;&gt;8. Percona-tookit 安装及pg-heartbeat使用&lt;/h2&gt;

&lt;p&gt;工具集中包含&lt;code&gt;pt-heartbeat&lt;/code&gt;, 安装依赖perl-DBD-MySQL， perl-IO-Socket-SSL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% rpm -ivh percona-toolkit-2.2.6-1.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;pg-heartbeat功能介绍：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;监控复制延迟&lt;/li&gt;
  &lt;li&gt;测量复制落后时间&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;实现机制：&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一步，pt-heartbeat的–update线程会在指定的时间间隔更新一个时间戳。&lt;/li&gt;
  &lt;li&gt;第二步，pt-heartbeat的–monitor线程或者–check线程连接到从上检查前面更新的时间戳，和主当地时间做比较，得出时间差。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;使用例子：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;1）初始化环境，创建一个后台进程定期更新主上的test库heartbeat表,默认是一秒可以–interval指定，执行后会生成一个heartbeat表，test为需要监控的同步库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pt-heartbeat -D test --update -u repl -p 123456 -h 192.168.0.108 --create-table --daemonize
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）监控并输出slave落后时间&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pt-heartbeat -D test --monitor -u repl -p 123456 -h 192.168.0.115
0.00s [  0.00s,  0.00s,  0.00s ]           ###瞬时延迟 [一分钟平均延迟，五分钟平均延迟，十五分钟平均延迟]
0.00s [  0.00s,  0.00s,  0.00s ]
0.00s [  0.00s,  0.00s,  0.00s ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果如下 会一直输出,断开一下连接得到如下结果，最后同步&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0.00s [  0.34s,  0.07s,  0.02s ]
0.00s [  0.00s,  0.07s,  0.02s ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3）只输出瞬时的差值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#pt-heartbeat -D test --test -u repl -p 123456 -h 192.168.0.115
0.00 ###只代表瞬时延迟
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;mysql&quot;&gt;9. mysql主从互换&lt;/h2&gt;

&lt;p&gt;(1) 停止从库复制,从新设置状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql-&amp;gt;stop slave;
mysql-&amp;gt;reset master;
mysql-&amp;gt;reset slave;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(2) 如配置文件相同的情况下，配置文件无需更改。否者主备的配置文件交换。&lt;/p&gt;

&lt;p&gt;(3) 原先的主变为备&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql-&amp;gt;reset master;
mysql-&amp;gt;reset slave;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从新配置change master to参数&lt;/p&gt;

&lt;p&gt;(4) 服务器重启&lt;/p&gt;

&lt;p&gt;如原先的主中有bin日志在从上为实现同步，可以认为读取bin日志的内容，在新的主中执行&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/04/06/mysql-config-for-master-slave-replication.html</link>
      <guid>http://blog.javachen.com/2014/04/06/mysql-config-for-master-slave-replication.html</guid>
      <pubDate>2014-04-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>RHEL系统安装MySql</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;环境说明&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统:linux6.4&lt;/li&gt;
  &lt;li&gt;MySql版本：percona 5.6.14&lt;/li&gt;
  &lt;li&gt;rpm包下载地址：&lt;a href=&quot;http://www.percona.com/downloads/Percona-Server-5.6/LATEST/RPM/rhel6/x86_64&quot;&gt;http://www.percona.com/downloads/Percona-Server-5.6/LATEST/RPM/rhel6/x86_64&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;安装步骤&lt;/h1&gt;

&lt;h2 id=&quot;rpm&quot;&gt;1. 安装所需要的rpm包&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;rpm -ivh Percona-Server-shared-56-5.6.14-rel62.0.483.rhel6.x86_64.rpm
rpm -ivh Percona-Server-client-56-5.6.14-rel62.0.483.rhel6.x86_64.rpm
rpm -ivh Percona-Server-server-56-5.6.14-rel62.0.483.rhel6.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;第三个包依赖前2个包，第三个包安装时可能会报错，需要将系统中原先的mysql-libs卸载（&lt;code&gt;yum remove mysql-libs&lt;/code&gt;）&lt;/p&gt;

&lt;p&gt;没有yum使用&lt;code&gt;rpm -e --nodeps&lt;/code&gt;的方式卸载安装包，可以使用&lt;code&gt;rpm -qa | grep mysql&lt;/code&gt;的方式查看安装的包&lt;/p&gt;

&lt;h2 id=&quot;mysql&quot;&gt;2. 启动mysql&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;#service mysql start 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止：stop 重启：restart，查看状态：status&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3. 设置远程登录&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;#mysql
mysql&amp;gt; grant all PRIVILEGES on test.* to test@&#39;%&#39; identified by &#39;test&#39;;
mysql&amp;gt;flush privileges;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将test数据库的权限授予test用户，登录密码是test，%代表所有ip。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;4. 配置文件&lt;/h2&gt;

&lt;p&gt;rpm包默认配置启动文件模板&lt;code&gt;/usr/share/doc/Percona-Server-server-56-5.6.14/my-default.cnf&lt;/code&gt;，可以将他拷贝到&lt;code&gt;/etc/my.cnf&lt;/code&gt;作为配置文件使用。&lt;/p&gt;

&lt;p&gt;配置文件修改举例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp /usr/share/doc/Percona-Server-server-56-5.6.14/my-default.cnf /etc/my.cnf
$ vim /etc/my.cnf 			#将需要修改的参数做如下填写
[mysqld]
# These are commonly set, remove the # and set as required.
# basedir = .....
# datadir = .....
# port = .....
# server_id = .....
# socket = .....
innodb_file_format=barracuda
innodb_file_per_table=true
innodb_large_prefix=on
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面参数作用，可以解决建索引时&lt;code&gt;Specified key was too long; max key length is 767 bytes&lt;/code&gt;的报错，拓展支持的最大索引长度，如使用上述功能建表时加&lt;code&gt;ROW_FORMAT=DYNAMIC&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;数据目录默认：&lt;code&gt;/var/lib/mysql/&lt;/code&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/04/06/install-mysql-on-rhel-system.html</link>
      <guid>http://blog.javachen.com/2014/04/06/install-mysql-on-rhel-system.html</guid>
      <pubDate>2014-04-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>BroadleafCommerce介绍</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 介绍&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://www.broadleafcommerce.org/&quot;&gt;BroadleafCommerce&lt;/a&gt;是一个Java开源电子商务网站框架。其目标是开发企业级商务网站，它提供健壮的数据和服务模型、富客户端管理平台、已经一些核心电子商务有关的工具。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 特性&lt;/h1&gt;

&lt;h2 id=&quot;catalog-&quot;&gt;2.1	Catalog （目录分类）&lt;/h2&gt;

&lt;p&gt;提供灵活的产品和类型管理，一个重要的特性是可以继承产品分类来满足特殊的商业需求。管理界面可以管理各种类别和产品。&lt;/p&gt;

&lt;h2 id=&quot;promotion-system&quot;&gt;2.2	Promotion System（促销系统）&lt;/h2&gt;

&lt;p&gt;可通过配置的方式管理促销。以下类促销示无需客制化而通过管理界面即可管理：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;百分比折扣、金额折扣、固定价格(Percent Off / Dollar Off / Fixed Price)&lt;/li&gt;
  &lt;li&gt;订单、订单项、快递级别促销&lt;/li&gt;
  &lt;li&gt;买一赠一促销&lt;/li&gt;
  &lt;li&gt;基于客户、购物车或类别属性的促销&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;content-management-system&quot;&gt;2.3	Content Management System（内容管理系统）&lt;/h2&gt;

&lt;p&gt;内容管理系统有以下特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;支持用户直接管理静态页面&lt;/li&gt;
  &lt;li&gt;可以配置顾客内容类型（如广告）&lt;/li&gt;
  &lt;li&gt;提供UI界面管理静态页面、结构化内容、图片以及其他内容；&lt;/li&gt;
  &lt;li&gt;结构化内容能够针对性的对某些客户显示（如对满足一定条件的客户显示广告）&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 架构&lt;/h1&gt;

&lt;h2 id=&quot;spring-framework&quot;&gt;3.1	Spring Framework&lt;/h2&gt;

&lt;p&gt;Spring提供诸多功能，包括依赖注入和事务管理&lt;/p&gt;

&lt;h2 id=&quot;security&quot;&gt;3.2	Security&lt;/h2&gt;

&lt;p&gt;Spring Security提供强健的安全认证框架，控制代码和页面级别的认证和授权。&lt;/p&gt;

&lt;h2 id=&quot;persistence&quot;&gt;3.3	Persistence&lt;/h2&gt;

&lt;p&gt;使用JPA和hibernate实现ORM基础&lt;/p&gt;

&lt;h2 id=&quot;asynchronous-messaging&quot;&gt;3.4	Asynchronous Messaging&lt;/h2&gt;

&lt;p&gt;使用spring JMS和一个现代的JMS代理交互来实现应用消息的异步处理。&lt;/p&gt;

&lt;h2 id=&quot;search&quot;&gt;3.5	Search&lt;/h2&gt;

&lt;p&gt;通过整合流行的Compass和lucene项目提供可灵活的domain查找功能。&lt;/p&gt;

&lt;h2 id=&quot;task-scheduling&quot;&gt;3.6	Task Scheduling&lt;/h2&gt;
&lt;p&gt;使用Quartz提供排程功能。&lt;/p&gt;

&lt;h2 id=&quot;email&quot;&gt;3.7	Email&lt;/h2&gt;
&lt;p&gt;Email功能分为同步和异步（jms）两种模式。Email内容可以通过velocity模块客制化。支持mail打开和连接点击跟踪。&lt;/p&gt;

&lt;h2 id=&quot;modular-design&quot;&gt;3.8	Modular Design（模块化设计）&lt;/h2&gt;
&lt;p&gt;提供各种模块，可以和电子商务的一些重要功能进行交互，如信用卡处理、税收服务、快递公司。&lt;br /&gt;
比如，USPS快递模块是一个好的案例。 客户模块可以很方便的开发并整合进来。&lt;/p&gt;

&lt;h2 id=&quot;configurable-workflows&quot;&gt;3.9	Configurable Workflows（可配置的工作流）&lt;/h2&gt;
&lt;p&gt;电子商务生命周期的关键表现在可配置的工作流。系统能够对这些关键的地方进行完全的控制，包括价格和结账，允许对订单、行为和客户执行模块进行操作。支持复杂内嵌行为的合成工作流。&lt;/p&gt;

&lt;h2 id=&quot;extensible-design&quot;&gt;3.10	Extensible Design（可扩展性设计）&lt;/h2&gt;
&lt;p&gt;扩展性是我们设计的核心，几乎broadleaf所有的组件都是可以继承、或添加、或者通过修改增强和改变默认的行为。 这些组件包括所有的service、数据访问对象、实体。&lt;/p&gt;

&lt;h2 id=&quot;configuration-merging&quot;&gt;3.11	Configuration Merging（配置合并）&lt;/h2&gt;
&lt;p&gt;我们以扩展模块的附加部分，为客户提供对spring配置文件进行合并的功能。它可以最小化配置，一个实现必须清楚它允许用户只需把精力放在他们自己的配置细节。 Broadleaf在运行时会智能的将实现者的配置信息和自己的配置信息进行合并。&lt;/p&gt;

&lt;h2 id=&quot;runtime-configuration-management&quot;&gt;3.12	Runtime Configuration Management（运行时配置管理）&lt;/h2&gt;

&lt;p&gt;services、模块和其他子系统的配置属性通过JMX暴露，这样管理者不用关闭系统就可以改变应用行为。&lt;/p&gt;

&lt;h2 id=&quot;presentation-layer-support&quot;&gt;3.13	Presentation Layer Support（表现层支持）&lt;/h2&gt;

&lt;p&gt;提供很多事先写好的spring MVC控制器来加快表现层的开发。&lt;/p&gt;

&lt;h2 id=&quot;qos&quot;&gt;3.14	QoS（服务质量）&lt;/h2&gt;
&lt;p&gt;提供对客户和默认模块的服务质量监控，同时支持外部日志和email。其他客户Qos处理器可以通过我们的open API添加。&lt;/p&gt;

&lt;h2 id=&quot;pci-considerationspci&quot;&gt;3.15	PCI Considerations（PCI注意事项）&lt;/h2&gt;

&lt;p&gt;我们的架构和设计经过了仔细的分析，帮助你在决定存储和使用敏感的客户金融账号信息的时候实现PCI遵从性。支付账号信息是分别引用的，允许你将机密的数据隔离存储到一个独立的安全的数据库平台。已经添加了API方法来包含PCI遵从性加密schema。另外，提供冗长的日志跟踪交易交互信息。&lt;/p&gt;

&lt;p&gt;PCI（Payment Card Industry）(Payment Card Industry (PCI) Data Security Standard).支付卡行业 (PCI) 数据安全标准 (DSS)是一组全面的要求，旨在确保持卡人的信用卡和借记卡信息保持安全，而不管这些信息是在何处以何种方法收集、处理、传输和存储。&lt;/p&gt;

&lt;p&gt;PCI DSS 由 PCI 安全标准委员会的创始成员（包括 American Express、Discover Financial Services、JCB、MasterCard Worldwide 和 Visa International）制定，旨在鼓励国际上采用一致的数据安全措施。&lt;/p&gt;

&lt;p&gt;PCI DSS 中的要求是针对在日常运营期间需要处理持卡人数据的公司和机构提出的。具体而言，PCI DSS 对在整个营业日中处理持卡人数据的金融机构、贸易商和服务提供商提出了要求。PCI DSS 包括有关安全管理、策略、过程、网络体系结构、软件设计的要求的列表，以及用来保护持卡人数据的其他措施。&lt;/p&gt;

&lt;h2 id=&quot;customizable-administration-platform-&quot;&gt;3.16	Customizable Administration Platform （客制化管理平台）&lt;/h2&gt;

&lt;p&gt;管理应用基于我们新的开放的管理平台，使用标准面向对象的技术提供一个清晰的客制化方式。管理平台和核心框架一样，都有很好扩展性。表现层是基于有名的可信赖的GWT和SmartGWT技术。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4. 源代码介绍&lt;/h1&gt;

&lt;h2 id=&quot;broadleaf&quot;&gt;4.1 BroadLeaf核心领域对象分析&lt;/h2&gt;

&lt;p&gt;核心领域对象详见broadleaf-framework模块org.broadleafcommerce.core.*.domain包下的领域类。&lt;/p&gt;

&lt;h2 id=&quot;broadleaf-1&quot;&gt;4.2 BroadLeaf的子系统及其职责&lt;/h2&gt;

&lt;p&gt;Broadleaf做电子商务网站需要两部分内容。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一部分：&lt;/strong&gt;Broadleaf框架   Broadleaf电子商务框架由有9大模块组成(不包括第三方模块)&lt;/p&gt;

&lt;p&gt;1) broadleaf-common  &lt;br /&gt;
各个模块共享的集合类. &amp;gt; 依赖于 broadleaf-instrument&lt;/p&gt;

&lt;p&gt;2) broadleaf-framework   &lt;br /&gt;
Broadleaf框架的核心类 &amp;gt; 依赖 于broadleaf-common, broadleaf-profile, broadleaf-contentmanagement-module&lt;/p&gt;

&lt;p&gt;3) broadleaf-framework-web   &lt;br /&gt;
Spring MVC 控制器和相关的项目 &amp;gt; 依赖于 broadleaf-framework, broadleaf-profile, broadleaf-profile-web&lt;/p&gt;

&lt;p&gt;4) broadleaf-profile   &lt;br /&gt;
客户资料相关的类,工具类, email, 配置合并 &amp;gt; 依赖于 broadleaf-common&lt;/p&gt;

&lt;p&gt;5) broadleaf-profile-web   &lt;br /&gt;
Spring MVC控制器和 相关项目的 profile 模块&amp;gt; 依赖于on broadleaf-profile&lt;/p&gt;

&lt;p&gt;6) broadleaf-instrument   &lt;br /&gt;
允许运行时检测到某些 Broadleaf的注解&amp;gt; 无依赖&lt;/p&gt;

&lt;p&gt;7) broadleaf-open-admin-platform   &lt;br /&gt;
为Hibernate管理的domains 创建可拓展的用户图形管理界面框架&amp;gt; 依赖于 broadleaf-common&lt;/p&gt;

&lt;p&gt;8) broadleaf-contentmanagement-module   &lt;br /&gt;
通过管理工具管理的一个功能齐全的内容管理系统 &amp;gt; 依赖于 broadleaf-open-admin-platform&lt;/p&gt;

&lt;p&gt;9) broadleaf-admin-module &lt;br /&gt;
内容: Broadleaf 电子商务框架特定的管理模块，插入开发的管理平台&amp;gt; 依赖于 broadleaf-framework, broadleaf-open-admin-platform, broadleaf-contentmanagement-module&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二部分：&lt;/strong&gt;需要在框架的基础上做web的开发，其框架已经封装了 已有功能的控制器方法，但是没有增加请求映射。&lt;a href=&quot;https://github.com/BroadleafCommerce/DemoSite&quot;&gt;DemoSite&lt;/a&gt;就是一个示例工程。&lt;/p&gt;

&lt;p&gt;DemoSite 总共三个模块&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;core:框架模块 集成了框架&lt;/li&gt;
  &lt;li&gt;admin:后台管理页面的封装&lt;/li&gt;
  &lt;li&gt;site:前台网站的封装&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;broadleaf-2&quot;&gt;4.3 BroadLeaf的分层及层之间的通讯机制&lt;/h2&gt;

&lt;p&gt;BroadLeaf的DemoSite 站点通过Maven集成BroadLeaf框架的各个模块，各个模块通过方法调用进行集成。就像添加普通的jar包一样。&lt;/p&gt;

&lt;p&gt;BroadLeaf框架采用常见的MVC架构  分为控制层 服务层  DAO层。&lt;/p&gt;

&lt;h2 id=&quot;broadleaf-3&quot;&gt;4.4 BroadLeaf的扩展机制&lt;/h2&gt;

&lt;p&gt;1) 界面拓展机制&lt;/p&gt;

&lt;p&gt;BroadLeaf的界面与框架式分离的（后台管理页面是集成在broadleaf-open-admin-platform模块里面），界面采用thymeleaf模板引擎开发。界面的拓展只需要美工针对于他们的模板修改样式就行。&lt;/p&gt;

&lt;p&gt;如果觉得他们的模板实在不好，直接自己开发一套模板 替换到一下就可以了 只需要在applicationContext-servlet.xml中修改替代的模板位置。&lt;/p&gt;

&lt;p&gt;2) 用户拓展机制&lt;/p&gt;

&lt;p&gt;BroadLeaf提供了 RegisterCustomerForm、CustomerAttributes两个类用户拓展用户属性。添加的属性只需要简单的配置一下就可以了（BroadLeaf采用的Hibernate的数据库字段同步，修改完实体后数据库会自动的修改）&lt;/p&gt;

&lt;p&gt;3) 订单处理流程拓展机制&lt;/p&gt;

&lt;p&gt;增加或者删除订单流程只需要配置blCheckoutWorkflow bean实体覆盖框架提供的。 然后再自定义Activiti任务  添加到新的blCheckoutWorkflow 的activities属性列表里面就可以了。&lt;/p&gt;

&lt;p&gt;4) 付款方式拓展机制&lt;/p&gt;

&lt;p&gt;增加新的付款实体 实现其付款接口。&lt;/p&gt;

&lt;p&gt;5) 数据库切换方式&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.修改pom里面涉及到数据库连接的dependency为要切换的数据库的jdbc连接驱动&lt;/li&gt;
  &lt;li&gt;2.修改properties文件里面的数据库连接信息（用户名、密码、URL、driverClassName、hibernate数据库方言）&lt;/li&gt;
  &lt;li&gt;3.修改jndi连接信息&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5. 缺点&lt;/h1&gt;

&lt;p&gt;目前已知的缺点或缺陷有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;实体之间存在双向关联，导致无法直接使用json的工具类序列化和反序列化实体类。如果的确需要序列化实体类，比如说你需要提供api获取一些信息，你可以参考DemoSite中的api，对实体的封装类使用webservice进行序列化。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-5&quot;&gt;6. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://www.broadleafcommerce.org/&quot;&gt;BroadleafCommerce&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;https://github.com/BroadleafCommerce/DemoSite&quot;&gt;DemoSite&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://sisopipo.com/blog/archives/553&quot;&gt;认识Java电子商务开源框架BroadleafCommerce&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/04/04/introduction-to-broadleaf-commerce.html</link>
      <guid>http://blog.javachen.com/2014/04/04/introduction-to-broadleaf-commerce.html</guid>
      <pubDate>2014-04-04T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用SaltStack安装JDK1.6</title>
      <description>&lt;h1 id=&quot;states&quot;&gt;创建states文件&lt;/h1&gt;

&lt;p&gt;在&lt;code&gt;/srv/salt&lt;/code&gt;目录下创建jdk目录，并在jdk目录创建init.sls文件，init.sls文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;jdk-file:
 file.managed:
   - source: salt://jdk/files/jdk1.6.0_39.tar.gz
   - name: /usr/java/jdk1.6.0_39.tar.gz
   - include_empty: True
 
jdk-install:
 cmd.run:
   - name: &#39;/bin/tar -zxf jdk1.6.0_39.tar.gz &amp;amp;&amp;amp; /bin/ln -s jdk1.6.0_39  latest &#39;
   - cwd: /usr/java
   - unless: &#39;test -e jdk1.6.0_39&#39;
   - require:
     - file: jdk-file
 
jdk-rmzip:
  file.absent:
    - name: /usr/java/jdk1.6.0_39.tar.gz
 
/root/.bashrc:
  file.append:
    - text:
      - export JAVA_HOME=/usr/java/latest
      - export PATH=$JAVA_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面sls文件需要引用&lt;code&gt;jdk1.6.0_39.tar.gz&lt;/code&gt;文件，故需要下载jdk1.6.0_39.bin安装之后将其打包成&lt;code&gt;jdk1.6.0_39.tar.gz&lt;/code&gt;拷贝到&lt;code&gt;/srv/salt/jdk/files&lt;/code&gt;目录。&lt;/p&gt;

&lt;p&gt;init.sls文件执行过程包括以下几个步骤：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;jdk-file，将&lt;code&gt;salt://jdk/files/jdk1.6.0_39.tar.gz&lt;/code&gt;文件拷贝到&lt;code&gt;/usr/java&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;jdk-install，解压文件&lt;/li&gt;
  &lt;li&gt;jdk-rmzip，删除压缩包&lt;/li&gt;
  &lt;li&gt;/root/.bashrc，设置JAVA_HOME&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改top.sls文件（该步骤为可选），添加jdk.init:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;base:
  &#39;*&#39;:
    - jdk.init
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;测试运行&lt;/h1&gt;

&lt;p&gt;在master上运行下面命令，并观察运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[root@sk1 vagrant]# salt &#39;*&#39; state.sls jdk
sk2:
----------
          ID: jdk-file
    Function: file.managed
        Name: /usr/java/jdk1.6.0_39.tar.gz
      Result: True
     Comment: File /usr/java/jdk1.6.0_39.tar.gz updated
     Changes:  
              ----------
              diff:
                  New file
              mode:
                  0644
----------
          ID: jdk-install
    Function: cmd.run
        Name: /bin/tar -zxf jdk1.6.0_39.tar.gz &amp;amp;&amp;amp; /bin/ln -s jdk1.6.0_39  latest
      Result: True
     Comment: unless execution succeeded
     Changes:  
----------
          ID: jdk-rmzip
    Function: file.absent
        Name: /usr/java/jdk1.6.0_39.tar.gz
      Result: True
     Comment: Removed file /usr/java/jdk1.6.0_39.tar.gz
     Changes:  
              ----------
              removed:
                  /usr/java/jdk1.6.0_39.tar.gz
----------
          ID: /root/.bashrc
    Function: file.append
      Result: True
     Comment: Appended 0 lines
     Changes:  
Summary
------------
Succeeded: 4
Failed:    0
------------
Total:     4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上可以看出成功了4个，失败为0。&lt;/p&gt;

&lt;p&gt;安装了jdk之后，需要重启minion(还需要&lt;strong&gt;修改minion启动脚本，让minion加载上系统环境变量&lt;/strong&gt;，详细说明，见&lt;a href=&quot;/2013/11/11/install-saltstack-and-halite.html&quot;&gt;安装SaltStack和halite&lt;/a&gt;)才能通过下面脚本运行java相关的命令，如java、jps等等：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;salt &#39;*&#39; cmd.run &#39;jps&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则，你需要通过下面脚本来执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;salt &#39;*&#39; cmd.run &#39;source /root/.bashrc ;jps&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;pillar&quot;&gt;设置pillar&lt;/h1&gt;

&lt;p&gt;将上面的&lt;code&gt;jdk/init.sls&lt;/code&gt;文件修改为通过pillar引用变量&lt;/p&gt;

&lt;p&gt;a.首先在&lt;code&gt;/srv/pillar&lt;/code&gt;目录创建jdk目录，并在jdk目录下创建init.sls文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;jdk:
  name: jdk1.6.0_39
  srvpath: salt://jdk/files 
  homepath: /usr/java
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b.在&lt;code&gt;/srv/pillar/top.sls&lt;/code&gt;中添加jdk.init&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;base:
  &#39;*&#39;:
    - jdk.init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c.修改&lt;code&gt;/srv/salt/jdk/init.sls&lt;/code&gt;文件为从pillar引入变量，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jdk-file:
  file.managed:
    - source: {{pillar[&#39;jdk&#39;][&#39;srvpath&#39;]}}/{{pillar[&#39;jdk&#39;][&#39;name&#39;]}}.tar.gz
    - name: {{pillar[&#39;jdk&#39;][&#39;homepath&#39;]}}/{{pillar[&#39;jdk&#39;][&#39;name&#39;]}}.tar.gz
    - makedirs: True
 
jdk-install:
  cmd.run:
    - name: &#39;unzip -q {{pillar[&#39;jdk&#39;][&#39;name&#39;]}}.tar.gz&#39;
    - cwd: {{pillar[&#39;jdk&#39;][&#39;homepath&#39;]}}
    - unless: &#39;test -e {{pillar[&#39;jdk&#39;][&#39;name&#39;]}}&#39;
    - require:
      - file: jdk-file
jdk-rmzip:
  file.absent:
    - name: {{pillar[&#39;jdk&#39;][&#39;homepath&#39;]}}/{{pillar[&#39;jdk&#39;][&#39;name&#39;]}}.tar.gz
{{pillar[&#39;userhome&#39;]}}/.bashrc:
  file.append:
    - text:
      - export JAVA_HOME={{pillar[&#39;jdk&#39;][&#39;homepath&#39;]}}/{{pillar[&#39;jdk&#39;][&#39;name&#39;]}}
      - export PATH=$JAVA_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;d.参考上面，再次测试一遍&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/04/01/install-jdk-with-saltstack.html</link>
      <guid>http://blog.javachen.com/2014/04/01/install-jdk-with-saltstack.html</guid>
      <pubDate>2014-04-01T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Lua和OpenResty搭建验证码服务器</title>
      <description>&lt;p&gt;&lt;a href=&quot;http://www.lua.org/&quot;&gt;Lua&lt;/a&gt;下有个&lt;a href=&quot;http://lua-gd.luaforge.net/&quot;&gt;Lua-GD&lt;/a&gt;图形库，通过简单的Lua语句就能控制、生成图片。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;环境说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：RHEL6.4&lt;/li&gt;
  &lt;li&gt;RHEL系统默认已安装RPM包的&lt;a href=&quot;http://www.lua.org/ftp/lua-5.1.4.tar.gz&quot;&gt;Lua-5.1.4&lt;/a&gt;，但其只具有Lua基本功能，不提供 &lt;code&gt;lua.h&lt;/code&gt; 等，但 Lua-GD 编译需要用到 &lt;code&gt;lua.h&lt;/code&gt;，故 Lua 需要编译安装。&lt;/li&gt;
  &lt;li&gt;Lua-GD 版本号格式为&lt;code&gt;X.Y.XrW&lt;/code&gt;，其中&lt;code&gt;X.Y.Z&lt;/code&gt;代表gd版本，&lt;code&gt;W&lt;/code&gt;代表效力版本，所以 lua-gd 版本：&lt;code&gt;lua-gd-2.0.33r2&lt;/code&gt; 相对应 gd 版本为：&lt;code&gt;gd-2.0.33&lt;/code&gt;，须注意保持一致。&lt;/li&gt;
  &lt;li&gt;因生成gif的lua脚本中用到md5加密，故需编译安装md5。&lt;/li&gt;
  &lt;li&gt;因为生成图片需要唯一命名，故依赖 UUID&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;另外：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;以下操作均以root用户运行，并且以下脚本的当前目录为&lt;code&gt;/opt&lt;/code&gt;，即所有的下载的文件都会保存在&lt;code&gt;/opt&lt;/code&gt;目录下。&lt;/p&gt;

&lt;p&gt;需要安装的软件如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OpenResty：WEB应用服务器，部署lua代码，提供URL供用户调用和访问&lt;/li&gt;
  &lt;li&gt;LuaJIT：LUA代码解释器，使用OpenResty中集成的版本&lt;/li&gt;
  &lt;li&gt;GD库：C图形库&lt;/li&gt;
  &lt;li&gt;Lua-GD库：Lua绑定的C图形库，使得lua可调用gd&lt;/li&gt;
  &lt;li&gt;Lua-Resty-UUID库：用于生成UUID，保证图片命名唯一性&lt;/li&gt;
  &lt;li&gt;LuaSocket：lua 的 socket 库&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;lua&quot;&gt;安装lua&lt;/h1&gt;

&lt;p&gt;安装编译所需软件包:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install -y make gcc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载并编译安装 lua-5.1：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install -y readline-devel
$ wget http://www.lua.org/ftp/lua-5.1.4.tar.gz
$ tar lua-5.1.4.tar.gz
$ cd lua-5.1.4
$ make linux
$ make linux install 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;gd&quot;&gt;安装 gd&lt;/h1&gt;

&lt;p&gt;GD版本：gd-2.0.33&lt;/p&gt;

&lt;p&gt;下载地址: &lt;a href=&quot;http://www.boutell.com/gd/http/gd-2.0.33.tar.gz&quot;&gt;http://www.boutell.com/gd/http/gd-2.0.33.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install -y libjpeg-devel libpng-devel freetype-devel fontconfig-devel libXpm-devel

$ wget http://www.boutell.com/gd/http/gd-2.0.33.tar.gz
$ tar zvxf gd-2.0.33.tar.gz
$ cd gd-2.0.33
$ ./configure
$ make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;lua-gd-&quot;&gt;安装 Lua-gd 库&lt;/h1&gt;

&lt;p&gt;Lua-GD版本：lua-gd-2.0.33r2&lt;/p&gt;

&lt;p&gt;下载地址: &lt;a href=&quot;http://jaist.dl.sourceforge.net/project/lua-gd/lua-gd/lua-gd-2.0.33r2%20%28for%20Lua%205.1%29/lua-gd-2.0.33r2.tar.gz&quot;&gt;http://jaist.dl.sourceforge.net/project/lua-gd/lua-gd/lua-gd-2.0.33r2%20%28for%20Lua%205.1%29/lua-gd-2.0.33r2.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;开发手册可参考: &lt;a href=&quot;http://ittner.github.io/lua-gd/manual.html&quot;&gt;http://ittner.github.io/lua-gd/manual.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;说明：&lt;/p&gt;

  &lt;p&gt;须先完成gd的安装，且版本号必须为gd-2.0.33 &lt;br /&gt;
调用Lua-GD库的lua代码须由OpenResty中集成的LuaJIT解释执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://sourceforge.net/projects/lua-gd/files/lua-gd/lua-gd-2.0.33r2%20(for%20Lua%205.1)/lua-gd-2.0.33r2.tar.gz/download?use_mirror=jaist
$ tar zvxf lua-gd-2.0.33r2.tar.gz
$ cd lua-gd-2.0.33r2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接写来修改Makefile文件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;注释第36～42行&lt;/li&gt;
  &lt;li&gt;打开第48～52行注释，并做如下修改&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;OUTFILE=gd.so
CFLAGS=-Wall `gdlib-config --cflags` -I/usr/local/include/lua -O3    //第49行，修改 lua 的 C 库头文件所在路径
GDFEATURES=`gdlib-config --features |sed -e &quot;s/GD_/-DGD_/g&quot;`
LFLAGS=-shared `gdlib-config --ldflags` `gdlib-config --libs` -llua -lgd  //第51行，取消lua库版本号51
INSTALL_PATH=/usr/local/lib/lua/5.1    //第52行，设置 gd.so 的安装路径

$(CC) -fPIC -o ...  //第70行，gcc 编译，添加 -fPIC 参数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后编译：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;md5&quot;&gt;安装 md5&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install unzip

$ wget https://github.com/keplerproject/md5/archive/master.zip -O md5-master.zip
$ unzip md5-master.zip
$ cd md5-master
$ make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;lua-resty-uuid-&quot;&gt;安装 Lua-resty-UUID 库&lt;/h1&gt;

&lt;p&gt;调用系统的UUID模块生成的由32位16进制（0-f）数组成的的串，本模块进一步压缩为62进制。正如你所想，生成的UUID越长，理论冲突率就越小，请根据业务需要自行斟酌。 基本思想为把系统生成的16字节（128bit）的UUID转换为62进制（a-zA-Z0-9），同时根据业务需要进行截断。&lt;/p&gt;

&lt;p&gt;下载地址: &lt;a href=&quot;https://github.com/dcshi/lua-resty-UUID/archive/master.zip&quot;&gt;https://github.com/dcshi/lua-resty-UUID/archive/master.zip&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum -y install libuuid-devel
$ wget https://github.com/dcshi/lua-resty-UUID/archive/master.zip -O lua-resty-UUID-master.zip
$ unzip lua-resty-UUID-master.zip
$ cd lua-resty-UUID-master/clib
$ make
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;nginx-sysguard&quot;&gt;下载nginx sysguard模块&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果nginx被攻击或者访问量突然变大，nginx会因为负载变高或者内存不够用导致服务器宕机，最终导致站点无法访问。&lt;br /&gt;
今天要谈到的解决方法来自淘宝开发的模块nginx-http-sysguard，主要用于当负载和内存达到一定的阀值之时，会执行相应的动作，比如直接返回503,504或者其他的。一直等到内存或者负载回到阀值的范围内，站点恢复可用。简单的说，这几个模块是让nginx有个缓冲时间，缓缓。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://github.com/alibaba/nginx-http-sysguard/archive/master.zip -O nginx-http-sysguard-master.zip
$ unzip nginx-http-sysguard-master.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;openresty&quot;&gt;安装 OpenResty&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;OpenResty（也称为 ngx_openresty）是一个全功能的 Web 应用服务器。它打包了标准的 Nginx 核心，很多的常用的第三方模块，以及它们的大多数依赖项。 &lt;br /&gt;
OpenResty 中的 LuaJIT 组件默认未激活，需使用 &lt;code&gt;--with-luajit&lt;/code&gt; 选项在编译 OpenResty 时激活,使用&lt;code&gt;--add-module&lt;/code&gt;，添加上sysguard模块&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;安装的版本：1.2.7.6&lt;/p&gt;

&lt;p&gt;下载地址：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://openresty.org/#Download&quot;&gt;http://openresty.org/#Download&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openresty.org/download/ngx_openresty-1.2.7.6.tar.gz&quot;&gt;http://openresty.org/download/ngx_openresty-1.2.7.6.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;先安装依赖软件，然后在编译代码，编译时使用&lt;code&gt;--perfix&lt;/code&gt;选项指定 OpenResty 的安装目录，&lt;code&gt;--with-luajit&lt;/code&gt; 选项激活 LuaJIT 组件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum -y install gcc make gmake openssl-devel pcre-devel readline-devel zlib-devel

$ wget http://openresty.org/download/ngx_openresty-1.2.7.6.tar.gz
$ tar zvxf ngx_openresty-1.2.7.6.tar.gz
$ cd ngx_openresty-1.2.7.6
$ ./configure --with-luajit --with-http_stub_status_module --add-module=/opt/nginx-http-sysguard-master/
$ gmake &amp;amp;&amp;amp; gmake install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建软连接：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ln -s /usr/local/openresty/nginx/sbin/nginx /usr/sbin/nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;redis-server&quot;&gt;安装 Redis Server&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lua 脚本功能是 Reids 2.6 版本的最大亮点， 通过内嵌对 Lua 环境的支持， Redis 解决了长久以来不能高效地处理 CAS （check-and-set）命令的缺点， 并且可以通过组合使用多个命令， 轻松实现以前很难实现或者不能高效实现的模式。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://redis.googlecode.com/files/redis-2.6.14.tar.gz
$ tar zvxf redis-2.6.14.tar.gz
$ cd redis-2.6.14
$ make &amp;amp;&amp;amp; make install

$ mkdir -p /usr/local/redis/conf
$ cp redis.conf /usr/local/redis/conf/
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;luasocket-&quot;&gt;安装 LuaSocket 库&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;LuaSocket是一个Lua扩展库，它能很方便地提供SMTP、HTTP、FTP等网络议访问操作。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;LuaSocket版本：luasocket-2.0-beta2&lt;/p&gt;

&lt;p&gt;下载地址: &lt;a href=&quot;http://files.luaforge.net/releases/luasocket/luasocket/luasocket-2.0-beta2/luasocket-2.0-beta2.tar.gz&quot;&gt;http://files.luaforge.net/releases/luasocket/luasocket/luasocket-2.0-beta2/luasocket-2.0-beta2.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://files.luaforge.net/releases/luasocket/luasocket/luasocket-2.0.2/luasocket-2.0.2.tar.gz
$ tar zvxf luasocket-2.0.2.tar.gz
$ cd luasocket-2.0.2
$ make -f makefile.Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;redis-lua-&quot;&gt;安装 redis-lua 库&lt;/h1&gt;

&lt;p&gt;Redis-Lua版本：2.0&lt;/p&gt;

&lt;p&gt;下载地址: &lt;a href=&quot;https://github.com/nrk/redis-lua/archive/version-2.0.zip&quot;&gt;https://github.com/nrk/redis-lua/archive/version-2.0.zip&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://github.com/nrk/redis-lua/archive/version-2.0.zip
$ unzip redis-lua-version-2.0.zip
$ cd redis-lua-version-2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，拷贝redis.lua至所需目录。&lt;/p&gt;

&lt;p&gt;lua调用方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;local redis = require(“redis”)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;zklua&quot;&gt;安装 zklua&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;zklua 仅依赖 zookeeper c API 实现，一般存在于 &lt;code&gt;zookeeper-X.Y.Z/src/c&lt;/code&gt;， 因此你需要首先安装 zookeeper c API。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;zookeeper c API 安装:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget http://mirrors.cnnic.cn/apache/zookeeper/zookeeper-3.4.5/
$ tar zvxf zookeeper-3.4.5
$ cd zookeeper-3.4.5/src/c
$ ./configure
$ make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后安装zklua：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://github.com/forhappy/zklua/archive/master.zip -O zklua-master.zip
$ unzip zklua-master.zip
$ cd zklua-master
$ make  &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;修改配置文件&lt;/h1&gt;

&lt;h2 id=&quot;openresty-1&quot;&gt;配置openresty&lt;/h2&gt;

&lt;p&gt;openresty安装在&lt;code&gt;/usr/local/openresty&lt;/code&gt;目录，在其目录下创建lualib，用于存放上面安装的一些动态连接库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p  /usr/local/openresty/lualib/captcha
cp lua-resty-UUID-master/clib/libuuidx.so /usr/local/openresty/lualib/captcha/  #拷贝uuid的库文件
cp -r lua-resty-UUID-master/lib/* /usr/local/openresty/lualib/captcha/

cp luasocket-2.0.2/luasocket.so.2.0 /usr/local/openresty/lualib/captcha/		#拷贝luasocket的库文件到/usr/local/openresty/lualib/captcha/
ln -s  /usr/local/openresty/lualib/captcha/luasocket.so.2.0 /usr/local/openresty/lualib/captcha/socket.so

cp redis-lua-version-2.0/src/redis.lua /usr/local/openresty/lualib/captcha/     #拷贝reis.lua到/usr/local/openresty/lualib/captcha/

mkdir -p /usr/local/openresty/lualib/zklua										#拷贝zklua文件到/usr/local/openresty/lualib/captcha/
cp cd zklua-master/zklua.so /usr/local/openresty/lualib/zklua/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;nginx&quot;&gt;配置nginx&lt;/h2&gt;

&lt;p&gt;创建www用户：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;useradd -M -s /sbin/nologin www
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑ngnix.conf，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    user  www;
    worker_processes  31;
    error_log  logs/error.log;
    pid        logs/nginx.pid;
    worker_rlimit_nofile 65535;
    events {
    	worker_connections 1024;
    	use epoll;
	   }

    http {
	include       	mime.types;
	default_type 	application/octet-stream;
	log_format  	main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;
			      &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;
	               	      &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;;
	access_log  	logs/access.log  main;
	sendfile        on;
	tcp_nopush      on;
	tcp_nodelay     on;
	keepalive_timeout  65;
	gzip  		on;
	gzip_min_length 1K;
	gzip_buffers 4 	8k;
	gzip_comp_level 2;
	gzip_types 	text/plain image/gif image/png image/jpg application/x-javascript text/css application/xml text/javascript;
	gzip_vary 	on;
	
	upstream redis-pool{
	        server 127.0.0.1:10005;
		keepalive 1024;
        }

    server {
        sysguard on;
        sysguard_load load=90 action=/50x.html;
        server_tokens off;
        listen       10002;
        server_name  localhost;
        charset utf-8;

        location / {
        root   html;
        index  index.html index.htm;
        }

	#-----------------------------------------------------------------------------------------
	
	# 验证码生成
	location /captcha {
		set $percent 0;
		set $modecount 1;
		content_by_lua_file /usr/local/openresty/nginx/luascripts/luajit/captcha.lua;
	}

	#-----------------------------------------------------------------------------------------
	
	# 验证码校验	
	location /captcha-check {
	        content_by_lua_file /usr/local/openresty/nginx/luascripts/luajit/captcha-check.lua;
        }

        # 验证码删除	
	location /captcha-delete {
		content_by_lua_file /usr/local/openresty/nginx/luascripts/luajit/captcha-delete.lua;
	}
	
	#-----------------------------------------------------------------------------------------

	# 样式1-静态图片
	location /mode1 {
		content_by_lua_file /usr/local/openresty/nginx/luascripts/luajit/mode/mode1.lua;
	} 
	
	#-----------------------------------------------------------------------------------------

	# redis中添加key-value
	location /redisSetQueue {
		internal;
		set_unescape_uri $key $arg_key;
		set_unescape_uri $val $arg_val; 
		redis2_query rpush $key $val;
		redis2_pass redis-pool;
	}
	# redis中获取captcha-string
	location /redisGetStr {
		internal;
		set_unescape_uri $key $arg_key;
		redis2_query lindex $key 0;
		redis2_pass redis-pool;
	}
	# redis中获取captcha-image
	location /redisGetImg {
		internal;
		set_unescape_uri $key $arg_key;
		redis2_query lindex $key 1;
		redis2_pass redis-pool;
	}
	
	#-----------------------------------------------------------------------------------------

        location ~.*.(gif|jpg|png)$ {
        expires 10s;
        }

        error_page  404              /404.html;
        error_page  500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面将 ngnix 的端口修改为10002。&lt;/p&gt;

&lt;p&gt;/usr/local/openresty/nginx/luascripts/luajit/captcha.lua 是用于生成验证码，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;--中控脚本
--
--部分应用预先生成
--部分应用实时生成，并且随机选择生成样式
--

----------------------------------------------------------------------------------------------
package.path = &quot;/usr/local/openresty/lualib/?.lua;/usr/local/openresty/lualib/captcha/?.lua;&quot;
package.cpath = &quot;/usr/local/openresty/lualib/?.so;/usr/local/openresty/lualib/captcha/?.so;&quot;
----------------------------------------------------------------------------------------------

--设置随机种子
local resty_uuid=require(&quot;resty.uuid&quot;)
math.randomseed(tonumber(resty_uuid.gennum20()))

-----------------------------------------------------------------------------------------
--
--[[ 预先生成 ]]
--
if math.random(1,99)&amp;lt;tonumber(ngx.var.percent) then
        
    --在redis的预先生成key中随机选择keyid
    local kid=math.random(1,ngx.var.pregencount)
        local res = ngx.location.capture(&#39;/redisGetImg&#39;,{ args = { key = kid } })
        
    if res.status==200 then
            local parser=require(&quot;redis.parser&quot;)
                local pic=parser.parse_reply(res.body)
                ngx.header.content_type=&quot;application/octet-stream&quot;
        
        --在header中返回用于去redis中查找记录的key
                ngx.header.picgid=kid
        
        --在body中返回captcha
                ngx.say(pic)

                ngx.exit(200)
        end
end

-----------------------------------------------------------------------------------------
--
--[[ 实时生成 ]]
--

--随机选择captcha模式X
local mode=math.random(1,ngx.var.modecount)

--调用modeX.lua，生成captcha
local res = ngx.location.capture(&quot;/mode&quot;..mode)
if res.status==200 then
    ngx.header.content_type=&quot;application/octet-stream&quot;

    --在header中返回用于去redis中查找记录的key
    ngx.header.picgid=res.header.picgid
        
    --在body中返回captcha
    ngx.say(res.body)

        ngx.exit(200)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/usr/local/openresty/nginx/luascripts/luajit/captcha-check.lua 用于校验验证码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;--[[captcha check]]

----------------------------------------------------------------------------------------------
package.path = &quot;/usr/local/openresty/lualib/?.lua;/usr/local/openresty/lualib/captcha/?.lua;&quot;
package.cpath = &quot;/usr/local/openresty/lualib/?.so;/usr/local/openresty/lualib/captcha/?.so;&quot;
----------------------------------------------------------------------------------------------

--获取请求中参数
local uriargs = ngx.req.get_uri_args()
local picgid = uriargs[&quot;image&quot;]
local ustr=string.lower(uriargs[&quot;str&quot;])

--查找redis中key为picgid的记录
local res = ngx.location.capture(&#39;/redisGetStr&#39;,{ args = { key = picgid } })
if res.status==200 then
    local parser=require(&quot;redis.parser&quot;)
    local reply=parser.parse_reply(res.body)
    local rstr=string.lower(reply)
    
    --匹配用户输入字符串与redis中记录的字符串，一致返回True，否则返回False
    ngx.header.content_type=&quot;text/plain&quot;
    if ustr == rstr then
        ngx.say(&quot;True&quot;)
    else
        ngx.say(&quot;False&quot;)
    end
    
    --匹配操作后删除redis中该key记录
    local redis = require(&#39;redis&#39;)
    local client = redis.connect(&#39;127.0.0.1&#39;, 10005)
    client:del(picgid)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/usr/local/openresty/nginx/luascripts/luajit/mode/mode1.lua 是生成静态验证码图片：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;--静态图片

------------------------------------------------------------------------------------------------
package.path = &quot;/usr/local/openresty/lualib/?.lua;/usr/local/openresty/lualib/captcha/?.lua;&quot;
package.cpath = &quot;/usr/local/openresty/lualib/?.so;/usr/local/openresty/lualib/captcha/?.so;&quot;
------------------------------------------------------------------------------------------------

--Redis中插入记录方法
function setRedis(skey, sval)
        local res = ngx.location.capture(&#39;/redisSetQueue&#39;, {args= {key=skey,val=sval}})
        if res.status == 200 then
                return true
        else
                return false
        end
end

--设置随机种子
local resty_uuid=require(&quot;resty.uuid&quot;)
math.randomseed(tonumber(resty_uuid.gennum20()))

--在32个备选字符中随机筛选4个作为captcha字符串
local dict={&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;,&#39;J&#39;,&#39;K&#39;,&#39;L&#39;,&#39;M&#39;,&#39;N&#39;,&#39;P&#39;,&#39;Q&#39;,&#39;R&#39;,&#39;S&#39;,&#39;T&#39;,&#39;U&#39;,&#39;V&#39;,&#39;W&#39;,&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;}
local stringmark=&quot;&quot;
for i=1,4 do
       stringmark=stringmark..dict[math.random(1,32)]
end

--图片基本info
--picgid
local filename= &quot;1&quot;..resty_uuid.gen20()..&quot;.png&quot;
--图片78x26
local xsize = 78
local ysize = 26
--字体大小
local wsize = 17.5
--干扰线(yes/no)
local line = &quot;yes&quot;

--加载模块
local gd=require(&#39;gd&#39;)
--创建面板
local im = gd.createTrueColor(xsize, ysize)
--定义颜色
local black = im:colorAllocate(0, 0, 0)
local grey = im:colorAllocate(202,202,202)
local color={}
for c=1,100 do
        color[c] = im:colorAllocate(math.random(100),math.random(100),math.random(100))
end
--画背景
x, y = im:sizeXY()
im:filledRectangle(0, 0, x, y, grey)
--画字符
gd.useFontConfig(true)
for i=1,4 do
    k=(i-1)*16+3
    im:stringFT(color[math.random(100)],&quot;Arial:bold&quot;,wsize,math.rad(math.random(-10,10)),k,22,string.sub(stringmark,i,i))
end
--干扰线点
if line==&quot;yes&quot; then
    for j=1,math.random(3) do
        im:line(math.random(xsize),math.random(ysize),math.random(xsize),math.random(ysize),color[math.random(100)])
    end
    for p=1,20 do
            im:setPixel(math.random(xsize),math.random(ysize),color[math.random(100)])
    end

end
--流输出
local fp=im:pngStr(75)

--redis中添加picgid为key,string为value的记录
setRedis(filename,stringmark)

--response header中传参picgid
ngx.header.content_type=&quot;text/plain&quot;
ngx.header.picgid=filename

--页面返回pic
ngx.say(fp)

--nginx退出
ngx.exit(200)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;redis&quot;&gt;配置redis&lt;/h2&gt;

&lt;p&gt;在&lt;code&gt;/usr/local/openresty/redis/conf/&lt;/code&gt;创建redis-10005.conf文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;daemonize yes
pidfile /usr/local/openresty/redis/redis-10005.pid
port 10005
timeout 300
tcp-keepalive 10
loglevel notice
logfile /usr/local/openresty/redis/redis-10005.log
databases 16
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump-10005.rdb
dir /usr/local/openresty/redis
slave-serve-stale-data yes
slave-read-only yes
repl-disable-tcp-nodelay no
slave-priority 100
appendonly no
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
lua-time-limit 5000
slowlog-log-slower-than 10000
slowlog-max-len 128
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-entries 512
list-max-ziplist-value 64
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;配置验证码服务器&lt;/h2&gt;

&lt;p&gt;在&lt;code&gt;/etc/ld.so.conf.d/&lt;/code&gt;目录创建captcha.conf，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/ld.so.conf.d/captcha.conf
/usr/local/lib
/usr/local/openresty/lualib
/usr/local/openresty/lualib/captcha
/usr/local/openresty/lualib/zklua
/usr/local/openresty/luajit/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;测试&lt;/h1&gt;

&lt;h2 id=&quot;section-3&quot;&gt;生成验证码&lt;/h2&gt;

&lt;p&gt;URL：http://IP:10002/captcha&lt;/p&gt;

&lt;p&gt;然后从响应Header中获取图片的picgid=XXXXX&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;验证码校验&lt;/h2&gt;

&lt;p&gt;URL：http://IP:10002/captcha-check?image=XXXXX&amp;amp;str=ABCD&lt;br /&gt;
http://IP:10002/captcha-check?image=XXXXX&amp;amp;str=ABCD&amp;amp;delete=true&lt;br /&gt;
或&lt;br /&gt;
http://IP:10002/captcha-check?image=XXXXX&amp;amp;str=ABCD&amp;amp;delete=false&lt;/p&gt;

&lt;p&gt;参数说明如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;参数image：要校验的验证码图片的picgid。&lt;/li&gt;
  &lt;li&gt;参数str：用户输入的验证码字符串。&lt;/li&gt;
  &lt;li&gt;参数delete：当且仅当传该参数且参数值为false时，校验完成之后该验证码记录不被删除，验证码未过期之前可多次校验，用于异步校验应用中；否则，若不传该参数或者其值为true，校验完成之后该验证码记录删除。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;验证码删除&lt;/h2&gt;

&lt;p&gt;URL：http://IP:10002/captcha-delete?image=XXXXX&lt;/p&gt;

&lt;p&gt;其中image为要删除的验证码图片的picgid。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2014/04/01/deploy-a-captcha-server-using-lua-and-openresty.html</link>
      <guid>http://blog.javachen.com/2014/04/01/deploy-a-captcha-server-using-lua-and-openresty.html</guid>
      <pubDate>2014-04-01T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>CDH4.5.0 新特性</title>
      <description>&lt;h1 id=&quot;apache-flume&quot;&gt;Apache Flume&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;新特性：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2190&quot;&gt;FLUME-2190&lt;/a&gt; - 引入一个新的Twitter firehose的feed源&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2109&quot;&gt;FLUME-2109&lt;/a&gt; - HTTP输入源支持HTTPS.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-1666&quot;&gt;FLUME-1666&lt;/a&gt; - 系统日志的TCP源现在可以保持时间戳和处理领域中的事件主体.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2202&quot;&gt;FLUME-2202&lt;/a&gt; - AsyncHBaseSink can now coalesce increments to the same row and column per transaction to reduce the number of RPC calls&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2189&quot;&gt;FLUME-2189&lt;/a&gt; - Avro Source can now accept events from a restricted set of peers&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2052&quot;&gt;FLUME-2052&lt;/a&gt; - Spooling Directory Source can now ignore or replace malformed characters.&lt;/li&gt;
  &lt;li&gt;Flume自动检测Cloudera Search依赖。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;变化的特性：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Memory Channel calculates byte capacity usage on transaction commits instead of puts to improve performance&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-hive&quot;&gt;Apache Hive&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;新特性：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;新增支持非Kerberos身份验证HiveServer2和JDBC客户端之间的SSL加密通信,请见&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Security-Guide/cdh4sg_topic_9_1.html#concept_rqh_sff_cm_unique_2&quot;&gt;Configuring Encrypted Client/Server Communication for non-Kerberos HiveServer2 Connections&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hue&quot;&gt;Hue&lt;/h1&gt;

&lt;h2 id=&quot;section-3&quot;&gt;新特性：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;增加了对SAML验证后端和其他安全修补程序支持.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;变化的特性:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HUE-1609&quot;&gt;HUE-1609&lt;/a&gt; - [core] LDAP后端和进口应不区分大小写.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HUE-1632&quot;&gt;HUE-1632&lt;/a&gt; - [oozie] Workflow with &amp;amp; in a property fails to submit.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HUE-1555&quot;&gt;HUE-1555&lt;/a&gt; - [hbase] Python 2.4 支持.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HUE-1521&quot;&gt;HUE-1521&lt;/a&gt; - [core] 改进 JobTracker HA.&lt;/li&gt;
  &lt;li&gt;[search] 默认的模板应显示的所有字段.&lt;/li&gt;
  &lt;li&gt;[core] 让搜索绑定认证可选的LDAP&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-mapreduce-v1-mrv1&quot;&gt;Apache MapReduce v1 (MRv1)&lt;/h1&gt;

&lt;h2 id=&quot;section-5&quot;&gt;新特性:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;HDFS访问追踪：当&lt;code&gt;mapreduce.job.token.tracking.ids&lt;/code&gt;设置为true时，MRv1任务根据持有的HDFS访问凭证来访问HDFS上的数据。而且，当MRv1其访问数据数据时HDFS日志会记录其访问信息。&lt;/li&gt;
  &lt;li&gt;堆栈跟踪的任务超时： 为了便于调试，当MR任务超时时会累记其堆栈信息.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;KeyOnlyTextInputWriter&lt;/code&gt; 和&lt;code&gt;KeyOnlyTextOutputReader&lt;/code&gt;使工作流不使用分隔符即可写入/读取文本.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-6&quot;&gt;变化的特性：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;用户在使用MRv1压缩包的&lt;code&gt;bin-mapreduce1&lt;/code&gt;目录下的脚本时，不再需要根据情况的不同而设置不同的环境变量了.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-mapreduce-v2-yarn&quot;&gt;Apache MapReduce v2 (YARN)&lt;/h1&gt;

&lt;h2 id=&quot;section-7&quot;&gt;新特性:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;HDFS访问追踪：当&lt;code&gt;mapreduce.job.token.tracking.ids&lt;/code&gt;设置为true时，MRv1任务根据持有的HDFS访问凭证来访问HDFS上的数据。而且，当MRv1其访问数据数据时HDFS日志会记录其访问信.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;KeyOnlyTextInputWriter&lt;/code&gt; 和&lt;code&gt;KeyOnlyTextOutputReader&lt;/code&gt;使工作流不使用分隔符即可写入/读取文本.&lt;/li&gt;
  &lt;li&gt;公平调度器现在可以不用受节点心跳检测的判断影响，从而可以更快的调度&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-oozie&quot;&gt;Apache Oozie&lt;/h1&gt;

&lt;h2 id=&quot;section-8&quot;&gt;新特性:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Pig和Hive现在无需手动操作或配置即可访问 Parquet 文件.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-sentry-&quot;&gt;Apache Sentry (孵化中)&lt;/h1&gt;

&lt;h2 id=&quot;section-9&quot;&gt;新特性:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Hive Metastore服务的访问可以不受&lt;code&gt;IPTables&lt;/code&gt;的限定。在HiveServer2和ImpalaD运行的用户必须要首先在&lt;code&gt;core-site.xml&lt;/code&gt;中配置，然后才可以访问Hive Metastore服务。&lt;br /&gt;
例如，hivemetastore 是Hive Metastore服务的用户。&lt;code&gt;hive&lt;/code&gt;和&lt;code&gt;impala&lt;/code&gt;分别是运行HiveServer2 和 ImpalaD不同用户。按如下的配置，这些用户将被允许访问Hive Metastore服务.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.proxyuser.hivemetastore.groups&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hive，impala&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sentry现在已经集成到Cloudera Search中，配置方法请参考：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Search/latest/Cloudera-Search-User-Guide/csug_sentry.html&quot;&gt; Configuring Sentry for Search&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;原文地址：&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Release-Notes/Whats_New_in_4-5.html&quot;&gt;What’s New in CDH4.5.0&lt;/a&gt;&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2014/03/31/what-is-new-in-CDH4.5.0.html</link>
      <guid>http://blog.javachen.com/2014/03/31/what-is-new-in-CDH4.5.0.html</guid>
      <pubDate>2014-03-31T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Python模拟新浪微博登录</title>
      <description>&lt;p&gt;看到一篇&lt;a href=&quot;http://qinxuye.me/article/simulate-weibo-login-in-python/&quot;&gt;Python模拟新浪微博登录&lt;/a&gt;的文章，想熟悉一下其中实现方式，并且顺便掌握python相关知识点。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;代码&lt;/h1&gt;

&lt;p&gt;下面的代码是来自上面这篇文章，并稍作修改添加了一些注释。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -*- coding: utf-8 -*

import urllib2
import urllib
import cookielib
 
import lxml.html as HTML
 
class Fetcher(object):
    def __init__(self, username=None, pwd=None, cookie_filename=None):
		#获取一个保存cookie的对象
        self.cj = cookielib.LWPCookieJar()
        if cookie_filename is not None:
            self.cj.load(cookie_filename)
		#将一个保存cookie对象，和一个HTTP的cookie的处理器绑定
        self.cookie_processor = urllib2.HTTPCookieProcessor(self.cj)
		#创建一个opener，将保存了cookie的http处理器，还有设置一个handler用于处理http的URL的打开
        self.opener = urllib2.build_opener(self.cookie_processor, urllib2.HTTPHandler)
		#将包含了cookie、http处理器、http的handler的资源和urllib2对象绑定在一起
        urllib2.install_opener(self.opener)
         
        self.username = username
        self.pwd = pwd
        self.headers = {&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 6.1; rv:14.0) Gecko/20100101 Firefox/14.0.1&#39;,
                        &#39;Referer&#39;:&#39;&#39;,&#39;Content-Type&#39;:&#39;application/x-www-form-urlencoded&#39;}
     
    def get_rand(self, url):
        headers = {&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows;U;Windows NT 5.1;zh-CN;rv:1.9.2.9)Gecko/20100824 Firefox/3.6.9&#39;,
                   &#39;Referer&#39;:&#39;&#39;}
        req = urllib2.Request(url ,&quot;&quot;, headers)
        login_page = urllib2.urlopen(req).read()
        rand = HTML.fromstring(login_page).xpath(&quot;//form/@action&quot;)[0]
        passwd = HTML.fromstring(login_page).xpath(&quot;//input[@type=&#39;password&#39;]/@name&quot;)[0]
        vk = HTML.fromstring(login_page).xpath(&quot;//input[@name=&#39;vk&#39;]/@value&quot;)[0]
        return rand, passwd, vk
     
    def login(self, username=None, pwd=None, cookie_filename=None):
        if self.username is None or self.pwd is None:
            self.username = username
            self.pwd = pwd
        assert self.username is not None and self.pwd is not None
         
        url = &#39;http://3g.sina.com.cn/prog/wapsite/sso/login.php?ns=1&amp;amp;revalid=2&amp;amp;backURL=http%3A%2F%2Fweibo.cn%2F&amp;amp;backTitle=%D0%C2%C0%CB%CE%A2%B2%A9&amp;amp;vt=&#39;
		# 获取随机数rand、password的name和vk
        rand, passwd, vk = self.get_rand(url)
        data = urllib.urlencode({&#39;mobile&#39;: self.username,
                                 passwd: self.pwd,
                                 &#39;remember&#39;: &#39;on&#39;,
                                 &#39;backURL&#39;: &#39;http://weibo.cn/&#39;,
                                 &#39;backTitle&#39;: &#39;新浪微博&#39;,
                                 &#39;vk&#39;: vk,
                                 &#39;submit&#39;: &#39;登录&#39;,
                                 &#39;encoding&#39;: &#39;utf-8&#39;})
        url = &#39;http://3g.sina.com.cn/prog/wapsite/sso/&#39; + rand
		
		# 模拟提交登陆
        page =self.fetch(url,data)
        link = HTML.fromstring(page).xpath(&quot;//a/@href&quot;)[0]
        if not link.startswith(&#39;http://&#39;): link = &#39;http://weibo.cn/%s&#39; % link

		# 手动跳转到微薄页面
        self.fetch(link,&quot;&quot;)
		
		# 保存cookie
        if cookie_filename is not None:
            self.cj.save(filename=cookie_filename)
        elif self.cj.filename is not None:
            self.cj.save()
        print &#39;login success!&#39;,data
         
    def fetch(self, url,data):
        print &#39;fetch url: &#39;, url
        req = urllib2.Request(url,data, headers=self.headers)
        return urllib2.urlopen(req).read()

# 开始运行
fet=Fetcher();
fet.login(&quot;huaiyu2006&quot;,&quot;XXXXXX&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上代码引入了一些python的模块，然后创建了一个class封装了login方法。&lt;/p&gt;

&lt;p&gt;以上代码的登录逻辑：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、进入到登陆页面，获取一些关键参数，包括随机数rand、password的name和vk。&lt;/li&gt;
  &lt;li&gt;2、模拟提交登陆，登陆之后跳到微薄页面。&lt;/li&gt;
  &lt;li&gt;3、手动跳转到微薄页面。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;以上代码是模拟手机版微博的登陆，如果你想&lt;strong&gt;模拟登陆网页版的微博&lt;/strong&gt;，你可以参考下面两个项目中的代码：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chineking/cola/blob/master/contrib/weibo/login.py&quot;&gt;cola&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/CnPaMeng/WeiboMsgBackupGUI/blob/master/sina/loginsinacom.py&quot;&gt;WeiboMsgBackupGUI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;python&quot;&gt;Python模块&lt;/h1&gt;
&lt;p&gt;## Python urllib模块&lt;/p&gt;

&lt;p&gt;Python urllib模块提供了一个从指定的URL地址获取网页数据，然后对其进行分析处理，获取想要的数据。&lt;/p&gt;

&lt;p&gt;1、urllib模块提供的urlopen函数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;➜  py-test  python
Python 2.7.5+ (default, Feb 27 2014, 19:37:08) 
[GCC 4.8.1] on linux2
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&amp;gt;&amp;gt;&amp;gt; import urllib
&amp;gt;&amp;gt;&amp;gt; help(urllib.urlopen)

Help on function urlopen in module urllib:

urlopen(url, data=None, proxies=None)
    Create a file-like object for the specified URL to read from.
(END)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;urllib.urlopen创建一个类文件对象为指定的url来读取：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;参数url表示远程数据的路径，一般是http或者ftp路径。&lt;/li&gt;
  &lt;li&gt;参数data表示以get或者post方式提交到url的数据。&lt;/li&gt;
  &lt;li&gt;参数proxies表示用于代理的设置。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import urllib

print urllib.urlopen(&#39;http://www.baidu.com&#39;).read()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;urlopen返回一个类文件对象，它提供了如下方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）read() , readline() , readlines()，fileno()和close()： 这些方法的使用与文件对象完全一样。&lt;/li&gt;
  &lt;li&gt;2）info()：返回一个httplib.HTTPMessage 对象，表示远程服务器返回的头信息。&lt;/li&gt;
  &lt;li&gt;3）getcode()：返回Http状态码，如果是http请求，200表示请求成功完成;404表示网址未找到。&lt;/li&gt;
  &lt;li&gt;4）geturl()：返回请求的url地址。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2.urllibe模块提供的urlretrieve函数。&lt;/p&gt;

&lt;p&gt;urlretrieve方法直接将远程数据下载到本地。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;参数finename指定了保存本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。）&lt;/li&gt;
  &lt;li&gt;参数reporthook是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。&lt;/li&gt;
  &lt;li&gt;参数data指post到服务器的数据，该方法返回一个包含两个元素的(filename, headers)元组，filename表示保存到本地的路径，header表示服务器的响应头。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;示例：&lt;/strong&gt;urlretrieve方法下载文件实例，可以显示下载进度。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#!/usr/bin/python
#encoding:utf-8
import urllib
import os
def Schedule(a,b,c):
    &#39;&#39;&#39;&#39;&#39;
    a:已经下载的数据块
    b:数据块的大小
    c:远程文件的大小
   &#39;&#39;&#39;
    per = 100.0 * a * b / c
    if per &amp;gt; 100 :
        per = 100
    print &#39;%.2f%%&#39; % per
url = &#39;http://www.python.org/ftp/python/2.7.5/Python-2.7.5.tar.bz2&#39;
#local = url.split(&#39;/&#39;)[-1]
local = os.path.join(&#39;/data/software&#39;,&#39;Python-2.7.5.tar.bz2&#39;)
urllib.urlretrieve(url,local,Schedule)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.辅助方法&lt;/p&gt;

&lt;p&gt;urllib中还提供了一些辅助方法，用于对url进行编码、解码。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;urllib.quote(string[, safe])：对字符串进行编码。参数safe指定了不需要编码的字符;&lt;/li&gt;
  &lt;li&gt;urllib.unquote(string) ：对字符串进行解码；&lt;/li&gt;
  &lt;li&gt;urllib.quote_plus(string [ , safe ] ) ：与urllib.quote类似，但这个方法用’+’来替换’ ‘，而quote用’%20’来代替’ ‘&lt;/li&gt;
  &lt;li&gt;urllib.unquote_plus(string ) ：对字符串进行解码；&lt;/li&gt;
  &lt;li&gt;urllib.urlencode(query[, doseq])：将dict或者包含两个元素的元组列表转换成url参数。例如 字典{‘name’: ‘dark-bull’, ‘age’: 200}将被转换为”name=dark-bull&amp;amp;age=200”&lt;/li&gt;
  &lt;li&gt;urllib.pathname2url(path)：将本地路径转换成url路径；&lt;/li&gt;
  &lt;li&gt;urllib.url2pathname(path)：将url路径转换成本地路径；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上面的练习可以知道，urlopen可以轻松获取远端html页面信息，然后通过python正则对所需要的数据进行分析，匹配出想要用的数据，在利用urlretrieve将数据下载到本地。对于访问受限或者对连接数有限制的远程url地址可以采用proxies（代理的方式）连接，如果远程数据量过大，单线程下载太慢的话可以采用多线程下载，这个就是传说中的爬虫。&lt;/p&gt;

&lt;h2 id=&quot;python-urllib2&quot;&gt;Python urllib2模块&lt;/h2&gt;

&lt;p&gt;客户端与服务器端通过request与response来沟通，客户端先向服务端发送request，然后接收服务端返回的response&lt;/p&gt;

&lt;p&gt;urllib2提供了request的类，可以让用户在发送请求前先构造一个request的对象，然后通过urllib2.urlopen方法来发送请求&lt;/p&gt;

&lt;p&gt;更详细的说明请参考：&lt;a href=&quot;http://zhuoqiang.me/python-urllib2-usage.html&quot;&gt;http://zhuoqiang.me/python-urllib2-usage.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;python--cookielib&quot;&gt;Python  cookielib模块&lt;/h2&gt;

&lt;p&gt;cookielib模块的主要作用是提供可存储cookie的对象，以便于与urllib2模块配合使用来访问Internet资源。例如可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送。coiokielib模块用到的对象主要有下面几个：CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CookieJar管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。&lt;/li&gt;
  &lt;li&gt;FileCookieJar检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。&lt;/li&gt;
  &lt;li&gt;MozillaCookieJar创建与Mozilla浏览器cookies.txt兼容的FileCookieJar实例。&lt;/li&gt;
  &lt;li&gt;LWPCookieJar创建与libwww-perl的Set-Cookie3文件格式兼容的FileCookieJar实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;cookielib模块一般与urllib2模块配合使用，主要用在urllib2.build_oper()函数中作为urllib2.HTTPCookieProcessor()的参数。&lt;/p&gt;

&lt;p&gt;使用方法如下面登录人人网的代码:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#! /usr/bin/env python
#coding=utf-8
import urllib2
import urllib
import cookielib
data={&quot;email&quot;:&quot;用户名&quot;,&quot;password&quot;:&quot;密码&quot;}  #登陆用户名和密码
post_data=urllib.urlencode(data)
cj=cookielib.CookieJar()
opener=urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
headers ={&quot;User-agent&quot;:&quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1&quot;}
req=urllib2.Request(&quot;http://www.renren.com/PLogin.do&quot;,post_data,headers)
content=opener.open(req)
print content2.read().decode(&quot;utf-8&quot;).encode(&quot;gbk&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;python-lxml&quot;&gt;Python lxml模块&lt;/h2&gt;

&lt;p&gt;具体用法可以参考 &lt;a href=&quot;http://www.ibm.com/developerworks/cn/xml/x-hiperfparse/&quot;&gt;使用由 Python 编写的 lxml 实现高性能 XML 解析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;上面python脚本主要是使用了lxml的xpath语法进行快速查找。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/03/18/simulate-weibo-login-in-python.html</link>
      <guid>http://blog.javachen.com/2014/03/18/simulate-weibo-login-in-python.html</guid>
      <pubDate>2014-03-18T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Solr的schema.xml</title>
      <description>&lt;p&gt;schema.xml是Solr一个配置文件，它包含了你的文档所有的字段，以及当文档被加入索引或查询字段时，这些字段是如何被处理的。这个文件被存储在Solr主文件夹下的conf目录下，默认的路径&lt;code&gt;./solr/conf/schema.xml&lt;/code&gt;，也可以是Solr webapp的类加载器所能确定的路径。在下载的Solr包里，有一个schema的样例文件，用户可以从那个文件出发，来观察如何编写自己的Schema.xml。&lt;/p&gt;

&lt;h1 id=&quot;type&quot;&gt;type节点&lt;/h1&gt;

&lt;p&gt;先来看下type节点，这里面定义FieldType子节点，包括name、class、positionIncrementGap等一些参数。必选参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;name：就是这个FieldType的名称。&lt;/li&gt;
  &lt;li&gt;class：指向org.apache.solr.analysis包里面对应的class名称，用来定义这个类型的行为。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其他可选的属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sortMissingLast，sortMissingFirst两个属性是用在可以内在使用String排序的类型上，默认false，适用于字段类型：string、boolean、sint、slong、sfloat、sdouble、pdate。&lt;/li&gt;
  &lt;li&gt;sortMissingLast=”true”，没有该field的数据排在有该field的数据之后，而不管请求时的排序规则，在Java中对应的意思就是，该字段为NULL，排在后面。&lt;/li&gt;
  &lt;li&gt;sortMissingFirst=”true”，排序规则与sortMissingLast相反。&lt;/li&gt;
  &lt;li&gt;positionIncrementGap：可选属性，定义在同一个文档中此类型数据的空白间隔，避免短语匹配错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在配置中，string类型的class是solr.StrField，而这个字段是不会被分析存储的，也就是说不会被分词。&lt;/p&gt;

&lt;p&gt;而对于文章或者长文本来说，我们必须对其进行分词才能保证搜索某些字段时能够给出正确的结果。这时我们就可以用到另外一个class，solr.TextField。它允许用户通过分析器来定制索引和查询，分析器包括一个分词器（tokenizer）和多个过滤器（filter） 。&lt;/p&gt;

&lt;p&gt;一个标准的分词：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;fieldType name=&quot;text_general&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
    &amp;lt;analyzer type=&quot;index&quot;&amp;gt;
        &amp;lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot; /&amp;gt;
        &amp;lt;filter class=&quot;solr.LowerCaseFilterFactory&quot; /&amp;gt;
    &amp;lt;/analyzer&amp;gt;
    &amp;lt;analyzer type=&quot;query&quot;&amp;gt;
        &amp;lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot; /&amp;gt;
        &amp;lt;filter class=&quot;solr.LowerCaseFilterFactory&quot; /&amp;gt;
        &amp;lt;filter class=&quot;solr.StopFilterFactory&quot; ignoreCase=&quot;true&quot; words=&quot;stopwords.txt&quot; enablePositionIncrements=&quot;true&quot; /&amp;gt;
    &amp;lt;/analyzer&amp;gt;
&amp;lt;/fieldType&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分词用的依旧是fieldType，为的是在下面的field中能够用到。有两个analyzer，一个是index，一个是query，index是针对于所有，query是针对于搜索。&lt;/p&gt;

&lt;p&gt;tokenizer节点当然就是对应分析链中的起点Tokenizer。接下来串联了2个filter，分别是solr.StopFilterFactory，solr.LowerCaseFilterFactory。stop word filter就是把那些the、 of、 on之类的词从token中去除掉，由于这类词在文档中出现的频率非常高，而对文档的特征又没什么影响，所以这类词对查询没什么意义。Lower case filter的作用是将所有的token转换成小写，也就是在最终的index中保存的都是小写&lt;/p&gt;

&lt;p&gt;你也可以定义一个analyzer，例如使用mmseg4j进行中文分词：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;fieldType name=&quot;text_zh&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
    &amp;lt;analyzer&amp;gt; 
        &amp;lt;tokenizer class=&quot;com.chenlb.mmseg4j.solr.MMSegTokenizerFactory&quot; mode=&quot;complex&quot; /&amp;gt;
    &amp;lt;/analyzer&amp;gt;
&amp;lt;/fieldType&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;filed&quot;&gt;filed节点&lt;/h1&gt;

&lt;p&gt;filed节点用于定义数据源字段所使用的搜索类型与相关设置。含有以下属性&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;name：数据源字段名，搜索使用到。&lt;/li&gt;
  &lt;li&gt;type：搜索类型名例如中文ika搜索名text_ika，对应于fieldType中的name。不需要分词的字符串类型，string即可，如果需要分词，用上面配置好的分词type。&lt;/li&gt;
  &lt;li&gt;indexed：是否被索引，只有设置为true的字段才能进行搜索排序分片(earchable、 sortable、 facetable)。&lt;/li&gt;
  &lt;li&gt;stored：是否存储内容，如果不需要存储字段值，尽量设置为false以提高效率。&lt;/li&gt;
  &lt;li&gt;multiValued：是否为多值类型，SOLR允许配置多个数据源字段存储到一个搜索字段中。多个值必须为true，否则有可能抛出异常。&lt;/li&gt;
  &lt;li&gt;omitNorms：是否忽略掉Norm，可以节省内存空间，只有全文本field和need an index-time boost的field需要norm。（具体没看懂，注释里有矛盾）&lt;/li&gt;
  &lt;li&gt;termVectors：当设置true，会存储 term vector。当使用MoreLikeThis，用来作为相似词的field应该存储起来。&lt;/li&gt;
  &lt;li&gt;termPositions：存储 term vector中的地址信息，会消耗存储开销。&lt;/li&gt;
  &lt;li&gt;termOffsets：存储 term vector 的偏移量，会消耗存储开销。&lt;/li&gt;
  &lt;li&gt;default：如果没有属性需要修改，就可以用这个标识下。&lt;/li&gt;
  &lt;li&gt;docValues：Solr 4.2中加入了该属性&lt;/li&gt;
  &lt;li&gt;docValuesFormat：可选的值为Disk或者Memory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;field name=&quot;manu_exact&quot; type=&quot;string&quot; indexed=&quot;false&quot; stored=&quot;false&quot; docValues=&quot;true&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;copyfield&quot;&gt;copyField节点&lt;/h1&gt;

&lt;p&gt;如果我们的搜索需要搜索多个字段该怎么办呢？这时候，我们就可以使用copyField。代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;copyField source=&quot;name&quot; dest=&quot;all&quot; maxChars=&quot;30000&quot;/&amp;gt;
&amp;lt;copyField source=&quot;address&quot; dest=&quot;all&quot; maxChars=&quot;30000&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;作用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将多个field的数据放在一起同时搜索，提供速度&lt;/li&gt;
  &lt;li&gt;将一个field的数据拷贝到另一个，可以用2种不同的方式来建立索引&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们将所有的中文分词字段全部拷贝至all中，当我们进行全文检索是，只用搜索all字段就OK了。&lt;/p&gt;

&lt;p&gt;其包含属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;source：源field字段&lt;/li&gt;
  &lt;li&gt;dest：目标field字段&lt;/li&gt;
  &lt;li&gt;maxChars：最多拷贝多少字符&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意，这里的目标字段必须支持多值，最好不要存储，因为他只是做搜索。indexed为true，stored为false。&lt;/p&gt;

&lt;p&gt;copyField节点和field节点都在fields节点之内。&lt;/p&gt;

&lt;h1 id=&quot;dynamicfield&quot;&gt;dynamicField节点&lt;/h1&gt;

&lt;p&gt;动态字段，没有具体名称的字段，用dynamicField字段&lt;/p&gt;

&lt;p&gt;如：name为&lt;code&gt;*_i&lt;/code&gt;，定义它的type为int，那么在使用这个字段的时候，任务以&lt;code&gt;_i&lt;/code&gt;结果的字段都被认为符合这个定义。如&lt;code&gt;name_i&lt;/code&gt;、 &lt;code&gt;school_i&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;dynamicField name=&quot;*_i&quot;  type=&quot;int&quot;    indexed=&quot;true&quot;  stored=&quot;true&quot;/&amp;gt; 
&amp;lt;dynamicField name=&quot;*_s&quot;  type=&quot;string&quot;  indexed=&quot;true&quot;  stored=&quot;true&quot;/&amp;gt;
&amp;lt;dynamicField name=&quot;*_l&quot;  type=&quot;long&quot;   indexed=&quot;true&quot;  stored=&quot;true&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;uniquekey&quot;&gt;uniqueKey节点&lt;/h1&gt;

&lt;p&gt;solr必须设置一个唯一字段，常设置为id，此唯一一段有uniqueKey节点指定。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;uniqueKey&amp;gt;id&amp;lt;/uniqueKey&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;defaultsearchfield&quot;&gt;defaultSearchField节点&lt;/h1&gt;

&lt;p&gt;默认搜索的字段，我们已经将需要搜索的字段拷贝至all字段了，在这里设为all即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;defaultSearchField&amp;gt;all&amp;lt;/defaultSearchField&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;solrqueryparser&quot;&gt;solrQueryParser节点&lt;/h1&gt;

&lt;p&gt;默认搜索操作符参数，及搜索短语间的逻辑，用AND增加准确率，用OR增加覆盖面，建议用AND，也可在搜索语句中定义。例如搜索“手机 苹果”，使用AND默认搜索为“手机AND苹果“。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;solrQueryParser defaultOperator=&quot;OR&quot;/&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;similarity&quot;&gt;similarity节点&lt;/h1&gt;

&lt;p&gt;Similarity式lucene中的一个类，用来在搜索过程中对一个文档进行评分。该类可以做些修改以支持自定义的排序。在Solr4中，你可以为每一个field配置一个不同的similarity，你也可以在schema.xml中使用DefaultSimilarityFactory类配置一个全局的similarity。&lt;/p&gt;

&lt;p&gt;你可以使用默认的工厂类来创建一个实例，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;similarity class=&quot;solr.DefaultSimilarityFactory&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以使用其他的工厂类，然后设置一些可选的初始化参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;similarity class=&quot;solr.DFRSimilarityFactory&quot;&amp;gt;
  &amp;lt;str name=&quot;basicModel&quot;&amp;gt;P&amp;lt;/str&amp;gt;
  &amp;lt;str name=&quot;afterEffect&quot;&amp;gt;L&amp;lt;/str&amp;gt;
  &amp;lt;str name=&quot;normalization&quot;&amp;gt;H2&amp;lt;/str&amp;gt;
  &amp;lt;float name=&quot;c&quot;&amp;gt;7&amp;lt;/float&amp;gt;
&amp;lt;/similarity&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Solr 4中，你可以为每一个field配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;fieldType name=&quot;text_ib&quot;&amp;gt;
   &amp;lt;analyzer/&amp;gt;
   &amp;lt;similarity class=&quot;solr.IBSimilarityFactory&quot;&amp;gt;
      &amp;lt;str name=&quot;distribution&quot;&amp;gt;SPL&amp;lt;/str&amp;gt;
      &amp;lt;str name=&quot;lambda&quot;&amp;gt;DF&amp;lt;/str&amp;gt;
      &amp;lt;str name=&quot;normalization&quot;&amp;gt;H2&amp;lt;/str&amp;gt;
   &amp;lt;/similarity&amp;gt;
&amp;lt;/fieldType&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面例子中，使用了DFRSimilarityFactory和IBSimilarityFactory，这里还有一些其他的实现类。在Solr 4.2中加入了SweetSpotSimilarityFactory。其他还有：BM25SimilarityFactory、SchemaSimilarityFactory等。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://www.cnblogs.com/wrt2010/archive/2012/11/14/2769521.html&quot;&gt;Solr配置，schema.xml的配置，以及中文分词&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;https://cwiki.apache.org/confluence/display/solr/Other+Schema+Elements&quot;&gt;Other Schema Elements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/03/15/schema-in-solr.html</link>
      <guid>http://blog.javachen.com/2014/03/15/schema-in-solr.html</guid>
      <pubDate>2014-03-15T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>IDH HBase中实现的一些特性</title>
      <description>&lt;p&gt;IDH为Intel’s Distribution of Hadoop的简称，中文为英特尔Hadoop发行版，目前应该没有人在维护该产品了。这里简单介绍一下IDH HBase中实现的一些特性。&lt;/p&gt;

&lt;p&gt;以下部分内容摘自IDH官方的一些文档，部分内容来自我的整理：&lt;/p&gt;

&lt;p&gt;1、 单调数据的加盐处理&lt;/p&gt;

&lt;p&gt;对于写入的rowkey是基本单调的（例如时序数据），IDH引入了一个新的接口：SaltedTableInterface&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提高近乎透明的“加盐”，方便使用&lt;/li&gt;
  &lt;li&gt;封装了get、scan、put、delete等操作&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、提供了Rolling Scanner应对HFile数量大量增加情况下的get、scan性能&lt;/p&gt;

&lt;p&gt;3、提供了ParallelClientScanner加速大范围查询性能&lt;/p&gt;

&lt;p&gt;具体实现，请参考&lt;a href=&quot;/2014/06/12/hbase-parallel-client-scanner.html&quot;&gt;HBase客户端实现并行扫描&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;4、使得协作器实用化，从而可使用协作器来进行计算&lt;/p&gt;

&lt;p&gt;相关说明，可以参考&lt;a href=&quot;/2014/06/12/hbase-aggregate-client.html&quot;&gt;HBase实现简单聚合计算&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;5、提供基于lucene的全文检索&lt;/p&gt;

&lt;p&gt;6、提供大对象的高效存储&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;类似Oracle的BLOB存储&lt;/li&gt;
  &lt;li&gt;对用户透明&lt;/li&gt;
  &lt;li&gt;2x以上的写入性能，还有些进步空间&lt;/li&gt;
  &lt;li&gt;2x的随机访问性能&lt;/li&gt;
  &lt;li&gt;1.3x的scan性能&lt;/li&gt;
  &lt;li&gt;接近直接写入hdfs性能&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;7、引入交互式的hive over hbase&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;完全的hive支持，常用功能（select、group by、top n等等）用hbase协作器实现，其余功能（大表关联等等）用mapreduce无缝对接&lt;/li&gt;
  &lt;li&gt;去除mapreduce的overhead，大大地减少了数据传输&lt;/li&gt;
  &lt;li&gt;性能有3x-10x提升&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体介绍，请参考&lt;a href=&quot;/2014/06/12/intro-of-hive-over-hbase.html&quot;&gt;Hive Over HBase的介绍&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;8、支持跨数据中心的大表&lt;/p&gt;

&lt;p&gt;9、HBase中支持对某列族设置副本数&lt;/p&gt;

&lt;p&gt;10、可以通过定时任务设置文件压缩合并频率&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2014/03/15/new-features-in-idh-hbase.html</link>
      <guid>http://blog.javachen.com/2014/03/15/new-features-in-idh-hbase.html</guid>
      <pubDate>2014-03-15T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>在Solr中使用中文分词</title>
      <description>&lt;p&gt;使用全文检索，中文分词是离不开的，这里我采用的是 &lt;strong&gt;mmseg4j&lt;/strong&gt; 分词器。mmseg4j分词器内置了对solr的支持，最新版本可支持4.X版本的sorl，使用起来很是方便。&lt;/p&gt;

&lt;h1 id=&quot;mmseg4j&quot;&gt;下载mmseg4j&lt;/h1&gt;

&lt;p&gt;GoogleCode地址：&lt;a href=&quot;http://code.google.com/p/mmseg4j/&quot;&gt;http://code.google.com/p/mmseg4j/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;请下载最新版本：mmseg4j-1.9.1，然后将mmseg4j-1.9.1/dist下的jar包拷贝至solr.war的lib目录，例如：&lt;em&gt;apache-tomcat-6.0.36/webapps/solr/WEB-INF/lib/&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;schemaxml&quot;&gt;配置schema.xml&lt;/h1&gt;

&lt;p&gt;使用mmseg4j中文分词器，首先需要在schema.xml文件中配置一个fieldType节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;fieldType name=&quot;text_zh&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
    &amp;lt;analyzer&amp;gt; 
        &amp;lt;tokenizer class=&quot;com.chenlb.mmseg4j.solr.MMSegTokenizerFactory&quot; mode=&quot;complex&quot; /&amp;gt;
    &amp;lt;/analyzer&amp;gt;
&amp;lt;/fieldType&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以在field节点中引用该filedType了，假设你有个字段叫content需要支持中文分词，则需要定义示例filed节点如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;field name=&quot;content&quot; type=&quot;text_zh&quot; indexed=&quot;true&quot; stored=&quot;false&quot; multiValued=&quot;true&quot;/&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，重启solr服务器。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;测试&lt;/h1&gt;

&lt;p&gt;我这里使用的是broadleaf项目(broadleaf是什么，请参考：&lt;a href=&quot;/2014/03/13/improve-the-search-function-in-broadleaf-project.html&quot;&gt;BroadLeaf项目搜索功能改进&lt;/a&gt;)中的schema.xml，需要修改成如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&amp;gt;
&amp;lt;schema name=&quot;example&quot; version=&quot;1.5&quot;&amp;gt;
    &amp;lt;fields&amp;gt;
        &amp;lt;field name=&quot;namespace&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;id&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;productId&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;category&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;explicitCategory&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;searchable&quot; type=&quot;text_zh&quot; indexed=&quot;true&quot; stored=&quot;false&quot; /&amp;gt;
        &amp;lt;field name=&quot;_version_&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;false&quot;/&amp;gt;
        &amp;lt;dynamicField name=&quot;*_searchable&quot; type=&quot;text_zh&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        
        &amp;lt;dynamicField name=&quot;*_i&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_is&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_s&quot; type=&quot;text_zh&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_ss&quot; type=&quot;text_zh&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_l&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_ls&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_t&quot; type=&quot;text_zh&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_txt&quot; type=&quot;text_zh&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_b&quot; type=&quot;boolean&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_bs&quot; type=&quot;boolean&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_d&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_ds&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_p&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;

        &amp;lt;dynamicField name=&quot;*_dt&quot; type=&quot;date&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_dts&quot; type=&quot;date&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;

        &amp;lt;!-- some trie-coded dynamic fields for faster range queries --&amp;gt;
        &amp;lt;dynamicField name=&quot;*_ti&quot; type=&quot;tint&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_tl&quot; type=&quot;tlong&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_td&quot; type=&quot;tdouble&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_tdt&quot; type=&quot;tdate&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
    &amp;lt;/fields&amp;gt;
    
    &amp;lt;uniqueKey&amp;gt;id&amp;lt;/uniqueKey&amp;gt;

    &amp;lt;types&amp;gt;
		&amp;lt;fieldType name=&quot;text_zh&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
			&amp;lt;analyzer&amp;gt; 
				&amp;lt;tokenizer class=&quot;com.chenlb.mmseg4j.solr.MMSegTokenizerFactory&quot; mode=&quot;complex&quot; /&amp;gt;
			&amp;lt;/analyzer&amp;gt;
		&amp;lt;/fieldType&amp;gt;
        &amp;lt;fieldType name=&quot;string&quot; class=&quot;solr.StrField&quot; sortMissingLast=&quot;true&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;boolean&quot; class=&quot;solr.BoolField&quot; sortMissingLast=&quot;true&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;int&quot; class=&quot;solr.TrieIntField&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;long&quot; class=&quot;solr.TrieLongField&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;double&quot; class=&quot;solr.TrieDoubleField&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;tint&quot; class=&quot;solr.TrieIntField&quot; precisionStep=&quot;8&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;tlong&quot; class=&quot;solr.TrieLongField&quot; precisionStep=&quot;8&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;tdouble&quot; class=&quot;solr.TrieDoubleField&quot; precisionStep=&quot;8&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;date&quot; class=&quot;solr.TrieDateField&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;tdate&quot; class=&quot;solr.TrieDateField&quot; precisionStep=&quot;6&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;

        &amp;lt;fieldType name=&quot;text_general&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
            &amp;lt;analyzer type=&quot;index&quot;&amp;gt;
                &amp;lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot; /&amp;gt;
                &amp;lt;filter class=&quot;solr.LowerCaseFilterFactory&quot; /&amp;gt;
            &amp;lt;/analyzer&amp;gt;
            &amp;lt;analyzer type=&quot;query&quot;&amp;gt;
                &amp;lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot; /&amp;gt;
                &amp;lt;filter class=&quot;solr.LowerCaseFilterFactory&quot; /&amp;gt;
            &amp;lt;/analyzer&amp;gt;
        &amp;lt;/fieldType&amp;gt;
    &amp;lt;/types&amp;gt;
&amp;lt;/schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，在浏览器中进行测试,输入下面url：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://192.168.56.123:8080/solr/primary_shard2_replica1/select?q=*%3A*&amp;amp;wt=json&amp;amp;indent=true&amp;amp;rows=6&amp;amp;start=0&amp;amp;fq=category%3A2002&amp;amp;fq=namespace%3Ad&amp;amp;fq=%7B%21tag%3Da%7D%28en_US_name_s%3A大理%29
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以上搜索的是category=2002,namespace=d，&lt;code&gt;en_US_name_s&lt;/code&gt;=大理的记录，查询结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;responseHeader&quot;:{
    &quot;status&quot;:0,
    &quot;QTime&quot;:20},
  &quot;response&quot;:{&quot;numFound&quot;:1,&quot;start&quot;:0,&quot;maxScore&quot;:1.0,&quot;docs&quot;:[
      {
        &quot;namespace&quot;:&quot;d&quot;,
        &quot;id&quot;:&quot;5&quot;,
        &quot;productId&quot;:5,
        &quot;explicitCategory&quot;:[2002],
        &quot;category_2002_sort_i&quot;:4,
        &quot;category&quot;:[2002,
          1,
          2],
        &quot;price_p&quot;:480.0,
        &quot;en_US_name_t&quot;:&quot;大理风情&quot;,
        &quot;en_name_t&quot;:&quot;大理风情&quot;,
        &quot;en_US_name_s&quot;:&quot;大理风情&quot;,
        &quot;en_name_s&quot;:&quot;大理风情&quot;,
        &quot;en_US_desc_t&quot;:&quot;体验不一样的风景&quot;,
        &quot;en_desc_t&quot;:&quot;体验不一样的风景&quot;,
        &quot;en_US_ldesc_t&quot;:&quot;大理风情养老基地坐落在美丽的洱海边，这里依山傍水，鲜花遍地，适合老年人居住、旅游。&quot;,
        &quot;en_ldesc_t&quot;:&quot;大理风情养老基地坐落在美丽的洱海边，这里依山傍水，鲜花遍地，适合老年人居住、旅游。&quot;,
        &quot;en_US_city_t&quot;:&quot;5329&quot;,
        &quot;en_city_t&quot;:&quot;5329&quot;,
        &quot;en_US_city_i&quot;:5329,
        &quot;en_city_i&quot;:5329,
        &quot;en_US_hotelType_t&quot;:&quot;A&quot;,
        &quot;en_hotelType_t&quot;:&quot;A&quot;,
        &quot;en_US_hotelType_s&quot;:&quot;A&quot;,
        &quot;en_hotelType_s&quot;:&quot;A&quot;,
        &quot;en_US_county_t&quot;:&quot;532901&quot;,
        &quot;en_county_t&quot;:&quot;532901&quot;,
        &quot;en_US_county_i&quot;:532901,
        &quot;en_county_i&quot;:532901,
        &quot;en_US_estatePrice_p&quot;:480.0,
        &quot;en_estatePrice_p&quot;:480.0,
        &quot;_version_&quot;:1462514915941023744}]
  }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过查询结果，可以知道：只搜索”大理”关键字，可以查询出&lt;code&gt;en_US_name_s&lt;/code&gt;为”大理风情”的记录。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://blog.csdn.net/zhyh1986/article/details/9856115&quot;&gt;Solr4.4的安装与配置&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/03/14/split-chinese-in-solr.html</link>
      <guid>http://blog.javachen.com/2014/03/14/split-chinese-in-solr.html</guid>
      <pubDate>2014-03-14T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>BroadLeaf项目集成SolrCloud</title>
      <description>&lt;p&gt;《&lt;a href=&quot;/2014/03/13/improve-the-search-function-in-broadleaf-project.html&quot;&gt;BroadLeaf项目搜索功能改进&lt;/a&gt;》一文中介绍了 BroadLeaf 项目中如何改进搜索引擎这一块的代码，其中使用的是单节点的 solr 服务器，这篇文章主要介绍 BroadLeaf 项目如何集成 SolrCloud 集群。&lt;/p&gt;

&lt;h1 id=&quot;solrcloud&quot;&gt;1、SolrCloud环境搭建&lt;/h1&gt;

&lt;p&gt;参考 《&lt;a href=&quot;/2014/03/10/how-to-install-solrcloud.html&quot;&gt;Apache SolrCloud安装&lt;/a&gt;》，搭建Solr集群环境，将 Demosite 所用的 Solr 配置文件 solrconfig.xml 和 schema.xml 上传到 zookeeper 集群中，保证成功启动 Solr 集群。&lt;/p&gt;

&lt;h1 id=&quot;searcheservice&quot;&gt;2、扩展SearcheService类&lt;/h1&gt;

&lt;p&gt;扩展SearchService类的步骤与单节点集成一致，此处不再叙述。&lt;/p&gt;

&lt;h1 id=&quot;solr&quot;&gt;3、修改Solr相关配置文件&lt;/h1&gt;

&lt;p&gt;a) 删除site模块中的site/src/main/webapp/WEB-INF/applicationContext.xml中的以下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;solrEmbedded&quot; class=&quot;java.lang.String&quot;&amp;gt;
          &amp;lt;constructor-arg value=&quot;solrhome&quot;/&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean id=&quot;blSearchService&quot; class=&quot;org.broadleafcommerce.core.search.service.solr.SolrSearchServiceImpl&quot;&amp;gt;
        &amp;lt;constructor-arg name=&quot;solrServer&quot; ref=&quot;${solr.source}&quot; /&amp;gt;
        &amp;lt;constructor-arg name=&quot;reindexServer&quot; ref=&quot;${solr.source.reindex}&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b)删除site模块的site/src/main/resources/runtime-properties/common.properties中以下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;solr.source=solrEmbedded
solr.source.reindex=solrEmbedded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c)在core模块中core/src/main/resources/applicationContext.xml添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;solrServer&quot; class=&quot;org.apache.solr.client.solrj.impl.CloudSolrServer&quot;&amp;gt;
        &amp;lt;constructor-arg value=&quot;${solr.url}&quot;/&amp;gt;
        &amp;lt;property name=&quot;defaultCollection&quot; value=&quot;product&quot; /&amp;gt;
        &amp;lt;property name=&quot;zkClientTimeout&quot; value=&quot;20000&quot; /&amp;gt;
        &amp;lt;property name=&quot;zkConnectTimeout&quot; value=&quot;1000&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean id=&quot;solrReindexServer&quot; class=&quot;org.apache.solr.client.solrj.impl.CloudSolrServer&quot;&amp;gt;
        &amp;lt;constructor-arg value=&quot;${solr.url.reindex}&quot; /&amp;gt;
        &amp;lt;property name=&quot;defaultCollection&quot; value=&quot;product&quot; /&amp;gt;
        &amp;lt;property name=&quot;zkClientTimeout&quot; value=&quot;20000&quot; /&amp;gt;
        &amp;lt;property name=&quot;zkConnectTimeout&quot; value=&quot;1000&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean id=&quot;blSearchService&quot;         class=&quot;org.broadleafcommerce.core.search.service.solr.ExtSolrSearchServiceImpl&quot;&amp;gt;
        &amp;lt;constructor-arg name=&quot;solrServer&quot; ref=&quot;${solr.source}&quot; /&amp;gt;
        &amp;lt;constructor-arg name=&quot;reindexServer&quot; ref=&quot;${solr.source.reindex}&quot;/&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：上述配置中的defaultCollection的值product对应solr集群的collection名字，根据实际情况修改此处的值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;d) 在 core模块中core/src/main/resources/runtime-properties/common-shared.properties添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;solr.url=192.168.56.121\:2181,192.168.56.122\:2181,1192.168.56.123\:2181
solr.url.reindex=192.168.56.121\:2181,192.168.56.122\:2181,1192.168.56.123\:2181
solr.source=solrServer
solr.source.reindex=solrReindexServer
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;rebuildindex&quot;&gt;4、重写rebuildIndex方法&lt;/h1&gt;

&lt;p&gt;在core模块的org.broadleafcommerce.core.search.service.solr包下添加LLSolrIndexServiceImpl，重写源码broadleaf-framework/SolrIndexServiceImpl中的rebuildIndex()方法，屏蔽如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//      // Swap the active and the reindex cores
//      shs.swapActiveCores();
//      // If we are not in single core mode, we delete the documents for the unused core after swapping
//      if (!SolrContext.isSingleCoreMode()) {
//          deleteAllDocuments();
//      }
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;5、修改定时任务&lt;/h1&gt;

&lt;p&gt;web系统启动时候，会查询数据库中商品，然后重建索引。该功能在applicationContext.xml中已经定义了定时任务，修改rebuildIndexJobDetail中的targetObject，对应rebuildIndex所在的服务类，如下：&lt;/p&gt;

&lt;p&gt;```xml&lt;/p&gt;
&lt;bean id=&quot;rebuildIndexJobDetail&quot; class=&quot;org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean&quot;&gt;
        &lt;property name=&quot;targetObject&quot; ref=&quot;llSolrIndexService&quot; /&gt;
        &lt;property name=&quot;targetMethod&quot; value=&quot;rebuildIndex&quot; /&gt;
    &lt;/bean&gt;
&lt;pre&gt;&lt;code&gt;
如果需要手动创建索引，则需要取消applicationContext.xml中定义的定时任务，步骤如下：

  a）去掉如下代码：

```xml  
&amp;lt;bean id=&quot;rebuildIndexJobDetail&quot; class=&quot;org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean&quot;&amp;gt;
    &amp;lt;property name=&quot;targetObject&quot; ref=&quot;blSearchService&quot; /&amp;gt;
    &amp;lt;property name=&quot;targetMethod&quot; value=&quot;rebuildIndex&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean id=&quot;rebuildIndexTrigger&quot; class=&quot;org.springframework.scheduling.quartz.SimpleTriggerBean&quot;&amp;gt;
    &amp;lt;property name=&quot;jobDetail&quot; ref=&quot;rebuildIndexJobDetail&quot; /&amp;gt;
    &amp;lt;property name=&quot;startDelay&quot; value=&quot;${solr.index.start.delay}&quot; /&amp;gt;
    &amp;lt;property name=&quot;repeatInterval&quot; value=&quot;${solr.index.repeat.interval}&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean class=&quot;org.springframework.scheduling.quartz.SchedulerFactoryBean&quot;&amp;gt;
    &amp;lt;property name=&quot;triggers&quot;&amp;gt;
     &amp;lt;list&amp;gt;
        &amp;lt;ref bean=&quot;rebuildIndexTrigger&quot; /&amp;gt;
     &amp;lt;/list&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b）编写main方法，打成jar包，然后编写shell脚本，用于手动重建索引或者设置定时任务。该类需要获取一个名称为blSearchService的bean，然后调用该bean的rebuildIndex方法，主要代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Resource(name = &quot;blSearchService&quot;)
private SearchService extSolrSe earchService;
public void doRebuild(){
    extSolrSearchService.rebuildIndex();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、扩展CatalogService&lt;/p&gt;

&lt;p&gt;添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Resource(name = &quot;blSearchService&quot;)
private ExtSolrSearchService extSolrSearchService;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改该类的saveProduct方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Override
@Transactional(&quot;blTransactionManager&quot;)
public Product saveProduct(Product product) {
    Product dbProduct = catalogService.saveProduct(product);
    try {
        extSolrSearchService.addProductIndex(dbProduct);
    } catch (ServiceException e) {
        e.printStackTrace();
        throw new RuntimeException(e);
    } catch (IOException e) {
        e.printStackTrace();
        throw new RuntimeException(e);
    }
    return dbProduct;
}
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2014/03/14/broadleaf-project-with-solrcloud.html</link>
      <guid>http://blog.javachen.com/2014/03/14/broadleaf-project-with-solrcloud.html</guid>
      <pubDate>2014-03-14T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>BroadLeaf项目搜索功能改进</title>
      <description>&lt;p&gt;Broadleaf Commerce 是一个开源的Java电子商务平台，基于Spring框架开发，提供一个可靠、可扩展的架构，可进行深度的定制和快速开发。&lt;/p&gt;

&lt;h1 id=&quot;solr&quot;&gt;关于Solr&lt;/h1&gt;

&lt;p&gt;Broadleaf项目中关于商品的搜索使用了嵌入式的Solr服务器，这个从配置文件中可以看出来。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;项目主页： &lt;a href=&quot;http://www.broadleafcommerce.com/&quot;&gt;http://www.broadleafcommerce.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;示例网站： &lt;a href=&quot;http://demo.broadleafcommerce.org/&quot;&gt;http://demo.broadleafcommerce.org/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;示例网站源代码： &lt;a href=&quot;https://github.com/BroadleafCommerce/DemoSite&quot;&gt;https://github.com/BroadleafCommerce/DemoSite&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从示例网站源代码的&lt;a href=&quot;https://github.com/BroadleafCommerce/DemoSite/blob/master/site/src/main/webapp/WEB-INF/applicationContext.xml&quot;&gt;applicationContext.xml文件&lt;/a&gt;中可以看到关于solr的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt; 	&amp;lt;bean id=&quot;solrEmbedded&quot; class=&quot;java.lang.String&quot;&amp;gt;
        &amp;lt;constructor-arg value=&quot;solrhome&quot;/&amp;gt;
    &amp;lt;/bean&amp;gt;

   &amp;lt;bean id=&quot;blSearchService&quot; class=&quot;org.broadleafcommerce.core.search.service.solr.SolrSearchServiceImpl&quot;&amp;gt;
        &amp;lt;constructor-arg name=&quot;solrServer&quot; ref=&quot;${solr.source}&quot; /&amp;gt;
        &amp;lt;constructor-arg name=&quot;reindexServer&quot; ref=&quot;${solr.source.reindex}&quot; /&amp;gt;
    &amp;lt;/bean&amp;gt; 
    &amp;lt;bean id=&quot;rebuildIndexJobDetail&quot; class=&quot;org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean&quot;&amp;gt;
        &amp;lt;property name=&quot;targetObject&quot; ref=&quot;blSearchService&quot; /&amp;gt;
        &amp;lt;property name=&quot;targetMethod&quot; value=&quot;rebuildIndex&quot; /&amp;gt;
    &amp;lt;/bean&amp;gt; 
    &amp;lt;bean id=&quot;rebuildIndexTrigger&quot; class=&quot;org.springframework.scheduling.quartz.SimpleTriggerFactoryBean&quot;&amp;gt;
        &amp;lt;property name=&quot;jobDetail&quot; ref=&quot;rebuildIndexJobDetail&quot; /&amp;gt;
        &amp;lt;property name=&quot;startDelay&quot; value=&quot;${solr.index.start.delay}&quot; /&amp;gt;
        &amp;lt;property name=&quot;repeatInterval&quot; value=&quot;${solr.index.repeat.interval}&quot; /&amp;gt;
    &amp;lt;/bean&amp;gt;
    &amp;lt;bean class=&quot;org.springframework.scheduling.quartz.SchedulerFactoryBean&quot;&amp;gt;
        &amp;lt;property name=&quot;triggers&quot;&amp;gt;
            &amp;lt;list&amp;gt;
                &amp;lt;ref bean=&quot;rebuildIndexTrigger&quot; /&amp;gt;
                &amp;lt;!--&amp;lt;ref bean=&quot;purgeCartTrigger&quot; /&amp;gt;--&amp;gt;
                &amp;lt;!--&amp;lt;ref bean=&quot;purgeCustomerTrigger&quot; /&amp;gt;--&amp;gt;
            &amp;lt;/list&amp;gt;
        &amp;lt;/property&amp;gt;
    &amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;资源配置文件在&lt;a href=&quot;https://github.com/BroadleafCommerce/DemoSite/blob/master/site/src/main/resources/runtime-properties/common.properties&quot;&gt;common.properties&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;web.defaultPageSize=15
web.maxPageSize=100

solr.source=solrEmbedded
solr.source.reindex=solrEmbedded
solr.index.start.delay=5000
solr.index.repeat.interval=3600000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上可以看出使用的Solr是嵌入式服务，Solr配置文件（schema.xml和solrconfig.xml）在 &lt;a href=&quot;https://github.com/BroadleafCommerce/DemoSite/tree/master/site/src/main/resources&quot;&gt;https://github.com/BroadleafCommerce/DemoSite/tree/master/site/src/main/resources&lt;/a&gt; 目录下。&lt;/p&gt;

&lt;p&gt;从源代码SolrSearchServiceImpl.java中可以看出,一共启动了两个Solr服务，分别对应primary和reindex两个solrcore，primary用于查询，reindex用于重建索引。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;改进搜索引擎&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;改进目标&lt;/h2&gt;

&lt;p&gt;本篇文章只是将嵌入式Solr服务换成独立运行的Solr服务，你还可以更进一步换成SolrCloud集群。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;单独搭建搜素引擎服务器&lt;/li&gt;
  &lt;li&gt;支持增量更新索引&lt;/li&gt;
  &lt;li&gt;支持手动重建索引&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;设计思路&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;1.修改原系统中的嵌入式搜索引擎为独立部署的搜索引擎，安装方法见下文。&lt;/li&gt;
  &lt;li&gt;2.扩展原系统中的SolrSearchServiceImpl类，添加增加索引的方法。&lt;/li&gt;
  &lt;li&gt;3.修改原来系统中商品的service类（我这里调用的是LLCatalogServiceImpl，该类是新添加的），在saveProduct方法中添加往搜索引擎添加索引的方法。&lt;/li&gt;
  &lt;li&gt;4.修改原系统中solr相关的配置文件。&lt;/li&gt;
  &lt;li&gt;5.修改原系统中的重建索引的定时任务，以支持手动重建索引。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;实现方法&lt;/h2&gt;

&lt;h3 id=&quot;solr-1&quot;&gt;1、搭建独立运行的solr服务器&lt;/h3&gt;

&lt;p&gt;你可以参考：&lt;a href=&quot;/2014/02/26/how-to-install-solr.html&quot;&gt;Apache Solr介绍及安装&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;关键在于solr/home的定义，以及在该目录下创建两个目录，分别为primary和reindex，两个目录下的配置文件都一样，solr/home目录结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  solrhome-solr  tree -L 3
.
├── primary
│   └── conf
│       ├── schema.xml
│       └── solrconfig.xml
├── reindex
│   └── conf
│       ├── schema.xml
│       └── solrconfig.xml
└── solr.xml

4 directories, 5 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;solr.xml文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&amp;gt;
&amp;lt;solr persistent=&quot;true&quot;&amp;gt;
  &amp;lt;cores defaultCoreName=&quot;primary&quot; adminPath=&quot;/admin/cores&quot;&amp;gt;
    &amp;lt;core instanceDir=&quot;reindex&quot; name=&quot;reindex&quot;/&amp;gt;
    &amp;lt;core instanceDir=&quot;primary&quot; name=&quot;primary&quot;/&amp;gt;
  &amp;lt;/cores&amp;gt;
&amp;lt;/solr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;schema.xml内容和&lt;a href=&quot;https://github.com/BroadleafCommerce/DemoSite/blob/master/site/src/main/resources/schema.xml&quot;&gt;原来的&lt;/a&gt;基本一样，只是添加了一行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;field name=&quot;_version_&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;false&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;solrconfig.xml内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&amp;gt;
&amp;lt;config&amp;gt;
  &amp;lt;luceneMatchVersion&amp;gt;4.4&amp;lt;/luceneMatchVersion&amp;gt;
  &amp;lt;directoryFactory name=&quot;DirectoryFactory&quot; class=&quot;${solr.directoryFactory:solr.StandardDirectoryFactory}&quot;/&amp;gt;

  &amp;lt;schemaFactory class=&quot;ClassicIndexSchemaFactory&quot;/&amp;gt;

  &amp;lt;updateHandler class=&quot;solr.DirectUpdateHandler2&quot;&amp;gt;
	&amp;lt;autoCommit&amp;gt;
		&amp;lt;maxDocs&amp;gt;2&amp;lt;/maxDocs&amp;gt;
		&amp;lt;maxTime&amp;gt;3000&amp;lt;/maxTime&amp;gt;
	&amp;lt;/autoCommit&amp;gt;
  &amp;lt;/updateHandler&amp;gt;

  &amp;lt;requestHandler name=&quot;/get&quot; class=&quot;solr.RealTimeGetHandler&quot;&amp;gt;
    &amp;lt;lst name=&quot;defaults&quot;&amp;gt;
      &amp;lt;str name=&quot;omitHeader&quot;&amp;gt;true&amp;lt;/str&amp;gt;
    &amp;lt;/lst&amp;gt;
  &amp;lt;/requestHandler&amp;gt;
  
  &amp;lt;requestHandler name=&quot;/replication&quot; class=&quot;solr.ReplicationHandler&quot; startup=&quot;lazy&quot; /&amp;gt; 

  &amp;lt;requestDispatcher handleSelect=&quot;true&quot; &amp;gt;
    &amp;lt;requestParsers enableRemoteStreaming=&quot;false&quot; multipartUploadLimitInKB=&quot;2048&quot; formdataUploadLimitInKB=&quot;2048&quot; /&amp;gt;
    &amp;lt;httpCaching never304=&quot;true&quot; /&amp;gt;
  &amp;lt;/requestDispatcher&amp;gt;
  
  &amp;lt;requestHandler name=&quot;standard&quot; class=&quot;solr.StandardRequestHandler&quot; default=&quot;true&quot; /&amp;gt;
  &amp;lt;requestHandler name=&quot;/analysis/field&quot; startup=&quot;lazy&quot; class=&quot;solr.FieldAnalysisRequestHandler&quot; /&amp;gt;
  &amp;lt;requestHandler name=&quot;/update&quot; class=&quot;solr.UpdateRequestHandler&quot;  /&amp;gt;
  &amp;lt;requestHandler name=&quot;/update/csv&quot; class=&quot;solr.CSVRequestHandler&quot; startup=&quot;lazy&quot; /&amp;gt;
  &amp;lt;requestHandler name=&quot;/update/json&quot; class=&quot;solr.JsonUpdateRequestHandler&quot; startup=&quot;lazy&quot; /&amp;gt;
  &amp;lt;requestHandler name=&quot;/admin/&quot; class=&quot;org.apache.solr.handler.admin.AdminHandlers&quot; /&amp;gt;

  &amp;lt;requestHandler name=&quot;/admin/ping&quot; class=&quot;solr.PingRequestHandler&quot;&amp;gt;
    &amp;lt;lst name=&quot;invariants&quot;&amp;gt;
      &amp;lt;str name=&quot;q&quot;&amp;gt;solrpingquery&amp;lt;/str&amp;gt;
    &amp;lt;/lst&amp;gt;
    &amp;lt;lst name=&quot;defaults&quot;&amp;gt;
      &amp;lt;str name=&quot;echoParams&quot;&amp;gt;all&amp;lt;/str&amp;gt;
    &amp;lt;/lst&amp;gt;
  &amp;lt;/requestHandler&amp;gt;

  &amp;lt;queryResponseWriter name=&quot;json&quot; class=&quot;solr.JSONResponseWriter&quot;&amp;gt;
        &amp;lt;str name=&quot;content-type&quot;&amp;gt;text/plain; charset=UTF-8&amp;lt;/str&amp;gt;
  &amp;lt;/queryResponseWriter&amp;gt;

  &amp;lt;!-- config for the admin interface --&amp;gt; 
  &amp;lt;admin&amp;gt;
    &amp;lt;defaultQuery&amp;gt;solr&amp;lt;/defaultQuery&amp;gt;
  &amp;lt;/admin&amp;gt;
&amp;lt;/config&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;solrsearchserviceimpl&quot;&gt;2、扩展SolrSearchServiceImpl类&lt;/h3&gt;

&lt;p&gt;在core模块创建org.broadleafcommerce.core.search.service.solr.ExtSolrSearchService接口，该接口定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package org.broadleafcommerce.core.search.service.solr;
import java.io.IOException;
import org.broadleafcommerce.common.exception.ServiceException;
import org.broadleafcommerce.core.catalog.domain.Product;
import org.broadleafcommerce.core.search.service.SearchService;
 
public interface ExtSolrSearchService extends SearchService {
    public void addProductIndex(Product product) throws ServiceException,
            IOException;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，创建其实现类org.broadleafcommerce.core.search.service.solr.ExtSolrSearchServiceImpl，该实现类定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package org.broadleafcommerce.core.search.service.solr;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import javax.annotation.Resource;
import javax.xml.parsers.ParserConfigurationException;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.solr.client.solrj.SolrServer;
import org.apache.solr.client.solrj.SolrServerException;
import org.apache.solr.client.solrj.response.QueryResponse;
import org.apache.solr.common.SolrDocument;
import org.apache.solr.common.SolrDocumentList;
import org.apache.solr.common.SolrInputDocument;
import org.broadleafcommerce.common.exception.ServiceException;
import org.broadleafcommerce.common.locale.domain.Locale;
import org.broadleafcommerce.common.util.StopWatch;
import org.broadleafcommerce.common.util.TransactionUtils;
import org.broadleafcommerce.core.catalog.domain.Product;
import org.broadleafcommerce.core.search.domain.Field;
import org.springframework.transaction.TransactionDefinition;
import org.springframework.transaction.TransactionStatus;
import org.xml.sax.SAXException;
 
public class ExtSolrSearchServiceImpl extends SolrSearchServiceImpl implements
        ExtSolrSearchService {
    private static final Log LOG = LogFactory
            .getLog(ExtSolrSearchServiceImpl.class);
    @Resource(name = &quot;blSolrIndexService&quot;)
    protected SolrIndexServiceImpl solrIndexServiceImpl;
    public ExtSolrSearchServiceImpl(SolrServer solrServer,
            SolrServer reindexServer) {
        super(solrServer, reindexServer);
    }
    public ExtSolrSearchServiceImpl(SolrServer solrServer) {
        super(solrServer);
    }
    public ExtSolrSearchServiceImpl(String solrServer, String reindexServer)
            throws IOException, ParserConfigurationException, SAXException {
        super(solrServer, reindexServer);
    }
    public ExtSolrSearchServiceImpl(String solrServer) throws IOException,
            ParserConfigurationException, SAXException {
        super(solrServer);
    }
    public void addProductIndex(Product product) throws ServiceException,
            IOException {
        TransactionStatus status = TransactionUtils.createTransaction(
                &quot;saveProduct&quot;, TransactionDefinition.PROPAGATION_REQUIRED,
                solrIndexServiceImpl.transactionManager, true);
        StopWatch s = new StopWatch();
        try {
            List&amp;lt;Field&amp;gt; fields = fieldDao.readAllProductFields();
            List&amp;lt;Locale&amp;gt; locales = solrIndexServiceImpl.getAllLocales();
            SolrInputDocument document = solrIndexServiceImpl.buildDocument(
                    product, fields, locales);
            if (LOG.isTraceEnabled()) {
                LOG.trace(document);
            }
            SolrContext.getServer().add(document);
            SolrContext.getServer().commit();
            TransactionUtils.finalizeTransaction(status,
                    solrIndexServiceImpl.transactionManager, false);
        } catch (SolrServerException e) {
            TransactionUtils.finalizeTransaction(status,
                    solrIndexServiceImpl.transactionManager, true);
            throw new ServiceException(&quot;Could not rebuild index&quot;, e);
        } catch (IOException e) {
            TransactionUtils.finalizeTransaction(status,
                    solrIndexServiceImpl.transactionManager, true);
            throw new ServiceException(&quot;Could not rebuild index&quot;, e);
        } catch (RuntimeException e) {
            TransactionUtils.finalizeTransaction(status,
                    solrIndexServiceImpl.transactionManager, true);
            throw e;
        }
        LOG.info(String.format(&quot;Finished adding index in %s&quot;, s.toLapString()));
    }
    protected List&amp;lt;Product&amp;gt; getProducts(QueryResponse response) {
        final List&amp;lt;Long&amp;gt; productIds = new ArrayList&amp;lt;Long&amp;gt;();
        SolrDocumentList docs = response.getResults();
        for (SolrDocument doc : docs) {
            productIds
                    .add((Long) doc.getFieldValue(shs.getProductIdFieldName()));
        }
        /**
         * TODO 请添加缓存相关代码
         */
        List&amp;lt;Product&amp;gt; products = productDao.readProductsByIds(productIds);
        // We have to sort the products list by the order of the productIds list
        // to maintain sortability in the UI
        if (products != null) {
            Collections.sort(products, new Comparator&amp;lt;Product&amp;gt;() {
                public int compare(Product o1, Product o2) {
                    return new Integer(productIds.indexOf(o1.getId()))
                            .compareTo(productIds.indexOf(o2.getId()));
                }
            });
        }
        return products;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;solr-2&quot;&gt;3、 修改solr相关配置文件&lt;/h3&gt;

&lt;p&gt;a. 删除web模块中/web/src/main/webapp/WEB-INF/applicationContext.xml的以下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;blSearchService&quot; class=&quot;org.broadleafcommerce.core.search.service.solr.SolrSearchServiceImpl&quot;&amp;gt;
    &amp;lt;constructor-arg name=&quot;solrServer&quot; ref=&quot;${solr.source}&quot; /&amp;gt;
    &amp;lt;constructor-arg name=&quot;reindexServer&quot; ref=&quot;${solr.source.reindex}&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b.删除web模块中web/src/main/resources/runtime-properties/common.properties的以下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;solr.source=solrEmbedded
solr.source.reindex=solrEmbedded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c. 在core模块中core/src/main/resources/applicationContext.xml添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;solrServer&quot; class=&quot;org.apache.solr.client.solrj.impl.HttpSolrServer&quot;&amp;gt;
    &amp;lt;constructor-arg value=&quot;${solr.url}&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean id=&quot;solrReindexServer&quot; class=&quot;org.apache.solr.client.solrj.impl.HttpSolrServer&quot;&amp;gt;
    &amp;lt;constructor-arg value=&quot;${solr.url.reindex}&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean id=&quot;blSearchService&quot;
    class=&quot;org.broadleafcommerce.core.search.service.solr.ExtSolrSearchServiceImpl&quot;&amp;gt;
    &amp;lt;constructor-arg name=&quot;solrServer&quot; ref=&quot;${solr.source}&quot; /&amp;gt;
    &amp;lt;constructor-arg name=&quot;reindexServer&quot; ref=&quot;${solr.source.reindex}&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;d. 在core模块中core/src/main/resources/runtime-properties/common-shared.properties添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;solr.url=http://localhost:8080/solr
solr.url.reindex=http://localhost:8080/solr/reindex
solr.source=solrServer
solr.source.reindex=solrReindexServer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;llcatalogserviceimpl&quot;&gt;4、 修改LLCatalogServiceImpl类&lt;/h3&gt;

&lt;p&gt;添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Resource(name = &quot;blSearchService&quot;)
private ExtSolrSearchService extSolrSearchService;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改该类的saveProduct方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Override
@Transactional(&quot;blTransactionManager&quot;)
public Product saveProduct(Product product) {
    Product dbProduct = catalogService.saveProduct(product);
    try {
        extSolrSearchService.addProductIndex(dbProduct);
    } catch (ServiceException e) {
        e.printStackTrace();
        throw new RuntimeException(e);
    } catch (IOException e) {
        e.printStackTrace();
        throw new RuntimeException(e);
    }
    return dbProduct;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-4&quot;&gt;5、 修改定时任务&lt;/h3&gt;

&lt;p&gt;a. web系统启动时候，会查询数据库中商品，然后重建索引。该功能在applicationContext.xml中已经定义了定时任务，建议取消该定时任务。去掉以下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;bean id=&quot;rebuildIndexJobDetail&quot;
    class=&quot;org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean&quot;&amp;gt;
    &amp;lt;property name=&quot;targetObject&quot; ref=&quot;blSearchService&quot; /&amp;gt;
    &amp;lt;property name=&quot;targetMethod&quot; value=&quot;rebuildIndex&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean id=&quot;rebuildIndexTrigger&quot; class=&quot;org.springframework.scheduling.quartz.SimpleTriggerBean&quot;&amp;gt;
    &amp;lt;property name=&quot;jobDetail&quot; ref=&quot;rebuildIndexJobDetail&quot; /&amp;gt;
    &amp;lt;property name=&quot;startDelay&quot; value=&quot;${solr.index.start.delay}&quot; /&amp;gt;
    &amp;lt;property name=&quot;repeatInterval&quot; value=&quot;${solr.index.repeat.interval}&quot; /&amp;gt;
&amp;lt;/bean&amp;gt;
&amp;lt;bean class=&quot;org.springframework.scheduling.quartz.SchedulerFactoryBean&quot;&amp;gt;
    &amp;lt;property name=&quot;triggers&quot;&amp;gt;
        &amp;lt;list&amp;gt;
            &amp;lt;ref bean=&quot;rebuildIndexTrigger&quot; /&amp;gt;
        &amp;lt;/list&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/bean&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b. 编写main方法，打成jar包，然后编写shell脚本，用于手动重建索引或者设置定时任务。该类需要获取一个名称为blSearchService的bean，然后调用该bean的rebuildIndex方法,主要代码如下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Resource(name = &quot;blSearchService&quot;)
private SearchService extSolrSearchService;
 
public void doRebuild(){
    extSolrSearchService.rebuildIndex();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-5&quot;&gt;6、单节点集成创建索引问题&lt;/h3&gt;

&lt;p&gt;a、创建索引异常&lt;br /&gt;
如果单节点Solr安装过程中有多个core，则创建索引的过程使用的是reindex的core，如果没有reindex这个core可能在启动项目时抛出如下异常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Caused by: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Server at http://localhost:8780/solr/reindex returned non ok status:404, message:Not Found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;p&gt;修改Solrhome中的配置文件solr.xml，确保配置solr中的core包含reindex，并且在solrhome目录下有reindex的目录以及配置文件，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;core name=&quot;reindex&quot; instanceDir=&quot;reindex&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b、solr查询异常&lt;/p&gt;

&lt;p&gt;如果单节点solr中多个core，默认的core为primary，查询使用的是primary，而创建索引使用的是reindex，此时访问web查询到的还是原来primary的数据，而不是创建的索引数据。&lt;/p&gt;

&lt;p&gt;解决办法：将创建索引的core作为默认使用的，修改solrhome/solr.xml如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;cores defaultCoreName=&quot;reindex&quot; adminPath=&quot;/admin/cores&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c、启动项目创建索引，solr中查询不到&lt;/p&gt;

&lt;p&gt;完成单节点集成，启动项目进行测试，启动完成，索引创建完成后，在solr的单节点中进行query，发现docs中没有添加索引。&lt;/p&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;p&gt;经过debug调试添加索引的过程，发现如下代码导致添加索引的docs被删除。原因是solr中配置的core是多个，则执行deleteAllDocuments方法，所以添加的索引都被删除，在solr中多个core的情况下，要屏蔽如下代码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//      // Swap the active and the reindex cores
//      shs.swapActiveCores();
//      // If we are not in single core mode, we delete the documents for the
//      // unused core after swapping
//      if (!SolrContext.isSingleCoreMode()) {
//        deleteAllDocuments();
//      }
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2014/03/13/improve-the-search-function-in-broadleaf-project.html</link>
      <guid>http://blog.javachen.com/2014/03/13/improve-the-search-function-in-broadleaf-project.html</guid>
      <pubDate>2014-03-13T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Apache SolrCloud安装</title>
      <description>&lt;p&gt;SolrCloud 通过 ZooKeeper 集群来进行协调，使一个索引进行分片，各个分片可以分布在不同的物理节点上，多个物理分片组成一个完成的索引 Collection。SolrCloud 自动支持 Solr Replication，可以同时对分片进行复制，冗余存储。下面，我们基于 Solr 最新的 4.4.0 版本进行安装配置 SolrCloud 集群。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装环境&lt;/h1&gt;

&lt;p&gt;我使用的安装程序各版本如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Solr： &lt;a href=&quot;http://archive.apache.org/dist/lucene/solr/4.4.0/&quot;&gt;Apache Solr-4.4.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tomcat： &lt;a href=&quot;http://archive.apache.org/dist/tomcat/tomcat-6/v6.0.36/&quot;&gt;Apache Tomcat 6.0.36&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ZooKeeper： &lt;a href=&quot;http://www.apache.org/dyn/closer.cgi/zookeeper/&quot;&gt;Apache ZooKeeper 3.4.5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各个目录说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;所有的程序安装在 &lt;code&gt;/opt&lt;/code&gt; 目录下，你可以依照你的实际情况下修改安装目录。&lt;/li&gt;
  &lt;li&gt;ZooKeeper的数据目录在： &lt;code&gt;/data/zookeeper/data&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;solrhome设置在： &lt;code&gt;/usr/local/solrhome&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;solrcloud&quot;&gt;2. 规划SolrCloud&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;单一SolrCloud数据集合： primary&lt;/li&gt;
  &lt;li&gt;ZooKeeper集群： 3台&lt;/li&gt;
  &lt;li&gt;SolrCloud实例： 3节点&lt;/li&gt;
  &lt;li&gt;索引分片： 3&lt;/li&gt;
  &lt;li&gt;复制因子： 2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;手动将3个索引分片(Shard)的复本(Replica)分布在3个 SolrCloud 节点上&lt;/p&gt;

&lt;p&gt;三个节点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;192.168.56.121&lt;/li&gt;
  &lt;li&gt;192.168.56.122&lt;/li&gt;
  &lt;li&gt;192.168.56.123&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;zookeeper&quot;&gt;3. 安装ZooKeeper集群&lt;/h1&gt;

&lt;p&gt;由于需要用到 ZooKeeper，故我们先安装好 ZooKeeper 集群。&lt;/p&gt;

&lt;p&gt;安装 ZooKeeper 集群之前，请确保每台机器上配置 &lt;code&gt;/etc/hosts&lt;/code&gt;文件，使每个节点都能通过机器名访问。&lt;/p&gt;

&lt;p&gt;首先，在第一个节点上将 zookeeper-3.4.5.tar.gz 解压到 &lt;code&gt;/opt&lt;/code&gt; 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tar zxvf zookeeper-3.4.5.tar.gz -C /opt/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 ZooKeeper 配置文件 zookeeper-3.4.5/conf/zoo.cfg，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;tickTime=2000
initLimit=10
syncLimit=5
dataDir=/data/zookeeper/data
clientPort=2181
server.1=192.168.56.121:2888:3888
server.2=192.168.56.122:2888:3888
server.3=192.168.56.123:2888:3888
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ZooKeeper 的数据目录指定在 &lt;code&gt;/data/zookeeper/data&lt;/code&gt; ，你也可以使用其他目录，通过下面命令进行创建该目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir /data/zookeeper/data -p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，初始化 myid ，三个节点编号依次为 &lt;code&gt;1，2，3&lt;/code&gt; ，在其余节点上分别执行命令（注意修改编号）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ echo &quot;1&quot; &amp;gt;/data/zookeeper/data/myid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在第二个和第三个节点上依次重复上面的操作。这样第一个节点中 myid 内容为1，第二个节点为2，第三个节点为3。&lt;/p&gt;

&lt;p&gt;最后，启动 ZooKeeper 集群，在每个节点上分别启动 ZooKeeper 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd /opt
$ sh zookeeper-3.4.5/bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以查看 ZooKeeper 集群的状态，保证集群启动没有问题：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[root@192.168.56.121 opt]# sh zookeeper-3.4.5/bin/zkServer.sh status
JMX enabled by default
Using config: /opt/zookeeper-3.4.5/bin/../conf/zoo.cfg
Mode: follower
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;solr&quot;&gt;4. 安装Solr&lt;/h1&gt;

&lt;p&gt;你可以参考&lt;a href=&quot;/2014/02/26/how-to-install-solr.html&quot;&gt;《Apache Solr介绍及安装》&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;简单来说，执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ unzip apache-tomcat-6.0.36.zip  -d /opt
$ unzip solr-4.4.0.zip  -d /opt

$ cd /opt
$ chmod +x apache-tomcat-6.0.36/bin/*.sh

$ cp solr-4.4.0/example/webapps/solr.war apache-tomcat-6.0.36/webapps/
$ cp solr-4.4.0/example/lib/ext/* apache-tomcat-6.0.36/webapps/solr/WEB-INF/lib/
$ cp solr-4.4.0/example/resources/log4j.properties apache-tomcat-6.0.36/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在其他节点上重复以上操作完成所有节点的 solr 的安装。&lt;/p&gt;

&lt;h1 id=&quot;solrcloud-&quot;&gt;5. 设置 SolrCloud 配置文件&lt;/h1&gt;

&lt;p&gt;1、 创建一个 SolrCloud 目录，并将 solr 的 lib 文件拷贝到这个目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p /usr/local/solrcloud/solr-lib/
$ cp apache-tomcat-6.0.36/webapps/solr/WEB-INF/lib/* /usr/local/solrcloud/solr-lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、 通过 bootstrap 设置 solrhome ：&lt;/p&gt;

&lt;p&gt;这里设置 solrhome 为 /usr/local/solrhome，创建该目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;mkdir -p /usr/local/solrhome 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，运行下面命令将 solrhome 下面的配置上传到 zookeeper：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -classpath .:/usr/local/solrcloud/solr-lib/* org.apache.solr.cloud.ZkCLI -zkhost 192.168.56.121:2181,192.168.56.122:2181,192.168.56.123:2181 -cmd bootstrap -solrhome 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SolrCloud 集群的所有的配置存储在 ZooKeeper。 一旦 SolrCloud 节点启动时配置了 &lt;code&gt;-Dbootstrap_confdir&lt;/code&gt; 参数， 该节点的配置信息将发送到 ZooKeeper 上存储。基它节点启动时会应用 ZooKeeper 上的配置信息，这样当我们改动配置时就不用一个个机子去更改了。&lt;/p&gt;

&lt;p&gt;3、SolrCloud 是通过 ZooKeeper 集群来保证配置文件的变更及时同步到各个节点上，所以，需要将我们自己的配置文件（在 /usr/local/solrcloud/conf/primary/conf 目录下）上传到 ZooKeeper 集群中，配置名称设为 primaryconf：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -classpath .:/usr/local/solrcloud/solr-lib/* org.apache.solr.cloud.ZkCLI -zkhost 192.168.56.121:2181,192.168.56.122:2181,192.168.56.123:2181 -cmd upconfig -confdir /usr/local/solrcloud/conf/primary/conf -confname primaryconf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;zkhost&lt;/code&gt; 指定 ZooKeeper 地址，逗号分割&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/usr/local/solrcloud/conf/&lt;/code&gt; 目录下存在名称为 primary 的目录，该目录下的配置是后面需要用到的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;primaryconf&lt;/code&gt; 为在 ZooKeeper 上的配置文件名称。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;/usr/local/solrcloud/conf 结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ tree /usr/local/solrcloud/conf
/usr/local/solrcloud/conf
├── primary
│   └── conf
│       ├── schema.xml
│       └── solrconfig.xml
└── solr.xml

2 directories, 3 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;schema.xml 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&amp;gt;
&amp;lt;schema name=&quot;example&quot; version=&quot;1.5&quot;&amp;gt;
    &amp;lt;fields&amp;gt;
        &amp;lt;field name=&quot;namespace&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;id&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;productId&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;category&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;explicitCategory&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;field name=&quot;searchable&quot; type=&quot;text_general&quot; indexed=&quot;true&quot; stored=&quot;false&quot; /&amp;gt;
        &amp;lt;field name=&quot;_version_&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;false&quot;/&amp;gt;
        &amp;lt;dynamicField name=&quot;*_searchable&quot; type=&quot;text_general&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;

        &amp;lt;dynamicField name=&quot;*_i&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_is&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_s&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_ss&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_l&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_ls&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_t&quot; type=&quot;text_general&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_txt&quot; type=&quot;text_general&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_b&quot; type=&quot;boolean&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_bs&quot; type=&quot;boolean&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_d&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_ds&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_p&quot; type=&quot;double&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;

        &amp;lt;dynamicField name=&quot;*_dt&quot; type=&quot;date&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_dts&quot; type=&quot;date&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&amp;gt;

        &amp;lt;!-- some trie-coded dynamic fields for faster range queries --&amp;gt;
        &amp;lt;dynamicField name=&quot;*_ti&quot; type=&quot;tint&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_tl&quot; type=&quot;tlong&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_td&quot; type=&quot;tdouble&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
        &amp;lt;dynamicField name=&quot;*_tdt&quot; type=&quot;tdate&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&amp;gt;
    &amp;lt;/fields&amp;gt;

    &amp;lt;uniqueKey&amp;gt;id&amp;lt;/uniqueKey&amp;gt;

    &amp;lt;types&amp;gt;
        &amp;lt;fieldType name=&quot;string&quot; class=&quot;solr.StrField&quot; sortMissingLast=&quot;true&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;boolean&quot; class=&quot;solr.BoolField&quot; sortMissingLast=&quot;true&quot; /&amp;gt;

        &amp;lt;fieldType name=&quot;int&quot; class=&quot;solr.TrieIntField&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;long&quot; class=&quot;solr.TrieLongField&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;double&quot; class=&quot;solr.TrieDoubleField&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;tint&quot; class=&quot;solr.TrieIntField&quot; precisionStep=&quot;8&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;tlong&quot; class=&quot;solr.TrieLongField&quot; precisionStep=&quot;8&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;fieldType name=&quot;tdouble&quot; class=&quot;solr.TrieDoubleField&quot; precisionStep=&quot;8&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;

        &amp;lt;fieldType name=&quot;date&quot; class=&quot;solr.TrieDateField&quot; precisionStep=&quot;0&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;
        &amp;lt;!-- A Trie based date field for faster date range queries and date faceting. --&amp;gt;
        &amp;lt;fieldType name=&quot;tdate&quot; class=&quot;solr.TrieDateField&quot; precisionStep=&quot;6&quot; positionIncrementGap=&quot;0&quot; /&amp;gt;

        &amp;lt;fieldType name=&quot;text_general&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
            &amp;lt;analyzer type=&quot;index&quot;&amp;gt;
                &amp;lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot; /&amp;gt;
                &amp;lt;filter class=&quot;solr.LowerCaseFilterFactory&quot; /&amp;gt;
            &amp;lt;/analyzer&amp;gt;
            &amp;lt;analyzer type=&quot;query&quot;&amp;gt;
                &amp;lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot; /&amp;gt;
                &amp;lt;filter class=&quot;solr.LowerCaseFilterFactory&quot; /&amp;gt;
            &amp;lt;/analyzer&amp;gt;
        &amp;lt;/fieldType&amp;gt;

    &amp;lt;/types&amp;gt;
&amp;lt;/schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;solrconfig.xml 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&amp;gt;
&amp;lt;config&amp;gt;
  &amp;lt;luceneMatchVersion&amp;gt;4.4&amp;lt;/luceneMatchVersion&amp;gt;
  &amp;lt;directoryFactory name=&quot;DirectoryFactory&quot; class=&quot;${solr.directoryFactory:solr.StandardDirectoryFactory}&quot;/&amp;gt;

  &amp;lt;schemaFactory class=&quot;ClassicIndexSchemaFactory&quot;/&amp;gt;

  &amp;lt;updateHandler class=&quot;solr.DirectUpdateHandler2&quot;&amp;gt;
    &amp;lt;updateLog&amp;gt;
            &amp;lt;str name=&quot;dir&quot;&amp;gt;${solr.data.dir:}&amp;lt;/str&amp;gt;
        &amp;lt;/updateLog&amp;gt;
  &amp;lt;/updateHandler&amp;gt;

  &amp;lt;requestHandler name=&quot;/get&quot; class=&quot;solr.RealTimeGetHandler&quot;&amp;gt;
    &amp;lt;lst name=&quot;defaults&quot;&amp;gt;
      &amp;lt;str name=&quot;omitHeader&quot;&amp;gt;true&amp;lt;/str&amp;gt;
    &amp;lt;/lst&amp;gt;
  &amp;lt;/requestHandler&amp;gt;

  &amp;lt;requestHandler name=&quot;/replication&quot; class=&quot;solr.ReplicationHandler&quot; startup=&quot;lazy&quot; /&amp;gt;

  &amp;lt;requestDispatcher handleSelect=&quot;true&quot; &amp;gt;
    &amp;lt;requestParsers enableRemoteStreaming=&quot;false&quot; multipartUploadLimitInKB=&quot;2048&quot; formdataUploadLimitInKB=&quot;2048&quot; /&amp;gt;
    &amp;lt;httpCaching never304=&quot;true&quot; /&amp;gt;
  &amp;lt;/requestDispatcher&amp;gt;

  &amp;lt;requestHandler name=&quot;standard&quot; class=&quot;solr.StandardRequestHandler&quot; default=&quot;true&quot; /&amp;gt;
  &amp;lt;requestHandler name=&quot;/analysis/field&quot; startup=&quot;lazy&quot; class=&quot;solr.FieldAnalysisRequestHandler&quot; /&amp;gt;
  &amp;lt;requestHandler name=&quot;/update&quot; class=&quot;solr.UpdateRequestHandler&quot;  /&amp;gt;
  &amp;lt;requestHandler name=&quot;/update/json&quot; class=&quot;solr.JsonUpdateRequestHandler&quot; startup=&quot;lazy&quot; /&amp;gt;
  &amp;lt;requestHandler name=&quot;/admin/&quot; class=&quot;org.apache.solr.handler.admin.AdminHandlers&quot; /&amp;gt;

  &amp;lt;requestHandler name=&quot;/admin/ping&quot; class=&quot;solr.PingRequestHandler&quot;&amp;gt;
    &amp;lt;lst name=&quot;invariants&quot;&amp;gt;
      &amp;lt;str name=&quot;q&quot;&amp;gt;solrpingquery&amp;lt;/str&amp;gt;
    &amp;lt;/lst&amp;gt;
    &amp;lt;lst name=&quot;defaults&quot;&amp;gt;
      &amp;lt;str name=&quot;echoParams&quot;&amp;gt;all&amp;lt;/str&amp;gt;
      &amp;lt;str name=&quot;df&quot;&amp;gt;id&amp;lt;/str&amp;gt;
    &amp;lt;/lst&amp;gt;
  &amp;lt;/requestHandler&amp;gt;

  &amp;lt;queryResponseWriter name=&quot;json&quot; class=&quot;solr.JSONResponseWriter&quot;&amp;gt;
        &amp;lt;str name=&quot;content-type&quot;&amp;gt;text/plain; charset=UTF-8&amp;lt;/str&amp;gt;
  &amp;lt;/queryResponseWriter&amp;gt;

  &amp;lt;updateRequestProcessorChain name=&quot;sample&quot;&amp;gt;
     &amp;lt;processor class=&quot;solr.LogUpdateProcessorFactory&quot; /&amp;gt;
     &amp;lt;processor class=&quot;solr.DistributedUpdateProcessorFactory&quot;/&amp;gt;
     &amp;lt;processor class=&quot;solr.RunUpdateProcessorFactory&quot; /&amp;gt;
  &amp;lt;/updateRequestProcessorChain&amp;gt;

  &amp;lt;!-- config for the admin interface --&amp;gt;
  &amp;lt;admin&amp;gt;
    &amp;lt;defaultQuery&amp;gt;solr&amp;lt;/defaultQuery&amp;gt;
    &amp;lt;pingQuery&amp;gt;q=solr&amp;amp;amp;version=2.0&amp;amp;amp;start=0&amp;amp;amp;rows=0&amp;lt;/pingQuery&amp;gt;
    &amp;lt;healthcheck type=&quot;file&quot;&amp;gt;server-enabled&amp;lt;/healthcheck&amp;gt;
  &amp;lt;/admin&amp;gt;

&amp;lt;/config&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、创建 collection 并和配置文件关联：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ java -classpath .:/usr/local/solrcloud/solr-lib/* org.apache.solr.cloud.ZkCLI -zkhost 192.168.56.121:2181,192.168.56.122:2181,192.168.56.123:2181 -cmd linkconfig -collection primary -confname primaryconf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建的 collection 叫做 primary，并指定和 primaryconf 连接&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5、查看 ZooKeeper 上状态&lt;/p&gt;

&lt;p&gt;在任意一个节点的 /opt 目录下执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$  zookeeper-3.4.5/bin/zkCli.sh 

[zk: localhost:2181(CONNECTED) 0] ls /
[configs,zookeeper,clusterstate.json,aliases.json,live_nodes,overseer,collections,overseer_elect]

[zk: localhost:2181(CONNECTED) 1] ls /configs
[primaryconf,]

[zk: localhost:2181(CONNECTED) 1] ls /collections
[primary]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 &lt;code&gt;/configs&lt;/code&gt; 和 &lt;code&gt;/collections&lt;/code&gt; 目录均有值，说明配置文件已经上传到 ZooKeeper 上了，接下来启动 solr。&lt;/p&gt;

&lt;h1 id=&quot;tomcat-&quot;&gt;6. Tomcat 配置与启动&lt;/h1&gt;

&lt;p&gt;1、修改每个节点上的 tomcat 配置文件，在环境变量中添加 &lt;code&gt;zkHost&lt;/code&gt; 变量&lt;/p&gt;

&lt;p&gt;编辑 &lt;code&gt;apache-tomcat-6.0.36/bin/catalina.sh&lt;/code&gt; ，添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;JAVA_OPTS=&#39;-Djetty.port=8080 -Dsolr.solr.home=/usr/local/solrhome -DzkHost=192.168.56.122:2181,192.168.56.122:2181,192.168.56.123:2181&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;/usr/local/solrhome/&lt;/code&gt; 目录创建 solr.xml ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&amp;gt;
&amp;lt;solr persistent=&quot;true&quot; sharedLib=&quot;lib&quot;&amp;gt;
    &amp;lt;cores adminPath=&quot;/admin/cores&quot; zkClientTimeout=&quot;${zkClientTimeout:15000}&quot; hostPort=&quot;${jetty.port:8080}&quot; hostContext=&quot;${hostContext:solr}&quot;&amp;gt;&amp;lt;/cores&amp;gt;
&amp;lt;/solr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;-Djetty.port&lt;/code&gt;：配置 solr 使用的端口，默认为 8983，这里我们使用的是 tomcat，端口为 8080&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-Dsolr.solr.home&lt;/code&gt;：配置 solr/home&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-zkHost&lt;/code&gt;: 配置 zookeeper 集群地址，多个地址逗号分隔&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后，在 /opt 目录下启动 tomcat：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sh apache-tomcat-6.0.36/bin/startup.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 &lt;a href=&quot;http://192.168.56.121:8080/solr/&quot;&gt;http://192.168.56.121:8080/solr/&lt;/a&gt; 进行访问，界面如图提示 &lt;code&gt;There are no SolrCores running. &lt;/code&gt;，这是因为配置文件尚未配置 solrcore。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/solr-no-solrcores.png&quot; alt=&quot;There are no SolrCores running&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;collectionshard--replication&quot;&gt;7. 创建 Collection、Shard 和 Replication&lt;/h1&gt;

&lt;h2 id=&quot;collection--shard&quot;&gt;手动创建 Collection 及初始 Shard&lt;/h2&gt;

&lt;p&gt;直接通过 REST 接口来创建 Collection，你也可以通过浏览器访问下面地址，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ curl &#39;http://192.168.56.121:8080/solr/admin/collections?action=CREATE&amp;amp;name=primary&amp;amp;numShards=3&amp;amp;replicationFactor=1&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果成功，会输出如下响应内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;response&amp;gt;
&amp;lt;lst name=&quot;responseHeader&quot;&amp;gt;
	&amp;lt;int name=&quot;status&quot;&amp;gt;0&amp;lt;/int&amp;gt;
	&amp;lt;int name=&quot;QTime&quot;&amp;gt;2649&amp;lt;/int&amp;gt;
&amp;lt;/lst&amp;gt;
&amp;lt;lst name=&quot;success&quot;&amp;gt;
	&amp;lt;lst&amp;gt;
		&amp;lt;lst name=&quot;responseHeader&quot;&amp;gt;
			&amp;lt;int name=&quot;status&quot;&amp;gt;0&amp;lt;/int&amp;gt;
			&amp;lt;int name=&quot;QTime&quot;&amp;gt;2521&amp;lt;/int&amp;gt;
		&amp;lt;/lst&amp;gt;
		&amp;lt;str name=&quot;core&quot;&amp;gt;primary_shard2_replica1&amp;lt;/str&amp;gt;
		&amp;lt;str name=&quot;saved&quot;&amp;gt;/usr/local/solrhome/solr.xml&amp;lt;/str&amp;gt;
	&amp;lt;/lst&amp;gt;
	&amp;lt;lst&amp;gt;
		&amp;lt;lst name=&quot;responseHeader&quot;&amp;gt;
			&amp;lt;int name=&quot;status&quot;&amp;gt;0&amp;lt;/int&amp;gt;
			&amp;lt;int name=&quot;QTime&quot;&amp;gt;2561&amp;lt;/int&amp;gt;
		&amp;lt;/lst&amp;gt;
		&amp;lt;str name=&quot;core&quot;&amp;gt;primary_shard3_replica1&amp;lt;/str&amp;gt;
		&amp;lt;str name=&quot;saved&quot;&amp;gt;/usr/local/solrhome/solr.xml&amp;lt;/str&amp;gt;
	&amp;lt;/lst&amp;gt;
	&amp;lt;lst&amp;gt;
		&amp;lt;lst name=&quot;responseHeader&quot;&amp;gt;
		&amp;lt;int name=&quot;status&quot;&amp;gt;0&amp;lt;/int&amp;gt;
		&amp;lt;int name=&quot;QTime&quot;&amp;gt;2607&amp;lt;/int&amp;gt;
		&amp;lt;/lst&amp;gt;
		&amp;lt;str name=&quot;core&quot;&amp;gt;primary_shard1_replica1&amp;lt;/str&amp;gt;
		&amp;lt;str name=&quot;saved&quot;&amp;gt;/usr/local/solrhome/solr.xml&amp;lt;/str&amp;gt;
	&amp;lt;/lst&amp;gt;
&amp;lt;/lst&amp;gt;
&amp;lt;/response&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面链接中的几个参数的含义，说明如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;name&lt;/code&gt;：               待创建Collection的名称&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;numShards&lt;/code&gt;：          分片的数量&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;replicationFactor&lt;/code&gt;：   复制副本的数量&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以通过 Web 管理页面，访问 &lt;code&gt;http://192.168.56.121:8080/solr/#/~cloud&lt;/code&gt;，查看 SolrCloud 集群的分片信息，如图所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/solrcloud-collection-shard.png&quot; alt=&quot;SolrCloud-collection-shard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;实际上，我们从192.168.56.121节点可以看到，SOLR 的配置文件内容，已经发生了变化，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&amp;gt;
&amp;lt;solr persistent=&quot;true&quot; sharedLib=&quot;lib&quot;&amp;gt;
  &amp;lt;cores adminPath=&quot;/admin/cores&quot; zkClientTimeout=&quot;20000&quot; hostPort=&quot;${jetty.port:8080}&quot; hostContext=&quot;${hostContext:solr}&quot;&amp;gt;
    &amp;lt;core shard=&quot;shard2&quot; instanceDir=&quot;primary_shard2_replica1/&quot; name=&quot;primary_shard2_replica1&quot; collection=&quot;primary&quot;/&amp;gt;
  &amp;lt;/cores&amp;gt;
&amp;lt;/solr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时，你还可以看另外两个节点上的 solr.xml 文件的变化。&lt;/p&gt;

&lt;h2 id=&quot;replication&quot;&gt;手动创建 Replication&lt;/h2&gt;

&lt;p&gt;下面对已经创建的初始分片进行复制。 shard1 已经在 192.168.56.123 上，我们复制分片到 192.168.56.121 和 192.168.56.122 上，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ curl &#39;http://192.168.56.121:8080/solr/admin/cores?action=CREATE&amp;amp;collection=primary&amp;amp;name=primary_shard1_replica_2&amp;amp;shard=shard1&#39;

$ curl &#39;http://192.168.56.122:8080/solr/admin/cores?action=CREATE&amp;amp;collection=primary&amp;amp;name=primary_shard1_replica_3&amp;amp;shard=shard1&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后的结果是，192.168.56.123 上的 shard1，在 192.168.56.121 节点上有1个副本，名称为 &lt;code&gt;primary_shard1_replica_2&lt;/code&gt; ，在 192.168.56.122 节点上有一个副本，名称为 &lt;code&gt;primary_shard1_replica_3&lt;/code&gt; 。也可以通过查看 192.168.56.121 和 192.168.56.122 上的目录变化，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$  ll /usr/local/solrhome/
total 16
drwxr-xr-x 3 root root 4096 Mar 10 17:11 primary_shard1_replica2
drwxr-xr-x 3 root root 4096 Mar 10 17:02 primary_shard2_replica1
-rw-r--r-- 1 root root  444 Mar 10 17:16 solr.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你还可以对 shard2 和 shard3 添加副本。shard2 已经在 192.168.56.121 上，我们复制分片到 192.168.56.122 和 192.168.56.123 上，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ curl &#39;http://192.168.56.122:8080/solr/admin/cores?action=CREATE&amp;amp;collection=primary&amp;amp;name=primary_shard2_replica_2&amp;amp;shard=shard2&#39;
$ curl &#39;http://192.168.56.123:8080/solr/admin/cores?action=CREATE&amp;amp;collection=primary&amp;amp;name=primary_shard2_replica_3&amp;amp;shard=shard2&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;shard3 已经在 192.168.56.122 上，我们复制分片到 192.168.56.121 和 192.168.56.123 上，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ curl &#39;http://192.168.56.121:8080/solr/admin/cores?action=CREATE&amp;amp;collection=primary&amp;amp;name=primary_shard3_replica_2&amp;amp;shard=shard3&#39;
$ curl &#39;http://192.168.56.123:8080/solr/admin/cores?action=CREATE&amp;amp;collection=primary&amp;amp;name=primary_shard3_replica_3&amp;amp;shard=shard3&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再次从 192.168.56.121 节点可以看到，SOLR 的配置文件内容，又发生了变化，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&amp;gt;
&amp;lt;solr persistent=&quot;true&quot; sharedLib=&quot;lib&quot;&amp;gt;
  &amp;lt;cores adminPath=&quot;/admin/cores&quot; zkClientTimeout=&quot;20000&quot; hostPort=&quot;${jetty.port:8080}&quot; hostContext=&quot;${hostContext:solr}&quot;&amp;gt;
    &amp;lt;core shard=&quot;shard1&quot; instanceDir=&quot;primary_shard1_replica2/&quot; name=&quot;primary_shard1_replica_2&quot; collection=&quot;primary&quot;/&amp;gt;
    &amp;lt;core shard=&quot;shard2&quot; instanceDir=&quot;primary_shard2_replica1/&quot; name=&quot;primary_shard2_replica_1&quot; collection=&quot;primary&quot;/&amp;gt;
    &amp;lt;core shard=&quot;shard3&quot; instanceDir=&quot;primary_shard2_replica2/&quot; name=&quot;primary_shard2_replica_2&quot; collection=&quot;primary&quot;/&amp;gt;
  &amp;lt;/cores&amp;gt;
&amp;lt;/solr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此为止，我们已经基于3个节点，配置完成了 SolrCloud 集群。最后效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/solrcloud-collection-shard-replica.png&quot; alt=&quot;solrcloud-collection-shard-replica&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;8. 其他说明&lt;/h1&gt;

&lt;h2 id=&quot;solrcloud--1&quot;&gt;8.1 SolrCloud 的一些必要配置&lt;/h2&gt;

&lt;h3 id=&quot;schemaxml&quot;&gt;schema.xml&lt;/h3&gt;

&lt;p&gt;必须定义 &lt;code&gt;_version_&lt;/code&gt; 字段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;field name=&quot;_version_&quot; type=&quot;long&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;false&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;solrconfigxml&quot;&gt;solrconfig.xml&lt;/h3&gt;

&lt;p&gt;updateHandler 节点下需要定义 updateLog：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;    &amp;lt;!-- Enables a transaction log， currently used for real-time get.
         &quot;dir&quot; - the target directory for transaction logs， defaults to the
         solr data directory.  --&amp;gt;
    &amp;lt;updateLog&amp;gt;
      &amp;lt;str name=&quot;dir&quot;&amp;gt;${solr.data.dir:}&amp;lt;/str&amp;gt;
      &amp;lt;!-- if you want to take control of the synchronization you may specify the syncLevel as one of the
           following where &#39;&#39;flush&#39;&#39; is the default. fsync will reduce throughput.
      &amp;lt;str name=&quot;syncLevel&quot;&amp;gt;flush|fsync|none&amp;lt;/str&amp;gt;
      --&amp;gt;
    &amp;lt;/updateLog&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要定义一个 &lt;code&gt;replication handler&lt;/code&gt;，名称为 &lt;code&gt;/replication&lt;/code&gt; ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;requestHandler name=&quot;/replication&quot; class=&quot;solr.ReplicationHandler&quot; startup=&quot;lazy&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要定义一个 &lt;code&gt;realtime get handler&lt;/code&gt;，名称为&lt;code&gt;/get&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;	&amp;lt;requestHandler name=&quot;/get&quot; class=&quot;solr.RealTimeGetHandler&quot;&amp;gt;
      &amp;lt;lst name=&quot;defaults&quot;&amp;gt;
        &amp;lt;str name=&quot;omitHeader&quot;&amp;gt;true&amp;lt;/str&amp;gt;
     &amp;lt;/lst&amp;gt;
    &amp;lt;/requestHandler&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要定义 &lt;code&gt;admin handlers&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;requestHandler name=&quot;/admin/&quot; class=&quot;solr.admin.AdminHandlers&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要定义 &lt;code&gt;updateRequestProcessorChain&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt; &amp;lt;updateRequestProcessorChain name=&quot;sample&quot;&amp;gt;
     &amp;lt;processor class=&quot;solr.LogUpdateProcessorFactory&quot; /&amp;gt;
     &amp;lt;processor class=&quot;solr.DistributedUpdateProcessorFactory&quot;/&amp;gt;
     &amp;lt;processor class=&quot;solr.RunUpdateProcessorFactory&quot; /&amp;gt;
   &amp;lt;/updateRequestProcessorChain&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;solrxml&quot;&gt;solr.xml&lt;/h3&gt;

&lt;p&gt;cores 节点需要定义 &lt;code&gt;adminPath&lt;/code&gt; 属性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;cores adminPath=&quot;/admin/cores&quot; &amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;solrcloud--shard&quot;&gt;8.2 SolrCloud 分布式检索时忽略宕机的 Shard&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;lst name=”error”&amp;gt;
	&amp;lt;str name=”msg”&amp;gt;no servers hosting shard:&amp;lt;/str&amp;gt;
	&amp;lt;int name=”code”&amp;gt;503&amp;lt;/int&amp;gt;
&amp;lt;/lst&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;加入下面参数，只从存活的 shards 获取数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;shards.tolerant=true 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如：&lt;code&gt;http://192.168.56.121:8080/solr/primary_shard2_replica1/select?q=*%3A*&amp;amp;wt=xml&amp;amp;indent=true&amp;amp;shards.tolerant=true&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;没有打此参数，如果集群内有挂掉的 shard，将显示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;no servers hosting shard
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;collection--shard-1&quot;&gt;8.3 自动创建 Collection 及初始 Shard&lt;/h2&gt;

&lt;p&gt;自动创建 Collection 及初始 Shard，不需要通过 zookeeper 手动上传配置文件并关联 collection。&lt;/p&gt;

&lt;p&gt;1、在第一个节点修改 tomcat 启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;JAVA_OPTS=&#39;-Djetty.port=8080 -Dsolr.solr.home=/usr/local/solrhome -DzkHost=192.168.56.122:2181,192.168.56.122:2181,192.168.56.123:2181 -DnumShards=3 -Dbootstrap_confdir=/usr/local/solrhome/primary/conf -Dcollection.configName=primaryconf &#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动 tomcat。这个步骤上传了集群的相关配置信息(&lt;code&gt;/usr/local/solrhome/primary/conf&lt;/code&gt;)到 ZooKeeper 中去，所以启动下一个节点时不用再指定配置文件了。&lt;/p&gt;

&lt;p&gt;2、在第二个和第三个节点修改 tomcat 启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;JAVA_OPTS=&#39;-Djetty.port=8080 -Dsolr.solr.home=/usr/local/solrhome -DzkHost=192.168.56.122:2181,192.168.56.122:2181,192.168.56.123:2181 -DnumShards=3&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动 tomcat。&lt;/p&gt;

&lt;p&gt;这样就会创建3个 shard 分别分布在三个节点上，如果你在增加一个节点，这节点会附加到一个 shard 上成为一个 replica，而不会创建新的 shard。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;9. 总结&lt;/h1&gt;

&lt;p&gt;本文记录了如何 zookeeper、SolrCloud 的安装和配置过程，solrcore 是通过 restapi 进行手动创建，然后又对自动创建 Collection 及初始 Shard 进行了说明。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;10. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://shiyanjun.cn/archives/100.html&quot;&gt;SolrCloud 4.3.1+Tomcat 7安装配置实践&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://wiki.apache.org/solr/SolrCloud&quot;&gt;SolrCloud Wiki&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://www.wxdl.cn/index/solrcloud.html&quot;&gt;SolrCloud使用教程、原理介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/03/10/how-to-install-solrcloud.html</link>
      <guid>http://blog.javachen.com/2014/03/10/how-to-install-solrcloud.html</guid>
      <pubDate>2014-03-10T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase源码：HRegionServer启动过程</title>
      <description>&lt;p&gt;版本：HBase 0.94.15-cdh4.7.0&lt;/p&gt;

&lt;p&gt;关于HMaster启动过程，请参考&lt;a href=&quot;/2014/03/09/hbase-note-about-hmaster-startup.html&quot;&gt;HBase源码：HMaster启动过程&lt;/a&gt;。先启动了HMaster之后，再启动HRegionServer。&lt;/p&gt;

&lt;p&gt;运行HRegionServerStarter类启动HRegionServer：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package my.test.start;

import org.apache.hadoop.hbase.regionserver.HRegionServer;

public class HRegionServerStarter {

    public static void main(String[] args) throws Exception {
        //new HMasterStarter.ZookeeperThread().start();

        HRegionServer.main(new String[] { &quot;start&quot; });
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样参考&lt;a href=&quot;/2014/03/09/hbase-note-about-hmaster-startup.html&quot;&gt;HBase源码：HMaster启动过程&lt;/a&gt;，运行HRegionServer.main方法，会通过反射创建一个HRegionServer实例，然后调用其run方法。&lt;/p&gt;

&lt;p&gt;HRegionServer类继承关系如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/hbase-hregionserver-class.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;构造方法&lt;/h1&gt;

&lt;p&gt;主要包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;设置服务端HConnection重试次数&lt;/li&gt;
  &lt;li&gt;检查压缩编码，通过hbase.regionserver.codecs可以配置编码类，一一检测，判断是否支持其压缩算法。&lt;/li&gt;
  &lt;li&gt;获取useHBaseChecksum值，是否开启hbase checksum校验&lt;/li&gt;
  &lt;li&gt;获取&lt;code&gt;hbase.regionserver.separate.hlog.for.meta&lt;/code&gt;参数值&lt;/li&gt;
  &lt;li&gt;获取客户端重复次数&lt;/li&gt;
  &lt;li&gt;获取threadWakeFrequency值&lt;/li&gt;
  &lt;li&gt;获取&lt;code&gt;hbase.regionserver.msginterval&lt;/code&gt;值&lt;/li&gt;
  &lt;li&gt;创建Sleeper对象，用于周期性休眠线程&lt;/li&gt;
  &lt;li&gt;获取最大扫描结果集大小，&lt;code&gt;hbase.client.scanner.max.result.size&lt;/code&gt;，默认无穷大&lt;/li&gt;
  &lt;li&gt;获取&lt;code&gt;hbase.regionserver.numregionstoreport&lt;/code&gt;值&lt;/li&gt;
  &lt;li&gt;获取rpctimeout值，&lt;code&gt;hbase.rpc.timeout&lt;/code&gt;，默认60000&lt;/li&gt;
  &lt;li&gt;获取主机名和绑定的ip和端口，端口默认为60020&lt;/li&gt;
  &lt;li&gt;创建rpcServer&lt;/li&gt;
  &lt;li&gt;zk授权登录和hbase授权&lt;/li&gt;
  &lt;li&gt;创建RegionServerAccounting&lt;/li&gt;
  &lt;li&gt;创建CacheConfig&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;run&quot;&gt;run方法&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;preRegistrationInitialization
    &lt;ul&gt;
      &lt;li&gt;initializeZooKeeper，此方法不会创建任何节点&lt;br /&gt;
 	- 创建ZooKeeperWatcher&lt;br /&gt;
 	- 创建MasterAddressTracker 并等到”/hbase/master”节点有数据为止&lt;br /&gt;
 	- 创建ClusterStatusTracker 并等到”/hbase/shutdown”节点有数据为止&lt;br /&gt;
 	- 创建CatalogTracker 不做任何等待&lt;br /&gt;
 	- 创建RegionServerSnapshotManager&lt;/li&gt;
      &lt;li&gt;设置集群id&lt;/li&gt;
      &lt;li&gt;初始化线程：initializeThreads&lt;br /&gt;
  	- 创建 cacheFlusher&lt;br /&gt;
    - 创建 compactSplitThread&lt;br /&gt;
    - 创建 compactionChecker&lt;br /&gt;
      - 创建 periodicFlusher&lt;br /&gt;
	- 创建 healthCheckChore 	&lt;br /&gt;
    - 创建 Leases&lt;br /&gt;
      - 判断是否启动 HRegionThriftServer&lt;/li&gt;
      &lt;li&gt;参数&lt;code&gt;hbase.regionserver.nbreservationblocks&lt;/code&gt;默认为4，默认会预留20M(每个5M,20M = 4*5M)的内存防止OOM&lt;/li&gt;
      &lt;li&gt;初始化rpcEngine = HBaseRPC.getProtocolEngine(conf)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;reportForDuty，轮询，向汇报master自己已经启动
    &lt;ul&gt;
      &lt;li&gt;getMaster()，取出”/hbase/master”节点中的数据，构造一个master的ServerName，然后基于此生成一个HMasterRegionInterface接口的代理，此代理用于调用master的方法&lt;/li&gt;
      &lt;li&gt;regionServerStartup&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;当轮询结果不为空时，调用handleReportForDutyResponse&lt;br /&gt;
	- regionServerStartup会返回来一个MapWritable，这个MapWritable有三个值，这三个key的值会覆盖rs原有的conf:&lt;br /&gt;
      - “hbase.regionserver.hostname.seen.by.master” = master为rs重新定义的hostname(通常跟rs的InetSocketAddress.getHostName一样)rs会用它重新得到serverNameFromMasterPOV&lt;br /&gt;
      - “fs.default.name” = “file:///”&lt;br /&gt;
      - “hbase.rootdir”	= “file:///E:/hbase/tmp”&lt;br /&gt;
	- 查看conf中是否有”mapred.task.id”，没有就自动设一个(格式: “hb_rs_“+serverNameFromMasterPOV)，例如: hb_rs_localhost,60050,1323525314060&lt;br /&gt;
	- createMyEphemeralNode：在zk中建立 短暂节点”/hbase/rs/localhost,60050,1323525314060”，也就是把当前rs的serverNameFromMasterPOV(为null的话用rs的InetSocketAddress、port、startcode构建新的ServerName)放到/hbase/rs节点下，”/hbase/rs/localhost,60050,1323525314060”节点没有数据&lt;br /&gt;
	- 设置fs.defaultFS值为hbase.rootdir的值&lt;br /&gt;
	- 生成一个只读的FSTableDescriptors&lt;br /&gt;
	- 调用setupWALAndReplication&lt;br /&gt;
	- 初始化 hlog、metrics、dynamicMetrics、rsHost&lt;br /&gt;
	- 调用startServiceThreads启动服务线程&lt;br /&gt;
	 	- 启动一些ExecutorService&lt;br /&gt;
	 	- 启动hlogRoller&lt;br /&gt;
	 	- 启动cacheFlusher&lt;br /&gt;
	 	- 启动compactionChecker&lt;br /&gt;
	 	- 启动healthCheckChore&lt;br /&gt;
   	- 启动periodicFlusher &lt;br /&gt;
	 	- leases.start()&lt;br /&gt;
	 	- 启动jetty的infoServer，默认端口为60030&lt;br /&gt;
	 	- 启动复制相关打的一些handler：replicationSourceHandler、replicationSourceHandler、replicationSinkHandler&lt;br /&gt;
	 	- rpcServer启动&lt;br /&gt;
	 	- 创建并启动SplitLogWorker&lt;/li&gt;
  &lt;li&gt;registerMBean&lt;/li&gt;
  &lt;li&gt;snapshotManager启动快照服务&lt;/li&gt;
  &lt;li&gt;在master上注册之后，进入运行模式，周期性(msgInterval默认3妙)调用doMetrics，tryRegionServerReport
    &lt;ul&gt;
      &lt;li&gt;isHealthy健康检查，只要Leases、MemStoreFlusher、LogRoller、periodicFlusher、CompactionChecker有一个线程退出，rs就停止&lt;/li&gt;
      &lt;li&gt;doMetrics&lt;/li&gt;
      &lt;li&gt;tryRegionServerReport向master汇报rs的负载HServerLoad&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;shutdown之后的一些操作
    &lt;ul&gt;
      &lt;li&gt;unregisterMBean&lt;br /&gt;
 	- 停掉thriftServer、leases、rpcServer、splitLogWorker、infoServer、cacheConfig
        &lt;ul&gt;
          &lt;li&gt;中断一些线程：cacheFlusher、compactSplitThread、hlogRoller、metaHLogRoller、compactionChecker、healthCheckChore&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;停掉napshotManager&lt;/li&gt;
      &lt;li&gt;停掉 catalogTracker、compactSplitThread&lt;/li&gt;
      &lt;li&gt;等待所有region关闭&lt;/li&gt;
      &lt;li&gt;关闭wal&lt;/li&gt;
      &lt;li&gt;删除zk上的一些临时节点，zooKeeper关闭&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结一下，HRegionServer主要干以下事情：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在zk上注册自己，表明自己上线了&lt;/li&gt;
  &lt;li&gt;跟master汇报&lt;/li&gt;
  &lt;li&gt;设置wal和复制&lt;/li&gt;
  &lt;li&gt;注册协作器RegionServerCoprocessorHost&lt;/li&gt;
  &lt;li&gt;启动hlogRoller&lt;/li&gt;
  &lt;li&gt;定期刷新memstore&lt;/li&gt;
  &lt;li&gt;定期检测是否需要压缩合并&lt;/li&gt;
  &lt;li&gt;启动租约&lt;/li&gt;
  &lt;li&gt;启动jetty的infoserver&lt;/li&gt;
  &lt;li&gt;创建SplitLogWorker，用于拆分HLog&lt;/li&gt;
  &lt;li&gt;快照管理&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;HRegionServer类中创建了一些对象：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;HBaseServer：处理客户端请求&lt;/li&gt;
  &lt;li&gt;Leases：租约&lt;/li&gt;
  &lt;li&gt;InfoServer：Jetty服务器&lt;/li&gt;
  &lt;li&gt;RegionServerMetrics：&lt;/li&gt;
  &lt;li&gt;RegionServerDynamicMetrics：&lt;/li&gt;
  &lt;li&gt;CompactSplitThread：合并文件线程&lt;/li&gt;
  &lt;li&gt;MemStoreFlusher：刷新memstore线程&lt;/li&gt;
  &lt;li&gt;两个Chore：compactionChecker、periodicFlusher&lt;/li&gt;
  &lt;li&gt;两个LogRoller：hlogRoller、metaHLogRoller&lt;/li&gt;
  &lt;li&gt;MasterAddressTracker：跟踪master地址&lt;/li&gt;
  &lt;li&gt;CatalogTracker：跟踪-ROOT-和.META.表&lt;/li&gt;
  &lt;li&gt;ClusterStatusTracker：跟踪集群状态&lt;/li&gt;
  &lt;li&gt;SplitLogWorker：拆分log&lt;/li&gt;
  &lt;li&gt;Sleeper：&lt;/li&gt;
  &lt;li&gt;ExecutorService：&lt;/li&gt;
  &lt;li&gt;ReplicationSourceService和ReplicationSinkService：复制服务&lt;/li&gt;
  &lt;li&gt;RegionServerAccounting：&lt;/li&gt;
  &lt;li&gt;CacheConfig：缓存配置和block&lt;/li&gt;
  &lt;li&gt;RegionServerCoprocessorHost：RegionServer协作器&lt;/li&gt;
  &lt;li&gt;HealthCheckChore：健康检查&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/03/09/hbase-note-about-hregionserver-startup.html</link>
      <guid>http://blog.javachen.com/2014/03/09/hbase-note-about-hregionserver-startup.html</guid>
      <pubDate>2014-03-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase源码：HMaster启动过程</title>
      <description>&lt;p&gt;版本：HBase 0.94.15-cdh4.7.0&lt;/p&gt;

&lt;h1 id=&quot;hmaster&quot;&gt;调试HMaster&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;这部分参考和使用了&lt;a href=&quot;https://github.com/codefollower/HBase-Research&quot;&gt;https://github.com/codefollower/HBase-Research&lt;/a&gt;上的代码（注意：原仓库已经被作者删除了），包括该作者自己写的一些&lt;a href=&quot;https://github.com/codefollower/HBase-Research/tree/0.94/src/test/java/my/test&quot;&gt;测试类&lt;/a&gt;和&lt;a href=&quot;https://github.com/codefollower/HBase-Research/blob/0.94/my-docs&quot;&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先，在IDE里启动HMaster和HRegionServer：&lt;/p&gt;

&lt;p&gt;运行&lt;code&gt;/hbase/src/test/java/my/test/start/HMasterStarter.java&lt;/code&gt;，当看到提示&lt;code&gt;Waiting for region servers count to settle&lt;/code&gt;时，&lt;br /&gt;
再打开同目录中的HRegionServerStarter，统一运行该类。&lt;/p&gt;

&lt;p&gt;此时会有两个Console，在HMasterStarter这个Console最后出现&lt;code&gt;Master has completed initialization&lt;/code&gt;，这样的信息时就表示它启动成功了，而HRegionServerStarter这个Console最后出现&lt;code&gt;Done with post open deploy task&lt;/code&gt;这样的信息时说明它启动成功了。&lt;/p&gt;

&lt;h1 id=&quot;main&quot;&gt;main方法&lt;/h1&gt;

&lt;p&gt;运行HMasterStarter类启动HMaster：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;package my.test.start;

import java.io.File;

import my.test.TestBase;

import org.apache.hadoop.hbase.HConstants;
import org.apache.hadoop.hbase.master.HMaster;
import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;

public class HMasterStarter {
    public static void deleteRecursive(File[] files) {
        if (files == null)
            return;
        for (File f : files) {
            if (f.isDirectory()) {
                deleteRecursive(f.listFiles());
            }
            f.delete();
        }
    }

    public static void main(String[] args) throws Exception {
        File f = TestBase.getTestDir();
        //删除临时测试目录
        deleteRecursive(f.listFiles());

        new ZookeeperThread().start();
        Thread.sleep(1000);
        HMaster.main(new String[] { &quot;start&quot; });
    }

    public static class ZookeeperThread extends Thread {
        public void run() {
            MiniZooKeeperCluster zooKeeperCluster = new MiniZooKeeperCluster();

            File zkDataPath = new File(TestBase.sharedConf.get(HConstants.ZOOKEEPER_DATA_DIR));
            int zkClientPort = TestBase.sharedConf.getInt(HConstants.ZOOKEEPER_CLIENT_PORT, 2181);
            zooKeeperCluster.setDefaultClientPort(zkClientPort);
            try {
                zooKeeperCluster.startup(zkDataPath);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HMaster的入口是main方法，main方法需要传递一个参数，start或者stop。&lt;/p&gt;

&lt;p&gt;main方法内首先打印hbase版本信息，然后在调用HMasterCommandLine的doMain方法。HMasterCommandLine继承自ServerCommandLine类并且ServerCommandLine类实现了Tool接口。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public void doMain(String args[]) throws Exception {
    int ret = ToolRunner.run(
      HBaseConfiguration.create(), this, args);
    if (ret != 0) {
      System.exit(ret);
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;doMain方法内会调用ToolRunner的run方法，查看ToolRunner类可以知道，实际上最后会调用HMasterCommandLine的run方法。&lt;/p&gt;

&lt;p&gt;接下来会解析参数，根据参数值判断是执行startMaster方法还是stopMaster方法。&lt;/p&gt;

&lt;p&gt;startMaster方法中分两种情况：本地模式和分布式模式。如果是分布式模式，通过反射调用HMaster的&lt;strong&gt;构造方法&lt;/strong&gt;，并调用其start和join方法。&lt;/p&gt;

&lt;p&gt;HMaster继承自HasThread类，而HasThread类实现了Runnable接口，故HMaster也是一个线程。&lt;/p&gt;

&lt;h1 id=&quot;hmaster-1&quot;&gt;HMaster类图&lt;/h1&gt;

&lt;p&gt;HMaster类继承关系如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/hbase-hmaster-class.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;hmaster-2&quot;&gt;HMaster的构造方法&lt;/h1&gt;

&lt;p&gt;1、构造方法总体过程&lt;/p&gt;

&lt;p&gt;创建Configuration并设置和获取一些参数。包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在master上禁止block cache&lt;/li&gt;
  &lt;li&gt;设置服务端重试次数&lt;/li&gt;
  &lt;li&gt;获取主机名称和master绑定的ip和端口号，端口号默认为60000&lt;/li&gt;
  &lt;li&gt;设置regionserver的coprocessorhandler线程数为0&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;创建rpcServer&lt;/strong&gt;（见下文分析）&lt;/li&gt;
  &lt;li&gt;初始化serverName，其值为：&lt;code&gt;192.168.1.129,60000,1404117936154&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;zk授权登录和hbase授权&lt;/li&gt;
  &lt;li&gt;设置当前线程名称：&lt;code&gt;master + &quot;-&quot; + this.serverName.toString()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;判断是否开启复制：&lt;code&gt;Replication.decorateMasterConfiguration(this.conf);&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;设置&lt;code&gt;mapred.task.id&lt;/code&gt;，如果其为空，则其值为：&lt;code&gt;&quot;hb_m_&quot; + this.serverName.toString()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;创建ZooKeeperWatcher监听器&lt;/strong&gt;（见下文分析），并在zookeeper上创建一些节点&lt;/li&gt;
  &lt;li&gt;启动rpcServer中的线程&lt;/li&gt;
  &lt;li&gt;创建一个MasterMetrics&lt;/li&gt;
  &lt;li&gt;判断是否进行健康检测：HealthCheckChore&lt;/li&gt;
  &lt;li&gt;另外还初始化两个参数：shouldSplitMetaSeparately、waitingOnLogSplitting&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;涉及到的参数有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hfile.block.cache.size
hbase.master.dns.interface
hbase.master.dns.nameserver
hbase.master.port
hbase.master.ipc.address
hbase.master.handler.count
hbase.regionserver.handler.count
hbase.master.buffer.for.rs.fatals
hbase.zookeeper.client.keytab.file
hbase.zookeeper.client.kerberos.principal
hbase.master.keytab.file
hbase.master.kerberos.principal
hbase.master.logcleaner.plugins
mapred.task.id
hbase.node.health.script.frequency
hbase.regionserver.separate.hlog.for.meta
hbase.master.wait.for.log.splitting
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、创建rpcServer并启动其中的线程：&lt;/p&gt;

&lt;p&gt;这部分涉及到RPC的使用，包括的知识点有&lt;code&gt;动态代理&lt;/code&gt;、&lt;code&gt;Java NIO&lt;/code&gt;等。&lt;/p&gt;

&lt;p&gt;通过反射创建RpcEngine的实现类，实现类可以在配置文件中配置（&lt;code&gt;hbase.rpc.engine&lt;/code&gt;），默认实现为WritableRpcEngine。&lt;br /&gt;
调用getServer方法，其实也就是new一个HBaseServer类。&lt;/p&gt;

&lt;p&gt;构造方法中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;启动一个Listener线程，功能是监听client的请求，将请求放入nio请求队列，逻辑如下：&lt;/li&gt;
  &lt;li&gt;–&amp;gt;创建n个selector，和一个n个线程的readpool，n由&lt;code&gt;ipc.server.read.threadpool.size&lt;/code&gt;决定，默认为10&lt;/li&gt;
  &lt;li&gt;–&amp;gt;读取每个请求的头和内容，将内容放入priorityQueue中&lt;/li&gt;
  &lt;li&gt;启动一个Responder线程，功能是将响应队列里的数据写给各个client的connection通道，逻辑如下：&lt;/li&gt;
  &lt;li&gt;–&amp;gt;创建nio selector&lt;/li&gt;
  &lt;li&gt;–&amp;gt;默认超时时间为15 mins&lt;/li&gt;
  &lt;li&gt;–&amp;gt;依次将responseQueue中的内容写回各通道，并关闭连接，buffer=8k&lt;/li&gt;
  &lt;li&gt;–&amp;gt;如果该请求的返回没有写完，则放回队列头，推迟再发送&lt;/li&gt;
  &lt;li&gt;–&amp;gt;对于超时未完成的响应，丢弃并关闭相应连接&lt;/li&gt;
  &lt;li&gt;启动N（n默认为10）个Handler线程，功能是处理请求队列，并将结果写到响应队列&lt;/li&gt;
  &lt;li&gt;–&amp;gt;读取priorityQueue中的call，调用对应的call方法获得value，写回out并调用doRespond方法，处理该请求，并唤醒writable　selector&lt;/li&gt;
  &lt;li&gt;–&amp;gt;启动M(m默认为0)个Handler线程以处理priority&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、创建ZooKeeperWatcher&lt;/p&gt;

&lt;p&gt;构造函数中生成如下持久节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/hbase
/hbase/root-region-server
/hbase/rs
/table/draining
/hbase/master
/hbase/backup-masters
/hbase/shutdown
/hbase/unassigned
/hbase/table94
/hbase/table
/hbase/hbaseid
/hbase/splitlog
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;run&quot;&gt;run方法&lt;/h1&gt;

&lt;p&gt;接下来看HMaster的run方法做了哪些事情。&lt;/p&gt;

&lt;p&gt;1、总体过程&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建MonitoredTask，并把HMaster的状态设置为Master startup&lt;/li&gt;
  &lt;li&gt;启动info server，即Jetty服务器，端口默认为60010，其对外提供两个接口：/master-status和/dump&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;调用becomeActiveMaster方法&lt;/strong&gt;（见下文分析），阻塞等待直至当前master成为active master&lt;/li&gt;
  &lt;li&gt;当成为了master之后并且当前master进程正在运行，则调用finishInitialization方法（见下文分析），并且调用loop方法循环等待，一直到stop发生&lt;/li&gt;
  &lt;li&gt;当HMaster停止运行时候，会做以下事情：
    &lt;ul&gt;
      &lt;li&gt;清理startupStatus&lt;/li&gt;
      &lt;li&gt;停止balancerChore和catalogJanitorChore&lt;/li&gt;
      &lt;li&gt;让RegionServers shutdown&lt;/li&gt;
      &lt;li&gt;停止服务线程：rpcServer、logCleaner、hfileCleaner、infoServer、executorService、healthCheckChore&lt;/li&gt;
      &lt;li&gt;停止以下线程：activeMasterManager、catalogTracker、serverManager、assignmentManager、fileSystemManager、snapshotManager、zooKeeper&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、becomeActiveMaster方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建ActiveMasterManager&lt;/li&gt;
  &lt;li&gt;ZooKeeperWatcher注册activeMasterManager监听器&lt;/li&gt;
  &lt;li&gt;调用stallIfBackupMaster：&lt;br /&gt;
  –&amp;gt;先检查配置项 “hbase.master.backup”，自己是否backup机器，如果是则直接block直至检查到系统中的active master挂掉（&lt;code&gt;zookeeper.session.timeout&lt;/code&gt;，默认每3分钟检查一次）&lt;/li&gt;
  &lt;li&gt;创建clusterStatusTracker并启动&lt;/li&gt;
  &lt;li&gt;调用activeMasterManager的blockUntilBecomingActiveMaster方法。
    &lt;ul&gt;
      &lt;li&gt;创建短暂的”/hbase/master”，此节点值为version+ServerName，如果创建成功，则删除备份节点；否则，创建备份节点&lt;/li&gt;
      &lt;li&gt;获得”/hbase/master”节点上的数据，如果不为null，则获得ServerName，并判断是否是在当前节点上创建了”/hbase/master”，如果是则删除该节点，这是因为该节点已经是备份节点了。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、finishInitialization方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建MasterFileSystem对象，封装了master常用的一些文件系统操作，包括splitlog file、删除region目录、删除table目录、删除cf目录、检查文件系统状态等.&lt;/li&gt;
  &lt;li&gt;创建FSTableDescriptors对象&lt;/li&gt;
  &lt;li&gt;设置集群id&lt;/li&gt;
  &lt;li&gt;如果不是备份master：
    &lt;ul&gt;
      &lt;li&gt;创建ExecutorService，维护一个ExecutorMap,一种Event对应一个Executor(线程池).可以提交EventHandler来执行异步事件；&lt;br /&gt;
 	- 创建serverManager，管理regionserver信息,维护着onlineregion server 和deadregion server列表，处理regionserver的startups、shutdowns、 deaths，同时也维护着每个regionserver rpc stub.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;调用initializeZKBasedSystemTrackers，初始化zk文件系统
    &lt;ul&gt;
      &lt;li&gt;创建CatalogTracker, 它包含RootRegionTracker和MetaNodeTracker，对应”/hbase/root-region-server”和/”hbase/unassigned/1028785192”这两个结点(1028785192是.META.的分区名)。如果之前从未启动过hbase，那么在start CatalogTracker时这两个结点不存在。”/hbase/root-region-server”是一个持久结点，在RootLocationEditor中建立&lt;/li&gt;
      &lt;li&gt;创建 LoadBalancer，负责region在regionserver之间的移动，关于balancer的策略，可以通过hbase.regions.slop来设置load区间&lt;/li&gt;
      &lt;li&gt;创建 AssignmentManager，负责管理和分配region，同时它也会接受zk上关于region的event，根据event来完成region的上下线、关闭打开等工作。&lt;/li&gt;
      &lt;li&gt;创建 RegionServerTracker: 监控”/hbase/rs”结点，通过ZK的Event来跟踪onlineregion servers， 如果有rs下线，删除ServerManager中对应的onlineregions.&lt;/li&gt;
      &lt;li&gt;创建 DrainingServerTracker: 监控”/hbase/draining”结点&lt;/li&gt;
      &lt;li&gt;创建 ClusterStatusTracker，监控”/hbase/shutdown”结点维护集群状态&lt;/li&gt;
      &lt;li&gt;创建SnapshotManager&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如果不是备份master，初始化MasterCoprocessorHost并执行startServiceThreads()。说明：&lt;code&gt;info server的启动移到构造函数了去了，这样可以早点通过Jetty服务器查看HMaster启动状态。&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;创建一些executorService&lt;/li&gt;
      &lt;li&gt;创建logCleaner并启动&lt;/li&gt;
      &lt;li&gt;创建hfileCleaner并启动&lt;/li&gt;
      &lt;li&gt;启动healthCheckChore&lt;/li&gt;
      &lt;li&gt;打开rpcServer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;等待RegionServer注册。满足以下这些条件后返回当前所有region server上的region数后继续：
    &lt;ul&gt;
      &lt;li&gt;a 至少等待4.5s，”hbase.master.wait.on.regionservers.timeout”&lt;/li&gt;
      &lt;li&gt;b 成功启动regionserver节点数&amp;gt;=1，”hbase.master.wait.on.regionservers.mintostart”&lt;/li&gt;
      &lt;li&gt;c 1.5s内没有regionsever死掉或重新启动，&lt;code&gt;hbase.master.wait.on.regionservers.interval&lt;/code&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;serverManager注册新的在线region server&lt;/li&gt;
  &lt;li&gt;如果不是备份master，启动assignmentManager&lt;/li&gt;
  &lt;li&gt;获取下线的Region server，然后拆分HLog
    &lt;ul&gt;
      &lt;li&gt;–&amp;gt;依次检查每一个hlog目录，查看它所属的region server是否online，如果是则不需要做任何动作，region server自己会恢复数据，如果不是，则需要将它分配给其它的region server&lt;/li&gt;
      &lt;li&gt;–&amp;gt;split是加锁操作:&lt;/li&gt;
      &lt;li&gt;–&amp;gt; 创建一个新的hlogsplitter,遍历每一个server目录下的所有hlog文件，依次做如下操作。（如果遇到文件损坏等无法跳过的错误，配 置 &lt;code&gt;hbase.hlog.split.skip.errors=true&lt;/code&gt; 以忽略之）&lt;/li&gt;
      &lt;li&gt;–&amp;gt;启动&lt;code&gt;hbase.regionserver.hlog.splitlog.writer.threads&lt;/code&gt;（默认为3）个线程，共使用128MB内存，启动这些写线程&lt;/li&gt;
      &lt;li&gt;–&amp;gt;先通过lease机制检查文件是否能够append，如果不能则死循环等待&lt;/li&gt;
      &lt;li&gt;–&amp;gt;把hlog中的内容全部加载到内存中（内存同时被几个写线程消费)）
        &lt;ul&gt;
          &lt;li&gt;–&amp;gt;把有损坏并且跳过的文件移到&lt;code&gt;/hbase/.corrupt/&lt;/code&gt;目录中&lt;/li&gt;
          &lt;li&gt;–&amp;gt; 把其余己经处理过的文件移到&lt;code&gt;/hbase/.oldlogs&lt;/code&gt;中，然后删除原有的server目录&lt;/li&gt;
          &lt;li&gt;–&amp;gt; 等待写线程结束，返回新写的所有路径&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;–&amp;gt;解锁&lt;/li&gt;
      &lt;li&gt;写线程逻辑：
        &lt;ul&gt;
          &lt;li&gt;–&amp;gt;从内存中读出每一行数据的key和value，然后查询相应的region路径。如果该region路径不存在，说明该region很可能己经被split了，则不处理这部分数据,因为此时忽略它们是安全的。&lt;/li&gt;
          &lt;li&gt;–&amp;gt;如果上一步能查到相应的路径，则到对应路径下创建”recovered.edits”文件夹(如果该文件夹存在则删除后覆盖之)，然后将数据写入该文件夹&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;调用assignRoot方法，检查是否分配了-ROOT-表，如果没有，则通过assignmentManager.assignRoot()来分配root表，并激活该表&lt;/li&gt;
  &lt;li&gt;运行this.serverManager.enableSSHForRoot()方法&lt;/li&gt;
  &lt;li&gt;拆分.META. server上的HLog&lt;/li&gt;
  &lt;li&gt;分配.META.表&lt;/li&gt;
  &lt;li&gt;enableServerShutdownHandler&lt;/li&gt;
  &lt;li&gt;处理dead的server&lt;/li&gt;
  &lt;li&gt;assignmentManager.joinCluster();&lt;/li&gt;
  &lt;li&gt;设置balancer&lt;/li&gt;
  &lt;li&gt;fixupDaughters(status)&lt;/li&gt;
  &lt;li&gt;如果不是备份master
    &lt;ul&gt;
      &lt;li&gt;启动balancerChore线程，运行LoadBalancer&lt;/li&gt;
      &lt;li&gt;启动startCatalogJanitorChore，周期性扫描&lt;code&gt;.META.&lt;/code&gt;表上未使用的region并回收&lt;/li&gt;
      &lt;li&gt;registerMBean&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;serverManager.clearDeadServersWithSameHostNameAndPortOfOnlineServer，清理dead的server&lt;/li&gt;
  &lt;li&gt;如果不是备份master，cpHost.postStartMaster&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;masterfilesystem&quot;&gt;MasterFileSystem构造方法&lt;/h1&gt;

&lt;p&gt;在&lt;code&gt;HMaster.finishInitialization&lt;/code&gt;方法中触发了MasterFileSystem的构造方法，该类在HMaster类中会被以下类使用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LogCleaner&lt;/li&gt;
  &lt;li&gt;HFileCleaner&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外该类可以完成拆分log的工作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;  /**
   * Override to change master&#39;s splitLogAfterStartup. Used testing
   * @param mfs
   */
  protected void splitLogAfterStartup(final MasterFileSystem mfs){
    mfs.splitLogAfterStartup();
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里主要是关心创建了哪些目录，其他用途暂不分析。&lt;/p&gt;

&lt;p&gt;1、接下来，看其构造方法运行过程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;获取rootdir：由参数&lt;code&gt;hbase.rootdir&lt;/code&gt;配置&lt;/li&gt;
  &lt;li&gt;获取tempdir：&lt;code&gt;${hbase.rootdir}/.tmp&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;获取文件系统的uri，并设置到&lt;code&gt;fs.default.name&lt;/code&gt;和&lt;code&gt;fs.defaultFS&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;判断是否进行分布式文件拆分，参数：&lt;code&gt;hbase.master.distributed.log.splitting&lt;/code&gt;，如果需要，则创建SplitLogManager&lt;/li&gt;
  &lt;li&gt;创建oldLogDir，调用createInitialFileSystemLayout方法
    &lt;ul&gt;
      &lt;li&gt;checkRootDir
        &lt;ul&gt;
          &lt;li&gt;等待fs退出安全模式(默认10秒钟轮循一次，可通过参数&lt;code&gt;hbase.server.thread.wakefrequency&lt;/code&gt;调整&lt;/li&gt;
          &lt;li&gt;如果hbase.rootdir目录不存在则创建它，然后在此目录中创建名为”hbase.version”的文件，内容是文件系统版本号，当前为7；如果hbase.rootdir目录已存在，则读出”hbase.version”文件的内容与当前的版本号相比，如果不相等，则打印错误信息(提示版本不对)，抛出异常FileSystemVersionException&lt;/li&gt;
          &lt;li&gt;检查&lt;code&gt;${hbase.rootdir}&lt;/code&gt;目录下是否有名为”hbase.id”的文件，如果没有则创建它，内容是随机生成的UUID(总长度36位，由5部份组成，用”-“分隔)，如：6c43f934-37a2-4cae-9d49-3f5abfdc113d&lt;/li&gt;
          &lt;li&gt;读出”hbase.id”的文件的内容存到clusterId字段&lt;/li&gt;
          &lt;li&gt;判断hbase.rootdir目录中是否有”-ROOT-/70236052”目录，没有的话说明是第一次启动hbase，进入&lt;strong&gt;bootstrap&lt;/strong&gt;方法&lt;/li&gt;
          &lt;li&gt;createRootTableInfo 建立”-ROOT-“表的描述文件，判断&lt;code&gt;hbase.rootdir/-ROOT-&lt;/code&gt;目录中是否存在tableinfo开头的文件，另外还创建了.tmp目录&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;checkTempDir&lt;/li&gt;
      &lt;li&gt;如果oldLogDir（&lt;code&gt;${hbase.rootdir}/.oldlogs&lt;/code&gt;）不存在，则创建&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、bootstrap方法运行过程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;调用HRegion.createHRegion建立”-ROOT-“分区和”.META.”分区&lt;/li&gt;
  &lt;li&gt;把”.META.”分区信息加到”-ROOT-“表，并关闭分区和hlog&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;经过上面分析之后，来看看zookeeper创建的一些目录分布式由哪个类来监控的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;/hbase&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/root-region-server&lt;/code&gt;：RootRegionTracker，监控root所在的regionserver&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/rs&lt;/code&gt;：RegionServerTracker，监控regionserver的上线和下线&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/table/draining&lt;/code&gt;：DrainingServerTracker，监听regionserver列表的变化&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/master&lt;/code&gt;：在HMaster中建立，并且是一个短暂结点，结点的值是HMaster的ServerName：&lt;code&gt;hostname,port,当前毫秒&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/backup-masters&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/shutdown&lt;/code&gt;：ClusterStatusTracker，当HMaster启动之后，会将当前时间（&lt;code&gt;Bytes.toBytes(new java.util.Date().toString())&lt;/code&gt;）存到该节点&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/unassigned&lt;/code&gt;：MetaNodeTracker&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/table94&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/table&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/hbaseid&lt;/code&gt;：在&lt;code&gt;HMaster.finishInitialization&lt;/code&gt;方法中调用ClusterId.setClusterId建立，结点值是UUID&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/hbase/splitlog&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在HMaster启动之后，&lt;code&gt;${hbase.rootdir}&lt;/code&gt;目录如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── -ROOT-                            //&quot;-ROOT-&quot;表名
│   ├── ..tableinfo.0000000001.crc    //crc校验文件
│   ├── .tableinfo.0000000001
│   ├── .tmp
│   └── 70236052                      //&quot;-ROOT-&quot;分区名
│       ├── ..regioninfo.crc
│       ├── .oldlogs                  //存放hlog文件
│       │   ├── .hlog.1402551641526.crc
│       │   └── hlog.1402551641526
│       ├── .regioninfo               //&quot;-ROOT-&quot;分区描述表件
│       ├── .tmp
│       └── info                      //列族名
│           ├── .5037e69a0c244bd78945aaa333d0230a.crc
│           └── 5037e69a0c244bd78945aaa333d0230a  //存放&quot;.META.&quot;分区信息的StoreFile
├── .META.
│   └── 1028785192
│       ├── ..regioninfo.crc
│       ├── .oldlogs
│       │   ├── .hlog.1402551641701.crc
│       │   └── hlog.1402551641701
│       ├── .regioninfo
│       └── info
├── .hbase.id.crc
├── .hbase.version.crc
├── .oldlogs
├── .tmp
├── hbase.id                         //集群uuid
└── hbase.version                    //hbase版本
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单总结一下HMaster启动过程做了哪些事情：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建rpcServer，及HBaseServer&lt;/li&gt;
  &lt;li&gt;创建ZooKeeperWatcher监听器&lt;/li&gt;
  &lt;li&gt;阻塞等待成为activeMaster&lt;/li&gt;
  &lt;li&gt;创建master的一些文件目录&lt;/li&gt;
  &lt;li&gt;初始化一些基于zk的跟踪器&lt;/li&gt;
  &lt;li&gt;创建LoadBalancer&lt;/li&gt;
  &lt;li&gt;创建SnapshotManager&lt;/li&gt;
  &lt;li&gt;如果不是备份master
    &lt;ul&gt;
      &lt;li&gt;创建logCleaner并启动&lt;/li&gt;
      &lt;li&gt;创建hfileCleaner并启动&lt;/li&gt;
      &lt;li&gt;创建jetty的infoServer并启动&lt;/li&gt;
      &lt;li&gt;启动健康检查&lt;/li&gt;
      &lt;li&gt;打开rpcServer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;等待RegionServer注册&lt;/li&gt;
  &lt;li&gt;从hlog中恢复数据&lt;/li&gt;
  &lt;li&gt;分配root和meta表&lt;/li&gt;
  &lt;li&gt;分配region&lt;/li&gt;
  &lt;li&gt;运行负载均衡线程&lt;/li&gt;
  &lt;li&gt;周期性扫描.META.表上未使用的region并回收&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/03/09/hbase-note-about-hmaster-startup.html</link>
      <guid>http://blog.javachen.com/2014/03/09/hbase-note-about-hmaster-startup.html</guid>
      <pubDate>2014-03-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>2013年度年终总结</title>
      <description>&lt;p&gt;回首2011年和2012年的年终总结，发现公司在2012年提到的一些不足仍然出现在2013年，不知道每年的总结是否有被认真阅读过、重视过。故虽谏且议，使人不得而知焉。&lt;/p&gt;

&lt;p&gt;2013年，通过了RHCE考试，掌握了shell编程，初识Python；&lt;/p&gt;

&lt;p&gt;2013年，不再负责、管理具体的项目，可还不是逃不过事后填坑的无奈；&lt;/p&gt;

&lt;p&gt;2013年，Cassandra不再，迎来Hadoop，满腔热血的学习Hadoop的安装、部署、原理、开发甚至还做了一些入门普及培训，但真的只是一个人在战斗；&lt;/p&gt;

&lt;p&gt;2013年，开始是一个人带着几个同事在探索和研究hadoop，慢慢地失去了自己的自主权，更多的时间是被花在了具体的项目上，自己的工作全是被计划安排着；&lt;/p&gt;

&lt;p&gt;2013年，开始做EAC，界面原型、客户需求、技术选型，花费了大量的时间去讨论、修改、论证，却错过了快速迭代出一个产品的最佳时机；有时候不是我们脚步太慢，而是我们想的太复杂；&lt;/p&gt;

&lt;p&gt;2013年，开发BMP，架构文档不断地被否定，被几个人反复修改，最后也还是没有捣鼓出一个无懈可击的文档出来；项目经理职责不明确，没有发挥应有的协调作用；&lt;/p&gt;

&lt;p&gt;2013年，做了一些测试的工作，从测试方法、过程和结果上来看，在测试方面还是不够专业。&lt;/p&gt;

&lt;p&gt;关于招聘。来来往往，来了不少人，也走了不少人，基本上没有来了后花些时间就能自己独立干活的。招三个人干四个人的事情发五个人的工资，想做到这个，不做一些改变，那只能是一厢情愿的事情了。&lt;/p&gt;

&lt;p&gt;关于团队建设。新人入职，无人指导，想仅靠wiki上的文章就能无师自通，这可是没那么容易的吧？跟优秀的人一起工作，才能变得更优秀。员工内部的分享与交流，至今都未形成；员工的归属感和存在感，不知道又遗失了多少。公司不可能只靠几个人单打独斗，而需要大家一起齐心协力同奋进。&lt;/p&gt;

&lt;p&gt;关于管理。项目的进度和质量管控不严，表现在任务的时间分配不合理，任务完成进度没跟踪和把控，代码质量和应用健壮性没审查和测试，代码开发不规范等等。另外，管理者的角色不明确以及执行力不够。&lt;/p&gt;

&lt;p&gt;还有其他的一些问题，所有这些问题主要都是不够【职业化】的问题。公司未来，应该朝职业化发展。&lt;/p&gt;

&lt;p&gt;博学而笃志，切问而近思，仁在其中矣。吾尝终日不食，终夜不寝，以思，无益，不如学也。每次都是年终的时候才总结一年的得失成败，费尽脑汁挤出的几条总结，能有多大的时效性、客观性和借鉴性呢？&lt;/p&gt;

&lt;p&gt;一样的月光，一样的照着新店溪；一样的冬天，一样的下着冰冷的雨；一样的尘埃，一样的在风中堆积，一样的笑容，一样的泪水，一样的日子，一样的我和你，什么时候蓝天白云都成了记忆，什么时候梦想变得如此的拥挤。谁能告诉我，是我们改变了公司，还是公司改变了我和你。&lt;/p&gt;

&lt;p&gt;2014年，等风来。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/03/06/summary-of-the-work-in-2013.html</link>
      <guid>http://blog.javachen.com/2014/03/06/summary-of-the-work-in-2013.html</guid>
      <pubDate>2014-03-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Apache Solr查询语法</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;查询参数&lt;/h1&gt;

&lt;p&gt;常用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;q&lt;/code&gt; - 查询字符串，必须的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;fl&lt;/code&gt; - 指定返回那些字段内容，用逗号或空格分隔多个。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;start&lt;/code&gt; - 返回第一条记录在完整找到结果中的偏移位置，0开始，一般分页用。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;rows&lt;/code&gt; - 指定返回结果最多有多少条记录，配合start来实现分页。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sort&lt;/code&gt; - 排序，格式：&lt;code&gt;sort=&amp;lt;field name&amp;gt;+&amp;lt;desc|asc&amp;gt;[,&amp;lt;field name&amp;gt;+&amp;lt;desc|asc&amp;gt;]&lt;/code&gt;。示例：（inStock desc, price asc）表示先 “inStock” 降序, 再 “price” 升序，默认是相关性降序。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;wt&lt;/code&gt; - (writer type)指定输出格式，可以有 xml, json, php, phps。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;fq&lt;/code&gt; - （filter query）过虑查询，作用：在q查询符合结果中同时是fq查询符合的，例如：&lt;code&gt;q=mm&amp;amp;fq=date_time:[20081001 TO 20091031]&lt;/code&gt;，找关键字mm，并且date_time是20081001到20091031之间的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不常用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;defType&lt;/code&gt;：&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;q.op&lt;/code&gt; - 覆盖schema.xml的defaultOperator（有空格时用”AND”还是用”OR”操作逻辑），一般默认指定&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;df&lt;/code&gt; - 默认的查询字段，一般默认指定&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;qt&lt;/code&gt; - （query type）指定那个类型来处理查询请求，一般不用指定，默认是standard。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其它：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;indent&lt;/code&gt; - 返回的结果是否缩进，默认关闭，用 &lt;code&gt;indent=true|on&lt;/code&gt; 开启，一般调试json,php,phps,ruby输出才有必要用这个参数。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;version &lt;/code&gt;- 查询语法的版本，建议不使用它，由服务器指定默认值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;检索运算符&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;:&lt;/code&gt; 指定字段查指定值，如返回所有值&lt;em&gt;:&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;?&lt;/code&gt; 表示单个任意字符的通配&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;*&lt;/code&gt; 表示多个任意字符的通配（不能在检索的项开始使用*或者?符号）&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;~&lt;/code&gt; 表示模糊检索，如检索拼写类似于”roam”的项这样写：roam~将找到形如foam和roams的单词；roam~0.8，检索返回相似度在0.8以上的记录。&lt;br /&gt;
  邻近检索，如检索相隔10个单词的”apache”和”jakarta”，”jakarta apache”~10&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;^&lt;/code&gt; 控制相关度检索，如检索jakarta apache，同时希望去让”jakarta”的相关度更加好，那么在其后加上”^”符号和增量值，即jakarta^4 apache&lt;/li&gt;
  &lt;li&gt;布尔操作符&lt;code&gt;AND、||&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;布尔操作符&lt;code&gt;OR、&amp;amp;&amp;amp;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;布尔操作符&lt;code&gt;NOT、!、-&lt;/code&gt;（排除操作符不能单独与项使用构成查询）&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;+&lt;/code&gt; 存在操作符，要求符号”+”后的项必须在文档相应的域中存在&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;()&lt;/code&gt; 用于构成子查询&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;[]&lt;/code&gt; 包含范围检索，如检索某时间段记录，包含头尾，date:[200707 TO 200710]&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;{}&lt;/code&gt;不包含范围检索，如检索某时间段记录，不包含头尾，date:{200707 TO 200710}&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;&quot;&lt;/code&gt; 转义操作符，特殊字符包括+ - &amp;amp;&amp;amp;&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;! ( ) { } [ ] ^ “ ~ * ? : “&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;示例&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;查询所有&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:8080/solr/primary/select?q=*:*
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;限定返回字段&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:8080/solr/primary/select?q=*:*&amp;amp;fl=productId
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示：查询所有记录，只返回productId字段&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;分页&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:8080/solr/primary/select?q=*:*&amp;amp;fl=productId&amp;amp;rows=6&amp;amp;start=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示：查询前六条记录，只返回productId字段&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;增加限定条件&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:8080/solr/primary/select?q=*:*&amp;amp;fl=productId&amp;amp;rows=6&amp;amp;start=0&amp;amp;fq=category:2002&amp;amp;fq=namespace:d&amp;amp;fl=productId+category&amp;amp;fq=en_US_city_i:1101
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示：查询category=2002、&lt;code&gt;en_US_city_i=110&lt;/code&gt;以及namespace=d的前六条记录，只返回productId和category字段&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;添加排序&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:8080/solr/primary/select?q=*:*&amp;amp;fl=productId&amp;amp;rows=6&amp;amp;start=0&amp;amp;fq=category:2002&amp;amp;fq=namespace:d&amp;amp;sort=category_2002_sort_i+asc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示：查询category=2002以及namespace=d并按&lt;code&gt;category_2002_sort_i&lt;/code&gt;升序排序的前六条记录，只返回productId字段&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;facet查询&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现实分组统计结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:8080/solr/primary/select?q=*:*&amp;amp;fl=productId&amp;amp;fq=category:2002&amp;amp;facet=true&amp;amp;facet.field=en_US_county_i&amp;amp;facet.field=en_US_hotelType_s&amp;amp;facet.field=price_p&amp;amp;facet.field=heatRange_i

http://localhost:8080/solr/primary/select?q=*:*&amp;amp;fl=productId&amp;amp;fq=category:2002&amp;amp;facet=true&amp;amp;facet.field=en_US_county_i&amp;amp;facet.field=en_US_hotelType_s&amp;amp;facet.field=price_p&amp;amp;facet.field=heatRange_i&amp;amp;facet.query=price_p:[300.00000+TO+*]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;高亮&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;hl-highlight&lt;/code&gt;，&lt;code&gt;h1=true&lt;/code&gt;，表示采用高亮。可以用&lt;code&gt;h1.fl=field1,field2&lt;/code&gt; 来设定高亮显示的字段。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;hl.fl&lt;/code&gt;:用空格或逗号隔开的字段列表。要启用某个字段的highlight功能，就得保证该字段在schema中是stored。如果该参数未被给出，那么就会高 亮默认字段 standard handler会用df参数，dismax字段用qf参数。你可以使用星号去方便的高亮所有字段。如果你使用了通配符，那么要考虑启用 。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.requireFieldMatch&lt;/code&gt;:如果置为true，除非该字段的查询结果不为空才会被高亮。它的默认值是false，意味 着它可能匹配某个字段却高亮一个不同的字段。如果hl.fl使用了通配符，那么就要启用该参数。尽管如此，如果你的查询是all字段（可能是使用 copy-field 指令），那么还是把它设为false，这样搜索结果能表明哪个字段的查询文本未被找到&lt;/li&gt;
  &lt;li&gt;h&lt;code&gt;l.usePhraseHighlighter&lt;/code&gt;:如果一个查询中含有短语（引号框起来的）那么会保证一定要完全匹配短语的才会被高亮。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.highlightMultiTerm&lt;/code&gt;&lt;br /&gt;
如果使用通配符和模糊搜索，那么会确保与通配符匹配的term会高亮。默认为false，同时&lt;code&gt;hl.usePhraseHighlighter&lt;/code&gt;要为true。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.snippets&lt;/code&gt;：&lt;br /&gt;
这是highlighted片段的最大数。默认值为1，也几乎不会修改。如果某个特定的字段的该值被置为0（如&lt;code&gt;f.allText.hl.snippets=0&lt;/code&gt;），这就表明该字段被禁用高亮了。你可能在hl.fl=*时会这么用。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.fragsize&lt;/code&gt;:&lt;br /&gt;
每个snippet返回的最大字符数。默认是100.如果为0，那么该字段不会被fragmented且整个字段的值会被返回。大字段时不会这么做。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.mergeContiguous&lt;/code&gt;:&lt;br /&gt;
如果被置为true，当snippet重叠时会merge起来。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.maxAnalyzedChars&lt;/code&gt;:&lt;br /&gt;
会搜索高亮的最大字符，默认值为51200，如果你想禁用，设为-1&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.alternateField&lt;/code&gt;:&lt;br /&gt;
如果没有生成snippet（没有terms 匹配），那么使用另一个字段值作为返回。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.maxAlternateFieldLength&lt;/code&gt;:&lt;br /&gt;
如果&lt;code&gt;hl.alternateField&lt;/code&gt;启用，则有时需要制定alternateField的最大字符长度，默认0是即没有限制。所以合理的值是应该为&lt;code&gt;hl.snippets * hl.fragsize&lt;/code&gt;这样返回结果的大小就能保持一致。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.formatter&lt;/code&gt;:一个提供可替换的formatting算法的扩展点。默认值是simple，这是目前仅有的选项。显然这不够用，你可以看看&lt;code&gt;org.apache.solr.highlight.HtmlFormatter.java&lt;/code&gt; 和 solrconfig.xml 中highlighting元素是如何配置的。&lt;br /&gt;
注意在不论原文中被高亮了什么值的情况下，如预先已存在的em tags，也不会被转义，所以在有时会导致假的高亮。&lt;br /&gt;
-&lt;code&gt;hl.fragmenter&lt;/code&gt;:这个是solr制定fragment算法的扩展点。gap是默认值。regex是另一种选项，这种选项指明highlight的边界由一个正则表达式确定。这是一种非典型 的高级选项。为了知道默认设置和fragmenters (and formatters)是如何配置的，可以看看 solrconfig.xml 中的highlight段。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.regex.pattern&lt;/code&gt;:正则表达式的pattern&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hl.regex.slop&lt;/code&gt;:这是hl.fragsize能变化以适应正则表达式的因子。默认值是0.6，意思是如果 &lt;code&gt;hlfragsize=100&lt;/code&gt; 那么fragment的大小会从40-160.&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/03/03/solr-query-syntax.html</link>
      <guid>http://blog.javachen.com/2014/03/03/solr-query-syntax.html</guid>
      <pubDate>2014-03-03T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Apache Solr介绍及安装</title>
      <description>&lt;h1 id=&quot;solr&quot;&gt;Solr是什么&lt;/h1&gt;

&lt;p&gt;Solr是一个基于Lucene java库的企业级搜索服务器，包含XML/HTTP，JSON API，高亮查询结果，缓存，复制，还有一个WEB管理界面。Solr运行在Servlet容器中，其架构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/solr-architecture.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主要功能包括全文检索，高亮命中，分面搜索(faceted search)，近实时索引，动态集群，数据库集成，富文本索引，空间搜索；通过提供分布式索引，复制，负载均衡查询，自动故障转移和恢复，集中配置等功能实现高可用，可伸缩和可容错。&lt;/p&gt;

&lt;p&gt;Solr和Lucene的本质区别有以下三点：搜索服务器、企业级和管理。Lucene本质上是搜索库，不是独立的应用程序，而Solr是。Lucene专注于搜索底层的建设，而Solr专注于企业应用。Lucene不负责支撑搜索服务所必须的管理，而Solr负责。所以说Solr是Lucene面向企业搜索应用的扩展。&lt;/p&gt;

&lt;p&gt;Solr目前有很多用户了，比较著名的用户有 AOL、 Disney、 Apple等，国内的有淘宝，淘宝的终搜就是基于Solr改造的，终搜用于淘宝的SNS、淘女郎等处的搜索。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;安装和部署&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1. 下载&lt;/h2&gt;

&lt;p&gt;官方网址：&lt;a href=&quot;http://lucene.apache.org/solr/&quot;&gt;http://lucene.apache.org/solr/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下载地址：&lt;a href=&quot;http://archive.apache.org/dist/lucene/solr/&quot;&gt;http://archive.apache.org/dist/lucene/solr/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2. 安装与配置&lt;/h2&gt;

&lt;p&gt;以solr-4.4.0为例，解压之后的目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  solr-4.4.0  tree -L 1
.
├── CHANGES.txt
├── contrib
├── dist
├── docs
├── example
├── licenses
├── LICENSE.txt
├── NOTICE.txt
├── README.txt
└── SYSTEM_REQUIREMENTS.txt

5 directories, 5 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;solr提供一个war包可以运行web界面，该文件位于&lt;code&gt;exmaple/webapps&lt;/code&gt;目录下，发布该war包之前需要配置solr home，solr home是索引和配置文件所在的目录。&lt;/p&gt;

&lt;p&gt;solr home的设置有好几种方式：&lt;/p&gt;

&lt;p&gt;1、 基于环境变量solr.solr.home&lt;/p&gt;

&lt;p&gt;直接修改JAVA全局环境变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_OPTS=&quot;$JAVA_OPTS -Dsolr.solr.home=/tmp/solrhome&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以修改&lt;code&gt;TOMCAT_HOME/bin/catalina.sh&lt;/code&gt;，在文件开头添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;JAVA_OPTS=&#39;-Dsolr.solr.home=/tmp/solrhome&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者，在启动时进行设置。start.jar在源码包中可以找到，内部包含jetty容器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -Dsolr.solr.home=/tmp/solrhome -jar start.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、 基于JNDI配置&lt;/p&gt;

&lt;p&gt;修改war中的web.xml文件，取消下面对下面的注视，并修改&lt;code&gt;env-entry-value&lt;/code&gt;的值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;env-entry&amp;gt;
   &amp;lt;env-entry-name&amp;gt;solr/home&amp;lt;/env-entry-name&amp;gt;
   &amp;lt;env-entry-value&amp;gt;/tmp/solrhome&amp;lt;/env-entry-value&amp;gt;
   &amp;lt;env-entry-type&amp;gt;java.lang.String&amp;lt;/env-entry-type&amp;gt;
&amp;lt;/env-entry&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者，创建solr.xml文件放于&lt;code&gt;TOMCAT_HOME/conf/Catalina/localhost&lt;/code&gt;，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;
&amp;lt;Context docBase=&quot;TOMCAT_HOME/webapps/solr.war&quot; debug=&quot;0&quot; crossContext=&quot;true&quot;&amp;gt;
   &amp;lt;Environment name=&quot;solr/home&quot; type=&quot;java.lang.String&quot; value=&quot;/tmp/solrhomehome&quot; override=&quot;true&quot;/&amp;gt;
&amp;lt;/Context&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、 基于当前路径的方式&lt;/p&gt;

&lt;p&gt;这种情况需要在example目录下去启动tomcat，Solr查找./solr，因此在启动时候需要切换到example目录下面&lt;/p&gt;

&lt;h2 id=&quot;jettysolr&quot;&gt;3. 在Jetty上运行Solr&lt;/h2&gt;

&lt;p&gt;在example目录下，运行下面命令即可启动一个内置的jetty容器：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -Dsolr.solr.home=/tmp/solrhome -jar start.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过&lt;code&gt;http://localhost:8983/solr&lt;/code&gt;即可访问。&lt;/p&gt;

&lt;h2 id=&quot;tomcatsolr&quot;&gt;4. 在tomcat中运行Solr&lt;/h2&gt;

&lt;p&gt;将&lt;code&gt;example/webapps/solr.war&lt;/code&gt;拷贝到tomcat的webapps目录下，然后参照上面的说明设置solr home值。tomcat版本可以使用tomcat-6.0.36。&lt;/p&gt;

&lt;p&gt;其次，将&lt;code&gt;example/lib/ext&lt;/code&gt;目录中的jar包拷贝到&lt;code&gt;tomcat-6.0.36/webapps/solr/WEB-INF/lib&lt;/code&gt;目录下。&lt;/p&gt;

&lt;p&gt;然后，将&lt;code&gt;example/resources/log4j.properties&lt;/code&gt;也拷到classpath，或者在tomcat-6.0.36/webapps/solr/目录下新建了一个classes目录，将log4j.properties放进去。&lt;/p&gt;

&lt;p&gt;这时候启动tomcat后访问&lt;code&gt;http://localhost:8080/solr&lt;/code&gt;会提示错误，这是因为solr home目录下没有solr的配置文件和一些目录。请将solr-4.4.0/example/solr/目录下的文件拷贝到solr home目录下，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp -r solr-4.4.0/example/solr/ /tmp/solrhome/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，启动tomcat，然后通过浏览器访问。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;5. 其他&lt;/h2&gt;

&lt;h3 id=&quot;section-4&quot;&gt;关于中文支持&lt;/h3&gt;

&lt;p&gt;关于中文，solr内核支持UTF-8编码，所以在tomcat里的server.xml需要进行配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;Connector port=&quot;8080&quot; maxHttpHeaderSize=&quot;8192&quot; URIEncoding=&quot;UTF-8&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，向solr Post请求的时候需要转为utf-8编码。对solr 返回的查询结果也需要进行一次utf-8的转码。检索数据时对查询的关键字也需要转码，然后用“+”连接。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;String[] array = StringUtils.split(query, null, 0);
for (String str : array) {
    result = result + URLEncoder.encode(str, &quot;UTF-8&quot;) + &quot;+&quot;;
}
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2014/02/26/how-to-install-solr.html</link>
      <guid>http://blog.javachen.com/2014/02/26/how-to-install-solr.html</guid>
      <pubDate>2014-02-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Vagrant创建虚拟机安装Hadoop</title>
      <description>&lt;h1 id=&quot;virtualbox&quot;&gt;安装VirtualBox&lt;/h1&gt;

&lt;p&gt;下载地址：&lt;a href=&quot;https://www.virtualbox.org/wiki/Downloads/&quot;&gt;https://www.virtualbox.org/wiki/Downloads/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;vagrant&quot;&gt;安装Vagrant&lt;/h1&gt;

&lt;p&gt;下载安装包：&lt;a href=&quot;http://downloads.vagrantup.com/&quot;&gt;http://downloads.vagrantup.com/&lt;/a&gt;，然后安装。&lt;/p&gt;

&lt;h1 id=&quot;box&quot;&gt;下载box&lt;/h1&gt;

&lt;p&gt;下载适合你的box，地址：&lt;a href=&quot;http://www.vagrantbox.es/&quot;&gt;http://www.vagrantbox.es/&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;例如下载 CentOS6.5：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://github.com/2creatives/vagrant-centos/releases/download/v6.5.3/centos65-x86_64-20140116.box
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;box-1&quot;&gt;添加box&lt;/h1&gt;

&lt;p&gt;首先查看已经添加的box：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ vagrant box list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加新的box，可以是远程地址也可以是本地文件，建议先下载到本地再进行添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ vagrant box add centos6.5 ./centos65-x86_64-20140116.box
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其语法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;vagrant box add {title} {url}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;box 被安装在 &lt;code&gt;~/.vagrant.d/boxes&lt;/code&gt; 目录下面。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;创建虚拟机&lt;/h1&gt;

&lt;p&gt;先创建一个目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p ~/workspace/vagrant/cdh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化，使用 centos6.5 box：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd ~/workspace/vagrant/cdh
$ vagrant init centos6.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出如下日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A `Vagrantfile` has been placed in this directory. You are now
ready to `vagrant up` your first virtual environment! Please read
the comments in the Vagrantfile as well as documentation on
`vagrantup.com` for more information on using Vagrant.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在当前目录生成了 Vagrantfile 文件。&lt;/p&gt;

&lt;h1 id=&quot;vagrantfile&quot;&gt;修改Vagrantfile&lt;/h1&gt;

&lt;p&gt;修改文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# -*- mode: ruby -*-
# vi: set ft=ruby :

# Vagrantfile API/syntax version. Don&#39;t touch unless you know what you&#39;re doing!
VAGRANTFILE_API_VERSION = &quot;2&quot;

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

  (1..3).each do |i|
    config.vm.define vm_name = &quot;cdh#{i}&quot;  do |config|
        config.vm.provider &quot;virtualbox&quot; do |v|
            v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, vm_name, &quot;--memory&quot;, &quot;2048&quot;,&#39;--cpus&#39;, 1]
        end
        config.vm.box = &quot;centos6.5&quot;
        config.vm.hostname =vm_name
        config.ssh.username = &quot;vagrant&quot;
        config.vm.network :private_network, ip: &quot;192.168.56.12#{i}&quot;
	  	config.vm.provision :shell, :path =&amp;gt; &quot;bootstrap.sh&quot;
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的文件中定义了三个虚拟机，三个虚拟机的名字和 hostname 分别为cdh1、cdh2、cdh3，网络使用的是 &lt;code&gt;host-only&lt;/code&gt; 网络。&lt;/p&gt;

&lt;p&gt;在启动成功之后，会运行 bootstrap.sh 脚本，你可以编写你自己的脚本。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;启动虚拟机&lt;/h1&gt;

&lt;p&gt;执行以下命令会依次启动三个虚拟机：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ vagrant up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动成功之后，就可以通过 ssh 登陆到虚拟机：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ vagrant ssh cdh1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;虚拟机的初始化设置&lt;/h1&gt;

&lt;p&gt;创建好的虚拟机有很多地方没有设置，有一些软件没有安装，可以编写一个shell脚本（例如，命名为 bootstrap.sh）进行手动执行,也可以通过provision启动之后自动运行。该脚本内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/usr/bin/env bash

# The output of all these installation steps is noisy. With this utility
# the progress report is nice and concise.
function install {
    echo Installing $1
    shift
    yum -y install &quot;$@&quot; &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
}

echo &quot;Update /etc/hosts&quot;
cat &amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF
127.0.0.1       localhost

192.168.56.121 cdh1
192.168.56.122 cdh2
192.168.56.123 cdh3
EOF

echo &quot;Remove unused logs&quot;
sudo rm -rf /root/anaconda-ks.cfg /root/install.log /root/install.log.syslog /root/install-post.log

echo &quot;Disable iptables&quot;
setenforce 0 &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 &amp;amp;&amp;amp; iptables -F

### Set env ###
echo &quot;export LC_ALL=en_US.UTF-8&quot;  &amp;gt;&amp;gt;  /etc/profile
cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime

echo &quot;Setup yum repos&quot;
rm -rf /etc/yum.repos.d/*
cp /vagrant/*.repo /etc/yum.repos.d/
yum clean all &amp;gt;/dev/null 2&amp;gt;&amp;amp;1

echo &quot;Setup root account&quot;
# Setup sudo to allow no-password sudo for &quot;admin&quot;. Additionally,
# make &quot;admin&quot; an exempt group so that the PATH is inherited.
cp /etc/sudoers /etc/sudoers.orig
echo &quot;root            ALL=(ALL)               NOPASSWD: ALL&quot; &amp;gt;&amp;gt; /etc/sudoers
echo &#39;redhat&#39;|passwd root --stdin &amp;gt;/dev/null 2&amp;gt;&amp;amp;1

echo &quot;Setup nameservers&quot;
# http://ithelpblog.com/os/linux/redhat/centos-redhat/howto-fix-couldnt-resolve-host-on-centos-redhat-rhel-fedora/
# http://stackoverflow.com/a/850731/1486325
echo &quot;nameserver 8.8.8.8&quot; | tee -a /etc/resolv.conf
echo &quot;nameserver 8.8.4.4&quot; | tee -a /etc/resolv.conf

echo &quot;Setup ssh&quot;
[ ! -d /root/.ssh ] &amp;amp;&amp;amp; ( mkdir /root/.ssh ) &amp;amp;&amp;amp; ( chmod 600 /root/.ssh  ) &amp;amp;&amp;amp; yes|ssh-keygen -f ~/.ssh/id_rsa -t rsa -N &quot;&quot;

install Git git
install &quot;Base tools&quot; vim wget curl
install &quot;Hadoop dependencies&quot; expect rsync pssh

install PostgreSQL postgresql-server postgresql-jdbc
sudo -u postgres createuser --superuser vagrant
sudo -u postgres createdb -O vagrant test1
sudo -u postgres createdb -O vagrant test2


echo &#39;All set, rock on!&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上脚本主要做了以下几件事：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;设置hosts文件&lt;/li&gt;
  &lt;li&gt;设置公网网络下的命名服务解析&lt;/li&gt;
  &lt;li&gt;关掉防火墙&lt;/li&gt;
  &lt;li&gt;设置虚拟机时区&lt;/li&gt;
  &lt;li&gt;修改root帐号密码为redhat&lt;/li&gt;
  &lt;li&gt;生成ssh公要文件&lt;/li&gt;
  &lt;li&gt;配置yum源并安装一些常用软件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上所有配置可以在 &lt;a href=&quot;https://github.com/javachen/snippets/tree/master/vagrant/cdh&quot;&gt;这里找&lt;/a&gt; 找到，其中 cdh.repo 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[cdh]
name=cdh
baseurl=http://192.168.56.1/cdh/5.2.0/
enabled=1
gpgcheck=0

[hadoop-repo]
name=hadoop-repo
baseurl=http://192.168.56.1/hadoop-repo/
enabled=1
gpgcheck=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面文件包括 cdh 和 hadoop 相关的一些依赖，这些需要通过 apache 服务在宿主机上配置好。&lt;/p&gt;

&lt;h1 id=&quot;hadoop&quot;&gt;安装hadoop&lt;/h1&gt;

&lt;p&gt;可以参考&lt;a href=&quot;http://blog.javachen.com/categories.html#hadoop-ref&quot;&gt;这些文章&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;你可以参考上面的文章手动安装 hadoop，也可以通过我写的 &lt;a href=&quot;https://github.com/javachen/hadoop-install/tree/master/shell&quot;&gt;shell&lt;/a&gt; 脚本来安装。&lt;/p&gt;

&lt;p&gt;步骤：&lt;/p&gt;

&lt;p&gt;1.在虚拟机中选择一个节点为管理节点，然后下载仓库&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone https://github.com/javachen/hadoop-install.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.进入 hadoop-install/shell 目录，参考 READEME.md 中说明来安装集群。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/02/23/create-virtualbox-by-vagrant.html</link>
      <guid>http://blog.javachen.com/2014/02/23/create-virtualbox-by-vagrant.html</guid>
      <pubDate>2014-02-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Python基础入门</title>
      <description>&lt;h1 id=&quot;python&quot;&gt;1. Python介绍&lt;/h1&gt;

&lt;p&gt;Python是一种解释性的面向对象的语言。Python使用C语言编写，不需要事先声明变量的类型（动态类型），但是一旦变量有了值，那么这个变脸是有一个类型的，不同类型的变量之间赋值需要类型转换（强类型）。&lt;/p&gt;

&lt;h2 id=&quot;python-1&quot;&gt;1.1 安装 Python&lt;/h2&gt;

&lt;p&gt;现在的操作系统都自带安装了 Python，要测试你是否安装了Python，你可以打开一个shell程序（就像konsole或gnome-terminal），然后输入如下所示的命令&lt;code&gt;python -V&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python -V
Python 2.7.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你看见向上面所示的那样一些版本信息，那么你已经安装了Python了。&lt;/p&gt;

&lt;h1 id=&quot;python-&quot;&gt;2. Python 基础&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;2.1 注释&lt;/h2&gt;

&lt;p&gt;无论是行注释还是段注释，均以 &lt;code&gt;#&lt;/code&gt; 加一个空格来注释。&lt;/p&gt;

&lt;p&gt;如果需要在代码中使用中文注释，必须在 python 文件的最前面加上如下注释说明：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -* - coding: UTF-8 -* -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如下注释用于指定解释器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#! /usr/bin/python
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2.2 数据类型和变量&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;python的数字类型分为整型、长整型、浮点型、布尔型、复数类型、None类型。&lt;/li&gt;
  &lt;li&gt;python没有字符类型&lt;/li&gt;
  &lt;li&gt;python内部没有普通类型，任何类型都是对象。&lt;/li&gt;
  &lt;li&gt;如果需要查看变量的类型，可以使用type类，该类可以返回变量的类型或创建一个新的类型。&lt;br /&gt;
 python有3种表示字符串类型的方式，即单引号、双引号、三引号。单引号和双引号的作用是相同的。python程序员更喜欢用单引号，C/Java程序员则习惯使用双引号表示字符串。三引号中可以输入单引号、双引号或换行等字符。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;变量命名规则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一个字符必须是字母表中的字母（大写或小写）或者一个下划线（‘ _ ’）&lt;/li&gt;
  &lt;li&gt;其他部分可以由字母（大写或小写）、下划线（‘ _ ’）或数字（0-9）组成&lt;/li&gt;
  &lt;li&gt;对大小写敏感&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;python中的变量不需要声明，变量的赋值操作即使变量声明和定义的过程。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt;a = 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么你的内存里就有了一个变量a， 它的值是10，它的类型是integer (整数)。 在此之前你不需要做什么特别的声明，而数据类型是Python自动决定的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt;print a
&amp;gt;&amp;gt;&amp;gt;print type(a)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么会有如下输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10
&amp;lt;type &#39;int&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里，我们学到一个内置函数type(), 用以查询变量的类型。&lt;/p&gt;

&lt;p&gt;如果你想让a存储不同的数据，你不需要删除原有变量就可以直接赋值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt;a = 1.3
&amp;gt;&amp;gt;&amp;gt;print a,type(a)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会有如下输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.3 &amp;lt;type &#39;float&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;python 中一次新的赋值，将创建一个新的变量。即使变量的名称相同，变量的标识并不相同。用id()函数可以获取变量标识：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;x = 1
print id(x)
x = 2
print id(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果变量没有赋值，则python认为该变量不存在。&lt;/p&gt;

&lt;p&gt;在函数之外定义的变量都可以称为全局变量。全局变量可以被文件内部的任何函数和外部文件访问。&lt;/p&gt;

&lt;p&gt;全局变量建议在文件的开头定义，也可以把全局变量放到一个专门的文件中，然后通过import来引用&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2.3 序列&lt;/h2&gt;

&lt;p&gt;sequence(序列)是一组有顺序的元素的集合。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;序列包括字符串、列表、元组。&lt;/li&gt;
  &lt;li&gt;序列可以包含一个或多个元素，也可以没有任何元素。&lt;/li&gt;
  &lt;li&gt;使用索引来访问序列，索引从0开始。&lt;/li&gt;
  &lt;li&gt;可以使用分片操作来访问一定范围内的元素。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;序列有两种：tuple（定值表； 也有翻译为元组） 和 list (表)。tuple 和 list 的主要区别在于，一旦建立，tuple 的各个元素不可再变更，而 list 的各个元素可以再变更。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt;s1 = (2, 1.3, &#39;love&#39;, 5.6, 9, 12, False)         # s1是一个tuple
&amp;gt;&amp;gt;&amp;gt;s2 = [True, 5, &#39;smile&#39;]                          # s2是一个list
&amp;gt;&amp;gt;&amp;gt;print s1,type(s1)
&amp;gt;&amp;gt;&amp;gt;print s2,type(s2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;序列可以进行相加和乘法等操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; [1,2]+[3,4]
  [1,2,3,4]

#列表和字符串是无法连接在一起的
&amp;gt;&amp;gt;&amp;gt; &quot;hello,&quot;+&quot;world!&quot;
  &quot;hello,world!&quot;

&amp;gt;&amp;gt;&amp;gt; &quot;a&quot; * 5
  &quot;aaaaa&quot;
&amp;gt;&amp;gt;&amp;gt; [2] * 5
  [2,2,2,2,2]

#初始化一个长度为3的空序列
&amp;gt;&amp;gt;&amp;gt; seq=[None] * 3
  [None,None,Noe]

#判断一个元素是否存在于序列中
&amp;gt;&amp;gt;&amp;gt; permissions=&#39;rw&#39;
&amp;gt;&amp;gt;&amp;gt; &#39;w&#39; in permissions
  True

&amp;gt;&amp;gt;&amp;gt; users=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]
&amp;gt;&amp;gt;&amp;gt; &#39;a&#39; in users
  True

&amp;gt;&amp;gt;&amp;gt; database=[
  [&#39;a&#39;,&#39;1234&#39;],
  [&#39;b&#39;,&#39;2344&#39;]
]
&amp;gt;&amp;gt;&amp;gt; [&#39;c&#39;,&#39;1234&#39;] in database
  False
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;2.4. 列表&lt;/h2&gt;

&lt;p&gt;列表类似于c语言中的数组，用于存储顺序结构。例如：&lt;code&gt;[1,2,3,4,5]&lt;/code&gt;。列表中的各个元素可以是任意类型，元素之间用逗号分隔。列表的下标从0开始，和c语言类似，但是增加了负下标的使用。&lt;/p&gt;

&lt;p&gt;对列表的操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; names=[&#39;zhangsan&#39;,&#39;lisi&#39;,&#39;lili&#39;,&#39;wangwu&#39;]

#删除
&amp;gt;&amp;gt;&amp;gt; del names[0]

#分片赋值
&amp;gt;&amp;gt;&amp;gt; names[1:]=list(&#39;ab&#39;) #list函数用于将字符串转换为列表
&amp;gt;&amp;gt;&amp;gt; names
  [&#39;lisi&#39;,&#39;a&#39;,&#39;b&#39;]

#在不需要替换任何原有元素的情况下插入新元素
&amp;gt;&amp;gt;&amp;gt; names[1:1]=[&#39;c&#39;,&#39;d&#39;]
&amp;gt;&amp;gt;&amp;gt; names
  [&#39;lisi&#39;,&#39;c&#39;,&#39;d&#39;,&#39;a&#39;,&#39;b&#39;]
&amp;gt;&amp;gt;&amp;gt; names[1:4]=[]
&amp;gt;&amp;gt;&amp;gt; names
  [&#39;lisi&#39;,&#39;b&#39;]

#添加
&amp;gt;&amp;gt;&amp;gt; names.append(&#39;bb&#39;)
&amp;gt;&amp;gt;&amp;gt; names.count(&#39;b&#39;)

#extend效率高于连接操作
&amp;gt;&amp;gt;&amp;gt; names.extend([&#39;c&#39;,&#39;e&#39;,&#39;d&#39;])
&amp;gt;&amp;gt;&amp;gt; names
  [&#39;lisi&#39;,&#39;b&#39;,&#39;bb&#39;,&#39;c&#39;,&#39;e&#39;,&#39;d&#39;]

&amp;gt;&amp;gt;&amp;gt; names.index(&#39;lisi&#39;)
&amp;gt;&amp;gt;&amp;gt; names.insert(1,&#39;a&#39;)
&amp;gt;&amp;gt;&amp;gt; names.pop()
  &#39;e&#39;
&amp;gt;&amp;gt;&amp;gt; names.remove(&#39;lisi&#39;)
&amp;gt;&amp;gt;&amp;gt; names
  [&#39;a&#39;,&#39;b&#39;,&#39;bb&#39;,&#39;c&#39;,&#39;e&#39;,&#39;d&#39;]
&amp;gt;&amp;gt;&amp;gt; names.reverse()
  [&#39;d&#39;,&#39;e&#39;,&#39;c&#39;,&#39;bb&#39;,&#39;b&#39;,&#39;a&#39;]

#排序
&amp;gt;&amp;gt;&amp;gt; names.sort()
  [&#39;a&#39;,&#39;b&#39;,&#39;bb&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;]
&amp;gt;&amp;gt;&amp;gt; sorted(names)
  [&#39;a&#39;,&#39;b&#39;,&#39;bb&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;]

#高级排序
&amp;gt;&amp;gt;&amp;gt; names.sort(cmp)
&amp;gt;&amp;gt;&amp;gt; names.sort(key=len)
&amp;gt;&amp;gt;&amp;gt; names.sort(reverse=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;2.5 元组&lt;/h2&gt;

&lt;p&gt;元组 tuple 是常量 list。tuple 不能 pop,remove,insert 等方法。&lt;br /&gt;
- tuple 用 &lt;code&gt;()&lt;/code&gt; 表示，如 &lt;code&gt;a=(1,2,3,4)&lt;/code&gt;,括号可以省略。&lt;br /&gt;
- tuple 可以用下标返回元素或者子 tuple&lt;br /&gt;
- tuple 可以用于多个变量的赋值。例如：&lt;code&gt;a,b=(1,2)&lt;/code&gt;&lt;br /&gt;
- 表示只含有一个元素的 tuple 的方法是：&lt;code&gt;(1,)&lt;/code&gt;,后面有个逗号，用来和单独的变量相区分。&lt;br /&gt;
- 字符串是一种特殊的元素，因此可以执行元组的相关操作。&lt;/p&gt;

&lt;p&gt;tuple 比 list 性能好，也就是不用提供动态内存管理的功能。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#定义一个元组
&amp;gt;&amp;gt;&amp;gt; 1,2,3
  (1,2,3)

#空的元组
&amp;gt;&amp;gt;&amp;gt; ()

&amp;gt;&amp;gt;&amp;gt; 42
  42
&amp;gt;&amp;gt;&amp;gt; 42,
  (42,)
&amp;gt;&amp;gt;&amp;gt; (42,)
  (42,)

#tuple函数
&amp;gt;&amp;gt;&amp;gt; tuple([1,2,3])
  (1,2,3)
&amp;gt;&amp;gt;&amp;gt; tuple(&#39;abc&#39;)
  (&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)
&amp;gt;&amp;gt;&amp;gt; tuple((1,2,3))
  (1,2,3)

&amp;gt;&amp;gt;&amp;gt; x=1,2,3
&amp;gt;&amp;gt;&amp;gt; x[1]
  2
&amp;gt;&amp;gt;&amp;gt; x[0:2]
  (1,2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;2.6 字典&lt;/h2&gt;

&lt;p&gt;字典是一个无序存储结构。每一个元素是一个 pair，包括 key 和 value 两个不服。key 的类型是 integer 或者 string 或者任何同时含有 &lt;code&gt;__hash__&lt;/code&gt;和&lt;code&gt;__cmp__&lt;/code&gt; 的对象。字典中没有重复的 key，其每一个元素是一个元组。&lt;/p&gt;

&lt;p&gt;创建和使用字典：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;phonebook = {&#39;zhangsan&#39;:&#39;1234&#39;,&quot;lisi&quot;:&#39;1231&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;dict函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; items = [(&#39;name&#39;,&#39;aa&#39;),(&#39;age&#39;,18)]
&amp;gt;&amp;gt;&amp;gt; d = dict(items)
&amp;gt;&amp;gt;&amp;gt; d
  {&#39;age&#39;:18,&#39;name&#39;:&#39;aa&#39;}

&amp;gt;&amp;gt;&amp;gt; d = dict(name=&#39;aa&#39;,age=18)
&amp;gt;&amp;gt;&amp;gt; d
  {&#39;age&#39;:18,&#39;name&#39;:&#39;aa&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字典的基本操作：&lt;/p&gt;

&lt;h2 id=&quot;set&quot;&gt;2.7 set&lt;/h2&gt;

&lt;p&gt;set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。&lt;/p&gt;

&lt;p&gt;要创建一个set，需要提供一个list作为输入集合：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; s = set([1, 2, 3])
&amp;gt;&amp;gt;&amp;gt; s
set([1, 2, 3])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，传入的参数[1, 2, 3]是一个list，而显示的set([1, 2, 3])只是告诉你这个set内部有1，2，3这3个元素，显示的[]不表示这是一个list。&lt;/p&gt;

&lt;p&gt;重复元素在set中自动被过滤：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; s = set([1, 1, 2, 2, 3, 3])
&amp;gt;&amp;gt;&amp;gt; s
set([1, 2, 3])

通过add(key)方法可以添加元素到set中，可以重复添加，但不会有效果：

```python
&amp;gt;&amp;gt;&amp;gt; s.add(4)
&amp;gt;&amp;gt;&amp;gt; s
set([1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; s.add(4)
&amp;gt;&amp;gt;&amp;gt; s
set([1, 2, 3, 4])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过remove(key)方法可以删除元素：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; s.remove(4)
&amp;gt;&amp;gt;&amp;gt; s
set([1, 2, 3])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; s1 = set([1, 2, 3])
&amp;gt;&amp;gt;&amp;gt; s2 = set([2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; s1 &amp;amp; s2
set([2, 3])
&amp;gt;&amp;gt;&amp;gt; s1 | s2
set([1, 2, 3, 4])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。试试把list放入set，看看是否会报错。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;2.8 字符串&lt;/h2&gt;

&lt;h3 id=&quot;section-7&quot;&gt;字符串&lt;/h3&gt;

&lt;p&gt;普通字符串使用双引号或者单引号或者 &lt;code&gt;&quot;&quot;&quot;&lt;/code&gt; 来表示。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;print &quot;hello,world !&quot;

print &quot;hello,\
world !&quot;

print &#39;&#39;&#39;
    hello,world!
&#39;&#39;&#39;

print &quot;&quot;&quot;
    hello,world!
&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-8&quot;&gt;自然字符串&lt;/h3&gt;

&lt;p&gt;如果你想要指示某些&lt;strong&gt;不需要如转义符那样的特别处理&lt;/strong&gt;的字符串，那么你需要指定一个自然字符串。自然字符串通过给字符串加上前缀 &lt;code&gt;r&lt;/code&gt; 或 &lt;code&gt;R&lt;/code&gt; 来指定。例如&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;r&quot;Newlines are indicated by \n&quot;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;strong&gt;自然字符串结尾不能输入反斜线&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;unicode&quot;&gt;Unicode字符串&lt;/h3&gt;

&lt;p&gt;Unicode 是书写国际文本的标准方法。如果你想要用你的母语如北印度语或阿拉伯语写文本，那么你需要有一个支持 Unicode  的编辑器。类似地，Python 允许你处理 Unicode 文本——你只需要在字符串前加上前缀 &lt;code&gt;u&lt;/code&gt; 或 &lt;code&gt;U&lt;/code&gt;。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;u&quot;This is a Unicode string.&quot;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python 中的普通字符串在内部是以8位的 ascii 码形式存储的，而 Unicode 字符串则存储为16位 Unicode 字符。&lt;/p&gt;

&lt;p&gt;记住，在你处理文本文件的时候使用 Unicode 字符串，特别是当你知道这个文件含有用非英语的语言写的文本。&lt;/p&gt;

&lt;h1 id=&quot;section-9&quot;&gt;3. 函数&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;python程序由包(package)、模块(module)和函数组成。包是由一系列模块组成的集合。模块是处理某一类问题的函数和类的集合。&lt;/li&gt;
  &lt;li&gt;包就是一个完成特定任务的工具箱。&lt;/li&gt;
  &lt;li&gt;包必须含有一个&lt;code&gt;__init__.py&lt;/code&gt;文件，它用于标识当前文件夹是一个包。&lt;/li&gt;
  &lt;li&gt;python的程序是由一个个模块组成的。模块把一组相关的函数或代码组织到一个文件中，一个文件即是一个模块。模块由代码、函数和类组成。导入模块使用import语句。&lt;/li&gt;
  &lt;li&gt;包的作用是实现程序的重用。&lt;/li&gt;
  &lt;li&gt;函数是一段可以重复多次调用的代码。&lt;/li&gt;
  &lt;li&gt;函数返回值可以用return来控制。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-10&quot;&gt;3.1 定义函数&lt;/h2&gt;

&lt;p&gt;函数定义示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def function_arithmetic(x,y,operator):
    &#39;&#39;&#39;
    usage: function for arithmetic
    &#39;&#39;&#39;
    result = {
      &quot;+&quot;:x+y,
      &quot;-&quot;:x-y,
      &quot;*&quot;:x*y,
      &quot;/&quot;:x/y
    }
    return result

# 函数名称
func_name = function_arithmetic.__name__
name = &#39;%s&#39; % func_name.replace(&#39;_&#39;, &#39;-&#39;).strip(&#39;-&#39;)
# 函数doc 文档
help_ = function_arithmetic.__doc__.strip()

print func_name
print name
print help_
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-11&quot;&gt;3.2 函数的参数&lt;/h2&gt;

&lt;h3 id=&quot;section-12&quot;&gt;默认参数&lt;/h3&gt;

&lt;p&gt;最有用的形式是为一个或更多参数指定默认值。这样创建的函式调用时可以不用给足参数.。例如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def ask_ok(prompt, retries=4, complaint=&#39;Yes or no, please!&#39;):
    pass
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-13&quot;&gt;关键字参数&lt;/h3&gt;

&lt;p&gt;函式也可以通过 keyword = value 形式的关键字参数来调用。例如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;ask_ok(&#39;ok?&#39;,complaint=&#39;Yes or no, please!&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-14&quot;&gt;可变参数&lt;/h3&gt;

&lt;p&gt;在Python函数中，还可以定义可变参数。顾名思义，可变参数就是传入的参数个数是可变的，可以是1个、2个到任意个，还可以是0个。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def calc(*numbers):
    sum = 0
    for n in numbers:
        sum = sum + n * n
    return sum

&amp;gt;&amp;gt;&amp;gt; calc(1, 2)
5
&amp;gt;&amp;gt;&amp;gt; calc()
0

&amp;gt;&amp;gt;&amp;gt; nums = [1, 2, 3]
&amp;gt;&amp;gt;&amp;gt; calc(*nums)
14
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-15&quot;&gt;关键字参数&lt;/h3&gt;

&lt;p&gt;可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def person(name, age, **kw):
    print &#39;name:&#39;, name, &#39;age:&#39;, age, &#39;other:&#39;, kw
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; person(&#39;Michael&#39;, 30)
name: Michael age: 30 other: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以传入任意个数的关键字参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; person(&#39;Bob&#39;, 35, city=&#39;Beijing&#39;)
name: Bob age: 35 other: {&#39;city&#39;: &#39;Beijing&#39;}
&amp;gt;&amp;gt;&amp;gt; person(&#39;Adam&#39;, 45, gender=&#39;M&#39;, job=&#39;Engineer&#39;)
name: Adam age: 45 other: {&#39;gender&#39;: &#39;M&#39;, &#39;job&#39;: &#39;Engineer&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-16&quot;&gt;参数组合&lt;/h3&gt;

&lt;p&gt;在Python中定义函数，可以用必选参数、默认参数、可变参数和关键字参数，这4种参数都可以一起使用，或者只用其中某些，但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数和关键字参数。&lt;/p&gt;

&lt;p&gt;比如定义一个函数，包含上述4种参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def func(a, b, c=0, *args, **kw):
    print &#39;a =&#39;, a, &#39;b =&#39;, b, &#39;c =&#39;, c, &#39;args =&#39;, args, &#39;kw =&#39;, kw
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在函数调用的时候，Python解释器自动按照参数位置和参数名把对应的参数传进去。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; func(1, 2)
a = 1 b = 2 c = 0 args = () kw = {}
&amp;gt;&amp;gt;&amp;gt; func(1, 2, c=3)
a = 1 b = 2 c = 3 args = () kw = {}
&amp;gt;&amp;gt;&amp;gt; func(1, 2, 3, &#39;a&#39;, &#39;b&#39;)
a = 1 b = 2 c = 3 args = (&#39;a&#39;, &#39;b&#39;) kw = {}
&amp;gt;&amp;gt;&amp;gt; func(1, 2, 3, &#39;a&#39;, &#39;b&#39;, x=99)
a = 1 b = 2 c = 3 args = (&#39;a&#39;, &#39;b&#39;) kw = {&#39;x&#39;: 99}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最神奇的是通过一个tuple和dict，你也可以调用该函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; args = (1, 2, 3, 4)
&amp;gt;&amp;gt;&amp;gt; kw = {&#39;x&#39;: 99}
&amp;gt;&amp;gt;&amp;gt; func(*args, **kw)
a = 1 b = 2 c = 3 args = (4,) kw = {&#39;x&#39;: 99}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以，对于任意函数，都可以通过类似&lt;code&gt;func(*args, **kw)&lt;/code&gt;的形式调用它，无论它的参数是如何定义的。&lt;/p&gt;

&lt;h3 id=&quot;section-17&quot;&gt;参数列表解包&lt;/h3&gt;

&lt;p&gt;也存在相反的情形: 当参数存在于一个既存的列表或者元组之中, 但却需要解包以若干位置参数的形式被函数调用. 例如, 内建的 range() 函数期望接收分别的开始和结束的位置参数. 如果它们不是分别可用 (而是同时存在于一个列表或者元组中), 下面是一个利用 * 操作符解从列表或者元组中解包参数以供函数调用的例子:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; x,y,z=1,2,3
&amp;gt;&amp;gt;&amp;gt; print x,y,z
  1 2 3

#交换值
&amp;gt;&amp;gt;&amp;gt; x,y=y,x

&amp;gt;&amp;gt;&amp;gt; phonebook = {&#39;zhangsan&#39;:&#39;1234&#39;,&quot;lisi&quot;:&#39;1231&#39;}
&amp;gt;&amp;gt;&amp;gt; key,value=phonebook.popitem()

&amp;gt;&amp;gt;&amp;gt; list(range(3, 6))            # 使用分离的参数正常调用
[3, 4, 5]
&amp;gt;&amp;gt;&amp;gt; args = [3, 6]
&amp;gt;&amp;gt;&amp;gt; list(range(*args))           # 通过解包列表参数调用
[3, 4, 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样的, 字典可以通过 ** 操作符来解包参数:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; def parrot(voltage, state=&#39;a stiff&#39;, action=&#39;voom&#39;):
     print(&quot;-- This parrot wouldn&#39;t&quot;, action, end=&#39; &#39;)
     print(&quot;if you put&quot;, voltage, &quot;volts through it.&quot;, end=&#39; &#39;)
     print(&quot;E&#39;s&quot;, state, &quot;!&quot;)

&amp;gt;&amp;gt;&amp;gt; d = {&quot;voltage&quot;: &quot;four million&quot;, &quot;state&quot;: &quot;bleedin&#39; demised&quot;, &quot;action&quot;: &quot;VOOM&quot;}
&amp;gt;&amp;gt;&amp;gt; parrot(**d)
 This parrot wouldn&#39;t VOOM if you put four million volts through it. E&#39;s bleedin&#39; demised
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-18&quot;&gt;4. 流程控制&lt;/h1&gt;

&lt;h2 id=&quot;if-&quot;&gt;4.1 if 语句&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; x = int(input(&quot;Please enter an integer: &quot;))
Please enter an integer: 42
&amp;gt;&amp;gt;&amp;gt; if x &amp;lt; 0:
      x = 0
      print(&#39;Negative changed to zero&#39;)
 elif x == 0:
      print(&#39;Zero&#39;)
 elif x == 1:
      print(&#39;Single&#39;)
 else:
      print(&#39;More&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;for-&quot;&gt;4.2 for 语句&lt;/h2&gt;

&lt;p&gt;Python中的 for 语句与你在 C 或是 Pascal 中使用的略有不同. 不同于在 Pascal 中总是依据一个等差的数值序列迭代, 也不同于在 C 中允许用户同时定义迭代步骤和终止条件, Python中的 for 语句在任意序列 (列表或者字符串) 中迭代时, 总是按照元素在序列中的出现顺序依次迭代.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; a = [&#39;cat&#39;, &#39;window&#39;, &#39;defenestrate&#39;]
&amp;gt;&amp;gt;&amp;gt; for x in a:
     print(x, len(x))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在循环过程中修改被迭代的对象是不安全的 (这只可能发生在可变序列类型上,如列表)。&lt;/p&gt;

&lt;p&gt;若想在循环体内修改你正迭代的序列 (例如复制序列中选定的项), 最好是先制作一个副本。&lt;/p&gt;

&lt;p&gt;而切片则让这种操作十分方便:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; for x in a[:]: # 制造整个列表的切片复本
    if len(x) &amp;gt; 6: a.insert(0, x)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;pass-&quot;&gt;4.4 pass 语句&lt;/h2&gt;

&lt;p&gt;pass 语句什么都不做. 当语法上需要一个语句, 但程序不要动作时, 就可以使用它. 例如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; while True:
     pass  # 忙等待键盘中断 (Ctrl+C)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般也可以用于创建最小类:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; class MyEmptyClass:
     pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另一个使用 pass 的地方是，作为函式或条件体的占位符，当你在新代码工作时，它让你能保持在更抽象的级别思考。 pass 会被默默地被忽略：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; def initlog(*args):
     pass   # 记得实现这里!
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-19&quot;&gt;5. 高级特性&lt;/h1&gt;

&lt;h2 id=&quot;section-20&quot;&gt;5.1 切片&lt;/h2&gt;

&lt;p&gt;下面这部分内容来自 &lt;a href=&quot;http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/0013868196352269f28f1f00aee485ea27e3c4e47f12bc7000&quot;&gt;廖雪峰的Python教程-切片&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;取一个list或tuple的部分元素是非常常见的操作。比如，一个list如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L = [&#39;Michael&#39;, &#39;Sarah&#39;, &#39;Tracy&#39;, &#39;Bob&#39;, &#39;Jack&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;取前3个元素，用一行代码就可以完成切片：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[0:3]
[&#39;Michael&#39;, &#39;Sarah&#39;, &#39;Tracy&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;L[0:3]&lt;/code&gt; 表示，从索引0开始取，直到索引3为止，但不包括索引3。即索引0，1，2，正好是3个元素。&lt;/p&gt;

&lt;p&gt;如果第一个索引是0，还可以省略：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[:3]
[&#39;Michael&#39;, &#39;Sarah&#39;, &#39;Tracy&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以从索引1开始，取出2个元素出来：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[1:3]
[&#39;Sarah&#39;, &#39;Tracy&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;类似的，既然Python支持 &lt;code&gt;L[-1]&lt;/code&gt; 取倒数第一个元素，那么它同样支持倒数切片，试试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[-2:]
[&#39;Bob&#39;, &#39;Jack&#39;]
&amp;gt;&amp;gt;&amp;gt; L[-2:-1]
[&#39;Bob&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记住倒数第一个元素的索引是 &lt;code&gt;-1&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;切片操作十分有用。我们先创建一个0-99的数列：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L = range(100)
&amp;gt;&amp;gt;&amp;gt; L
[0, 1, 2, 3, ..., 99]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过切片轻松取出某一段数列。比如前10个数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[:10]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;后10个数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[-10:]
[90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前11-20个数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[10:20]
[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前10个数，每两个取一个：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[:10:2]
[0, 2, 4, 6, 8]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有数，每5个取一个：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[::5]
[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;甚至什么都不写，只写 &lt;code&gt;[:]&lt;/code&gt; 就可以原样复制一个list：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; L[:]
[0, 1, 2, 3, ..., 99]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tuple 也是一种 list，唯一区别是 tuple 不可变。因此，tuple 也可以用切片操作，只是操作的结果仍是 tuple：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; (0, 1, 2, 3, 4, 5)[:3]
(0, 1, 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字符串 ‘xxx’ 或 Unicode 字符串 u’xxx’ 也可以看成是一种 list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &#39;ABCDEFG&#39;[:3]
&#39;ABC&#39;
&amp;gt;&amp;gt;&amp;gt; &#39;ABCDEFG&#39;[::2]
&#39;ACEG&#39;
Try
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-21&quot;&gt;5.2 迭代&lt;/h2&gt;

&lt;p&gt;使用 &lt;code&gt;for&lt;/code&gt; 关键字来迭代序列、字段或者可迭代的对象。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;d = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}

# 迭代字段的 key
for key in d:
  print key

# 迭代字段的 value
for value in d.itervalues():
  print print

# 迭代字符串
for ch in &#39;ABC&#39;:
  print ch

# Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身
for i, value in enumerate([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]):
  print i, value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;判断一个对象是否支持迭代：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from collections import Iterable
&amp;gt;&amp;gt;&amp;gt; isinstance(&#39;abc&#39;, Iterable) # str是否可迭代
True
&amp;gt;&amp;gt;&amp;gt; isinstance([1,2,3], Iterable) # list是否可迭代
True
&amp;gt;&amp;gt;&amp;gt; isinstance(123, Iterable) # 整数是否可迭代
False
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-22&quot;&gt;5.3 列表推导式&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; [x*x for x in range(7)]
 [0,1,4,9,16,25,36]

&amp;gt;&amp;gt;&amp;gt; [x*x for x in range(7) if x%3 ==0 ]
 [0,9,36]

&amp;gt;&amp;gt;&amp;gt; [(x,y) for x in range(3) for y in range(2)]
  [(0,0),(0,1),(1,0),(1,1),(2,0),(2,1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另一个使用列表推导式的代码示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;num = [1, 4, -5, 10, -7, 2, 3, -1]

def square_generator(optional_parameter):
    return (x ** 2 for x in num if x &amp;gt; optional_parameter)

print square_generator(0)
# &amp;lt;generator object &amp;lt;genexpr&amp;gt; at 0x004E6418&amp;gt;

# Option I
for k in square_generator(0):
    print k
# 1, 16, 100, 4, 9

# Option II
g = list(square_generator(0))
print g
# [1, 16, 100, 4, 9]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-23&quot;&gt;5.4 生成器&lt;/h2&gt;

&lt;p&gt;除非特殊的原因，应该经常在代码中使用生成器表达式。但除非是面对非常大的列表，否则是不会看出明显区别的。&lt;/p&gt;

&lt;p&gt;使用生成器得到当前目录及其子目录中的所有文件的代码，下面代码来自 &lt;a href=&quot;http://blog.jobbole.com/61171/&quot;&gt;伯乐在线-python高级编程技巧&lt;/a&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
def tree(top):
    #path,folder list,file list
    for path, names, fnames in os.walk(top):
        for fname in fnames:
            yield os.path.join(path, fname)

for name in tree(os.getcwd()):
    print name
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-24&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000&quot;&gt;Python教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hujiaweibujidao.github.io/blog/2014/05/10/python-tips1/&quot;&gt;Python Basics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/02/22/python-introduction-of-basics.html</link>
      <guid>http://blog.javachen.com/2014/02/22/python-introduction-of-basics.html</guid>
      <pubDate>2014-02-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Confluence 5.4.2安装</title>
      <description>&lt;p&gt;Confluence是Atlassian公司出品的团队协同与知识管理工具。 Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。通过它可以实现团队成员之间的协作和知识共享。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1、下载&lt;/h1&gt;

&lt;p&gt;下载指定版本Confluence&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p /data/confluence
$ cd /data/confluence
$ wget www.atlassian.com/software/confluence/downloads/binary/atlassian-confluence-5.4.2.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2、安装&lt;/h1&gt;

&lt;p&gt;解压：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tar -zxvf atlassian-confluence-5.4.2.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd atlassian-confluence-5.4.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置confluence安装目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim confluence/WEB-INF/classes/confluence-init.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在属性文件中添加安装目录路径：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;###########################
# Note for Unix Users     #
###########################
# - For example:
confluence.home=/data/confluence/confluence-data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir confluence-data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建完成后，运行启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sh bin/start-confluence.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问ip+portnum，默认端口为8090，如果出现破解界面，以上步骤即为成功&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3、启停服务&lt;/h1&gt;

&lt;p&gt;使用压缩包形式的Confluence无法直接使用服务启停，需要配合目录的sh文件&lt;/p&gt;

&lt;p&gt;启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sh bin/start-confluence.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sh bin/stop-confluence.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4、修改默认端口&lt;/h1&gt;

&lt;p&gt;配置文件位置：&lt;code&gt;atlassian-confluence-5.4.2/conf/server.xml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;打开文件，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;Connector className=&quot;org.apache.coyote.tomcat4.CoyoteConnector&quot; port=&quot;8090&quot; minProcessors=&quot;5&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 port 为需要的端口。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5、更换默认的数据库&lt;/h1&gt;

&lt;p&gt;在完成之前的步骤后，数据库使用的是一个 confluence 默认的 hsql 数据库，此数据库缺陷较多，例如：不支持事务管理。因此需要将数据库迁移为指定的数据库类型。&lt;/p&gt;

&lt;p&gt;进入 confluence-data 目录，修改 &lt;code&gt;confluence-cfg.xml&lt;/code&gt; 文件中数据库相关的连接信息。&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;6、安装中文字体：&lt;/h1&gt;

&lt;p&gt;默认情况下 Confluence 导出 PDF 不支持中文，需要修改如下：&lt;/p&gt;

&lt;p&gt;管理员登录 “Confluence Admin”，选择左边菜单 “CONFIGURATION”-“PDF Export Language Support”，选择安装中文字体，例如：simsun.ttc&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;7、破解&lt;/h1&gt;

&lt;p&gt;请参考：&lt;a href=&quot;http://582033.vicp.net/?p=1085&quot;&gt;http://582033.vicp.net/?p=1085&lt;/a&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/02/21/install-confluence5-4-2.html</link>
      <guid>http://blog.javachen.com/2014/02/21/install-confluence5-4-2.html</guid>
      <pubDate>2014-02-21T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>CDH 5 Beta 2 的新变化</title>
      <description>&lt;p&gt;本文是同事对&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Release-Notes/cdh5rn_whats_new_in_b2.html&quot;&gt;CDH 5.0.0 Beta 2&lt;/a&gt;的翻译，仅供大家参考。&lt;/p&gt;

&lt;p&gt;这是 CDH 5.0.0 Beta 2的初稿。鉴于 CDH 5 目前的发布版本是测试版，它不应用于生产环境中；它只是用来评估、测试的。对于生产环境，请使用 CDH 4,最近的文档在 &lt;a href=&quot;http://www.cloudera.com/content/support/en/documentation/cdh4-documentation/cdh4-documentation-v4-latest.html&quot;&gt;CDH Documentation&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;apache-crunch&quot;&gt;Apache Crunch&lt;/h1&gt;

&lt;p&gt;Apache Crunch 项目开发了新的 Java API，简化了在 Apache Hadoop 之上的数据管道的创建过程。&lt;/p&gt;

&lt;p&gt;Crunch APIs 是以 FlumeJava 为蓝本开发的，FlumeJava 是 Google 用来在他们自己的 MapReduce 实现之上构建数据管道的工具库。&lt;/p&gt;

&lt;p&gt;更多信息和安装指南，见 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_Crunch_install.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7ed4&quot;&gt;Crunch Installation&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;apache-datefu&quot;&gt;Apache DateFu&lt;/h1&gt;

&lt;p&gt;从 0.4 更新到 1.1.0(此更新不向后兼容)。&lt;/p&gt;

&lt;p&gt;新特性包含了 UDFS SHA, SimpleRandomSample, COALESCE, ReservoirSample, EmptyBagToNullFields 以及很多其他部分。&lt;/p&gt;

&lt;h1 id=&quot;apache-flume&quot;&gt;Apache Flume&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2294&quot;&gt;FLUME-2294&lt;/a&gt; - 添加了一个 支持 Kite dataset 的 sink&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2056&quot;&gt;FLUME-2056&lt;/a&gt; - Spooling Directory Source 现在只能向 event headers 中传入文件名(而不是文件的完整路径名)。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2155&quot;&gt;FLUME-2155&lt;/a&gt; - 在文件回放时对文件通道建立索引来提高性能，使它能更快地启动&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2217&quot;&gt;FLUME-2217&lt;/a&gt; - Syslog Source 可选择性地在消息体中保存所有的系统日志头部属性信息(syslog headers)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2052&quot;&gt;FLUME-2052&lt;/a&gt; - Spooling Directory Source 现在可以替换或忽略所有输入文件中不正常的(malformed)字符&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-hbase&quot;&gt;Apache HBase&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;支持在线修改表结构&lt;/li&gt;
  &lt;li&gt;支持在线合并 Region&lt;/li&gt;
  &lt;li&gt;命名空间:CDH 5 Beta 2 包含的命名空间特性可以使不同的管理员用户分别管理不同的表。所有被更新的表都会被放在 “hbase” 命名空间中。超级管理员可以创建新命名空间和表。对命名空间拥有权限的用户可以管理里面的表的权限&lt;/li&gt;
  &lt;li&gt;已经有了几个针对 HBase 在 Master 或 RegionServer 出故障时的的平均恢复时间 的改进:
    &lt;ul&gt;
      &lt;li&gt;分布式日志分割(split)已经成熟，并且总是被启用的。原来的比较慢的分割机制现在已经没有了&lt;/li&gt;
      &lt;li&gt;失败检测时间有了提升。若 RegionServer 或 Master 发生故障会很快触发补救措施，此时会发送新的通知。&lt;/li&gt;
      &lt;li&gt;元数据表有一个专用的 WAL(write ahead log) ,此时如果 RegionServer 的元数据已经保存，它能使 region 更快地恢复&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Region Balancer 有了显著更新，兼顾了更多的负载特性。&lt;/li&gt;
  &lt;li&gt;添加了 TableSnapshotInputFormat 和 TableSnapshotScanner 来扫描 HBase 表的客户端快照，而不是服务器上的,后者将引发一个 MapReduce 作业，前者只会对客户端快照文件做单一的扫描。它们也可用于拥有快照文件的离线 HBase，或直接用于导出的快照文件&lt;/li&gt;
  &lt;li&gt;KeyValue API 已经废弃，取而代之的是 Cell 接口。更新到 HBase 0.96 的用户仍可以使用 KeyValue，但以后的更新可能会去掉这个类或它的部分功能。建议用户更新更新他们的应用来使用新的 Cell 接口。&lt;/li&gt;
  &lt;li&gt;当前试验性的一些特性：
    &lt;ul&gt;
      &lt;li&gt;分布式日志回放：这个机制使 RegionServer 能更快地从失败中恢复，但它会导致一个特殊情况：它不能保证 ACID(原子性、一致性、隔离性、持久性) 要素Cloudera 现在不建议启用这个特性&lt;/li&gt;
      &lt;li&gt;大缓存(Bucket cache): 这是个 off-heap 缓存机制，它使用额外的 RAM 和 块设备(如闪存盘等)来极大地增强了 BlockCache 提供的读取缓存的能力。Cloudera 现在不建议启用这个特性&lt;/li&gt;
      &lt;li&gt;垂青节点(Favored nodes,客户端希望存储数据的节点):为了在发生故障后更好地恢复现场(preserve performance)，这个特性使 HBase 能更好地控制它的数据是从哪里写入HDFS的。目前它还不能很好地与 HBase Balancer 和 HDFS Balancer 交互，所以目前还不可用。Cloudera 现在不建议启用这个特性&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多细节见这个&lt;a href=&quot;https://blogs.apache.org/hbase/entry/hbase_0_96_0_released&quot;&gt;博客&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;apache-hdfs&quot;&gt;Apache HDFS&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;新特性及改进：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;从 CDH 5 Beta 2 开始，你可以更新 HDFS 并启用 HA 特性，如果你使用集群存储(Quorum-based storage)(CDH 5 不支持 NFS 共享存储，唯一的方法是集群存储)。从 CDH 4 到 CDH 5 的更新说明见 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_topic_6.html#topic_6&quot;&gt;Upgrading to CDH 5 from CDH 4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4949&quot;&gt;HDFS-4949&lt;/a&gt; - CDH 5 Beta 2 支持 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_hdfs_caching.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7e7d&quot;&gt;HDFS 的集中化缓存管理&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;从 CDH 5 Beta 2 开始，你可以配置一个 NFSv3 网关，它允许任何 NFSv3 兼容的客户端将 HDFS 挂载为一个客户端的本地文件系统。更多信息和说明，见 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_NFSv3_gateway.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7ef4&quot;&gt;配置 NFSv3 网关&lt;/a&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-5709&quot;&gt;HDFS-5709&lt;/a&gt; - 改进对名为 .snapshot 的目录和文件的更新&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bug&quot;&gt;主要的Bug修复:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-5449&quot;&gt;HDFS-5449&lt;/a&gt;- 修股 WebHDFS 的不兼容问题&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-5671&quot;&gt;HDFS-5671&lt;/a&gt;- 修复 &lt;code&gt;DFSInputStream#getBlockReader&lt;/code&gt; 中的 socket 泄漏&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-5353&quot;&gt;HDFS-5353&lt;/a&gt;- 启用 &lt;code&gt;dfs.encrypt.data.transfer&lt;/code&gt; 导致 Short circuit reads 失败的问题&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-5438&quot;&gt;HDFS-5438&lt;/a&gt;- 处理文件块报告过程中的导致数据丢失的缺陷&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;改变的行为:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;从 CDH 5 Beta 2 开始，为了让 NameNode 在一个安全的集群中启动，应该在 &lt;code&gt;hdfs-site.xml&lt;/code&gt; 中设置 &lt;code&gt;dfs.web.authentication.kerberos.principal&lt;/code&gt;	 属性。&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Security-Guide/cdh5sg_topic_3_7.html#topic_3_7_unique_2&quot;&gt;CDH 5 的安全指南&lt;/a&gt;已经包含了这点。Cloudera Manager 管理的集群则无需显式定义这个属性。&lt;/li&gt;
  &lt;li&gt;HDFS-5037 - 启动 NameNode 的操作应该触发它进行日志滚动。当碰到 NameNode 处在安全模式时，客户端会在配置的时间范围内重试。&lt;/li&gt;
  &lt;li&gt;mkdir 命令的默认行为有了变化。从 CDH 5 Beta 2 开始，如果父目录不存在就必须使用 &lt;code&gt;-p&lt;/code&gt; 选项，否则操作将会失败。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-hive&quot;&gt;Apache Hive&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;新特性:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;改进的 JDBC 规范包括：
    &lt;ul&gt;
      &lt;li&gt;改进 &lt;code&gt;getDatabaseMajorVersion()&lt;/code&gt;, &lt;code&gt;getDatabaseMinorVersion()&lt;/code&gt; APIs (&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3181&quot;&gt;HIVE-3181&lt;/a&gt;)&lt;/li&gt;
      &lt;li&gt;添加了对新的数据类型的支持:Char (&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-5209&quot;&gt;HIVE-5683&lt;/a&gt;), Decimal (&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-5355&quot;&gt;HIVE-5355&lt;/a&gt;) and Varchar (&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-5209&quot;&gt;HIVE-5209&lt;/a&gt;)&lt;/li&gt;
      &lt;li&gt;现在可以在 HiveServer2 连接地址(connection URL) 中为会话指定要连接的数据库&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hive Server 和 Clients 的加密通信。 包括 HiveServer2 对 SSL 加密的支持(非 Kerberos 连接).(&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-5351&quot;&gt;HIVE-5351&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;CDH 5 Beta 2 包含了一个可用的本地 Parquet SerDe。用户不需要依赖任何外部包即可直接创建一个 Parquet 格式的表。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;改变的行为:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-4256&quot;&gt;HIVE-4256&lt;/a&gt;- 启用 Sentry 的情况下，与 HiveServer2 建立连接的工作包含了 use &lt;database&gt; 这个命令 的执行。因此，没有权限访问相应数据库的用户不允许连接到 HiveServer2&lt;/database&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hue&quot;&gt;Hue&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Hue 版本更新到 3.5.0.&lt;/li&gt;
  &lt;li&gt;添加了一个新的 &lt;a href=&quot;http://gethue.tumblr.com/post/71963991256/a-new-spark-web-ui-spark-app&quot;&gt;Spark Editor&lt;/a&gt; 应用。&lt;/li&gt;
  &lt;li&gt;Impala 和 Hive Editor 现在都是单页应用(one-page apps). 编辑器、进度、Table 列表和结果都在同一个页面。&lt;/li&gt;
  &lt;li&gt;用图展示 Impala 和 Hive Editor 的结果数据&lt;/li&gt;
  &lt;li&gt;Oozie SLA 的 Editor 和 Dashboard，定时任务，认证信息&lt;/li&gt;
  &lt;li&gt;Sqoop2 应用支持数据库和表名/属性的自动补全&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gethue.tumblr.com/post/66661074125/dbquery-app-mysql-postgresql-oracle-and-sqlite-query&quot;&gt;DBQuery App&lt;/a&gt;: MySQL 和 PostgreSQL Query Editors.&lt;/li&gt;
  &lt;li&gt;新的搜索特性：&lt;a href=&quot;http://gethue.tumblr.com/post/66351828212/new-search-feature-graphical-facets&quot;&gt;支持图形化的套件&lt;/a&gt;(Graphical facets)&lt;/li&gt;
  &lt;li&gt;集成外部 Web 应用，应用可以是基于任何语言的。更多信息见 &lt;a href=&quot;http://gethue.tumblr.com/post/66367939672/integrate-external-web-applications-in-any-language&quot;&gt;blog post&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;创建 Hive 表，加载 quoted CSV data。&lt;a href=&quot;http://gethue.tumblr.com/post/68282571607/hadoop-tutorial-create-hive-tables-with-headers-and&quot;&gt;参考手册&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;从 HDFS 直接提交任意 Oozie 作业。&lt;a href=&quot;http://gethue.tumblr.com/post/68781982681/hadoop-tutorial-submit-any-oozie-jobs-directly-from&quot;&gt;参考手册&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;新的 &lt;a href=&quot;http://gethue.tumblr.com/post/62273866476/sso-with-hue-new-saml-backend&quot;&gt;SAML backend&lt;/a&gt; 支持使用 Hue 单点登录&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-mapreduce-mrv1-and-yarn&quot;&gt;Apache MapReduce (MRv1 and YARN)&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;公平调度器支持自动将应用存入队列的高级配置&lt;/li&gt;
  &lt;li&gt;MapReduce 支持在 uber 模式和本地 job runner 中运行多个 reducer&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-oozie&quot;&gt;Apache Oozie&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Oozie now supports cron-style scheduling capability.&lt;/li&gt;
  &lt;li&gt;Oozie 现在支持安全的 HA(High Availability with security)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-pig&quot;&gt;Apache Pig&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;重写了 AvroStorage 以提升性能，并且从 piggybank 移到了 core Pig.&lt;/li&gt;
  &lt;li&gt;添加了 ASSERT, IN, 和 CASE 操作&lt;/li&gt;
  &lt;li&gt;添加了 ParquetStorage 来与 Parquet 集成&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;apache-spark-&quot;&gt;Apache Spark (孵化中)&lt;/h1&gt;

&lt;p&gt;Spark 是一个快速，通用用于大规模数据处理的引擎。安装和配置指南见 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_Spark_install.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7eff&quot;&gt;Spark Installation&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;apache-sqoop2&quot;&gt;Apache Sqoop2&lt;/h1&gt;

&lt;p&gt;版本从 1.99.2 更新到 1.99.3.&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2014/02/21/cdh5rn_whats_new_in_b2.html</link>
      <guid>http://blog.javachen.com/2014/02/21/cdh5rn_whats_new_in_b2.html</guid>
      <pubDate>2014-02-21T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Backbone中的模型</title>
      <description>&lt;h1 id=&quot;model&quot;&gt;创建model&lt;/h1&gt;

&lt;p&gt;模型是所有Javascript应用程序的核心，包括交互数据及相关的大量逻辑： 转换、验证、计算属性和访问控制。你可以用特定的方法扩展&lt;code&gt;Backbone.Model&lt;/code&gt;，模型也提供了一组基本的管理变化的功能。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;Person = Backbone.Model.extend({
    initialize: function(){
        alert(&quot;Welcome to this world&quot;);
    }
});

var person = new Person;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;new一个model的实例后就会触发initialize()函数。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;设置属性&lt;/h1&gt;

&lt;p&gt;现在我们想设置一些属性，有两种方式，可以在创建model实例时进行传参，也可以在实例生成后通过&lt;code&gt;model.set(obj)&lt;/code&gt;来进行设置。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;Person = Backbone.Model.extend({
    initialize: function(){
        alert(&quot;Welcome to this world&quot;);
    }
});

var person = new Person({ name: &quot;Thomas&quot;, age: 67});
// or we can set afterwards, these operations are equivelent
var person = new Person();
person.set({ name: &quot;Thomas&quot;, age: 67});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;# 获取属性&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;Person = Backbone.Model.extend({
    initialize: function(){
        alert(&quot;Welcome to this world&quot;);
    }
});

var person = new Person({ name: &quot;Thomas&quot;, age: 67, child: &#39;Ryan&#39;});

var age = person.get(&quot;age&quot;); // 67
var name = person.get(&quot;name&quot;); // &quot;Thomas&quot;
var child = person.get(&quot;child&quot;); // &#39;Ryan&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;model-1&quot;&gt;设置model默认属性&lt;/h1&gt;
&lt;p&gt;有的时候你可能会想让model有默认属性值，只要在进行model声明的时候设置个&lt;code&gt;defaults&lt;/code&gt;就行了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;Person = Backbone.Model.extend({
    defaults: {
        name: &#39;Fetus&#39;,
        age: 0,
        child: &#39;&#39;
    },
    initialize: function(){
        alert(&quot;Welcome to this world&quot;);
    }
});

var person = new Person({ name: &quot;Thomas&quot;, age: 67, child: &#39;Ryan&#39;});

var age = person.get(&quot;age&quot;); // 67
var name = person.get(&quot;name&quot;); // &quot;Thomas&quot;
var child = person.get(&quot;child&quot;); // &#39;Ryan&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;model-2&quot;&gt;监听model的属性改变&lt;/h1&gt;
&lt;p&gt;我们可以通过&lt;code&gt;model.bind(event,callback)&lt;/code&gt;方法来绑定change事件来监听属性改变。下面的这个例子就是在initialize方法中绑定了一个name属性改变的事件监听。&lt;br /&gt;
如果person的name属性改变了，就会弹出个对话框显示新值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;Person = Backbone.Model.extend({
    defaults: {
        name: &#39;Fetus&#39;,
        age: 0
    },
    initialize: function(){
        alert(&quot;Welcome to this world&quot;);
        this.on(&quot;change:name&quot;, function(model){
            var name = model.get(&quot;name&quot;); // &#39;Stewie Griffin&#39;
            alert(&quot;Changed my name to &quot; + name );
        });
    }
});

var person = new Person({ name: &quot;Thomas&quot;, age: 67});
person.set({name: &#39;Stewie Griffin&#39;}); // This triggers a change and will alert()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;和服务端交互&lt;/h1&gt;

&lt;p&gt;服务端实现一个RESTful的url例如/user，可以允许我们通过他与后台交互。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var UserModel = Backbone.Model.extend({
    urlRoot: &#39;/user&#39;,
    defaults: {
        name: &#39;&#39;,
        email: &#39;&#39;
    }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;model.urlRoot&lt;/code&gt;:如果使用的集合外部的模型，通过指定 urlRoot 来设置生成基于模型 id 的 URLs 的默认 url 函数。 “/[urlRoot]/id”&lt;/p&gt;

&lt;h1 id=&quot;model-3&quot;&gt;创建一个新model&lt;/h1&gt;

&lt;p&gt;如果id为null，则会提交一个POST请求到/user。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var UserModel = Backbone.Model.extend({
    urlRoot: &#39;/user&#39;,
    defaults: {
        name: &#39;&#39;,
        email: &#39;&#39;
    }
});
var user = new Usermodel();
// Notice that we haven&#39;t set an `id`
var userDetails = {
    name: &#39;Thomas&#39;,
    email: &#39;thomasalwyndavis@gmail.com&#39;
};
// Because we have not set a `id` the server will call
// POST /user with a payload of {name:&#39;Thomas&#39;, email: &#39;thomasalwyndavis@gmail.com&#39;}
// The server should save the data and return a response containing the new `id`
user.save(userDetails, {
    success: function (user) {
        alert(user.toJSON());
    }
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;model.save([attributes], [options]) &lt;/code&gt;: 通过委托&lt;code&gt;Backbone.sync&lt;/code&gt;保存模型到数据库（或可替代的持久层）。 attributes 散列表 (在 set) 应当包含想要改变的属性，不涉及的键不会被修改。 如果模型含有&lt;code&gt;validate&lt;/code&gt;方法，并且验证失败，模型不会保存。 如果模型&lt;code&gt;isNew&lt;/code&gt;, 保存将采用 “create” (HTTP POST) 方法, 如果模型已经在服务器存在，保存将采用 “update” (HTTP PUT) 方法.&lt;/p&gt;

&lt;h1 id=&quot;model-4&quot;&gt;获取一个model&lt;/h1&gt;

&lt;p&gt;初始化一个model实例并设置其id属性，并调用fetch方法，这样会请求&lt;code&gt;urlRoot + &#39;/id&#39;&lt;/code&gt;地址到后台。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// Here we have set the `id` of the model
var user = new Usermodel({id: 1});

// The fetch below will perform GET /user/1
// The server should return the id, name and email from the database
user.fetch({
    success: function (user) {
        alert(user.toJSON());
    }
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;model.fetch([options])&lt;/code&gt;: 从服务器重置模型状态。这对模型尚未填充数据，或者服务器端已有最新状态的情况很有用处。 如果服务器端状态与当前属性不同，则触发&lt;code&gt;change&lt;/code&gt;事件。 选项的散列表参数接受&lt;code&gt;success&lt;/code&gt;和&lt;code&gt;error&lt;/code&gt;回调函数， 回调函数中可以传入&lt;code&gt;(model,response)&lt;/code&gt;作为参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// 每隔 10 秒从服务器拉取数据以保持模型是最新的
setInterval(function() {
  user.fetch();
}, 10000);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;# 更新一个model&lt;br /&gt;
当保存的model对象的id不为空时，则会提交一个PUT请求到urlRoot。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// Here we have set the `id` of the model
var user = new Usermodel({
    id: 1,
    name: &#39;Thomas&#39;,
    email: &#39;thomasalwyndavis@gmail.com&#39;
});

// Let&#39;s change the name and update the server
// Because there is `id` present, Backbone.js will fire
// PUT /user/1 with a payload of `{name: &#39;Davis&#39;, email: &#39;thomasalwyndavis@gmail.com&#39;}`
user.save({name: &#39;Davis&#39;}, {
    success: function (model) {
        alert(user.toJSON());
    }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;model-5&quot;&gt;删除一个model&lt;/h1&gt;
&lt;p&gt;调用model的destroy方法时，则会提交请求到urlRoot+”/id”&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// Here we have set the `id` of the model
var user = new Usermodel({
    id: 1,
    name: &#39;Thomas&#39;,
    email: &#39;thomasalwyndavis@gmail.com&#39;
});

// Because there is `id` present, Backbone.js will fire
// DELETE /user/1 
user.destroy({
    success: function () {
        alert(&#39;Destroyed&#39;);
    }
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;model.destroy([options])&lt;/code&gt;:通过委托&lt;code&gt;HTTP DELETE&lt;/code&gt;请求到&lt;code&gt;Backbone.sync&lt;/code&gt;销毁服务器上的模型. 接受&lt;code&gt;success&lt;/code&gt;和&lt;code&gt;error&lt;/code&gt;回调函数作为选项散列表参数。将在模型上触发&lt;code&gt;destroy&lt;/code&gt;事件，该事件可以通过任意包含它的集合向上冒泡。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;其他方法&lt;/h1&gt;

&lt;p&gt;model还有一些其他的方法，可以参考api：&lt;a href=&quot;http://www.csser.com/tools/backbone/backbone.js.html#manual/Model&quot;&gt;Backbone.js API中文文档&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;var person = new Person({ name: &quot;Thomas&quot;, age: 67});
var attributes = person.toJSON(); // { name: &quot;Thomas&quot;, age: 67}
/* This simply returns a copy of the current attributes. */

var attributes = person.attributes;
/* The line above gives a direct reference to the attributes and you should be careful when playing with it.   Best practise would suggest that you use .set() to edit attributes of a model to take advantage of backbone listeners. */
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://backbonetutorials.com/what-is-a-model/&quot;&gt;what-is-a-model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csser.com/tools/backbone/backbone.js.html&quot;&gt;Backbone.js API中文文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/02/16/backbone-model.html</link>
      <guid>http://blog.javachen.com/2014/02/16/backbone-model.html</guid>
      <pubDate>2014-02-16T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>在CentOs6系统上安装Ganglia</title>
      <description>&lt;p&gt;Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，由RRDTool工具处理数据，并生成相应的的图形显示，以Web方式直观的提供给客户端。如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。&lt;/p&gt;

&lt;h1 id=&quot;yum&quot;&gt;配置yum源&lt;/h1&gt;

&lt;p&gt;首先配置好CentOs系统的yum源，然后需要包含有ganglia的yum源。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;/etc/yum.repos.d&lt;/code&gt;下创建&lt;code&gt;ganglia.repo&lt;/code&gt;，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ganglia]
name= ganglia
baseurl = http://vuksan.com/centos/RPMS/
enabled = 1
gpgcheck = 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;为了方便离线使用，你可以下载该yum源内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /opt
$ reposync -r ganglia
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- more --&gt;

&lt;p&gt;然后在&lt;code&gt;/opt/ganglia&lt;/code&gt;下执行如下的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ createrepo .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样你就可以将&lt;code&gt;ganglia.repo&lt;/code&gt;修改为本地yum的方式。&lt;/p&gt;

&lt;h1 id=&quot;gmetad&quot;&gt;管理机上安装gmetad&lt;/h1&gt;

&lt;p&gt;执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum -y install ganglia-gmetad
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装时遇到如下的错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: Package: rrdtool-1.4.5-1.x86_64 (ganglia)
          Requires: dejavu-lgc-fonts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;rrdtool依赖&lt;code&gt;dejavu-lgc-fonts&lt;/code&gt;，但是系统源并不包含这个，你可以从网上下载，然后安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rpm -Uvh http://mirror.steadfast.net/centos/5/os/x86_64//CentOS/dejavu-lgc-fonts-2.10-1.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;# 管理机上安装ganglia-web&lt;/p&gt;

&lt;p&gt;先安装apache和php等依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum install php* httpd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后下载ganglia-web:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget http://sourceforge.net/projects/ganglia/files/ganglia-web/3.5.12/ganglia-web-3.5.12.tar.gz/download

$ tar zxvf ganglia-web-3.5.12.tar.gz
$ cd ganglia-web-3.5.12
$ make install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将ganglia-web拷贝或者添加软链接到apache的目录下去，以下是拷贝：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir /var/www/html/ganglia
$ cp -a  /usr/share/ganglia/   /var/www/html/ganglia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在httpd的conf.d目录下添加ganglia.conf，命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/httpd/conf.d/ganglia.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Location /ganglia&amp;gt;
    Order deny,allow
    Deny from all
    ALLOW from all
#    Allow from 127.0.0.1
#    Allow from ::1
#    Allow from .example.com
&amp;lt;/Location&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;gmond&quot;&gt;客户端机器上安装gmond&lt;/h1&gt;

&lt;p&gt;执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum install gmond
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;启动服务&lt;/h1&gt;

&lt;p&gt;在管理机上启动gmetad&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /etc/init.d/gmetad start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在客户端机器上启动gmond&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /etc/init.d/gmond start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在管理机上启动httpd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /etc/init.d/httpd start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过web界面（&lt;code&gt;http://manager-ip/ganglia&lt;/code&gt;）访问ganglia-web&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://www.elain.org/?p=359&quot;&gt;ganglia监控的搭建部署(从源码安装)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/01/25/how-to-install-ganglia-on-centos6.html</link>
      <guid>http://blog.javachen.com/2014/01/25/how-to-install-ganglia-on-centos6.html</guid>
      <pubDate>2014-01-25T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>在RHEL系统上安装Nagios</title>
      <description>&lt;h1 id=&quot;rpm&quot;&gt;在管理机上安装rpm包&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
$ rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm
$ yum -y install nagios nagios-plugins-all nagios-plugins-nrpe nrpe php httpd
$ chkconfig httpd on &amp;amp;&amp;amp; chkconfig nagios on
$ service httpd start &amp;amp;&amp;amp; service nagios start
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;设置管理界面密码&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ htpasswd -c /etc/nagios/passwd nagiosadmin
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- more --&gt;

&lt;p&gt;密码和用户名保持一致（都设置为nagiosadmin），否则你需要修改&lt;code&gt;/etc/nagios/cgi.cfg&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;nagios&quot;&gt;访问Nagios&lt;/h1&gt;

&lt;p&gt;打开&lt;code&gt;http://ip/nagios&lt;/code&gt;，输入用户名和密码即可访问&lt;/p&gt;

&lt;h1 id=&quot;nrpe&quot;&gt;在客户端上安装NRPE&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
$ rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm
$ yum -y install nagios nagios-plugins-all nrpe
$ chkconfig nrpe on
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置&lt;code&gt;/etc/nagios/nrpe.cfg&lt;/code&gt;，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log_facility=daemon
pid_file=/var/run/nrpe/nrpe.pid
server_port=5666
nrpe_user=nrpe
nrpe_group=nrpe
allowed_hosts=198.211.117.251
dont_blame_nrpe=1
debug=0
command_timeout=60
connection_timeout=300
include_dir=/etc/nrpe.d/
command[check_users]=/usr/lib64/nagios/plugins/check_users -w 5 -c 10
command[check_load]=/usr/lib64/nagios/plugins/check_load -w 15,10,5 -c 30,25,20
command[check_disk]=/usr/lib64/nagios/plugins/check_disk -w 20% -c 10% -p /dev/vda
command[check_zombie_procs]=/usr/lib64/nagios/plugins/check_procs -w 5 -c 10 -s Z
command[check_total_procs]=/usr/lib64/nagios/plugins/check_procs -w 150 -c 200
command[check_procs]=/usr/lib64/nagios/plugins/check_procs -w $ARG1$ -c $ARG2$ -s $ARG3$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请注意修改&lt;code&gt;allowed_hosts&lt;/code&gt;值为你的nagios监控机ip&lt;/p&gt;

&lt;p&gt;设置iptables：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ iptables -N NRPE
$ iptables -I INPUT -s 0/0 -p tcp --dport 5666 -j NRPE
$ iptables -I NRPE -s 198.211.117.251 -j ACCEPT
$ iptables -A NRPE -s 0/0 -j DROP
$ /etc/init.d/iptables save
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者，关闭iptables：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /etc/init.d/iptables stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动NRPE：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ service nrpe start
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;在管理机上添加配置文件&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ echo &quot;cfg_dir=/etc/nagios/servers&quot; &amp;gt;&amp;gt; /etc/nagios/nagios.cfg
$ cd /etc/nagios/servers
$ touch hadoop.tk.cfg
$ touch hbase.tk.cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后修改每一个配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/nagios/servers/hadoop.tk.cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加内容如下，你也可以稍作修改：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;define host {
        use                     linux-server
        host_name               cloudmail.tk
        alias                   cloudmail.tk
        address                 192.168.56.122
        }

define service {
        use                             generic-service
        host_name                       cloudmail.tk
        service_description             PING
        check_command                   check_ping!100.0,20%!500.0,60%
        }

define service {
        use                             generic-service
        host_name                       cloudmail.tk
        service_description             SSH
        check_command                   check_ssh
        notifications_enabled           0
        }

define service {
        use                             generic-service
        host_name                       cloudmail.tk
        service_description             Current Load
        check_command                   check_local_load!5.0,4.0,3.0!10.0,6.0,4.0
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启nagios：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chown -R nagios. /etc/nagios
$ service nagios restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;其他资源&lt;/h1&gt;

&lt;p&gt;nagios客户端：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://code.google.com/p/nagioschecker/&quot;&gt;nagioschecker&lt;/a&gt; Firefox extension made as the statusbar indicator of the events from the network monitoring system Nagios.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sourceforge.net/projects/nagstamon/files/latest/download&quot;&gt;nagstamon Nagios status monitor&lt;/a&gt; Nagstamon is a Nagios status monitor which resides in systray or desktop (GNOME, KDE, Windows) as floating statusbar to inform you in realtime about the status of your hosts and services.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://code.google.com/p/nagmondroid/&quot;&gt;Nagios Monitor for Android&lt;/a&gt; NagMonDroid retrieves the current problems from your Nagios install and displays them. It has a variable update frequency and can be set to vibrate on new update.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;资料：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.elain.org/?p=467&quot;&gt;CentOS下nagios报警飞信部署四步走&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/01/24/how-to-install-nagios-on-rhel6.html</link>
      <guid>http://blog.javachen.com/2014/01/24/how-to-install-nagios-on-rhel6.html</guid>
      <pubDate>2014-01-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>All Things OpenTSDB</title>
      <description>&lt;h1 id=&quot;opentsdb&quot;&gt;1. OpenTSDB介绍&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://opentsdb.net/index.html&quot;&gt;OpenTSDB&lt;/a&gt;用HBase存储所有的时序（无须采样）来构建一个&lt;strong&gt;分布式、可伸缩的时间序列数据库&lt;/strong&gt;。它支持秒级数据采集所有metrics，支持永久存储，可以做容量规划，并很容易的接入到现有的报警系统里。OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化、图形化等。&lt;/p&gt;

&lt;p&gt;对于运维工程师而言，OpenTSDB可以获取基础设施和服务的实时状态信息，展示集群的各种软硬件错误，性能变化以及性能瓶颈。对于管理者而言，OpenTSDB可以衡量系统的SLA，理解复杂系统间的相互作用，展示资源消耗情况。集群的整体作业情况，可以用以辅助预算和集群资源协调。对于开发者而言，OpenTSDB可以展示集群的主要性能瓶颈，经常出现的错误，从而可以着力重点解决重要问题。&lt;/p&gt;

&lt;p&gt;OpenTSDB使用LGPLv2.1+开源协议,目前版本为2.X。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;官网地址：&lt;a href=&quot;http://opentsdb.net/&quot;&gt;http://opentsdb.net/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;源代码：&lt;a href=&quot;https://github.com/OpenTSDB/opentsdb/&quot;&gt;https://github.com/OpenTSDB/opentsdb/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;opentsdb-1&quot;&gt;2. 安装OpenTSDB&lt;/h1&gt;
&lt;p&gt;## 2.1 依赖&lt;br /&gt;
OpenTSDB依赖jdk和&lt;a href=&quot;http://www.gnuplot.info/&quot;&gt;Gnuplot&lt;/a&gt;，Gnuplot需要提前安装，版本要求为最小4.2,最大4.4,执行以下命令安装即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install gnuplot autoconf
apt-get install gnuplot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OpenTSDB是用java编写的，但是项目构建不是用的java的方式而是使用的C、C++程序员构建项目的方式。运行时依赖：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;JDK 1.6&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/OpenTSDB/asynchbase&quot;&gt;asynchbase&lt;/a&gt; 1.3.0 (BSD)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://code.google.com/p/guava-libraries/&quot;&gt;Guava&lt;/a&gt; 12.0 (ASLv2)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://logback.qos.ch/&quot;&gt;logback&lt;/a&gt; 1.0 (LGPLv2.1 / EPL)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jboss.org/netty&quot;&gt;Netty&lt;/a&gt; 3.4 (ASLv2)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://slf4j.org/&quot;&gt;SLF4J&lt;/a&gt; 1.6 (MIT) with Log4J and JCL adapters&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/OpenTSDB/async&quot;&gt;suasync&lt;/a&gt; 1.2 (BSD)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/zookeeper/&quot;&gt;ZooKeeper&lt;/a&gt; 3.3 (ASLv2)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可选的编译时依赖：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://gwt.google.com/&quot;&gt;GWT&lt;/a&gt; 2.4 (ASLv2)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可选的单元测试依赖：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.javassist.org/&quot;&gt;Javassist&lt;/a&gt; 3.15 (MPL / LGPL)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.junit.org/&quot;&gt;JUnit&lt;/a&gt; 4.10 (CPL)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mockito.org/&quot;&gt;Mockito&lt;/a&gt; 1.9 (MIT)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://code.google.com/p/powermock/&quot;&gt;PowerMock&lt;/a&gt; 1.4 (ASLv2)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;2.2 下载并编译源代码&lt;/h2&gt;

&lt;p&gt;首先安装必要依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install gnuplot automake autoconf git -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载源代码，可以指定最新版本或者手动checkout&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone git://github.com/OpenTSDB/opentsdb.git
cd opentsdb
./build.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2.3 安装&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;首先安装一个单节点或者多节点集群的hbase环境，hbase版本要求为0.94&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;设置环境变量并创建opentsdb使用的表，需要设置的环境变量为&lt;code&gt;COMPRESSION&lt;/code&gt;和&lt;code&gt;HBASE_HOME&lt;/code&gt;，前者设置是否启用压缩，或者设置hbase home目录。如果使用压缩，则还需要安装lzo&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;执行建表语句&lt;code&gt;src/create_table.sh&lt;/code&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;启动TSD&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;tsdtmp=${TMPDIR-&#39;/tmp&#39;}/tsd    # For best performance, make sure
mkdir -p &quot;$tsdtmp&quot;             # your temporary directory uses tmpfs
./build/tsdb tsd --port=4242 --staticroot=build/staticroot --cachedir=&quot;$tsdtmp&quot; --auto-metric
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你使用的是hbase集群，则你还需要设置&lt;code&gt;--zkquorum&lt;/code&gt;，&lt;code&gt;--cachedir&lt;/code&gt;对应的目录会产生一些临时文件，你可以设置cron定时任务进行删除。添加&lt;code&gt;--auto-metric&lt;/code&gt;，则当新的数据被搜集时自动创建指标。&lt;/p&gt;

&lt;p&gt;你可以将这些参数编写到配置文件中，然后通过&lt;code&gt;--config&lt;/code&gt;指定该文件所在路径。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;启动成功之后，你可以通过&lt;a href=&quot;http://127.0.0.1:4242&quot;&gt;127.0.0.1:4242&lt;/a&gt;进行访问。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从源代码安装gnuplot、autoconf、opentsdb以及tcollector，可以参考：&lt;a href=&quot;http://www.adintellig.com/blog/14&quot;&gt;OpenTSDB &amp;amp; tcollector 安装部署（Installation and Deployment）&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 使用向导&lt;/h1&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3.1 配置&lt;/h2&gt;

&lt;p&gt;OpenTSDB的配置参数可以在命令行指定，也可以在配置文件中指定。配置文件使用的是java的properties文件，文件中key为小写，支持逗号连接字符串但是不能有空格。所有的OpenTSDB属性都以tsdb开头，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# List of Zookeeper hosts that manage the HBase cluster
tsd.storage.hbase.zk_quorum = 192.168.1.100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置参数优先级：&lt;/p&gt;

&lt;p&gt;命令行参数 &amp;gt; 配置文件 &amp;gt; 默认值&lt;/p&gt;

&lt;p&gt;你可以在命令行中通过&lt;code&gt;--config&lt;/code&gt;指定配置文件所在路径，如果没有指定，OpenTSDB会从以下路径寻找配置文件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;./opentsdb.conf&lt;/li&gt;
  &lt;li&gt;/etc/opentsdb.conf&lt;/li&gt;
  &lt;li&gt;/etc/opentsdb/opentsdb.conf&lt;/li&gt;
  &lt;li&gt;/opt/opentsdb/opentsdb.conf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果一个合法的配置文件没有找到并且一些必须参数没有设置，TSD进程将不会启动。&lt;/p&gt;

&lt;p&gt;配置文件中可配置的属性请参考：&lt;a href=&quot;http://opentsdb.net/docs/build/html/user_guide/configuration.html#properties&quot;&gt;Properties&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;3.2 基本概念&lt;/h2&gt;

&lt;p&gt;在深入理解OpenTSDB之前，需要了解一些基本概念。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cardinality&lt;/strong&gt;。基数，在数学中定义为一个集合中的一些元素，在数据库中定义为一个索引的一些唯一元素，在OpenTSDB定义为：&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;一个给定指标的一些唯一时间序列&lt;/li&gt;
  &lt;li&gt;和一个标签名称相关联的一些唯一标签值&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在OpenTSDB中拥有高基数的指标在查询过程中返回的值要多于低基数的指标，这样花费的时间也就越多。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Compaction&lt;/strong&gt;。在OpenTSDB中，会将多列合并到一列之中以减少磁盘占用空间，这和hbase中的Compaction不一样。这个过程会在TSD写数据或者查询过程中不定期的发生。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Point&lt;/strong&gt;。每一个指标可以被记录为某一个时间点的一个数值。Data Point包括以下部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一个指标：metric&lt;/li&gt;
  &lt;li&gt;一个数值&lt;/li&gt;
  &lt;li&gt;这个数值被记录的时间戳&lt;/li&gt;
  &lt;li&gt;多个标签&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Metric&lt;/strong&gt;。一个可测量的单位的标称。&lt;code&gt;metric&lt;/code&gt;不包括一个数值或一个时间，其仅仅是一个标签，包含数值和时间的叫&lt;code&gt;datapoints&lt;/code&gt;，metric是用逗号连接的不允许有空格，例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hours.worked&lt;/li&gt;
  &lt;li&gt;webserver.downloads&lt;/li&gt;
  &lt;li&gt;accumulation.snow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Tags&lt;/strong&gt;。一个metric应该描述什么东西被测量，在OpenTSDB中，其不应该定义的太简单。通常，更好的做法是用Tags来描述具有相同维度的metric。Tags由tagk和tagv组成，前者表示一个分组，后者表示一个特定的项。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time Series&lt;/strong&gt;。一个metric的带有多个tag的data point集合。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Timestamp&lt;/strong&gt;。一个绝对时间，用来描述一个数值或者一个给定的metric是在什么时候定义的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt;。一个Value表示一个metric的实际数值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UID&lt;/strong&gt;。在OpenTSDB中，每一个metric、tagk或者tagv在创建的时候被分配一个唯一标识叫做UID，他们组合在一起可以创建一个序列的UID或者&lt;code&gt;TSUID&lt;/code&gt;。在OpenTSDB的存储中，对于每一个metric、tagk或者tagv都存在从0开始的计数器，每来一个新的metric、tagk或者tagv，对应的计数器就会加1。当data point写到TSD时，UID是自动分配的。你也可以手动分配UID，前提是&lt;code&gt;auto metric&lt;/code&gt;被设置为true。默认地，UID被编码为3Bytes，每一种UID类型最多可以有16,777,215个UID。你也可以修改源代码改为4Bytes。UID的展示有几种方式，最常见的方式是通过http api访问时，3 bytes的UID被编码为16进制的字符串。例如，UID为1的写为二进制的形式为&lt;code&gt;000000000000000000000001&lt;/code&gt;，最为一个无符号的byte数组，其可以表示为&lt;code&gt;[0,0,1]&lt;/code&gt;，编码为16进制字符串为&lt;code&gt;000001&lt;/code&gt;,其中每一位左边都被补上0,如果其不足两位。故，UID为255的会显示为&lt;code&gt;[0,0,255]&lt;/code&gt;和&lt;code&gt;0000FF&lt;/code&gt;。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;关于为什么使用UID而不使用hashes，可以参考：&lt;a href=&quot;http://opentsdb.net/docs/build/html/user_guide/uids.html#why-uids&quot;&gt;why-uids&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;TSUID&lt;/strong&gt;。当一个data point被写到OpenTSDB时，其row key格式为：&lt;code&gt;&amp;lt;metric_UID&amp;gt;&amp;lt;timestamp&amp;gt;&amp;lt;tagk1_UID&amp;gt;&amp;lt;tagv1_UID&amp;gt;[...&amp;lt;tagkN_UID&amp;gt;&amp;lt;tagvN_UID&amp;gt;]&lt;/code&gt;，不考虑时间戳的话，将其余部分都转换为UID，然后拼在一起，就可以组成为TSUID。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Metadata&lt;/strong&gt;。主要用于记录data point的一些附加的信息，方便搜索和跟踪，分为UIDMeta和TSMeta。&lt;/p&gt;

&lt;p&gt;每一个UID都有一个metadata记录保存在&lt;code&gt;tsdb-uid&lt;/code&gt;表中，每一个UID包括一些不可变的字段，如&lt;code&gt;uid&lt;/code&gt;、&lt;code&gt;type&lt;/code&gt;、&lt;code&gt;name&lt;/code&gt;和&lt;code&gt;created&lt;/code&gt;字段表示什么时候被创建，还可以有一些额外字段，如&lt;code&gt;description&lt;/code&gt;、&lt;code&gt;notes&lt;/code&gt;、&lt;code&gt;displayName&lt;/code&gt;和一些&lt;code&gt;custom&lt;/code&gt; key/value对，详细信息，可以查看&lt;a href=&quot;http://opentsdb.net/docs/build/html/api_http/uid/uidmeta.html&quot;&gt; /api/uid/uidmeta&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;同样，每一个TSUID可以对应一个TSMeta，记录在&lt;code&gt;tsdb-uid&lt;/code&gt;中，其包括的字段有&lt;code&gt;tsuid&lt;/code&gt;、&lt;code&gt;metric&lt;/code&gt;、&lt;code&gt;tags&lt;/code&gt;、&lt;code&gt;lastReceived&lt;/code&gt;和&lt;code&gt;created&lt;/code&gt;，可选的字段有&lt;code&gt;description&lt;/code&gt;, &lt;code&gt;notes&lt;/code&gt;，详细信息，可以查看&lt;a href=&quot;http://opentsdb.net/docs/build/html/api_http/uid/tsmeta.html&quot;&gt;/api/uid/tsmeta&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;开启Metadata有以下几个参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;tsd.core.meta.enable_realtime_uid&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tsd.core.meta.enable_tsuid_tracking&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tsd.core.meta.enable_tsuid_incrementing&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tsd.core.meta.enable_realtime_ts&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;metadata的另外一个形式是&lt;code&gt;Annotations&lt;/code&gt;，详细说明，请参考&lt;a href=&quot;http://opentsdb.net/docs/build/html/user_guide/metadata.html#annotations&quot;&gt;annotations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tree&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;3.3 数据存储方式&lt;/h2&gt;

&lt;p&gt;OpenTSDB使用HBase作为后端存储，在安装OpenTSDB之前，需要先启动一个hbase节点或者集群，然后再执行建表语句&lt;code&gt;src/create_table.sh&lt;/code&gt;创建hbase表。建表语句如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create &#39;$UID_TABLE&#39;,
  {NAME =&amp;gt; &#39;id&#39;, COMPRESSION =&amp;gt; &#39;$COMPRESSION&#39;, BLOOMFILTER =&amp;gt; &#39;$BLOOMFILTER&#39;},
  {NAME =&amp;gt; &#39;name&#39;, COMPRESSION =&amp;gt; &#39;$COMPRESSION&#39;, BLOOMFILTER =&amp;gt; &#39;$BLOOMFILTER&#39;}

create &#39;$TSDB_TABLE&#39;,
  {NAME =&amp;gt; &#39;t&#39;, VERSIONS =&amp;gt; 1, COMPRESSION =&amp;gt; &#39;$COMPRESSION&#39;, BLOOMFILTER =&amp;gt; &#39;$BLOOMFILTER&#39;}
  
create &#39;$TREE_TABLE&#39;,
  {NAME =&amp;gt; &#39;t&#39;, VERSIONS =&amp;gt; 1, COMPRESSION =&amp;gt; &#39;$COMPRESSION&#39;, BLOOMFILTER =&amp;gt; &#39;$BLOOMFILTER&#39;}
  
create &#39;$META_TABLE&#39;,
  {NAME =&amp;gt; &#39;name&#39;, COMPRESSION =&amp;gt; &#39;$COMPRESSION&#39;, BLOOMFILTER =&amp;gt; &#39;$BLOOMFILTER&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面可以看出一共创建了4张表，并且可以设置是否压缩、是否启用布隆过滤、保存版本号等等，如果追求hbase读写性能，还可以预建分区。&lt;/p&gt;

&lt;h3 id=&quot;data-table-schema&quot;&gt;3.3.1 Data Table Schema&lt;/h3&gt;

&lt;p&gt;在OpenTSDB中，所有数据存储在一张叫做&lt;code&gt;tsdb&lt;/code&gt;的表中，这是为了充分利用hbase有序和region分布式的特点。所有的值都保存在列族&lt;code&gt;t&lt;/code&gt;中。&lt;/p&gt;

&lt;p&gt;rowkey为&lt;code&gt;&amp;lt;metric_uid&amp;gt;&amp;lt;timestamp&amp;gt;&amp;lt;tagk1&amp;gt;&amp;lt;tagv1&amp;gt;[...&amp;lt;tagkN&amp;gt;&amp;lt;tagvN&amp;gt;]&lt;/code&gt;，UID默认编码为3 Bytes，而时间戳会编码为4 Bytes&lt;/p&gt;

&lt;p&gt;OpenTSDB的tsdb启动之后，会监控指定的socket端口（默认为4242），接收到监控数据，包括指标、时间戳、数据、tag标签，tag标签包括tag名称ID和tag值ID。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;myservice.latency.avg 1292148123 42 reqtype=foo host=web42
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于指标myservice.latency.avg的ID为：[0, 0, -69],reqtype标签名称的ID为：[0, 0, 1], foo标签值的ID为：[0, 1, 11], 标签名称的ID为：[0, 0, 2] web42标签值的ID为：[0, -7, 42]，他们组成rowkey：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[0, 0, -69, 77, 4, -99, 32, 0, 0, 1, 0, 1, 11, 0, 0, 2, 0, -7, 42]
 `-------&#39;  `------------&#39;  `-----&#39;  `------&#39;  `-----&#39;  `-------&#39;
 metric ID  base timestamp  name ID  value ID  name ID  value ID
                            `---------------&#39;  `---------------&#39;
                                first tag         second tag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;row表示格式为： 每个数字对应1 byte&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[0, 0, -69] metric ID&lt;/li&gt;
  &lt;li&gt;[77, 4, -99, 32] base timestamp = 1292148000. timestamps in the row key are rounded down to a 60 minute boundary。也就是说对于同一个小时的metric + tags相同的数据都会存放在一个row下面&lt;/li&gt;
  &lt;li&gt;[0, 0, 1] “reqtype” index&lt;/li&gt;
  &lt;li&gt;[0, 1, 11] “foo” index&lt;/li&gt;
  &lt;li&gt;[0, 0, 2] “host” index&lt;/li&gt;
  &lt;li&gt;[0, -7, 42] “web42” index&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;：可以看到，对于metric + tags相同的数据都会连续存放，且metic相同的数据也会连续存放，这样对于scan以及做aggregation都非常有帮助&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;column qualifier&lt;/strong&gt; 占用2 bytes或者4 bytes，占用2 bytes时表示以秒为单位的偏移，格式为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;12 bits:相对row表示的小时的delta, 最多2^ 12 = 4096 &amp;gt; 3600因此没有问题&lt;/li&gt;
  &lt;li&gt;4 bits:format flags&lt;/li&gt;
  &lt;li&gt;1 bit: an integer or floating point&lt;/li&gt;
  &lt;li&gt;3 bits: 标明数据的长度，其长度必须是1、2、4、8。&lt;code&gt;000&lt;/code&gt;表示1个byte,&lt;code&gt;010&lt;/code&gt;表示2byte，&lt;code&gt;011&lt;/code&gt;表示4byte，&lt;code&gt;100&lt;/code&gt;表示8byte&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;占用4 bytes时表示以毫秒为单位的偏移，格式为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;4 bits：十六进制的&lt;code&gt;1&lt;/code&gt;或者&lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;22 bits:毫秒偏移&lt;/li&gt;
  &lt;li&gt;2 bit:保留&lt;/li&gt;
  &lt;li&gt;4 bits: format flags&lt;/li&gt;
  &lt;li&gt;1 bit: an integer or floating point，0表示整数,1表示浮点数&lt;/li&gt;
  &lt;li&gt;3 bits: 标明数据的长度，其长度必须是1、2、4、8。&lt;code&gt;000&lt;/code&gt;表示1个byte,&lt;code&gt;010&lt;/code&gt;表示2byte，&lt;code&gt;011&lt;/code&gt;表示4byte，&lt;code&gt;100&lt;/code&gt;表示8byte&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;举例：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于时间戳为1292148123的数据点来说，其转换为以小时为单位的基准时间(去掉小时后的秒）为129214800,偏移为123,转换为二进制为&lt;code&gt;1111011&lt;/code&gt;，因为该值为整数且长度为8位（对应为2byte，故最后3bit为&lt;code&gt;100&lt;/code&gt;）,故其对应的列族名为：&lt;code&gt;0000011110110100&lt;/code&gt;，将其转换为十六进制为&lt;code&gt;07B4&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;value&lt;/strong&gt; 使用8bytes存储，既可以存储long,也可以存储double。&lt;/p&gt;

&lt;p&gt;总结一下，&lt;code&gt;tsdb&lt;/code&gt;表结构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/opentsdb-tsdb-schema.png&quot; alt=&quot;opentsdb-tsdb-schema&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;uid-table-schema&quot;&gt;3.3.2 UID Table Schema&lt;/h3&gt;

&lt;p&gt;一个单独的较小的表叫做&lt;code&gt;tsdb-uid&lt;/code&gt;用来存储UID映射，包括正向的和反向的。存在两列族，一列族叫做&lt;code&gt;name&lt;/code&gt;用来将一个UID映射到一个字符串，另一个列族叫做&lt;code&gt;id&lt;/code&gt;，用来将字符串映射到UID。列族的每一行都至少有以下三列中的一个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;metrics&lt;/code&gt; 将metric的名称映射到UID&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tagk&lt;/code&gt; 将tag名称映射到UID&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tagv&lt;/code&gt; 将tag的值映射到UID&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果配置了metadata，则&lt;code&gt;name&lt;/code&gt;列族还可以包括额外的metatata列。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;id 列族&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Row Key&lt;/strong&gt; - 将会是一个分配到UID的字符串，例如，对于一个指标可能有一个值为&lt;code&gt;sys.cpu.user&lt;/code&gt;或者对于一个标签其值可能为&lt;code&gt;42&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Column Qualifiers&lt;/strong&gt; - 上面三种列类型中一种。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Column Value&lt;/strong&gt; - 一个无符号的整数，默认被编码为3个byte，其值为UID。&lt;/p&gt;

&lt;p&gt;例如以下几行数据是从&lt;code&gt;tsdb-uid&lt;/code&gt;表中查询出来的数据，第一个列为row key，第二列为”列族:列名”，第三列为值，对应为UID&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;proc.stat.cpu id:metrics \x00\x00\x01
host id:tagk \x00\x00\x01
cdh1 id:tagv \x00\x00\x01
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;name 列族&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Row Key&lt;/strong&gt; - 为UID&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Column Qualifiers&lt;/strong&gt; - 上面三种列类型中一种或者为&lt;code&gt;metrics_meta&lt;/code&gt;、&lt;code&gt;tagk_meta&lt;/code&gt;、&lt;code&gt;tagv_meta&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Column Value&lt;/strong&gt; - 与UID对应的字符串，对于一个&lt;code&gt;*_meta&lt;/code&gt;列，其值将会是一个UTF-8编码的JSON格式字符串。不要在OpenTSDB外部去修改该值，其中的字段顺序会影响&lt;code&gt;CAS&lt;/code&gt;调用。&lt;/p&gt;

&lt;p&gt;例如,以下几行数据是从&lt;code&gt;tsdb-uid&lt;/code&gt;表中查询出来的数据，第一个列为row key，第二列为”列族:列名”，第三列为值，对应为UID&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\x00\x00\x01 name:metrics proc.stat.cpu
\x00\x00\x01 name:tagk host
\x00\x00\x01 name:tagv cdh1
\x00\x00\x01 name:tagk_meta {&quot;uid&quot;:&quot;000001&quot;,&quot;type&quot;:&quot;TAGK&quot;,&quot;name&quot;:&quot;host&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213193,&quot;custom&quot;:null,&quot;displayName&quot;:&quot;&quot;}
\x00\x00\x01 name:tagv_meta {&quot;uid&quot;:&quot;000001&quot;,&quot;type&quot;:&quot;TAGV&quot;,&quot;name&quot;:&quot;cdh1&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213193,&quot;custom&quot;:null,&quot;displayName&quot;:&quot;&quot;}
\x00\x00\x01 name:metric_meta {&quot;uid&quot;:&quot;000001&quot;,&quot;type&quot;:&quot;METRIC&quot;,&quot;name&quot;:&quot;metrics proc.stat.cpu&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213193,&quot;custom&quot;:null,&quot;displayName&quot;:&quot;&quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总结一下，&lt;code&gt;tsdb-uid&lt;/code&gt;表结构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/opentsdb-tsdb-uid-schema.png&quot; alt=&quot;opentsdb-tsdb-uid-schema&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图对应的一个datapoint如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;proc.stat.cpu 1292148123 80 host=cdh1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上图可以看出&lt;code&gt;tsdb-uid&lt;/code&gt;的表结构以及数据存储方式，对于一个data point来说，其被保存到opentsdb之前，会对&lt;code&gt;metrics&lt;/code&gt;、&lt;code&gt;tagk&lt;/code&gt;、&lt;code&gt;tagv&lt;/code&gt;、&lt;code&gt;metric_meta&lt;/code&gt;、&lt;code&gt;tagk_meta&lt;/code&gt;、&lt;code&gt;tagv_meta&lt;/code&gt;生成一个UID（如上图中的&lt;code&gt;000001&lt;/code&gt;）,然后将其插入hbase表中，rowkey为UID，同时会存储多行记录，分别保存&lt;code&gt;metrics&lt;/code&gt;、&lt;code&gt;tagk&lt;/code&gt;、&lt;code&gt;tagv&lt;/code&gt;、&lt;code&gt;metric_meta&lt;/code&gt;、&lt;code&gt;tagk_meta&lt;/code&gt;、&lt;code&gt;tagv_meta&lt;/code&gt;到UID的映射。&lt;/p&gt;

&lt;h3 id=&quot;meta-table-schema&quot;&gt;3.3.3 Meta Table Schema&lt;/h3&gt;

&lt;p&gt;这个表是OpenTSDB中不同时间序列的一个索引，可以用来存储一些额外的信息。这个表名称叫做&lt;code&gt;tsdb-meta&lt;/code&gt;，该表只有一个列族&lt;code&gt;name&lt;/code&gt;，两个列，分别为&lt;code&gt;ts_meta&lt;/code&gt;、&lt;code&gt;ts_ctr&lt;/code&gt;，该表中数据如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\x00\x00\x01\x00\x00\x01\x00\x00\x01 name:ts_ctr \x00\x00\x00\x00\x00\x00\x00p
\x00\x00\x01\x00\x00\x01\x00\x00\x01 name:ts_meta {&quot;tsuid&quot;:&quot;000001000001000001&quot;,&quot;displayName&quot;:&quot;&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213196,&quot;custom&quot;:null,&quot;units&quot;:&quot;&quot;,&quot;dataType&quot;:&quot;&quot;,&quot;retention&quot;:0,&quot;max&quot;:&quot;NaN&quot;,&quot;min&quot;:&quot;NaN&quot;}

\x00\x00\x02\x00\x00\x01\x00\x00\x01 name:ts_ctr \x00\x00\x00\x00\x00\x00\x00p
\x00\x00\x02\x00\x00\x01\x00\x00\x01 name:ts_meta {&quot;tsuid&quot;:&quot;000002000001000001&quot;,&quot;displayName&quot;:&quot;&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213196,&quot;custom&quot;:null,&quot;units&quot;:&quot;&quot;,&quot;dataType&quot;:&quot;&quot;,&quot;retention&quot;:0,&quot;max&quot;:&quot;NaN&quot;,&quot;min&quot;:&quot;NaN&quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Row Key&lt;/strong&gt; 和&lt;code&gt;tsdb&lt;/code&gt;表一样，其中不包含时间戳，&lt;code&gt;&amp;lt;metric_uid&amp;gt;&amp;lt;tagk1&amp;gt;&amp;lt;tagv1&amp;gt;[...&amp;lt;tagkN&amp;gt;&amp;lt;tagvN&amp;gt;]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TSMeta Column&lt;/strong&gt; 和UIDMeta相似，其为UTF-8编码的JSON格式字符串&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ts_ctr Column&lt;/strong&gt; 计数器，用来记录一个时间序列中存储的数据个数，其列名为&lt;code&gt;ts_ctr&lt;/code&gt;，为8位有符号的整数。&lt;/p&gt;

&lt;h3 id=&quot;tree-table-schema&quot;&gt;3.3.4 Tree Table Schema&lt;/h3&gt;

&lt;p&gt;索引表，用于展示树状结构的，类似于文件系统，以方便其他系统使用，例如：&lt;code&gt;Graphite&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;3.4 如何写数据&lt;/h2&gt;
&lt;p&gt;## 3.5 如何查询数据&lt;br /&gt;
## 3.6 CLI Tools&lt;/p&gt;

&lt;p&gt;tsdb支持以下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@cdh1 build]# ./tsdb 
usage: tsdb &amp;lt;command&amp;gt; [args]
Valid commands: fsck, import, mkmetric, query, tsd, scan, uid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过以下命令创建指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./tsdb mkmetric mysql.bytes_received mysql.bytes_sent
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行上述命令的结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics mysql.bytes_received: [0, 0, -93]
metrics mysql.bytes_sent: [0, 0, -92]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;utilities&quot;&gt;3.11 Utilities&lt;/h2&gt;
&lt;p&gt;## 3.12 Logging&lt;/p&gt;

&lt;h1 id=&quot;http-api&quot;&gt;4. HTTP API&lt;/h1&gt;
&lt;p&gt;# 5. 谁在用OpenTSDB&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.stumbleupon.com/&quot;&gt;StumbleUpon&lt;/a&gt; StumbleUpon is the easiest way to find cool new websites, videos, photos and images from across the Web&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.box.com/&quot;&gt;box&lt;/a&gt; Box simplifies online file storage, replaces FTP and connects teams in online workspaces.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.tumblr.com/&quot;&gt;tumblr&lt;/a&gt; 一个轻量级博客，用户可以跟进其他的会员并在自己的页面上看到跟进会员发表的文章，还可以转发他人在Tumblr上的文章&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;kairosdb&quot;&gt;6. KairosDB&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;KairosDB是一个快速可靠的分布式时间序列数据库，主要用于Cassandra当然也可以适用与HBase。KairosDB是在OpenTSDB基础上重写的，他不仅可以在HBase上存储数据还支持Cassandra。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KairosDB主页：&lt;a href=&quot;https://code.google.com/p/kairosdb/&quot;&gt;https://code.google.com/p/kairosdb/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-7&quot;&gt;7. 参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://luoshi0801.iteye.com/blog/1938835&quot;&gt;tlog数据存储&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/bingjie1217/article/category/1751285&quot;&gt;OpenTSDB源码分析系列文章&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.binospace.com/index.php/opentsdb-design-road/&quot;&gt;OpenTSDB的设计之道&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dirlt.com/opentsdb.html&quot;&gt;opentsdb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/01/22/all-things-opentsdb.html</link>
      <guid>http://blog.javachen.com/2014/01/22/all-things-opentsdb.html</guid>
      <pubDate>2014-01-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>All Things Jekyll</title>
      <description>&lt;p&gt;Jekyll是一个简洁的、特别针对博客平台的静态网站生成器。它使用一个模板目录作为网站布局的基础框架，并在其上运行Textile、Markdown或Liquid标记语言的转换器，最终生成一个完整的静态Web站点，可以被放置在Apache或者你喜欢的其他任何Web服务器上。它同时也是GitHub Pages、一个由GitHub提供的用于托管项目主页或博客的服务，在后台所运行的引擎。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 安装&lt;/h1&gt;
&lt;p&gt;Jekyll使用动态脚本语言Ruby写成。请首先下载并安装Ruby，目前需要的ruby版本为&lt;code&gt;1.9.1&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在使用Jekyll之前，你可能想要对Ruby语言有一些初步了解（非必需）。&lt;/p&gt;

&lt;p&gt;安装Jekyll的最好方式是通过&lt;code&gt;RubyGems&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gem install jekyll
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Jekyll依赖以下的gems模块：liquid、fast-stemmer、classifier、directory_watcher、syntax、maruku、kramdown、posix-spawn和albino。它们会被&lt;code&gt;gem install&lt;/code&gt;命令自动安装。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 模板引擎&lt;/h1&gt;

&lt;h2 id=&quot;rdiscount&quot;&gt;2.1 RDiscount&lt;/h2&gt;
&lt;p&gt;如果你想用 RDiscount 取代 Maruku 作为你的Markdown标记语言转换引擎，只需确认安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gem install rdiscount
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在你站点下的&lt;code&gt;_config.yml&lt;/code&gt;文件中加入以下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markdown: rdiscount
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;redcloth&quot;&gt;2.2 RedCloth&lt;/h2&gt;

&lt;p&gt;若要使用Textile标记语言，需要安装相应的转换引擎&lt;a href=&quot;http://redcloth.org/&quot;&gt;RedCloth&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gem install RedCloth
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;redcarpet&quot;&gt;2.3 Redcarpet&lt;/h2&gt;

&lt;p&gt;Redcarpet是由GitHub自己人开发的，一直以来它被用于在GitHub上渲染Markdown格式文本（也就是&lt;a href=&quot;http://github.github.com/github-flavored-markdown/&quot;&gt;GitHub Flavored Markdown&lt;/a&gt;）。由于API兼容性的原因，Jekyll以前的版本并不支持Redcarpet。现在，Jekyll 0.12.0终于增加了对Redcarpet 2引擎的支持，只需安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gem install redcarpet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把&lt;code&gt;_config.yml&lt;/code&gt;中的Markdown引擎设置从：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markdown: rdiscount
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markdown: redcarpet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以迁移到Redcarpet上了。&lt;/p&gt;

&lt;p&gt;Redcarpet所支持的GitHub Flavored Markdown比起标准的Markdown语法来增加了不少便利之处，诸如围栏式代码块（Fenced code blocks）。&lt;/p&gt;

&lt;p&gt;redcarpet 有很多很多的options可以设置，见：&lt;a href=&quot;https://github.com/vmg/redcarpet&quot;&gt;https://github.com/vmg/redcarpet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;edcarpet 只和 markdown parser有关，如果需要设置高亮，可以使用&lt;a href=&quot;http://pygments.org/&quot;&gt;pygments&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;你可能会用到的标记语言和模板引擎：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[Textile](http://en.wikipedia.org/wiki/Textile_(markup_language) 可读性好的轻量级标记语言，可以被转换成XHTML格式。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.textism.com/tools/textile/&quot;&gt;Textile Home Page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://redcloth.org/hobix.com/textile/&quot;&gt;A Textile Reference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://redcloth.org/&quot;&gt;RedCloth&lt;/a&gt; Ruby的Textile实现引擎。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Markdown&quot;&gt;Markdown&lt;/a&gt; 另一种Jekyll所支持的轻量级标记语言。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://daringfireball.net/projects/markdown/&quot;&gt;Markdown Home Page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://deveiate.org/projects/BlueCloth&quot;&gt;BlueCloth&lt;/a&gt; Ruby的Markdown实现引擎。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://maruku.rubyforge.org/&quot;&gt;Maruku&lt;/a&gt; Ruby的另一个Markdown实现引擎，效率较高。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/rtomayko/rdiscount/&quot;&gt;RDiscount&lt;/a&gt; Ruby的另一个Markdown实现引擎，效率比Maruku更高。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://liquidmarkup.org/&quot;&gt;Liquid&lt;/a&gt; Ruby的模板渲染引擎。它也是Jekyll所使用的模板引擎。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Shopify/liquid/wiki/Liquid-for-Designers&quot;&gt;Liquid for Designers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Shopify/liquid/wiki/Liquid-for-Programmers&quot;&gt;Liquid for Programmers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 基本结构&lt;/h1&gt;

&lt;p&gt;Jekyll从核心上来说是一个文本转换引擎。该系统内部的工作原理是：你输入一些用自己喜爱的标记语言格式书写的文本，可以是Markdown、Textile或纯粹的HTML，它将这些文本混合后放入一个或一整套页面布局当中。在整个过程中，你可以自行决定你的站点URL的模式、以及哪些数据将被显示在页面中，等等。这一切都将通过严格的文本编辑完成，而生成的Web界面则是最终的产品。&lt;/p&gt;

&lt;p&gt;一个典型的Jekyll站点通常具有如下结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
|-- _config.yml
|-- _includes
|-- _layouts
|   |-- default.html
|   `-- post.html
|-- _posts
|-- _site
`-- index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下是每部分功能的简述：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;_config.yml&lt;/code&gt;。保存Jekyll配置的文件。虽然绝大部分选项可以通过命令行参数指定，但将它们写入配置文件可以使你在每次执行时不必记住它们。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;_includes&lt;/code&gt;。该目录存放可以与&lt;code&gt;_layouts&lt;/code&gt;和&lt;code&gt;_posts&lt;/code&gt;混合、匹配并重用的文件。Liquid标签&lt;code&gt;{ % include % }&lt;/code&gt;可以用于嵌入文件&lt;code&gt;_includes/file.ext&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;_layouts&lt;/code&gt;。该目录存放用来插入帖子的网页布局模板。页面布局基于类似博客平台的一个帖子接一个帖子的原则，通过YAML前置数据定义。Liquid标签用于在页面上插入帖子的文本内容。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;_posts&lt;/code&gt;。该目录下存放的可以说成是你的”动态内容”。这些文件的格式很重要，它们的命名模式必须遵循 &lt;code&gt;YEAR-MONTH-DATE-title.MARKUP&lt;/code&gt; 。每一个帖子的固定链接URL可以作弹性的调整，但帖子的发布日期和转换所使用的标记语言会根据且仅根据文件名中的相应部分来识别。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;_site&lt;/code&gt;。这里是Jekyll用以存放最终生成站点的根路径位置。也许把它加到你的 &lt;code&gt;.gitignore&lt;/code&gt; 列表中会是个不错的主意。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4. 运行、部署&lt;/h1&gt;
&lt;p&gt;通常直接在命令行下使用可执行的Ruby脚本 jekyll ，它可以从gem安装。如果要启动一个临时的Web服务器并测试你的Jekyll站点，执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jekyll --server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在浏览器中访问 &lt;code&gt;http://localhost:4000&lt;/code&gt; 或 &lt;code&gt;http://0.0.0.0:4000&lt;/code&gt; 。当然这里还有其他许多参数选项可以使用。&lt;/p&gt;

&lt;p&gt;由于Jekyll所做的仅仅是生成一个包含HTML等静态网站文件的目录（_site），它可以通过简单的拷贝（scp）、远程同步（rsync）、ftp上传或git等方式部署到任何Web服务器上，例如github、gitcafe、qiniu。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5. 一些技巧&lt;/h1&gt;

&lt;h2 id=&quot;section-5&quot;&gt;使用表格&lt;/h2&gt;

&lt;p&gt;使用redcarpet模板引擎，通过gem安装redcarpet并修改&lt;code&gt;_config.yml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markdown: redcarpet
redcarpet: 
    extensions: [&quot;tables&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Markdwon 文件中可以依据以下语法进行书写&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;|head1 head1 head1|head2 head2 head2|head3 head3 head3|head4 head4 head4|
|---|:---|:---:|---:|
|row1text1|row1text3|row1text3|row1text4|
|row2text1|row2text3|row2text3|row2text4|
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后添加如下样式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;table {
margin-bottom: 20px;
max-width: 100%;
border-collapse: collapse;
transition: all 0.3s;
border: 1px solid rgba(0,0,0,0.167);
}
table {
border-collapse: collapse;
}
table thead&amp;gt;tr {
background-color: rgba(249,249,249,0.9);
}
table thead tr th {
border: 1px solid rgba(0,0,0,0.167);
border-top: 0px none;
border-bottom-width: 2px;
vertical-align: bottom;
padding: 8px;
}
table tbody tr td {
border: 1px solid rgba(0,0,0,0.167);
vertical-align: top;
padding: 8px;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-6&quot;&gt;6. 其他静态网站生成器&lt;/h1&gt;
&lt;p&gt;如果想要尝试一些其他的静态网页生成器，这里是一个简略的列表：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ruby&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tinytree.info/&quot;&gt;Bonsai&lt;/a&gt; 一个非常简单（但实用）的小脚本&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://webgen.rubyforge.org/&quot;&gt;Webgen&lt;/a&gt; 一个较复杂的生成器&lt;/li&gt;
  &lt;li&gt;Python&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ringce.com/hyde&quot;&gt;Hyde&lt;/a&gt; Jekyll的Python语言实现版本&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pypi.python.org/pypi/cyrax&quot;&gt;Cyrax&lt;/a&gt; 使用Jinja2模板引擎的生成器&lt;/li&gt;
  &lt;li&gt;PHP&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.phrozn.info/&quot;&gt;Phrozn&lt;/a&gt; PHP语言实现的静态网站生成器&lt;/li&gt;
  &lt;li&gt;nodejs&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tommy351/hexo&quot;&gt;hexo&lt;/a&gt;一个台湾人实现的生成器&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ericzhang-cn/papery&quot;&gt;papery&lt;/a&gt; 纯nodejs编写&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docpad.org/&quot;&gt;DocPad&lt;/a&gt;  static web sites generator using Node.js&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更详细的列表和介绍请参见：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.subspacefield.org/~travis/static_blog_generators.html&quot;&gt;Static Blog Generators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://iwantmyname.com/blog/2011/02/list-static-website-generators.html&quot;&gt;32 Static Website Generators For Your Site, Blog Or Wiki&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-7&quot;&gt;7. 资源&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://jekyllrb.com/&quot;&gt;jekyll官网&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://jekyllbootstrap.com/‎&quot;&gt;Jekyll Bootstrap&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2012/08/blogging_with_jekyll.html&quot;&gt;搭建一个免费的，无限流量的Blog—-github Pages和Jekyll入门&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yangzhiping.com/tech/wordpress-to-jekyll.html&quot;&gt;告别wordpress，拥抱jekyll&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.soimort.org/posts/101/&quot;&gt;像黑客一样写博客——Jekyll入门&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zhihu.com/question/19996679&quot;&gt;用 Jekyll 和 Octopress 搭建博客，哪个更合适？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.skydark.info/programming/2012/03/23/play-with-jekyll/&quot;&gt;Play with Jekyll&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.zhuoqun.net/&quot;&gt;创造者&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.liulantao.com/&quot;&gt;非常规思维研究所&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://yihui.name/cn/&quot;&gt;Keep on Fighting!&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://havee.me/&quot;&gt;Havee’s Space&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mytharcher.github.io/&quot;&gt;闭门造轮子&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/01/21/all-things-about-jekyll.html</link>
      <guid>http://blog.javachen.com/2014/01/21/all-things-about-jekyll.html</guid>
      <pubDate>2014-01-21T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>SSH远程连接时环境变量问题</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 问题&lt;/h1&gt;

&lt;p&gt;RHEL服务器A有个启动脚本（普通用户user01运行），里面使用ifconfig获取ip地址如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Localhost_ip=$(ifconfig |awk -F &#39;addr:|Bcast&#39; &#39;/Bcast/{print $2}&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于普通用户user01不能直接识别ifconfig命令，只能使用全路径&lt;code&gt;/sbin/ifconfig&lt;/code&gt;，目前处理方式为修改&lt;code&gt;~/.bash_profile&lt;/code&gt;文件添加环境变量如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PATH=$PATH:$HOME/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;改成如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PATH=$PATH:$HOME/bin:/sbin
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- more --&gt;

&lt;p&gt;经过如上配置后服务器本机user01用户登录执行XX.sh脚本可以识别ifconfig命令。&lt;/p&gt;

&lt;p&gt;出现如下问题：&lt;/p&gt;

&lt;p&gt;远程主机B通过ssh远程执行启动脚本XX.sh，报错如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash: ifconfg: command not found
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 问题分析&lt;/h1&gt;

&lt;p&gt;测试前准备，追加调用说明语句，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/etc/profile
[root@node3 ~]# vim /etc/profile
echo &quot;/etc/profile begin:&quot;
echo &quot;$PATH&quot;
...
 
unset i
unset pathmunge
 
echo &quot;invoke /etc/profile&quot;
echo &quot;$PATH&quot;
echo &quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node3 ~]# vim /etc/bashrc
...
# vim:ts=4:sw=4
 
echo &quot;invoke /etc/bashrc&quot;
echo &quot;$PATH&quot;
echo &quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node3 ~]# vim /root/.bash_profile
# User specific environment and startup programs
PATH=$PATH:$HOME/bin
export PATH
 
echo &quot;invoke ~/.bash_profile&quot;
echo &quot;$PATH&quot;
echo &quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node3 ~]# vim /root/.bashrc
...
# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
 
echo &quot;invoke ~/.bashrc&quot;
echo &quot;$PATH&quot;
echo &quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node3 ~]# vim /home/user01/.bash_profile
...
# User specific environment and startup programs
PATH=$PATH:$HOME/bin
export PATH
 
echo &quot;invoke ~/.bashrc&quot;
echo &quot;$PATH&quot;
echo &quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node3 ~]# vim /home/user01/.bashrc
...
# User specific aliases and functions
 
echo &quot;invoke ~/.bashrc&quot;
echo &quot;$PATH&quot;
echo &quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分 user 和 root 用户，3 种场景进行测试，如下：&lt;/p&gt;

&lt;h2 id=&quot;user&quot;&gt;普通用户 User&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;场景1：&lt;/h3&gt;

&lt;p&gt;本机使用 su 命令切换到普通用户 （属于 Login 方式）&lt;/p&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Login 之前，系统 PATH 为：&lt;code&gt;/usr/local/bin:/bin:/usr/bin&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Login 方式，文件调用顺序为： &lt;code&gt;/etc/profile -&amp;gt; /etc/bashrc -&amp;gt; ~/.bashrc -&amp;gt; ~/.bash_profile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Login 之后，系统 PATH 为：&lt;code&gt;/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/user01/bin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;su - user
[root@node3 ~]# hostname -i
192.168.122.33
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node3 ~]# su - user01
/etc/profile begin:
/usr/local/bin:/bin:/usr/bin
invoke /etc/profile
/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
 
invoke /etc/bashrc
/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
 
invoke ~/.bashrc
/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
 
invoke ~/.bash_profile
/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/user01/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-3&quot;&gt;场景2：&lt;/h3&gt;

&lt;p&gt;远程机使用 ssh 命令以普通用户身份登陆到主机 （属于 Login 方式）&lt;/p&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;与在本机使用 su 命令切换到普通用户的效果完全一样！&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;ssh user@remote_server_ip
[root@node1 ~]# hostname -i
192.168.122.31
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ssh user01@192.168.122.33
user01@192.168.122.33&#39;s password: 
Last login: Tue Jul  9 16:23:33 2013 from 192.168.122.31
/etc/profile begin:
/usr/local/bin:/bin:/usr/bin
invoke /etc/profile
/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
 
invoke /etc/bashrc
/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
 
invoke ~/.bashrc
/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
 
invoke ~/.bash_profile
/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/user01/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;场景3：&lt;/h2&gt;

&lt;p&gt;远程机使用 ssh 命令以普通用户身份连接到主机执行获取 PATH 的命令 （属于 NoLogin 方式）&lt;/p&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NoLogin 方式，命令获取的 PATH 为该远程机的，并未拿到目标主机的 PATH&lt;/li&gt;
  &lt;li&gt;NoLogin 方式，文件调用顺序为：&lt;code&gt;/etc/bashrc -&amp;gt; ~/.bashrc&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;NoLogin 方式，目标主机 User 用户 PATH 为：&lt;code&gt;/usr/local/bin:/bin:/usr/bin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;ssh user@remote_server_ip command
[root@node1 ~]# hostname -i
192.168.122.31
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# echo $PATH
/usr/local/rabbitmq/sbin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
 
[root@node1 ~]# ssh user01@192.168.122.33 &quot;echo $PATH&quot;
user01@192.168.122.33&#39;s password: 
 
invoke /etc/bashrc
/usr/local/bin:/bin:/usr/bin
 
invoke ~/.bashrc
/usr/local/bin:/bin:/usr/bin
 
/usr/local/rabbitmq/sbin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;root-&quot;&gt;对比 root 用户&lt;/h2&gt;

&lt;h3 id=&quot;section-5&quot;&gt;场景1：&lt;/h3&gt;

&lt;p&gt;本机使用 su 命令切换到 root （属于 Login 方式）&lt;/p&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Login 之前，系统 PATH 为：&lt;code&gt;/usr/local/bin:/bin:/usr/bin&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Login 方式，root 用户，文件调用顺序为：&lt;code&gt;/etc/profile -&amp;gt; /etc/bashrc -&amp;gt; ~/.bashrc -&amp;gt; ~/.bash_profile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Login 之后，系统 PATH 为：&lt;code&gt;/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;su - root
[root@node3 ~]# hostname -i
192.168.122.33
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node3 ~]# su - root
/etc/profile begin:
/usr/local/bin:/bin:/usr/bin
invoke /etc/profile
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
 
invoke /etc/bashrc
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
 
invoke ~/.bashrc
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
 
invoke ~/.bash_profile
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-6&quot;&gt;场景2：&lt;/h3&gt;

&lt;p&gt;远程机使用 ssh 命令以 root 用户身份登陆到主机 （属于 Login 方式）&lt;/p&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;与在本机使用 su 命令切换到 root 用户的效果完全一样！&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;ssh root@remote_server_ip
[root@node1 ~]# hostname -i
192.168.122.31
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ssh root@192.168.122.33
root@192.168.122.33&#39;s password: 
Last login: Tue Jul  9 15:54:53 2013 from 192.168.122.1
/etc/profile begin:
/usr/local/bin:/bin:/usr/bin
invoke /etc/profile
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
 
invoke /etc/bashrc
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
 
invoke ~/.bashrc
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
 
invoke ~/.bash_profile
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-7&quot;&gt;场景3&lt;/h3&gt;

&lt;p&gt;远程机使用 ssh 命令以 root 用户身份连接到主机执行获取 PATH 的命令 （属于 NoLogin 方式）&lt;/p&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NoLogin 方式，命令获取的 PATH 为该远程机的，并未拿到目标主机的 PATH&lt;/li&gt;
  &lt;li&gt;NoLogin 方式，文件调用顺序为：&lt;code&gt;/etc/bashrc -&amp;gt; ~/.bashrc&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;NoLogin 方式，目标主机 root 用户 PATH 为：&lt;code&gt;/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;ssh root@remote_server_ip command
[root@node1 ~]# hostname -i
192.168.122.31
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# echo $PATH
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
 
[root@node1 ~]# ssh root@192.168.122.33 &quot;echo $PATH&quot;
root@192.168.122.33&#39;s password: 
invoke /etc/bashrc
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
 
invoke ~/.bashrc
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
 
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;etcprofile-&quot;&gt;关于/etc/profile 文件部分代码分析&lt;/h3&gt;

&lt;p&gt;结论：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;无论 root 还是 user ，只有调用此文件，其 PATH 中才会被追加 sbin 相关路径。而由以上测试场景可知，只有 Login 时，&lt;code&gt;/etc/profile&lt;/code&gt; 文件才会被调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;pathmunge () {
    case &quot;:${PATH}:&quot; in
        *:&quot;$1&quot;:*)
            ;;
        *)
            if [ &quot;$2&quot; = &quot;after&quot; ] ; then
                PATH=$PATH:$1
            else
                PATH=$1:$PATH
            fi
    esac
}

if [ -x /usr/bin/id ]; then
    if [ -z &quot;$EUID&quot; ]; then
        # ksh workaround
        EUID=`id -u`
        UID=`id -ru`
    fi
    USER=&quot;`id -un`&quot;
    LOGNAME=$USER
    MAIL=&quot;/var/spool/mail/$USER&quot;
fi


# Path manipulation
if [ &quot;$EUID&quot; = &quot;0&quot; ]; then
    pathmunge /sbin
    pathmunge /usr/sbin
    pathmunge /usr/local/sbin
else
    pathmunge /usr/local/sbin after
    pathmunge /usr/sbin after
    pathmunge /sbin after

fi
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-8&quot;&gt;3. 总结&lt;/h1&gt;

&lt;p&gt;综上，如需修改 PATH，建议修改 bashrc 文件，从而保证任何方式访问时 PATH 的正确性。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2014/01/18/bash-problem-when-ssh-access.html</link>
      <guid>http://blog.javachen.com/2014/01/18/bash-problem-when-ssh-access.html</guid>
      <pubDate>2014-01-18T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase笔记：Region拆分策略</title>
      <description>&lt;h1 id=&quot;region-&quot;&gt;Region 概念&lt;/h1&gt;

&lt;p&gt;Region是表获取和分布的基本元素，由每个列族的一个Store组成。对象层级图如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Table       (HBase table)
    Region       (Regions for the table)
         Store          (Store per ColumnFamily for each Region for the table)
              MemStore        (MemStore for each Store for each Region for the table)
              StoreFile       (StoreFiles for each Store for each Region for the table)
                    Block     (Blocks within a StoreFile within a Store for each Region for the table)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;region--1&quot;&gt;Region 大小&lt;/h1&gt;

&lt;p&gt;Region的大小是一个棘手的问题，需要考量如下几个因素。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Region是HBase中分布式存储和负载均衡的最小单元。不同Region分布到不同RegionServer上，但并不是存储的最小单元。&lt;/li&gt;
  &lt;li&gt;Region由一个或者多个Store组成，每个store保存一个columns family，每个Strore又由一个memStore和0至多个StoreFile 组成。memStore存储在内存中， StoreFile存储在HDFS上。&lt;/li&gt;
  &lt;li&gt;HBase通过将region切分在许多机器上实现分布式。也就是说，你如果有16GB的数据，只分了2个region， 你却有20台机器，有18台就浪费了。&lt;/li&gt;
  &lt;li&gt;region数目太多就会造成性能下降，现在比以前好多了。但是对于同样大小的数据，700个region比3000个要好。&lt;/li&gt;
  &lt;li&gt;region数目太少就会妨碍可扩展性，降低并行能力。有的时候导致压力不够分散。这就是为什么，你向一个10节点的HBase集群导入200MB的数据，大部分的节点是idle的。&lt;/li&gt;
  &lt;li&gt;RegionServer中1个region和10个region索引需要的内存量没有太多的差别。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最好是使用默认的配置，可以把热的表配小一点(或者受到split热点的region把压力分散到集群中)。如果你的cell的大小比较大(100KB或更大)，就可以把region的大小调到1GB。region的最大大小在hbase配置文件中定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt; &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.hregion.max.filesize&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;10 * 1024 * 1024 * 1024&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;说明：&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;当region中的StoreFile大小超过了上面配置的值的时候，该region就会被拆分，具体的拆分策略见下文。&lt;/li&gt;
  &lt;li&gt;上面的值也可以针对每个表单独设置，例如在hbase shell中设置：&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;create &#39;t&#39;,&#39;f&#39;
disable &#39;t&#39;
alter &#39;t&#39;, METHOD =&amp;gt; &#39;table_att&#39;, MAX_FILESIZE =&amp;gt; &#39;134217728&#39;
enable &#39;t&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;region--2&quot;&gt;Region 拆分策略&lt;/h1&gt;

&lt;p&gt;Region的分割操作是不可见的，因为Master不会参与其中。RegionServer拆分region的步骤是，先将该region下线，然后拆分，将其子region加入到META元信息中，再将他们加入到原本的RegionServer中，最后汇报Master。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;执行split的线程是CompactSplitThread。&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;自定义拆分策略&lt;/h2&gt;

&lt;p&gt;可以通过设置&lt;code&gt;RegionSplitPolicy&lt;/code&gt;的实现类来指定拆分策略，RegionSplitPolicy类的实现类有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ConstantSizeRegionSplitPolicy
	IncreasingToUpperBoundRegionSplitPolicy
		DelimitedKeyPrefixRegionSplitPolicy
		KeyPrefixRegionSplitPolicy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于split，并不是设置了&lt;code&gt;hbase.hregion.max.filesize&lt;/code&gt;（默认10G）为很大就保证不split了，需要有以下的算法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IncreasingToUpperBoundRegionSplitPolicy，&lt;strong&gt;0.94.0默认region split策略&lt;/strong&gt;。根据公式min(r^2*flushSize，maxFileSize)确定split的maxFileSize，其中r为在线region个数，maxFileSize由&lt;code&gt;hbase.hregion.max.filesize&lt;/code&gt;指定。&lt;/li&gt;
  &lt;li&gt;ConstantSizeRegionSplitPolicy，仅仅当region大小超过常量值（&lt;code&gt;hbase.hregion.max.filesize&lt;/code&gt;大小）时，才进行拆分。&lt;/li&gt;
  &lt;li&gt;DelimitedKeyPrefixRegionSplitPolicy，保证以分隔符前面的前缀为splitPoint，保证相同RowKey前缀的数据在一个Region中&lt;/li&gt;
  &lt;li&gt;KeyPrefixRegionSplitPolicy，保证具有相同前缀的row在一个region中（要求设计中前缀具有同样长度）。指定rowkey前缀位数划分region，通过读取&lt;code&gt;table的prefix_split_key_policy.prefix_length&lt;/code&gt;属性，该属性为数字类型，表示前缀长度，在进行split时，按此长度对splitPoint进行截取。此种策略比较适合固定前缀的rowkey。当table中没有设置该属性，或其属性不为Integer类型时，指定此策略效果等同与使用IncreasingToUpperBoundRegionSplitPolicy。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;increasingtoupperboundregionsplitpolicy&quot;&gt;IncreasingToUpperBoundRegionSplitPolicy&lt;/h3&gt;

&lt;p&gt;这是&lt;strong&gt;0.94.0默认region split策略&lt;/strong&gt;。根据根据公式min(r^2*flushSize，maxFileSize)确定split的maxFileSize，这里假设flushSize为128M：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;第一次拆分大小为：min(10G，1*1*128M)=128M
第二次拆分大小为：min(10G，3*3*128M)=1152M
第三次拆分大小为：min(10G，5*5*128M)=3200M
第四次拆分大小为：min(10G，7*7*128M)=6272M
第五次拆分大小为：min(10G，9*9*128M)=10G
第五次拆分大小为：min(10G，11*11*128M)=10G
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，只有在第四次之后的拆分大小才为10G&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;配置拆分策略&lt;/h2&gt;

&lt;p&gt;你可以在hbase配置文件中定义全局的拆分策略，设置&lt;code&gt;hbase.regionserver.region.split.policy&lt;/code&gt;的值即可，也可以在创建和修改表时候指定：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// 更新现有表的split策略
HBaseAdmin admin = new HBaseAdmin( conf);
HTable hTable = new HTable( conf, &quot;test&quot; );
HTableDescriptor htd = hTable.getTableDescriptor();
HTableDescriptor newHtd = new HTableDescriptor(htd);
newHtd.setValue(HTableDescriptor. SPLIT_POLICY, KeyPrefixRegionSplitPolicy.class .getName());// 指定策略
newHtd.setValue(&quot;prefix_split_key_policy.prefix_length&quot;, &quot;2&quot;);
newHtd.setValue(&quot;MEMSTORE_FLUSHSIZE&quot;, &quot;5242880&quot;); // 5M
admin.disableTable( &quot;test&quot;);
admin.modifyTable(Bytes. toBytes(&quot;test&quot;), newHtd);
admin.enableTable( &quot;test&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;说明：&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;上面的不同策略可以在不同的业务场景下使用，特别是第三种和第四种一般关注和使用的比较少。&lt;/li&gt;
  &lt;li&gt;如果想关闭自动拆分改为手动拆分，建议同时修改&lt;code&gt;hbase.hregion.max.filesize&lt;/code&gt;和&lt;code&gt;hbase.regionserver.region.split.policy&lt;/code&gt;值。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://blog.csdn.net/doliu6/article/details/13505319&quot;&gt;HBase的Compact和Split源码分析与应用–基于0.94.5&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://blog.csdn.net/yangbutao/article/details/8930126&quot;&gt;HBase源码分析之org.apache.hadoop.hbase.regionserver包&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://abloz.com/hbase/book.html&quot;&gt;HBase 官方文档中文版&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[4] &lt;a href=&quot;http://blog.toby941.sinaapp.com/hbase-region-split.html&quot;&gt;hbase region split策略&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/01/16/hbase-region-split-policy.html</link>
      <guid>http://blog.javachen.com/2014/01/16/hbase-region-split-policy.html</guid>
      <pubDate>2014-01-16T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Vim配置和插件管理</title>
      <description>&lt;p&gt;这篇文章主要是记录vim配置中各个配置项的含义并且收藏一些常用的插件及其使用方法。&lt;/p&gt;

&lt;h1 id=&quot;vim&quot;&gt;1. Vim配置&lt;/h1&gt;

&lt;p&gt;目前我的vimrc配置放置在:&lt;a href=&quot;https://github.com/javachen/snippets/blob/master/dotfiles/.vimrc&quot;&gt;https://github.com/javachen/snippets/blob/master/dotfiles/.vimrc&lt;/a&gt;，其中大多数用英文注释。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;2. 插件管理&lt;/h1&gt;

&lt;p&gt;使用 pathogen来管理插件&lt;/p&gt;

&lt;p&gt;项目地址:	&lt;a href=&quot;https://github.com/tpope/vim-pathogen&quot;&gt;https://github.com/tpope/vim-pathogen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p ~/.vim/autoload ~/.vim/bundle &amp;amp;&amp;amp; \
$ curl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要记得把以下内容加入到vimrc文件中:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;execute pathogen#infect()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;3. 安装插件&lt;/h1&gt;

&lt;h2 id=&quot;nerdtree&quot;&gt;3.1 NERDTree&lt;/h2&gt;

&lt;p&gt;NERD tree允许你在Vim编辑器中以树状方式浏览系统中的文件和目录, 支持快捷键与鼠标操作, 使用起来十分方便. NERD tree能够以不同颜色高亮显示节点类型, 并包含书签, 过滤等实用功能. 配合taglist或txtviewer插件, 右边窗口显示本文件夹的文件, 左边窗口显示本文的文档结构, 将会使管理一个工程变得相当容易.&lt;/p&gt;

&lt;p&gt;项目地址:	&lt;a href=&quot;https://github.com/scrooloose/nerdtree&quot;&gt;https://github.com/scrooloose/nerdtree&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装方法很简单，只要把项目clone一份到bundle目录就可以了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ~/.vim/bundle
git clone https://github.com/scrooloose/nerdtree.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后的插件也都是这么安装。&lt;/p&gt;

&lt;p&gt;使用：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在linux命令行界面，输入vim&lt;/li&gt;
  &lt;li&gt;输入&lt;code&gt;:NERDTree&lt;/code&gt; ，回车，默认打开当前目录，当然可以打开指定目录，如 &lt;code&gt;:NERDTree /home/&lt;/code&gt; 打开&lt;/li&gt;
  &lt;li&gt;入当前目录的树形界面，通过小键盘上下键，能移动选中的目录或文件&lt;/li&gt;
  &lt;li&gt;目录前面有&lt;code&gt;+&lt;/code&gt;号，摁 &lt;code&gt;Enter&lt;/code&gt; 会展开目录，文件前面是&lt;code&gt;-&lt;/code&gt;号，摁 &lt;code&gt;Enter&lt;/code&gt; 会在右侧窗口展现该文件的内容，并光标的焦点focus右侧。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ctr+w+h&lt;/code&gt; 光标 focus 左侧树形目录，&lt;code&gt;ctrl+w+l&lt;/code&gt; 光标 focus 右侧文件显示窗口。多次摁 &lt;code&gt;ctrl+w&lt;/code&gt;，光标自动在左右侧窗口切换&lt;/li&gt;
  &lt;li&gt;光标focus左侧树形窗口，按 &lt;code&gt;?&lt;/code&gt; 弹出NERDTree的帮助，再次按 &lt;code&gt;？&lt;/code&gt;关闭帮助显示&lt;/li&gt;
  &lt;li&gt;输入 &lt;code&gt;:q&lt;/code&gt; 回车，关闭光标所在窗口&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;除了使用鼠标可以基本操作以外，还可以使用键盘。下下面列出常用的快捷键：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;j&lt;/code&gt;、&lt;code&gt;k&lt;/code&gt; 分别下、上移动光标&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;o&lt;/code&gt; 或者回车打开文件或是文件夹，如果是文件的话，光标直接定位到文件中，想回到目录结构中，按住 &lt;code&gt;Ctrl&lt;/code&gt;，然后点两下 &lt;code&gt;w&lt;/code&gt; 就回来了&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;go&lt;/code&gt; 打开文件，但是光标不动，仍然在目录结构中&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;i&lt;/code&gt;、&lt;code&gt;s&lt;/code&gt; 分别是水平、垂直打开文件，就像vim命令的 &lt;code&gt;:vs&lt;/code&gt;、&lt;code&gt;:sp&lt;/code&gt;一样&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;gi&lt;/code&gt;、&lt;code&gt;gs&lt;/code&gt; 水平、垂直打开文件，光标不动&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;p&lt;/code&gt; 快速定位到上层目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;P&lt;/code&gt; 快速定位到根目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;K&lt;/code&gt;、&lt;code&gt;J&lt;/code&gt; 快速定位到同层目录第一个、最后一个节点&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;q&lt;/code&gt; 关闭&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nerdtree-tabs&quot;&gt;3.2 NERDTree-Tabs&lt;/h2&gt;

&lt;p&gt;项目地址:&lt;a href=&quot;https://github.com/jistr/vim-nerdtree-tabs&quot;&gt;https://github.com/jistr/vim-nerdtree-tabs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装完 NERDTree 以后我觉得还需要安装一下 NERDTree-Tabs 这个插件，提供了很多 NERDTree 的加强功能，包括保持 目录树状态、优化tab标题等等。&lt;/p&gt;

&lt;p&gt;安装方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd ~/.vim/bundle
$ git clone https://github.com/jistr/vim-nerdtree-tabs.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以把一下内容添加到 vimrc 文件中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let g:nerdtree_tabs_open_on_console_startup=1       &quot;设置打开vim的时候默认打开目录树
map &amp;lt;leader&amp;gt;n &amp;lt;plug&amp;gt;NERDTreeTabsToggle &amp;lt;CR&amp;gt;         &quot;设置打开目录树的快捷键
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;supertab&quot;&gt;3.3 supertab&lt;/h2&gt;

&lt;p&gt;SuperTab使键入Tab键时具有上下文提示及补全功能。如下图（图片来自 &lt;a href=&quot;http://www.ituring.com.cn/article/124970&quot;&gt;图灵社区&lt;/a&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.ituring.com.cn/download/01g6WiEUEKaO&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;项目地址:	&lt;a href=&quot;https://github.com/ervandew/supertab&quot;&gt;https://github.com/ervandew/supertab&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd ~/.vim/bundle
$ git clone git@github.com:ervandew/supertab.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开vim配置文件，&lt;code&gt;vim ~/.vimrc&lt;/code&gt;，在最后加上一行内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;let g:SuperTabDefaultCompletionType=&quot;context&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ctrlp&quot;&gt;3.4 ctrlp&lt;/h2&gt;

&lt;p&gt;项目地址:	&lt;a href=&quot;https://github.com/kien/ctrlp.vim&quot;&gt;https://github.com/kien/ctrlp.vim&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd ~/.vim/bundle
$ git clone git@github.com:kien/ctrlp.vim.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;快捷键：&lt;code&gt;ctrl+p&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;4. 参考&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;https://github.com/wklken/k-vim&quot;&gt;https://github.com/wklken/k-vim&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://www.zhihu.com/question/19989337&quot;&gt;http://www.zhihu.com/question/19989337&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://www.cnblogs.com/ma6174/archive/2011/12/10/2283393.html&quot;&gt;http://www.cnblogs.com/ma6174/archive/2011/12/10/2283393.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/01/14/vim-config-and-plugins.html</link>
      <guid>http://blog.javachen.com/2014/01/14/vim-config-and-plugins.html</guid>
      <pubDate>2014-01-14T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>SiteMesh介绍</title>
      <description>&lt;h1 id=&quot;sitemesh&quot;&gt;1. SiteMesh简介&lt;/h1&gt;

&lt;p&gt;SiteMesh是由一个基于Web页面布局、装饰以及与现存Web应用整合的框架。它能帮助我们在由大量页面构成的项目中创建一致的页面布局和外观，如一致的导航条，一致的banner，一致的版权等等。它不仅仅能处理动态的内容，如jsp，php，asp等产生的内容，它也能处理静态的内容，如htm的内容，使得它的内容也符合你的页面结构的要求。甚至于它能将HTML文件象include那样将该文件作为一个面板的形式嵌入到别的文件中去。所有的这些，都是GOF的Decorator模式的最生动的实现。尽管它是由java语言来实现的，但它能与其他Web应用很好地集成。&lt;/p&gt;

&lt;h1 id=&quot;sitemesh-1&quot;&gt;2. SiteMesh原理&lt;/h1&gt;

&lt;p&gt;SiteMesh框架是OpenSymphony团队开发的一个非常优秀的页面装饰器框架，它通过对用户请求进行过滤，并对服务器向客户端响应也进行过滤，然后给原始页面加入一定的装饰(header,footer等)，然后把结果返回给客户端。通过SiteMesh的页面装饰，可以提供更好的代码复用，所有的页面装饰效果耦合在目标页面中，无需再使用include指令来包含装饰效果，目标页与装饰页完全分离，如果所有页面使用相同的装饰器，可以是整个Web应用具有统一的风格。&lt;/p&gt;

&lt;h1 id=&quot;sitemesh3&quot;&gt;3. SiteMesh3&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/sitemesh/sitemesh3&quot;&gt;SiteMesh3&lt;/a&gt;只有Alpha2版本而且很久没更新了,它号称性能快3倍内存少1倍。&lt;/p&gt;

&lt;p&gt;另外，Atlassian公司因为在自己的产品中使用了SiteMesh2，所以后来也一直有维护，出到了2.5-atlassian-9版，不过不在maven中央库中，可以在&lt;a href=&quot;https://maven-us.nuxeo.org/nexus/content/groups/public/opensymphony/sitemesh/2.5-atlassian-9/&quot;&gt;这里&lt;/a&gt;下载。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h1 id=&quot;sitemesh2&quot;&gt;4. SiteMesh2的使用&lt;/h1&gt;

&lt;h2 id=&quot;sitemesh-2&quot;&gt;引入SiteMesh的依赖&lt;/h2&gt;

&lt;p&gt;在pom.xml中添加如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;dependency&amp;gt;
	&amp;lt;groupId&amp;gt;opensymphony&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;sitemesh&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;2.4.2&amp;lt;/version&amp;gt;
	&amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;添加过滤器&lt;/h2&gt;

&lt;p&gt;在web.xml中添加过滤器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;filter&amp;gt;
	&amp;lt;filter-name&amp;gt;sitemesh&amp;lt;/filter-name&amp;gt;
	&amp;lt;filter-class&amp;gt;com.opensymphony.module.sitemesh.filter.PageFilter&amp;lt;/filter-class&amp;gt;
&amp;lt;/filter&amp;gt;
&amp;lt;filter-mapping&amp;gt;
	&amp;lt;filter-name&amp;gt;sitemesh&amp;lt;/filter-name&amp;gt;
	&amp;lt;url-pattern&amp;gt;/*&amp;lt;/url-pattern&amp;gt;
&amp;lt;/filter-mapping&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;decoratorsxml&quot;&gt;新建decorators.xml文件&lt;/h2&gt;

&lt;p&gt;在WEB-INF下新建decorators.xml文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;
&amp;lt;decorators defaultdir=&quot;/WEB-INF/layouts/&quot;&amp;gt;
    &amp;lt;!-- 此处用来定义不需要过滤的页面 --&amp;gt;
    &amp;lt;excludes&amp;gt;
        &amp;lt;pattern&amp;gt;/static/*&amp;lt;/pattern&amp;gt;
    &amp;lt;/excludes&amp;gt;

    &amp;lt;!-- 用来定义装饰器要过滤的页面 --&amp;gt;
    &amp;lt;decorator name=&quot;default&quot; page=&quot;default.jsp&quot;&amp;gt;
        &amp;lt;pattern&amp;gt;/*&amp;lt;/pattern&amp;gt;
    &amp;lt;/decorator&amp;gt;
&amp;lt;/decorators&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;理论上SiteMesh只会搞那些MIME type为html的页面，但在配置里先exclude掉一些静态内容和API的路径会更省心。&lt;/p&gt;

&lt;h2 id=&quot;defaultjsp&quot;&gt;创建装饰器页面default.jsp&lt;/h2&gt;

&lt;p&gt;在decorators.xml中定义了一个装饰页面default.jsp：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt; %@ taglib uri=&quot;http://www.opensymphony.com/sitemesh/decorator&quot; prefix=&quot;decorator&quot; %&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;My Site - &amp;lt;decorator:title default=&quot;Welcome!&quot; /&amp;gt;&amp;lt;/title&amp;gt;
    &amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot; /&amp;gt;
    &amp;lt;meta http-equiv=&quot;Cache-Control&quot; content=&quot;no-store&quot; /&amp;gt;
    &amp;lt;meta http-equiv=&quot;Pragma&quot; content=&quot;no-cache&quot; /&amp;gt;
    &amp;lt;meta http-equiv=&quot;Expires&quot; content=&quot;0&quot; /&amp;gt;
    &amp;lt;decorator:head /&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    &amp;lt;div id=&quot;content&quot;&amp;gt;
    	&amp;lt;decorator:body /&amp;gt;
    &amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;装饰模板中可以使用的Sitemesh标签有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;取出被装饰页面的head标签中的内容。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;decorator:head /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;取出被装饰页面的body标签中的内容。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;decorator:body /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;取出被装饰页面的title标签中的内容。default为默认值&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;decorator:title default=&quot;&quot;  /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;取出被装饰页面相关标签的属性值。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;decorator:getProperty property=&quot;&quot; default=&quot;&quot;  writeEntireProperty=&quot;&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;将被装饰页面构造为一个对象，可以在装饰页面的JSP中直接引用。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;decorator:usePage id=&quot;&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;testjsp&quot;&gt;创建被装饰页面test.jsp&lt;/h2&gt;

&lt;p&gt;创建test.jsp如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;Hello world&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    &amp;lt;p&amp;gt;hello world.&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;测试&lt;/h2&gt;

&lt;p&gt;访问test.jsp页面，查看源代码会显示如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;My Site - Hello world&amp;lt;/title&amp;gt;
    &amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot; /&amp;gt;
    &amp;lt;meta http-equiv=&quot;Cache-Control&quot; content=&quot;no-store&quot; /&amp;gt;
    &amp;lt;meta http-equiv=&quot;Pragma&quot; content=&quot;no-cache&quot; /&amp;gt;
    &amp;lt;meta http-equiv=&quot;Expires&quot; content=&quot;0&quot; /&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
	&amp;lt;div id=&quot;content&quot;&amp;gt;
   		&amp;lt;p&amp;gt;hello world.&amp;lt;/p&amp;gt;
	&amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;5. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;https://github.com/sitemesh/sitemesh2#readme&quot;&gt;OpenSymphony SiteMesh Readme&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;https://github.com/springside/springside4/wiki/SiteMesh&quot;&gt;springside4 wiki:SiteMesh&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/01/13/about-sitemesh.html</link>
      <guid>http://blog.javachen.com/2014/01/13/about-sitemesh.html</guid>
      <pubDate>2014-01-13T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>如何创建一个Django网站</title>
      <description>&lt;p&gt;本文参考&lt;a href=&quot;https://docs.djangoproject.com/en/1.7/intro/&quot;&gt;官方文档&lt;/a&gt;演示如何创建一个简单的 django 网站，使用的 django 版本为1.7。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 创建项目&lt;/h1&gt;

&lt;p&gt;运行下面命令就可以创建一个 django 项目，项目名称叫 mysite ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ django-admin.py startproject mysite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建后的项目目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysite
├── manage.py
└── mysite
    ├── __init__.py
    ├── settings.py
    ├── urls.py
    └── wsgi.py

1 directory, 5 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;__init__.py&lt;/code&gt; ：让 Python 把该目录当成一个开发包 (即一组模块)所需的文件。 这是一个空文件，一般你不需要修改它。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;manage.py&lt;/code&gt; ：一种命令行工具，允许你以多种方式与该 Django 项目进行交互。 键入&lt;code&gt;python manage.py help&lt;/code&gt;，看一下它能做什么。 你应当不需要编辑这个文件；在这个目录下生成它纯是为了方便。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;settings.py&lt;/code&gt; ：该 Django 项目的设置或配置。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;urls.py&lt;/code&gt;：Django项目的URL路由设置。目前，它是空的。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;wsgi.py&lt;/code&gt;：WSGI web 应用服务器的配置文件。更多细节，查看 &lt;a href=&quot;https://docs.djangoproject.com/en/1.7/howto/deployment/wsgi/&quot;&gt;How to deploy with WSGI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接下来，你可以修改 settings.py 文件，例如：修改 &lt;code&gt;LANGUAGE_CODE&lt;/code&gt;、设置时区 &lt;code&gt;TIME_ZONE&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;SITE_ID = 1

LANGUAGE_CODE = &#39;zh_CN&#39;

TIME_ZONE = &#39;Asia/Shanghai&#39;

USE_TZ = True 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面开启了 &lt;a href=&quot;https://docs.djangoproject.com/en/1.7/topics/i18n/timezones/&quot;&gt;Time zone&lt;/a&gt; 特性，需要安装 pytz：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo pip install pytz
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 运行项目&lt;/h1&gt;

&lt;p&gt;在运行项目之前，我们需要创建数据库和表结构，这里我使用的默认数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py migrate
Operations to perform:
  Apply all migrations: admin, contenttypes, auth, sessions
Running migrations:
  Applying contenttypes.0001_initial... OK
  Applying auth.0001_initial... OK
  Applying admin.0001_initial... OK
  Applying sessions.0001_initial... OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py runserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你会看到下面的输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Performing system checks...

System check identified no issues (0 silenced).
January 28, 2015 - 02:08:33
Django version 1.7.1, using settings &#39;mysite.settings&#39;
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这将会在端口8000启动一个本地服务器, 并且只能从你的这台电脑连接和访问。 既然服务器已经运行起来了，现在用网页浏览器访问 &lt;a href=&quot;http://127.0.0.1:8000/&quot;&gt;http://127.0.0.1:8000/&lt;/a&gt;。你应该可以看到一个令人赏心悦目的淡蓝色 Django 欢迎页面它开始工作了。&lt;/p&gt;

&lt;p&gt;你也可以指定启动端口:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py runserver 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以及指定 ip：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py runserver 0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;app&quot;&gt;3. 创建 app&lt;/h1&gt;

&lt;p&gt;前面创建了一个项目并且成功运行，现在来创建一个 app，一个 app 相当于项目的一个子模块。&lt;/p&gt;

&lt;p&gt;在项目目录下创建一个 app：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py startapp polls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果操作成功，你会在 mysite 文件夹下看到已经多了一个叫 polls 的文件夹，目录结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;polls
├── __init__.py
├── admin.py
├── migrations
│   └── __init__.py
├── models.py
├── tests.py
└── views.py

1 directory, 6 files
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;4. 创建模型&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;每一个 Django Model 都继承自 django.db.models.Model&lt;/li&gt;
  &lt;li&gt;在 Model 当中每一个属性 attribute 都代表一个 database field&lt;/li&gt;
  &lt;li&gt;通过 Django Model API 可以执行数据库的增删改查, 而不需要写一些数据库的查询语句&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;打开 polls 文件夹下的 models.py 文件。创建两个模型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import datetime
from django.db import models
from django.utils import timezone

class Question(models.Model):
    question_text = models.CharField(max_length=200)
    pub_date = models.DateTimeField(&#39;date published&#39;)

    def was_published_recently(self):
        return self.pub_date &amp;gt;= timezone.now() - datetime.timedelta(days=1)


class Choice(models.Model):
    question = models.ForeignKey(Question)
    choice_text = models.CharField(max_length=200)
    votes = models.IntegerField(default=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在 mysite/settings.py 中修改 ` INSTALLED_APPS` 添加 polls：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;INSTALLED_APPS = (
    &#39;django.contrib.admin&#39;,
    &#39;django.contrib.auth&#39;,
    &#39;django.contrib.contenttypes&#39;,
    &#39;django.contrib.sessions&#39;,
    &#39;django.contrib.messages&#39;,
    &#39;django.contrib.staticfiles&#39;,
    &#39;polls&#39;,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在添加了新的 app 之后，我们需要运行下面命令告诉 Django 你的模型做了改变，需要迁移数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py makemigrations polls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你会看到下面的输出日志：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;Migrations for &#39;polls&#39;:
  0001_initial.py:
    - Create model Choice
    - Create model Question
    - Add field question to choice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以从 polls/migrations/0001_initial.py 查看迁移语句。&lt;/p&gt;

&lt;p&gt;运行下面语句，你可以查看迁移的 sql 语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py sqlmigrate polls 0001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;BEGIN;
CREATE TABLE &quot;polls_choice&quot; (&quot;id&quot; integer NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;choice_text&quot; varchar(200) NOT NULL, &quot;votes&quot; integer NOT NULL);
CREATE TABLE &quot;polls_question&quot; (&quot;id&quot; integer NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;question_text&quot; varchar(200) NOT NULL, &quot;pub_date&quot; datetime NOT NULL);
CREATE TABLE &quot;polls_choice__new&quot; (&quot;id&quot; integer NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;choice_text&quot; varchar(200) NOT NULL, &quot;votes&quot; integer NOT NULL, &quot;question_id&quot; integer NOT NULL REFERENCES &quot;polls_question&quot; (&quot;id&quot;));
INSERT INTO &quot;polls_choice__new&quot; (&quot;choice_text&quot;, &quot;votes&quot;, &quot;id&quot;) SELECT &quot;choice_text&quot;, &quot;votes&quot;, &quot;id&quot; FROM &quot;polls_choice&quot;;
DROP TABLE &quot;polls_choice&quot;;
ALTER TABLE &quot;polls_choice__new&quot; RENAME TO &quot;polls_choice&quot;;
CREATE INDEX polls_choice_7aa0f6ee ON &quot;polls_choice&quot; (&quot;question_id&quot;);

COMMIT;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以运行下面命令，来检查数据库是否有问题：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py check
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次运行下面的命令，来创建新添加的模型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py migrate
Operations to perform:
  Apply all migrations: admin, contenttypes, polls, auth, sessions
Running migrations:
  Applying polls.0001_initial... OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总结一下，当修改一个模型时，需要做以下几个步骤：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;修改 models.py 文件&lt;/li&gt;
  &lt;li&gt;运行 &lt;code&gt;python manage.py makemigrations&lt;/code&gt; 创建迁移语句&lt;/li&gt;
  &lt;li&gt;运行 &lt;code&gt;python manage.py migrate&lt;/code&gt;，将模型的改变迁移到数据库中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;你可以阅读 &lt;a href=&quot;https://docs.djangoproject.com/en/1.7/ref/django-admin/&quot;&gt;django-admin.py documentation&lt;/a&gt;，查看更多 manage.py 的用法。&lt;/p&gt;

&lt;p&gt;创建了模型之后，我们可以通过 Django 提供的 API 来做测试。运行下面命令可以进入到 python shell 的交互模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py shell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是一些测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from polls.models import Question, Choice   # Import the model classes we just wrote.

# No questions are in the system yet.
&amp;gt;&amp;gt;&amp;gt; Question.objects.all()
[]

# Create a new Question.
# Support for time zones is enabled in the default settings file, so
# Django expects a datetime with tzinfo for pub_date. Use timezone.now()
# instead of datetime.datetime.now() and it will do the right thing.
&amp;gt;&amp;gt;&amp;gt; from django.utils import timezone
&amp;gt;&amp;gt;&amp;gt; q = Question(question_text=&quot;What&#39;s new?&quot;, pub_date=timezone.now())

# Save the object into the database. You have to call save() explicitly.
&amp;gt;&amp;gt;&amp;gt; q.save()

# Now it has an ID. Note that this might say &quot;1L&quot; instead of &quot;1&quot;, depending
# on which database you&#39;re using. That&#39;s no biggie; it just means your
# database backend prefers to return integers as Python long integer
# objects.
&amp;gt;&amp;gt;&amp;gt; q.id
1

# Access model field values via Python attributes.
&amp;gt;&amp;gt;&amp;gt; q.question_text
&quot;What&#39;s new?&quot;
&amp;gt;&amp;gt;&amp;gt; q.pub_date
datetime.datetime(2012, 2, 26, 13, 0, 0, 775217, tzinfo=&amp;lt;UTC&amp;gt;)

# Change values by changing the attributes, then calling save().
&amp;gt;&amp;gt;&amp;gt; q.question_text = &quot;What&#39;s up?&quot;
&amp;gt;&amp;gt;&amp;gt; q.save()

# objects.all() displays all the questions in the database.
&amp;gt;&amp;gt;&amp;gt; Question.objects.all()
[&amp;lt;Question: Question object&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打印所有的 Question 时，输出的结果是 &lt;code&gt;[&amp;lt;Question: Question object&amp;gt;]&lt;/code&gt;，我们可以修改模型类，使其输出更为易懂的描述。修改模型类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from django.db import models

class Question(models.Model):
    # ...
    def __str__(self):              # __unicode__ on Python 2
        return self.question_text

class Choice(models.Model):
    # ...
    def __str__(self):              # __unicode__ on Python 2
        return self.choice_text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来继续测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from polls.models import Question, Choice

# Make sure our __str__() addition worked.
&amp;gt;&amp;gt;&amp;gt; Question.objects.all()
[&amp;lt;Question: What&#39;s up?&amp;gt;]

# Django provides a rich database lookup API that&#39;s entirely driven by
# keyword arguments.
&amp;gt;&amp;gt;&amp;gt; Question.objects.filter(id=1)
[&amp;lt;Question: What&#39;s up?&amp;gt;]
&amp;gt;&amp;gt;&amp;gt; Question.objects.filter(question_text__startswith=&#39;What&#39;)
[&amp;lt;Question: What&#39;s up?&amp;gt;]

# Get the question that was published this year.
&amp;gt;&amp;gt;&amp;gt; from django.utils import timezone
&amp;gt;&amp;gt;&amp;gt; current_year = timezone.now().year
&amp;gt;&amp;gt;&amp;gt; Question.objects.get(pub_date__year=current_year)
&amp;lt;Question: What&#39;s up?&amp;gt;

# Request an ID that doesn&#39;t exist, this will raise an exception.
&amp;gt;&amp;gt;&amp;gt; Question.objects.get(id=2)
Traceback (most recent call last):
    ...
DoesNotExist: Question matching query does not exist.

# Lookup by a primary key is the most common case, so Django provides a
# shortcut for primary-key exact lookups.
# The following is identical to Question.objects.get(id=1).
&amp;gt;&amp;gt;&amp;gt; Question.objects.get(pk=1)
&amp;lt;Question: What&#39;s up?&amp;gt;

# Make sure our custom method worked.
&amp;gt;&amp;gt;&amp;gt; q = Question.objects.get(pk=1)

# Give the Question a couple of Choices. The create call constructs a new
# Choice object, does the INSERT statement, adds the choice to the set
# of available choices and returns the new Choice object. Django creates
# a set to hold the &quot;other side&quot; of a ForeignKey relation
# (e.g. a question&#39;s choice) which can be accessed via the API.
&amp;gt;&amp;gt;&amp;gt; q = Question.objects.get(pk=1)

# Display any choices from the related object set -- none so far.
&amp;gt;&amp;gt;&amp;gt; q.choice_set.all()
[]

# Create three choices.
&amp;gt;&amp;gt;&amp;gt; q.choice_set.create(choice_text=&#39;Not much&#39;, votes=0)
&amp;lt;Choice: Not much&amp;gt;
&amp;gt;&amp;gt;&amp;gt; q.choice_set.create(choice_text=&#39;The sky&#39;, votes=0)
&amp;lt;Choice: The sky&amp;gt;
&amp;gt;&amp;gt;&amp;gt; c = q.choice_set.create(choice_text=&#39;Just hacking again&#39;, votes=0)

# Choice objects have API access to their related Question objects.
&amp;gt;&amp;gt;&amp;gt; c.question
&amp;lt;Question: What&#39;s up?&amp;gt;

# And vice versa: Question objects get access to Choice objects.
&amp;gt;&amp;gt;&amp;gt; q.choice_set.all()
[&amp;lt;Choice: Not much&amp;gt;, &amp;lt;Choice: The sky&amp;gt;, &amp;lt;Choice: Just hacking again&amp;gt;]
&amp;gt;&amp;gt;&amp;gt; q.choice_set.count()
3

# The API automatically follows relationships as far as you need.
# Use double underscores to separate relationships.
# This works as many levels deep as you want; there&#39;s no limit.
# Find all Choices for any question whose pub_date is in this year
# (reusing the &#39;current_year&#39; variable we created above).
&amp;gt;&amp;gt;&amp;gt; Choice.objects.filter(question__pub_date__year=current_year)
[&amp;lt;Choice: Not much&amp;gt;, &amp;lt;Choice: The sky&amp;gt;, &amp;lt;Choice: Just hacking again&amp;gt;]

# Let&#39;s delete one of the choices. Use delete() for that.
&amp;gt;&amp;gt;&amp;gt; c = q.choice_set.filter(choice_text__startswith=&#39;Just hacking&#39;)
&amp;gt;&amp;gt;&amp;gt; c.delete()
&amp;gt;&amp;gt;&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这部分测试，涉及到  django orm 相关的知识，详细说明可以参考 &lt;a href=&quot;/2015/01/15/django-orm.html&quot;&gt;Django中的ORM&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;admin&quot;&gt;5. 管理 admin&lt;/h1&gt;

&lt;p&gt;Django有一个优秀的特性, 内置了Django admin后台管理界面, 方便管理者进行添加和删除网站的内容.&lt;/p&gt;

&lt;p&gt;新建的项目系统已经为我们设置好了后台管理功能，见 mysite/settings.py：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;INSTALLED_APPS = (
    &#39;django.contrib.admin&#39;, #默认添加后台管理功能
    &#39;django.contrib.auth&#39;,
    &#39;django.contrib.contenttypes&#39;,
    &#39;django.contrib.sessions&#39;,
    &#39;django.contrib.messages&#39;,
    &#39;django.contrib.staticfiles&#39;,
    &#39;polls&#39;,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时也已经添加了进入后台管理的 url, 可以在  mysite/urls.py 中查看：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;url(r&#39;^admin/&#39;, include(admin.site.urls)), #可以使用设置好的url进入网站后台
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来我们需要创建一个管理用户来登录 admin 后台管理界面：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ python manage.py createsuperuser
Username (leave blank to use &#39;june&#39;): admin
Email address:
Password:
Password (again):
Superuser created successfully.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面创建了一个 admin 的超级用户，密码也为 admin。&lt;/p&gt;

&lt;p&gt;再次运行项目：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python manage.py runserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，访问web 页面 &lt;a href=&quot;http://127.0.0.1:8000/admin/&quot;&gt;http://127.0.0.1:8000/admin/&lt;/a&gt;，你将会看到：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.djangoproject.com/en/1.7/_images/admin01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;输入用户名和密码，你可以看到：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.djangoproject.com/en/1.7/_images/admin02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这时候你可以看到，你可以修改 groups 和 users 两个对象，但是你不能修改你添加的模型。下面来使你添加的模型也能修改，修改 polls/admin.py：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.contrib import admin
from polls.models import Choice, Question

class ChoiceInline(admin.StackedInline):
    model = Choice
    extra = 3

class QuestionAdmin(admin.ModelAdmin):
    fieldsets = [
        (None,               {&#39;fields&#39;: [&#39;question_text&#39;]}),
        (&#39;Date information&#39;, {&#39;fields&#39;: [&#39;pub_date&#39;], &#39;classes&#39;: [&#39;collapse&#39;]}),
    ]
    inlines = [ChoiceInline]
    list_filter = [&#39;pub_date&#39;]
    list_display = (&#39;question_text&#39;, &#39;pub_date&#39;, &#39;was_published_recently&#39;)

admin.site.register(Choice)
admin.site.register(Question, QuestionAdmin)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你单击表头时，会发现 was_published_recently 这一列无法排序，我们可以修改  polls/models.py 中的 Question 模型，额外添加一些属性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Question(models.Model):
    # ...
    def was_published_recently(self):
        return self.pub_date &amp;gt;= timezone.now() - datetime.timedelta(days=1)
    was_published_recently.admin_order_field = &#39;pub_date&#39;
    was_published_recently.boolean = True
    was_published_recently.short_description = &#39;Published recently?&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;定制外观&lt;/h2&gt;

&lt;p&gt;在项目根路径创建一个 templates 目录，并修改 mysite/settings.py 文件，添加下面设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;TEMPLATE_DIRS = [os.path.join(BASE_DIR, &#39;templates&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 templates  下面创建一个 admin 目录，并从 Django admin 模板路径下拷贝 admin/base_site.html 文件到 admin 目录下，修改该文件：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% block branding %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;h1&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;site-name&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% url &amp;#39;admin:index&amp;#39; %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Polls Administration&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;
{% endblock %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;同样，你可以参考上面定制 admin/index.html 内容。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;使用第三方插件&lt;/h2&gt;

&lt;p&gt;Django现在已经相对成熟，已经有许多不错的可以使用的第三方插件可以使用，这些插件各种各样，现在我们使用一个第三方插件使后台管理界面更加美观，目前大部分第三方插件可以在 &lt;a href=&quot;https://www.djangopackages.com/&quot;&gt;Django Packages&lt;/a&gt; 中查看，尝试使用 &lt;a href=&quot;https://github.com/douglasmiranda/django-admin-bootstrap&quot;&gt;django-admin-bootstrap&lt;/a&gt; 美化后台管理界面。&lt;/p&gt;

&lt;p&gt;安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pip install bootstrap-admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在 mysite/settings.py 中修改 INSTALLED_APPS：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;INSTALLED_APPS = (
    &#39;bootstrap_admin&#39;,  #一定要放在`django.contrib.admin`前面
    &#39;django.contrib.admin&#39;,
    &#39;django.contrib.auth&#39;,
    &#39;django.contrib.contenttypes&#39;,
    &#39;django.contrib.sessions&#39;,
    &#39;django.contrib.messages&#39;,
    &#39;django.contrib.staticfiles&#39;,
    &#39;polls&#39;,
)

from django.conf import global_settings
TEMPLATE_CONTEXT_PROCESSORS = global_settings.TEMPLATE_CONTEXT_PROCESSORS + (
    &#39;django.core.context_processors.request&#39;,
)
BOOTSTRAP_ADMIN_SIDEBAR_MENU = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保存后，再次刷新页面，访问 &lt;a href=&quot;http://127.0.0.1:8000/admin&quot;&gt;http://127.0.0.1:8000/admin&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;url&quot;&gt;6. 视图和URL配置&lt;/h1&gt;

&lt;p&gt;Django 中 views 里面的代码就是一个一个函数逻辑，处理客户端(浏览器)发送的 HTTPRequest，然后返回 HTTPResponse。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;第一个视图&lt;/h2&gt;

&lt;p&gt;创建 &lt;code&gt;polls/views.py &lt;/code&gt; 文件并添加如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.http import HttpResponse

def index(request):
    return HttpResponse(&quot;Hello, world. You&#39;re at the polls index.&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;polls/urls.py&lt;/code&gt; 中添加一个 url 映射：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.conf.urls import patterns, url
from polls import views

urlpatterns = patterns(&#39;&#39;,
    url(r&#39;^$&#39;, views.index, name=&#39;index&#39;),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候 polls 目录结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;polls
├── __init__.py
├── admin.py
├── migrations
│   ├── 0001_initial.py
│   ├── __init__.py
├── models.py
├── tests.py
├── urls.py
└── views.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上面视图文件中，我们只是告诉 Django，所有指向 URL &lt;code&gt;/index/&lt;/code&gt; 的请求都应由 index 这个视图函数来处理。&lt;/p&gt;

&lt;p&gt;Django 在检查 URL 模式前，移除每一个申请的URL开头的斜杠。 这意味着我们为 &lt;code&gt;/index/&lt;/code&gt; 写URL模式不用包含斜杠。&lt;/p&gt;

&lt;p&gt;模式包含了一个尖号(&lt;code&gt;^&lt;/code&gt;)和一个美元符号(&lt;code&gt;$&lt;/code&gt;)。这些都是正则表达式符号，并且有特定的含义： ^要求表达式对字符串的头部进行匹配，$符号则要求表达式对字符串的尾部进行匹配。&lt;/p&gt;

&lt;p&gt;如果你访问 &lt;code&gt;/index&lt;/code&gt;，默认会重定向到末尾带有反斜杠的请求上去，这是受配置文件setting中 &lt;code&gt;APPEND_SLASH&lt;/code&gt; 项控制的。&lt;/p&gt;

&lt;p&gt;如果你是喜欢所有URL都以 &lt;code&gt;/&lt;/code&gt; 结尾的人（Django开发者的偏爱），那么你只需要在每个 URL 后添加斜杠，并且设置 &lt;code&gt;APPEND_SLASH&lt;/code&gt; 为 “True”。如果不喜欢URL以斜杠结尾或者根据每个 URL 来决定，那么需要设置 &lt;code&gt;APPEND_SLASH&lt;/code&gt; 为 “False”，并且根据你自己的意愿来添加结尾斜杠/在URL模式后。&lt;/p&gt;

&lt;p&gt;接下来配置项目根路径的 URL 映射，修改  mysite/urls.py 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.conf.urls import patterns, include, url
from django.contrib import admin

urlpatterns = patterns(&#39;&#39;,
    url(r&#39;^polls/&#39;, include(&#39;polls.urls&#39;)),
    url(r&#39;^admin/&#39;, include(admin.site.urls)),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开浏览器，访问 &lt;a href=&quot;http://localhost:8000/polls/&quot;&gt;http://localhost:8000/polls/&lt;/a&gt;，你将会看到 &lt;code&gt;Hello, world. You’re at the polls index.&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;编写更多视图&lt;/h2&gt;

&lt;p&gt;修改  polls/views.py 文件，添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def detail(request, question_id):
    return HttpResponse(&quot;You&#39;re looking at question %s.&quot; % question_id)

def results(request, question_id):
    response = &quot;You&#39;re looking at the results of question %s.&quot;
    return HttpResponse(response % question_id)

def vote(request, question_id):
    return HttpResponse(&quot;You&#39;re voting on question %s.&quot; % question_id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，修改 polls/urls.py 添加映射：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.conf.urls import patterns, url

from polls import views

urlpatterns = patterns(&#39;&#39;,
    # ex: /polls/
    url(r&#39;^$&#39;, views.index, name=&#39;index&#39;),
    # ex: /polls/5/
    url(r&#39;^(?P&amp;lt;question_id&amp;gt;\d+)/$&#39;, views.detail, name=&#39;detail&#39;),
    # ex: /polls/5/results/
    url(r&#39;^(?P&amp;lt;question_id&amp;gt;\d+)/results/$&#39;, views.results, name=&#39;results&#39;),
    # ex: /polls/5/vote/
    url(r&#39;^(?P&amp;lt;question_id&amp;gt;\d+)/vote/$&#39;, views.vote, name=&#39;vote&#39;),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-7&quot;&gt;正则表达式&lt;/h2&gt;

&lt;p&gt;正则表达式是通用的文本模式匹配的方法。 Django URLconfs 允许你使用任意的正则表达式来做强有力的 URL 映射，不过通常你实际上可能只需要使用很少的一部分功能。这里是一些基本的语法。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;符号&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;匹配&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;任意单一字符&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;\d&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;任意一位数字&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;[A-Z]&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;A 到 Z中任意一个字符（大写）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;[a-z]&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;a 到 z中任意一个字符（小写）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;[A-Za-z]&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;a 到 z中任意一个字符（不区分大小写）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;+&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;匹配一个或更多 (例如, \d+ 匹配一个或 多个数字字符)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;[^/]+&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;一个或多个不为‘/’的字符&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;*&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;零个或一个之前的表达式（例如：\d? 匹配零个或一个数字）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;*&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;匹配0个或更多 (例如, \d* 匹配0个 或更多数字字符)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;{1,3}&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;介于一个和三个（包含）之前的表达式（例如，\d{1,3}匹配一个或两个或三个数字）&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;有关正则表达式的更多内容，请访问&lt;a href=&quot;http://www.djangoproject.com/r/python/re-module/&quot;&gt;http://www.djangoproject.com/r/python/re-module/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;动态内容&lt;/h2&gt;

&lt;p&gt;接下来使视图返回动态内容，修改 mysite/views.py 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.http import HttpResponse

from polls.models import Question

def index(request):
    latest_question_list = Question.objects.order_by(&#39;-pub_date&#39;)[:5]
    output = &#39;, &#39;.join([p.question_text for p in latest_question_list])
    return HttpResponse(output)

# Leave the rest of the views (detail, results, vote) unchanged
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候访问 &lt;a href=&quot;http://127.0.0.1:8000/polls/&quot;&gt;http://127.0.0.1:8000/polls/&lt;/a&gt; 可以看到输出内容为所有的问题通过逗号连接。&lt;/p&gt;

&lt;h2 id=&quot;section-9&quot;&gt;使用模板&lt;/h2&gt;

&lt;p&gt;接下来在视图中使用模板，在 settings.py 中有一个 &lt;code&gt;TEMPLATE_LOADERS&lt;/code&gt; 属性，并且有一个默认值  &lt;code&gt;django.template.loaders.app_directories.Loader&lt;/code&gt;，该值定义了从每一个安装的 app 的 templates 目录下寻找模板，故我们可以在 polls 目录下创建 templates 目录以及其子目录 polls。&lt;/p&gt;

&lt;p&gt;现在创建一个首页页面  polls/templates/polls/index.html：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;{% if latest_question_list %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;ul&amp;gt;&lt;/span&gt;
    {% for question in latest_question_list %}
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;li&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/polls/{{ question.id }}/&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ question.question_text }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
    {% endfor %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/ul&amp;gt;&lt;/span&gt;
{% else %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;No polls are available.&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
{% endif %}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;br /&gt;
页面中的 latest_question_list 变量来自视图返回的上下文，见下面 context 变量&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后更新 polls/views.py 视图，来使用模板：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.http import HttpResponse
from django.template import RequestContext, loader

from polls.models import Question

def index(request):
    latest_question_list = Question.objects.order_by(&#39;-pub_date&#39;)[:5]
    template = loader.get_template(&#39;polls/index.html&#39;)
    context = RequestContext(request, {
        &#39;latest_question_list&#39;: latest_question_list,
    })
    return HttpResponse(template.render(context))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;通过 loader 来加载模板页面，如前面提到的，这里是相对 polls/templates 目录&lt;/li&gt;
  &lt;li&gt;创建上下文，将需要传递到页面的变量放入上下文变量 context&lt;/li&gt;
  &lt;li&gt;使用 template 通过上下文来渲染页面&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面代码，可以使用  &lt;code&gt;render()&lt;/code&gt; 来简化：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.shortcuts import render
from polls.models import Question

def index(request):
    latest_question_list = Question.objects.order_by(&#39;-pub_date&#39;)[:5]
    context = {&#39;latest_question_list&#39;: latest_question_list}
    return render(request, &#39;polls/index.html&#39;, context)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当查询数据返回错误时，可以抛出异常，让页面重定向到 404 页面：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.http import Http404
from django.shortcuts import render

from polls.models import Question
# ...
def detail(request, question_id):
    try:
        question = Question.objects.get(pk=question_id)
    except Question.DoesNotExist:
        raise Http404(&quot;Question does not exist&quot;)
    return render(request, &#39;polls/detail.html&#39;, {&#39;question&#39;: question})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，我们可以回过头来修改  polls/index.html 页面中下面代码：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;li&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/polls/{{ question.id }}/&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ question.question_text }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;上面代码中的 url 为硬编码的，我们可以将其修改为：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;li&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{ % url  &amp;#39;detail&amp;#39; question.id  %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ question.question_text }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;为了避免多个视图的名称，例如 detail 视图，的冲突，我们可以给 URL 添加命名空间，修改 mysite/urls.py：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.conf.urls import patterns, include, url
from django.contrib import admin

urlpatterns = patterns(&#39;&#39;,
    url(r&#39;^polls/&#39;, include(&#39;polls.urls&#39;, namespace=&quot;polls&quot;)),
    url(r&#39;^admin/&#39;, include(admin.site.urls)),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，修改 polls/templates/polls/index.html 为：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;li&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{ % url &amp;#39;polls:detail&amp;#39; question.id % }&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ question.question_text }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;接下来创建明细页面 polls/templates/polls/detail.html：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;h1&amp;gt;&lt;/span&gt;{{ question.question_text }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;

{% if error_message %}&lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;&lt;/span&gt;{{ error_message }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/strong&amp;gt;&amp;lt;/p&amp;gt;&lt;/span&gt;{% endif %}

&lt;span class=&quot;nt&quot;&gt;&amp;lt;form&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;action=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% url &amp;#39;polls:vote&amp;#39; question.id %}&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;method=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
{% csrf_token %}
{% for choice in question.choice_set.all %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;input&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;radio&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;choice&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;choice{{ forloop.counter }}&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{{ choice.id }}&amp;quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;label&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;for=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;choice{{ forloop.counter }}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ choice.choice_text }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/label&amp;gt;&amp;lt;br&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
{% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;input&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;submit&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Vote&amp;quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/form&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;创建一个显示投票结果页面 polls/templates/polls/results.html ：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;h1&amp;gt;&lt;/span&gt;{{ question.question_text }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;ul&amp;gt;&lt;/span&gt;
{% for choice in question.choice_set.all %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;li&amp;gt;&lt;/span&gt;{{ choice.choice_text }} -- {{ choice.votes }} vote{{ choice.votes|pluralize }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
{% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/ul&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{% url &amp;#39;polls:detail&amp;#39; question.id %}&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Vote again?&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;接下来修改 polls/views.py 添加 detail 、 vote 和 results 三个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.shortcuts import get_object_or_404, render
from django.http import HttpResponseRedirect, HttpResponse
from django.core.urlresolvers import reverse

from polls.models import Question,Choice

# ...
def detail(request, question_id):
    question = get_object_or_404(Question, pk=question_id)
    return render(request, &#39;polls/detail.html&#39;, {&#39;question&#39;: question})

def vote(request, question_id):
    p = get_object_or_404(Question, pk=question_id)
    try:
        selected_choice = p.choice_set.get(pk=request.POST[&#39;choice&#39;])
    except (KeyError, Choice.DoesNotExist):
        # Redisplay the question voting form.
        return render(request, &#39;polls/detail.html&#39;, {
            &#39;question&#39;: p,
            &#39;error_message&#39;: &quot;You didn&#39;t select a choice.&quot;,
        })
    else:
        selected_choice.votes += 1
        selected_choice.save()
        # Always return an HttpResponseRedirect after successfully dealing
        # with POST data. This prevents data from being posted twice if a
        # user hits the Back button.
        return HttpResponseRedirect(reverse(&#39;polls:results&#39;, args=(p.id,)))    

def results(request, question_id):
    question = get_object_or_404(Question, pk=question_id)
    return render(request, &#39;polls/results.html&#39;, {&#39;question&#39;: question})        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启服务进行测试，检查是否有错误。&lt;/p&gt;

&lt;h2 id=&quot;section-10&quot;&gt;简化代码&lt;/h2&gt;

&lt;p&gt;如果我们要针对很多模型编写视图，则有很多代码都是重复的，我们可以使用 Django 中的通用视图来简化代码。首先，我们将  polls/urls.py 改成如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.conf.urls import patterns, url

from polls import views

urlpatterns = patterns(&#39;&#39;,
    url(r&#39;^$&#39;, views.IndexView.as_view(), name=&#39;index&#39;),
    url(r&#39;^(?P&amp;lt;pk&amp;gt;\d+)/$&#39;, views.DetailView.as_view(), name=&#39;detail&#39;),
    url(r&#39;^(?P&amp;lt;pk&amp;gt;\d+)/results/$&#39;, views.ResultsView.as_view(), name=&#39;results&#39;),
    url(r&#39;^(?P&amp;lt;question_id&amp;gt;\d+)/vote/$&#39;, views.vote, name=&#39;vote&#39;),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，我们删除 polls/views.py 中的 index、detail 和 results 三个函数，vote 函数和之前保持一致，最后改为如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from django.shortcuts import get_object_or_404, render
from django.http import HttpResponseRedirect
from django.core.urlresolvers import reverse
from django.views import generic

from polls.models import Choice, Question


class IndexView(generic.ListView):
    template_name = &#39;polls/index.html&#39;
    context_object_name = &#39;latest_question_list&#39;

    def get_queryset(self):
        &quot;&quot;&quot;Return the last five published questions.&quot;&quot;&quot;
        return Question.objects.order_by(&#39;-pub_date&#39;)[:5]


class DetailView(generic.DetailView):
    model = Question
    template_name = &#39;polls/detail.html&#39;


class ResultsView(generic.DetailView):
    model = Question
    template_name = &#39;polls/results.html&#39;


def vote(request, question_id):
    ... # same as above
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这里我们使用了两个通用视图 : ListView 和 DetailView，来和模型相关联。&lt;/li&gt;
  &lt;li&gt;DetailView 视图需要传递一个名称为 pk 表示模型主键的参数，所以我们需要把 URL 映射中的 question_id  改为 pk。&lt;/li&gt;
  &lt;li&gt;默认情况下， DetailView使用  &lt;code&gt;&amp;lt;app name&amp;gt;/&amp;lt;model name&amp;gt;_detail.html&lt;/code&gt; 模板，我们可以使用 &lt;code&gt;template_name&lt;/code&gt; 变量重新定义模板路径；类似的， ListView 默认使用 &lt;code&gt;&amp;lt;app name&amp;gt;/&amp;lt;model name&amp;gt;_list.html&lt;/code&gt; 模板。&lt;/li&gt;
  &lt;li&gt;DetailView 视图中的上下文默认就包含了对模型的引用，如 question 变量；对于 ListView 视图，为 question_list 变量，如果我们想修改该变量名称，可以通过 &lt;code&gt;context_object_name 变量覆盖默认的名称&lt;/code&gt;，如使用 latest_question_list 代替 question_list。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多的文档，请参考 &lt;a href=&quot;https://docs.djangoproject.com/en/1.7/topics/class-based-views/&quot;&gt;generic views documentation&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-11&quot;&gt;7. 总结&lt;/h1&gt;

&lt;p&gt;最后，来看项目目录结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysite
├── db.sqlite3
├── manage.py
├── mysite
│   ├── __init__.py
│   ├── settings.py
│   ├── urls.py
│   ├── wsgi.py
├── polls
│   ├── __init__.py
│   ├── admin.py
│   ├── migrations
│   │   ├── 0001_initial.py
│   │   ├── __init__.py
│   ├── models.py
│   ├── templates
│   │   └── polls
│   │       ├── detail.html
│   │       ├── index.html
│   │       └── results.html
│   ├── tests.py
│   ├── urls.py
│   ├── views.py
└── templates
    └── admin
        └── base_site.htm      
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过上面的介绍，对 django 的安装、运行以及如何创建视图和模型有了一个清晰的认识，接下来就可以深入的学习 django 的自动化测试、持久化、中间件、国际化等知识。&lt;/p&gt;

&lt;h1 id=&quot;section-12&quot;&gt;8. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.djangoproject.com/en/1.7/intro/&quot;&gt;Django1.7官方文档&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://markchen.me/django-instance-tutorial-blog-1/&quot;&gt;django实例教程–blog(1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://djangobook.py3k.cn/2.0/&quot;&gt;The Django book 2.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://andrew-liu.gitbooks.io/django-blog/content/&quot;&gt;Django搭建简易博客&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2014/01/11/how-to-create-a-django-site.html</link>
      <guid>http://blog.javachen.com/2014/01/11/how-to-create-a-django-site.html</guid>
      <pubDate>2014-01-11T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>重装Linux-Mint系统之后</title>
      <description>&lt;p&gt;本文主要记录重装Linux-Mint系统之后的一些软件安装和环境变量配置。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;安装常用工具&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install ctags curl vsftpd git vim tmux meld htop putty subversion  nload  iptraf iftop tree openssh-server gconf-editor gnome-tweak-tool
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;exfat&quot;&gt;挂载exfat格式磁盘&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install exfat-fuse exfat-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;ibus&quot;&gt;安装ibus&lt;/h1&gt;

&lt;p&gt;在终端输入命令:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo add-apt-repository ppa:shawn-p-huang/ppa
sudo apt-get update
sudo apt-get install ibus-gtk ibus-pinyin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启用IBus框架:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;im-switch -s ibus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动ibus：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ibus-daemon
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;gedit-markdown&quot;&gt;安装gedit-markdown&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;wget https://gitorious.org/projets-divers/gedit-markdown/archive/master.zip
unzip master.zip
cd projets-divers-gedit-markdown
./gedit-markdown.sh install
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;wiz&quot;&gt;安装wiz&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;sudo add-apt-repository ppa:wiznote-team
sudo apt-get update
sudo apt-get install wiznote
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;oh-my-zsh&quot;&gt;安装 oh-my-zsh&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install zsh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把默认 Shell 换为 zsh。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chsh -s /bin/zsh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后用下面的两句（任选其一）可以自动安装 oh-my-zsh：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;wget --no-check-certificate https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑&lt;code&gt;~/.zshrc&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;source ~/.bashrc&#39; &amp;gt;&amp;gt;~/.zshrc
echo &#39;source ~/.bash_profile&#39; &amp;gt;&amp;gt;~/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;博客相关&lt;/h1&gt;
&lt;p&gt;## 安装Ruby&lt;/p&gt;

&lt;p&gt;通过rvm安装ruby：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -L get.rvm.io | bash -s stable
source ~/.bash_profile
sed -i -e &#39;s/ftp\.ruby-lang\.org\/pub\/ruby/ruby\.taobao\.org\/mirrors\/ruby/g&#39; ~/.rvm/config/db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装rvm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install clang -y
rvm install 1.9.3 --with-gcc=clang
rvm --default 1.9.3
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;jekyll&quot;&gt;安装jekyll&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;gem update --system
gem install rdoc jekyll redcarpet
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;安装七牛同步脚本&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;wget http://devtools.qiniudn.com/linux_amd64/qiniu-devtools.zip
unzip qiniu-devtools.zip
mv _package_linux_amd64/* /usr/bin/
source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ssh-key&quot;&gt;更新ssh-key&lt;/h2&gt;

&lt;p&gt;更新github和gitcafe的ssh-key&lt;/p&gt;

&lt;h1 id=&quot;virtualbox&quot;&gt;安装 virtualbox&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;wget -q http://download.virtualbox.org/virtualbox/debian/oracle_vbox.asc -O- | sudo apt-key add -
sudo apt-get update
sudo apt-get install virtualbox-4.3
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;fortune-zh&quot;&gt;安装fortune-zh&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install fortune-zh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/usr/bin/mint-fortune&lt;/code&gt; 中调用语句改为:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/usr/games/fortune 70% tang300 30% song100 | $command -f $cow -n
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的70%与30%是显示唐诗与宋词的概率。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gsettings set com.linuxmint.terminal show-fortunes true
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;修改分区权限&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;sudo chown -R june:june /chan
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;home&quot;&gt;重命名home下目录&lt;/h1&gt;

&lt;p&gt;先手动重命名:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;文档   ---&amp;gt;  projects
音乐   ---&amp;gt;  app
图片   ---&amp;gt;  codes
视频   ---&amp;gt;  workspace
下载   ---&amp;gt;  downloads
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后,删除这些目录,建立软连接:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm -rf projects app codes workspace downloads
ln -s /chan/app  ~/app
ln -s /chan/codes   ~/codes
ln -s /chan/projects  ~/projects
ln -s /chan/workspace  ~/workspace
ln -s /chan/downloads    ~/downloads
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;安装开发环境&lt;/h1&gt;

&lt;p&gt;配置ant、maven和ivy仓库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod +x /chan/app/apache/apache-maven-3.0.5/bin/mvn
chmod +x /chan/app/apache/apache-ant-1.9.4/bin/ant

rm -rf /home/june/.ivy2/cache /home/june/.m2/repository
mkdir -p /home/june/.ivy2 /home/june/.m2
ln -s /chan/app/repository/cache/  /home/june/.ivy2/cache
ln -s /chan/app/repository/m2/  /home/june/.m2/repository
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装jdk1.6&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://archive.cloudera.com/cm4/ubuntu/precise/amd64/cm/pool/contrib/o/oracle-j2sdk1.6/oracle-j2sdk1.6_1.6.0+update31_amd64.deb
dpkg -i oracle-j2sdk1.6_1.6.0+update31_amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置环境变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo mkdir -p /usr/java/
sudo ln -s /usr/lib/jvm/j2sdk1.6-oracle /usr/java/latest
sudo update-alternatives --install /usr/bin/java java /usr/java/latest/bin/java 5
sudo update-alternatives --install /usr/bin/javac javac /usr/java/latest/bin/javac 5
sudo update-alternatives --set java /usr/java/latest/bin/java


if [ -f ~/.bashrc ] ; then
    sed -i &#39;/^export[[:space:]]\{1,\}JAVA_HOME[[:space:]]\{0,\}=/d&#39; ~/.bashrc
    sed -i &#39;/^export[[:space:]]\{1,\}CLASSPATH[[:space:]]\{0,\}=/d&#39; ~/.bashrc
    sed -i &#39;/^export[[:space:]]\{1,\}PATH[[:space:]]\{0,\}=/d&#39; ~/.bashrc
fi
echo &quot;export JAVA_HOME=/usr/java/latest&quot; &amp;gt;&amp;gt; ~/.bashrc
echo &quot;export CLASSPATH=.:\$JAVA_HOME/lib/tools.jar:\$JAVA_HOME/lib/dt.jar&quot;&amp;gt;&amp;gt;~/.bashrc
echo &quot;export MVN_HOME=/chan/app/apache/apache-maven-3.0.5&quot; &amp;gt;&amp;gt; ~/.bashrc

echo &quot;export ANT_HOME=/chan/app/apache/apache-ant-1.9.4&quot; &amp;gt;&amp;gt; ~/.bashrc
echo &quot;export PATH=\$JAVA_HOME/bin:\$MVN_HOME/bin:\$ANT_HOME/bin:\$PATH&quot; &amp;gt;&amp;gt; ~/.bashrc
source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2014/01/09/after-reinstall-the-system.html</link>
      <guid>http://blog.javachen.com/2014/01/09/after-reinstall-the-system.html</guid>
      <pubDate>2014-01-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive使用HAProxy配置HA</title>
      <description>&lt;p&gt;HAProxy是一款提供高可用性、负载均衡以及基于TCP（第四层）和HTTP（第七层）应用的代理软件，HAProxy是完全免费的、借助HAProxy可以快速并且可靠的提供基于TCP和HTTP应用的代理解决方案。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;免费开源，稳定性也是非常好，这个可通过我做的一些小项目可以看出来，单Haproxy也跑得不错，稳定性可以与硬件级的F5相媲美。&lt;br /&gt;
根据官方文档，HAProxy可以跑满10Gbps-New benchmark of HAProxy at 10 Gbps using Myricom’s 10GbE NICs （Myri-10G PCI-Express），这个数值作为软件级负载均衡器是相当惊人的。&lt;/li&gt;
  &lt;li&gt;HAProxy 支持连接拒绝 : 因为维护一个连接的打开的开销是很低的，有时我们很需要限制攻击蠕虫（attack bots），也就是说限制它们的连接打开从而限制它们的危害。 这个已经为一个陷于小型DDoS攻击的网站开发了而且已经拯救了很多站点，这个优点也是其它负载均衡器没有的。&lt;/li&gt;
  &lt;li&gt;HAProxy 支持全透明代理（已具备硬件防火墙的典型特点）: 可以用客户端IP地址或者任何其他地址来连接后端服务器. 这个特性仅在Linux 2.4/2.6内核打了cttproxy补丁后才可以使用. 这个特性也使得为某特殊服务器处理部分流量同时又不修改服务器的地址成为可能。&lt;/li&gt;
  &lt;li&gt;HAProxy现多于线上的Mysql集群环境，我们常用于它作为MySQL（读）负载均衡；&lt;/li&gt;
  &lt;li&gt;自带强大的监控服务器状态的页面，实际环境中我们结合Nagios进行邮件或短信报警，这个也是我非常喜欢它的原因之一；&lt;/li&gt;
  &lt;li&gt;HAProxy支持虚拟主机，许多朋友说它不支持虚拟主机是错误的，通过测试我们知道，HAProxy是支持虚拟主机的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HAProxy特别适用于那些负载特大的web站点， 这些站点通常又需要会话保持或七层处理。HAProxy运行在当前的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;安装配置&lt;/h1&gt;

&lt;p&gt;在&lt;a href=&quot;http://haproxy.1wt.eu/&quot;&gt;HAProxy&lt;/a&gt;官网下载安装包并编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://haproxy.1wt.eu/download/1.4/src/haproxy-1.4.24.tar.gz|tar zxvf
mv haproxy-1.4.24 /opt/haproxy-1.4.24
cd /opt/haproxy-1.4.24
make TARGET=linux26
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;添加配置文件&lt;/h1&gt;

&lt;p&gt;在/opt/haproxy-1.4.24目录下创建一个config.cfg文件，添加如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global
        daemon
        nbproc 1
        pidfile /var/run/haproxy.pid
        ulimit-n 65535

defaults
        mode tcp                        #mode { tcp|http|health }，tcp 表示4层，http表示7层，health仅作为健康检查使用
        retries 2                       #尝试2次失败则从集群摘除
        option redispatch               #如果失效则强制转换其他服务器
        option abortonclose          	#连接数过大自动关闭
        maxconn 1024                  	#最大连接数
        timeout connect 1d           	#连接超时时间，重要，hive查询数据能返回结果的保证
        timeout client 1d               #同上
        timeout server 1d              	#同上
        timeout check 2000           	#健康检查时间
        log 127.0.0.1 local0 err #[err warning info debug]

listen  admin_stats                     #定义管理界面
        bind 0.0.0.0:1090               #管理界面访问IP和端口
        mode http                       #管理界面所使用的协议
        maxconn 10			#最大连接数
        stats refresh 30s               #30秒自动刷新
        stats uri /                     #访问url
        stats realm Hive\ Haproxy    	#验证窗口提示
        stats auth admin:123456     	#401验证用户名密码

listen hive				#hive后端定义
        bind 0.0.0.0:10001              #ha作为proxy所绑定的IP和端口
        mode tcp                        #以4层方式代理，重要
        balance leastconn               #调度算法 &#39;leastconn&#39; 最少连接数分配，或者 &#39;roundrobin&#39;，轮询分配
        maxconn 1024                    #最大连接数
        server hive_1 192.168.1.1:10000 check inter 180000 rise 1 fall 2
        server hive_2 192.168.1.2:10000 check inter 180000 rise 1 fall 2
	#释义：server 主机代名(你自己能看懂就行)，IP:端口 每180000毫秒检查一次。也就是三分钟。
	#hive每有10000端口的请求就会创建一个log，设置短了，/tmp下面会有无数个log文件，删不完。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;如何启动&lt;/h1&gt;

&lt;p&gt;在HAProxy目录下执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;haproxy -f conf.cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;如何使用&lt;/h1&gt;

&lt;p&gt;在hive-server或者hive-server2中jdbc的连接信息修改url和port，如hive-server2:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jdbc:hive2://${haproxy.hostname}:${haproxy.hive.bind.port}/${hive.database}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面&lt;code&gt;haproxy.hostname&lt;/code&gt;为你安装haproxy的机器名；&lt;code&gt;haproxy.hive.bind.port&lt;/code&gt;为&lt;code&gt;conf.cfg&lt;/code&gt;中定义的监听hive的端口（上面中定义的为10001）&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://blog.csdn.net/xiyf2046/article/details/11686873&quot;&gt;HAProxy—HAProxy简介&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://slaytanic.blog.51cto.com/2057708/803626&quot;&gt;HAProxy+Hive构建高可用数据挖掘集群&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2014/01/08/hive-ha-by-haproxy.html</link>
      <guid>http://blog.javachen.com/2014/01/08/hive-ha-by-haproxy.html</guid>
      <pubDate>2014-01-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Git配置和一些常用命令</title>
      <description>&lt;p&gt;Git是一个分布式版本控制／软件配置管理软件，原来是linux内核开发者林纳斯·托瓦兹（Linus Torvalds）为了更好地管理linux内核开发而创立的。&lt;/p&gt;

&lt;h1 id=&quot;git&quot;&gt;Git配置&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;git config --global user.name &quot;javachen&quot;
git config --global user.email &quot;june.chan@foxmail.com&quot;
git config --global color.ui true
git config --global alias.co checkout
git config --global alias.ci commit
git config --global alias.st status
git config --global alias.br branch
git config -l  # 列举所有配置
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- more --&gt;

&lt;p&gt;用户的git配置文件在&lt;code&gt;~/.gitconfig&lt;/code&gt;，我的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;june@june-mint ~/workspace/snippets/dotfiles $ cat .gitconfig 
[user]
	email = june.chan@foxmail.com
	name = javachen
[color]
    ui = auto
[color &quot;branch&quot;]
    current = yellow reverse
    local = yellow
    remote = green
[color &quot;diff&quot;]
    meta = yellow bold
    frag = magenta bold
    old = red bold
    new = green bold
[color &quot;status&quot;]
    added = yellow
    changed = green
    untracked = cyan
[alias]
    st = &quot;status&quot;
    co = checkout
    ls = &quot;ls-files&quot;
    ci = commit
    br = branch
    rt = reset --hard
    unstage = reset HEAD
    uncommit = reset --soft HEAD^
    l = log --pretty=oneline --abbrev-commit --graph --decorate
    amend = commit --amend 
    who = shortlog -n -s --no-merges 
    g = grep -n --color -E 
    cp = cherry-pick -x 
    cb = checkout -b 
[core]
    filemode = true
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;git-1&quot;&gt;Git常用命令&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;查看、帮助命令&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git help &amp;lt;command&amp;gt;  # 显示command的help
git show            # 显示某次提交的内容
git show $id
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;查看提交记录&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git log
git log &amp;lt;file&amp;gt;      # 查看该文件每次提交记录
git log -p &amp;lt;file&amp;gt;   # 显示版本历史，以及版本间的内容差异
git log -p -2       # 查看最近两次详细修改内容的diff
git log --stat      # 查看提交统计信息
git log --since=&quot;6 hours&quot;  # 显示最近6小时提交
git log --before=&quot;2 days&quot;  # 显示2天前提交
git log -1 HEAD~3          # 显示比HEAD早3个提交的那个提交
git log -1 HEAD^^^
git reflog				   # 查看操作记录
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;添加、提交、删除、找回，重置修改文件&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;git add &amp;lt;file&amp;gt;      # 将工作文件修改提交到本地暂存区
git add .           # 将所有修改过的工作文件提交暂存区
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;git co  -- &amp;lt;file&amp;gt;    # 抛弃工作区修改
git co  .               # 抛弃工作区修改
git co HEAD &amp;lt;file&amp;gt;  # 抛弃工作目录区中文件的修改
git co HEAD~3       # 回退三个版本
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;git ci &amp;lt;file&amp;gt;
git ci .
git ci -a           # 将git add, git rm和git ci等操作都合并在一起做
git ci -am &quot;some comments&quot;
git ci --amend      # 修改最后一次提交记录
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;git rm &amp;lt;file&amp;gt;       # 从版本库中删除文件
git rm &amp;lt;file&amp;gt; --cached  # 从版本库中删除文件，但不删除文件
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;git mv &amp;lt;file1&amp;gt;  &amp;lt;file2&amp;gt;    # 重命名文件
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;git reset --hard  HEAD^ # 恢复最近一次提交过的状态，即放弃上次提交后的所有本次修改
git reset --hard &amp;lt;commit id&amp;gt;  # 恢复到某一次提交的状态
git reset HEAD &amp;lt;file&amp;gt; # 抛弃暂存区中文件的修改
git reset &amp;lt;file&amp;gt;    # 从暂存区恢复到工作文件
git reset -- .      # 从暂存区恢复到工作文件
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;git revert &amp;lt;$id&amp;gt;    # 恢复某次提交的状态，恢复动作本身也创建了一次提交对象
git revert HEAD     # 恢复最后一次提交的状态
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;diff&quot;&gt;查看文件diff&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git diff &amp;lt;file&amp;gt;     # 比较当前文件和暂存区文件差异
git diff
git diff &amp;lt;$id1&amp;gt; &amp;lt;$id2&amp;gt;   	# 比较两次提交之间的差异
git diff &amp;lt;branch1&amp;gt; &amp;lt;branch2&amp;gt;   # 在两个分支之间比较 
git diff --staged   # 比较暂存区和版本库差异
git diff --cached   # 比较暂存区和版本库差异
git diff --stat     # 仅仅比较统计信息

git diff &quot;@{yesterday}&quot;     # 查看昨天的改变
git diff 1b6d &quot;master~2&quot;    # 查看一个特定版本与倒数第二个变更之间的改变
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;git-&quot;&gt;Git 本地分支管理&lt;/h1&gt;

&lt;h2 id=&quot;section-3&quot;&gt;查看、切换、创建和删除分支&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git br -r           # 查看远程分支
git br -v           # 查看各个分支最后提交信息
git br -a           # 列出所有分支
git br --merged     # 查看已经被合并到当前分支的分支
git br --no-merged  # 查看尚未被合并到当前分支的分支
git br &amp;lt;new_branch&amp;gt; # 基于当前分支创建新的分支
git br &amp;lt;new_branch&amp;gt;  &amp;lt;start_point&amp;gt;		# 基于另一个起点（分支名称，提交名称或则标签名称），创建新的分支
git br -f &amp;lt;existing_branch&amp;gt;  &amp;lt;start_point&amp;gt;	# 创建同名新分支，覆盖已有分支
git br -d &amp;lt;branch&amp;gt;  # 删除某个分支
git br -D &amp;lt;branch&amp;gt;  # 强制删除某个分支 (未被合并的分支被删除的时候需要强制)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;git co &amp;lt;branch&amp;gt;     	# 切换到某个分支
git co -b &amp;lt;new_branch&amp;gt; 	# 创建新的分支，并且切换过去
git co -b &amp;lt;new_branch&amp;gt; &amp;lt;branch&amp;gt;  	  # 基于branch创建新的new_branch
git co -m &amp;lt;existing_branch&amp;gt; &amp;lt;new_branch&amp;gt;  # 移动或重命名分支，当新分支不存在时
git co -M &amp;lt;existing_branch&amp;gt; &amp;lt;new_branch&amp;gt;  # 移动或重命名分支，当新分支存在时就覆盖

git co $id         		 # 把某次历史提交记录checkout出来，但无分支信息，切换到其他分支会自动删除
git co $id -b &amp;lt;new_branch&amp;gt;       # 把某次历史提交记录checkout出来，创建成一个分支
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;rebase&quot;&gt;分支合并和rebase&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git merge &amp;lt;branch&amp;gt;                  # 将branch分支合并到当前分支
git merge origin/master --no-ff     # 不要Fast-Foward合并，这样可以生成merge提交
git merge --no-commit &amp;lt;branch&amp;gt;      # 合并但不提交
git merge --squash &amp;lt;branch&amp;gt;         # 把一条分支上的内容合并到另一个分支上的一个提交

git rebase master &amp;lt;branch&amp;gt;          # 将master rebase到branch，相当于：
git co &amp;lt;branch&amp;gt; &amp;amp;&amp;amp; git rebase master &amp;amp;&amp;amp; git co master &amp;amp;&amp;amp; git merge &amp;lt;branch&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;git-2&quot;&gt;Git补丁管理&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git diff &amp;gt; ../sync.patch         # 生成补丁
git apply ../sync.patch          # 打补丁
git apply --check ../sync.patch  # 测试补丁能否成功
git format-patch -X              # 根据提交的log生成patch，X为数字，表示最近的几个日志
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;git-3&quot;&gt;Git暂存管理&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git stash                        # 暂存
git stash list                   # 列所有stash
git stash apply                  # 恢复暂存的内容
git stash drop                   # 删除暂存区
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;git-4&quot;&gt;Git远程分支管理&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git pull                         # 抓取远程仓库所有分支更新并合并到本地
git pull --no-ff                 # 抓取远程仓库所有分支更新并合并到本地，不要快进合并
git fetch origin                 # 抓取远程仓库所有更新
git fetch origin remote-branch:local-branch #抓取remote-branch分支的更新
git fetch origin --tags 		 # 抓取远程上的所有分支
git checkout -b &amp;lt;new-branch&amp;gt; &amp;lt;remote_tag&amp;gt; # 抓取远程上的分支
git merge origin/master          # 将远程主分支合并到本地当前分支
git co --track origin/branch     # 跟踪某个远程分支创建相应的本地分支
git co -b &amp;lt;local_branch&amp;gt; origin/&amp;lt;remote_branch&amp;gt;  # 基于远程分支创建本地分支，功能同上
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;git push                         # push所有分支
git push origin master           # 将本地主分支推到远程主分支
git push -u origin master        # 将本地主分支推到远程(如无远程主分支则创建，用于初始化远程仓库)
git push origin &amp;lt;local_branch&amp;gt;   # 创建远程分支， origin是远程仓库名
git push origin &amp;lt;local_branch&amp;gt;:&amp;lt;remote_branch&amp;gt;  # 创建远程分支
git push origin :&amp;lt;remote_branch&amp;gt;  #先删除本地分支(git br -d &amp;lt;branch&amp;gt;)，然后再push删除远程分支
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;git-5&quot;&gt;Git远程仓库管理&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git remote -v                    # 查看远程服务器地址和仓库名称
git remote show origin           # 查看远程服务器仓库状态
git remote add origin git@github:XXX/test.git         # 添加远程仓库地址
git remote set-url origin git@github.com:XXX/test.git # 设置远程仓库地址(用于修改远程仓库地址)
git remote rm &amp;lt;repository&amp;gt;       # 删除远程仓库
git remote set-head origin master   # 设置远程仓库的HEAD指向master分支

git branch --set-upstream master origin/master
git branch --set-upstream develop origin/develop
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;实例&lt;/h1&gt;
&lt;p&gt;## 打patch过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git add .
git status
git diff --cached &amp;gt;XXX.patch
git ci -m &#39;add patch&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-5&quot;&gt;分支策略&lt;/h1&gt;

&lt;p&gt;在实际开发中，我们应该按照几个基本原则进行分支管理：&lt;/p&gt;

&lt;p&gt;首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；&lt;/p&gt;

&lt;p&gt;那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本；&lt;/p&gt;

&lt;p&gt;你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2013/12/27/some-git-configs-and-cammands.html</link>
      <guid>http://blog.javachen.com/2013/12/27/some-git-configs-and-cammands.html</guid>
      <pubDate>2013-12-27T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>SaltStack学习笔记</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1. 关于本文档&lt;/h1&gt;

&lt;p&gt;这份文档如其名，是我自己整理的学习 SaltStack 的过程记录。只是过程记录，没有刻意像教程那样去做。所以呢，从前至后，中间不免有一些概念不清不明的地方。因为事实上，在某个阶段对于一些概念本来就不可能明白。所以，整个过程只求在形式上的能用即可。前面就不要太纠结概念和原理，知道怎么用就好。&lt;/p&gt;

&lt;p&gt;希望这篇文章能够让你快速了解并使用saltstack。文章还在编写中。&lt;/p&gt;

&lt;h1 id=&quot;saltstack&quot;&gt;2. 关于SaltStack&lt;/h1&gt;

&lt;h2 id=&quot;saltstack-1&quot;&gt;2.1. 什么是SaltStack&lt;/h2&gt;
&lt;p&gt;SaltStack是开源的管理基础设置的轻量级工具，容易搭建，为远程管理服务器提供一种更好、更快速、更有扩展性的解决方案。通过简单、可管理的接口，Salt实现了可以管理成千上百的服务器和处理大数据的能力。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;轻量级配置管理系统，能够维持远端节点运行在预定状态（例如，确保指定的软件包已经安装和特定的系统服务正在运行）&lt;/li&gt;
  &lt;li&gt;分布式远程执行系统，用于在远端节点执行命令和查询数据，可以是单独，也可以是选定的条件&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;saltstack-2&quot;&gt;2.2. SaltStack特点&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;简单&lt;/h3&gt;

&lt;p&gt;兼顾大规模部署与更小的系统的同时提供多功能性是很困难的，Salt是非常简单配置和维护，不管项目的大小。Salt可以胜任管理任意的数量的服务器，不管是本地网络，还是跨数据中心。架构采用C/S模式，在一个后台程序中集成必要功能。默认不需要复杂的配置就可以工作，同时可以定制用于特殊的需求。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;并行执行&lt;/h3&gt;

&lt;p&gt;Salt的核心功能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;通过并行方式让远端节点执行命令&lt;/li&gt;
  &lt;li&gt;采用安全的加密/解析协议&lt;/li&gt;
  &lt;li&gt;最小化使用网络和负载&lt;/li&gt;
  &lt;li&gt;提供简单的程序接口&lt;/li&gt;
  &lt;li&gt;Salt引入了更细粒度的控制，允许不通过目标名字，二是通过系统属性分类&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;构建在成熟技术之上&lt;/h3&gt;

&lt;p&gt;Salt采用了很多技术和技巧。网络层采用优秀的ZeroMQ库，所以守护进程里面包含AMQ代理。Salt采用公钥和主控通讯，同时使用更快的AES加密通信，验证和加密都已经集成在Salt里面。Salt使用msgpack通讯，所以更快速和更轻量网络交换。&lt;/p&gt;

&lt;h3 id=&quot;python-&quot;&gt;Python 客户端接口&lt;/h3&gt;

&lt;p&gt;为了实现简单的扩展，Salt执行例程可以写成简单的Python模块。客户端程序收集的数据可以发送回主控端，可以是其他任意程序。可以通过Python API调用Salt程序，或者命令行，因此，Salt可以用来执行一次性命令，或者大型应用程序中的一部分模块。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;快速，灵活，可扩展&lt;/h3&gt;

&lt;p&gt;结果是一个系统可以高速在一台或者一组服务器执行命令。Salt速度很快，配置简单，扩展性好，提供了一个远程执行架构，可以管理多样化需求的任何数量的服务器。整合了世界上最好的远程执行方法，增强处理能力，扩展使用范围，使得可以适用任何多样化复杂的网络。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;开源&lt;/h3&gt;

&lt;p&gt;Salt基于Apache 2.0 licence开发，可以用于开源或者自有项目。请反馈你的扩展给项目组，以便更多人受益，共同促进Salt发展。请在你的系统部署 系统，让运维更便捷。&lt;/p&gt;

&lt;p&gt;开发语言：Python&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;2.3. 支持的系统&lt;/h2&gt;

&lt;p&gt;常见的系统包可以直接下载安装使用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://admin.fedoraproject.org/pkgdb/acls/name/salt&quot;&gt;Fedora&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;RedHat Enterprise Linux / Centos (&lt;a href=&quot;http://dl.fedoraproject.org/pub/epel/5/x86_64/repoview/salt.html&quot;&gt;EPEL 5&lt;/a&gt;, &lt;a href=&quot;http://dl.fedoraproject.org/pub/epel/6/x86_64/repoview/salt.html&quot;&gt;EPEL 6&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://launchpad.net/~saltstack&quot;&gt;Ubuntu (PPA)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://aur.archlinux.org/packages.php?ID=47512&quot;&gt;Arch (AUR)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.freebsd.org/cgi/cvsweb.cgi/ports/sysutils/salt/&quot;&gt;FreeBSD&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://packages.gentoo.org/package/app-admin/salt&quot;&gt;Gentoo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://packages.debian.org/sid/salt-master&quot;&gt;Debian (sid)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://packages.debian.org/experimental/salt-master&quot;&gt;Debian (experimental)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;saltstack-3&quot;&gt;3. 安装SaltStack&lt;/h1&gt;

&lt;h2 id=&quot;section-7&quot;&gt;3.1. 依赖&lt;/h2&gt;
&lt;p&gt;SaltStack只能安装在类unix的操作系统上并依赖以下组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://python.org/download/&quot;&gt;Python 2.6&lt;/a&gt; &amp;gt;= 2.6 &amp;lt;3.0&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zeromq.org/&quot;&gt;ZeroMQ&lt;/a&gt; &amp;gt;= 2.1.9&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zeromq/pyzmq&quot;&gt;pyzmq&lt;/a&gt; &amp;gt;= 2.1.9 - ZeroMQ Python bindings&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.dlitz.net/software/pycrypto/&quot;&gt;PyCrypto&lt;/a&gt; - The Python cryptography toolkit&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pypi.python.org/pypi/msgpack-python/0.1.12&quot;&gt;msgpack-python&lt;/a&gt; - High-performance message interchange format&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pyyaml.org/&quot;&gt;YAML&lt;/a&gt; - Python YAML bindings&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jinja.pocoo.org/&quot;&gt;Jinja2&lt;/a&gt; - parsing Salt States (configurable in the master settings)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可选的依赖：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.makotemplates.org/&quot;&gt;mako&lt;/a&gt; - an optional parser for Salt States (configurable in the master settings)&lt;/li&gt;
  &lt;li&gt;gcc - dynamic &lt;a href=&quot;http://cython.org/&quot;&gt;Cython&lt;/a&gt; module compiling&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-8&quot;&gt;3.2. 快速安装&lt;/h2&gt;
&lt;p&gt;可以使用官方提供的&lt;a href=&quot;https://github.com/saltstack/salt-bootstrap&quot;&gt;Salt Bootstrap&lt;/a&gt;来快速安装SaltStack。&lt;/p&gt;

&lt;p&gt;安装master：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -L http://bootstrap.saltstack.org | sudo sh -s -- -M -N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装minion：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget -O - http://bootstrap.saltstack.org | sudo sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当前&lt;a href=&quot;https://github.com/saltstack/salt-bootstrap&quot;&gt;Salt Bootstrap&lt;/a&gt;已经在以下操作系统测试通过：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ubuntu 10.x/11.x/12.x&lt;/li&gt;
  &lt;li&gt;Debian 6.x&lt;/li&gt;
  &lt;li&gt;CentOS 6.3&lt;/li&gt;
  &lt;li&gt;Fedora&lt;/li&gt;
  &lt;li&gt;Arch&lt;/li&gt;
  &lt;li&gt;FreeBSD 9.0&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rpm&quot;&gt;3.3. 通过rpm安装&lt;/h2&gt;

&lt;h3 id=&quot;epel-yum&quot;&gt;3.3.1. 下载EPEL yum源：&lt;/h3&gt;

&lt;p&gt;RHEL 5系统:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm -Uvh http://mirror.pnl.gov/epel/5/i386/epel-release-5-4.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RHEL 6系统:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm -Uvh http://ftp.linux.ncsu.edu/pub/epel/6/i386/epel-release-6-8.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-9&quot;&gt;3.3.2. 安装包&lt;/h3&gt;
&lt;p&gt;在master上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install salt-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在minion上运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install salt-minion
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-10&quot;&gt;3.3.3. 安装后&lt;/h3&gt;
&lt;p&gt;master上设置开启启动并启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chkconfig salt-master on
service salt-master start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;minion上设置开启启动并启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chkconfig salt-minion on
service salt-minion start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-11&quot;&gt;3.4. 排错&lt;/h3&gt;
&lt;p&gt;当前最新版为0.17.1，如果你也是使用的这个版本并且启动提示如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 vagrant]# /etc/init.d/salt-master start
Starting salt-master daemon: Traceback (most recent call last):
 File &quot;/usr/bin/salt-master&quot;, line 10, in &amp;lt;module&amp;gt;
   salt_master()
 File &quot;/usr/lib/python2.6/site-packages/salt/scripts.py&quot;, line 20, in salt_master
   master.start()
 File &quot;/usr/lib/python2.6/site-packages/salt/__init__.py&quot;, line 114, in start
   if check_user(self.config[&#39;user&#39;]):
 File &quot;/usr/lib/python2.6/site-packages/salt/utils/verify.py&quot;, line 296, in check_user
   if user in e.gr_mem] + [pwuser.gid])
AttributeError: &#39;pwd.struct_passwd&#39; object has no attribute &#39;gid&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过google搜索关键字&lt;code&gt;saltstack &#39;pwd.struct_passwd&#39; object has no attribute &#39;gid&#39;&lt;/code&gt;，可以找到这个&lt;a href=&quot;https://github.com/saltstack/salt/issues/8176&quot;&gt;issues&lt;/a&gt;，查看评论可以发现这是一个bug，将会在0.17.2中被修复。&lt;/p&gt;

&lt;p&gt;我的解决方法是：下载saltstack源码重新编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://github.com/saltstack/salt/archive/develop.zip
unzip develop
cd salt-develop/
python2.6 setup.py install
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;saltstack-4&quot;&gt;4. 配置SaltStack&lt;/h1&gt;

&lt;h2 id=&quot;master&quot;&gt;4.1. master配置&lt;/h2&gt;
&lt;p&gt;master不修改配置文件就可以运行，而minion必须修改配置文件中的master id才能和master通讯，配置文件分别在&lt;code&gt;/etc/salt/master&lt;/code&gt; 和 &lt;code&gt;/etc/salt/minion&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;master默认监控0.0.0.0上4505和4506端口，你可以在&lt;code&gt;/etc/salt/master&lt;/code&gt;中修改为指定ip。&lt;/p&gt;

&lt;p&gt;master关键配置：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;interface&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;publish_port&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;user&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;max_open_files&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;worker_threads&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ret_port&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pidfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;root_dir&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pki_dir&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cachedir&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;keep_jobs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;job_cache&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ext_job_cache&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;minion_data_cache&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;enforce_mine_cache&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sock_dir&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Master Security配置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;open_mode&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;auto_accept&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;autosign_file&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;client_acl&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;client_acl_blacklist&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;external_auth&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;token_expire&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Master Module管理&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;runner_dirs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cython_enable&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Master State设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;state_verbose&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;state_output&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;state_top&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;external_nodes&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;renderer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;failhard&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;test&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Master File Server设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;fileserver_backend&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;file_roots&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hash_type&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;file_buffer_size&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pillar配置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;pillar_roots&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ext_pillar&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Syndic Server设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;order_masters&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;syndic_master&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;syndic_master_port&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;syndic_log_file&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;syndic_pidfile&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Peer发布设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;peer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;peer_run&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Node Groups设置&lt;/p&gt;

&lt;p&gt;Master Logging设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;log_file&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_level&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_level_logfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_datefmt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_datefmt_logfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_fmt_console&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_fmt_logfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_granular_levels&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Include配置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;default_include&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;include&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;minion&quot;&gt;4.4. minion配置&lt;/h2&gt;

&lt;p&gt;Minion主要配置：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;master&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;master_port&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;user&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pidfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;root_dir&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pki_dir&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;id&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;append_domain&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cachedir&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;verify_env&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cache_jobs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;sock_dir&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;backup_mode&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;acceptance_wait_time&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;random_reauth_delay&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cceptance_wait_time_max&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dns_check&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ipc_mode&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tcp_pub_port&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tcp_pull_port&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Minion Module管理&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;disable_modules&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;disable_returners&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;module_dirs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;returner_dirs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;states_dirs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;render_dirs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cython_enable&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;providers&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;State Management 设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;renderer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;state_verbose&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;state_output&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;autoload_dynamic_modules&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;environment&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;File目录设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;file_client&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;file_roots&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hash_type&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pillar_roots&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Security设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;open_mode&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;线程设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;multiprocessing&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Minion日志设置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;log_file&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_level&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_level_logfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_datefmt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_datefmt_logfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_fmt_console&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_fmt_logfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;log_granular_levels&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Include配置&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;default_include&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;include&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Frozen Build Update Settings&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;update_url&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;update_restart_services&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;saltstack-5&quot;&gt;5. 初识SaltStack&lt;/h1&gt;

&lt;h2 id=&quot;section-12&quot;&gt;5.1. 配置&lt;/h2&gt;
&lt;p&gt;参考上面的配置文件说明修改master和minion配置。这里我的master主机名为sk1，minion主机名为sk2。&lt;/p&gt;

&lt;p&gt;修改master配置文件/etc/salt/master&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;interface: 0.0.0.0
auto_accept: True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改minion配置文件/etc/salt/minion&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master: sk1
id: sk2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;salt&quot;&gt;5.2. 运行salt&lt;/h2&gt;
&lt;p&gt;然后运行salt，启动master（添加&lt;code&gt;-d&lt;/code&gt;参数可以让其后台运行）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动minion（添加-d参数可以让其后台运行）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt-minion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果需要排错，可以添加设置日志级别：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt-master --log-level=debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想以非用户运行，可以添加&lt;code&gt;--user&lt;/code&gt;参数&lt;/p&gt;

&lt;h2 id=&quot;key&quot;&gt;5.3. 管理Key&lt;/h2&gt;
&lt;p&gt;Salt在master和minion通信之间使用AES加密。在master和minion通信之前，minion上的key需要发送到master并被master接受。可以在master上查看已经接受的key：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 pillar]# salt-key -L
Accepted Keys:
Unaccepted Keys:
sk2
Rejected Keys:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后运行下面命令可以接受所有未被接受的key：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 pillar]# salt-key -A
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-13&quot;&gt;5.4. 发送命令&lt;/h2&gt;

&lt;p&gt;在master上发送ping命令检测minon是否被认证成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 pillar]# salt &#39;*&#39; test.ping
sk2:salt &#39;*&#39; test.ping
    True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;True表明测试成功。&lt;/p&gt;

&lt;h1 id=&quot;section-14&quot;&gt;6. 配置管理&lt;/h1&gt;
&lt;p&gt;## 6.1 states&lt;br /&gt;
## 6.1.1 states文件&lt;/p&gt;

&lt;p&gt;salt states的核心是sls文件，该文件使用YAML语法定义了一些k/v的数据。&lt;/p&gt;

&lt;p&gt;sls文件存放根路径在master配置文件中定义，默认为&lt;code&gt;/srv/salt&lt;/code&gt;,该目录在操作系统上不存在，需要手动创建。&lt;/p&gt;

&lt;p&gt;在salt中可以通过&lt;code&gt;salt://&lt;/code&gt;代替根路径，例如你可以通过&lt;code&gt;salt://top.sls&lt;/code&gt;访问&lt;code&gt;/srv/salt/top.sls&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在states中top文件也由master配置文件定义，默认为top.sls，该文件为states的入口文件。&lt;/p&gt;

&lt;p&gt;一个简单的sls文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; apache:
  pkg:
    - installed
  service:
    - running
    - require:
      - pkg: apache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：此SLS数据确保叫做”apache”的软件包(package)已经安装,并且”apache”服务(service)正在运行中。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一行，被称为ID说明(ID Declaration)。ID说明表明可以操控的名字。&lt;/li&gt;
  &lt;li&gt;第二行和第四行是State说明(State Declaration)，它们分别使用了pkg和service states。pkg state通过系统的包管理其管理关键包，service state管理系统服务(daemon)。 在pkg及service列下边是运行的方法。方法定义包和服务应该怎么做。此处是软件包应该被安装，服务应该处于运行中。&lt;/li&gt;
  &lt;li&gt;第六行使用require。本方法称为”必须指令”(Requisite Statement)，表明只有当apache软件包安装成功时，apache服务才启动起来。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;state和方法可以通过点连起来，上面sls文件和下面文件意思相同。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; apache:
  pkg.installed
  service.running
    - require:
      - pkg: apache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面sls保存为init.sls并放置&lt;code&gt;在sal://apache&lt;/code&gt;目录下，结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/srv/salt
├── apache
│   └── init.sls
└── top.sls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;top.sls如何定义呢？&lt;/p&gt;

&lt;p&gt;master配置文件中定义了三种环境，每种环境都可以定义多个目录，但是要避免冲突，分别如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# file_roots:
#   base:
#     - /srv/salt/
#   dev:
#     - /srv/salt/dev/services
#     - /srv/salt/dev/states
#   prod:
#     - /srv/salt/prod/services
#     - /srv/salt/prod/states
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;top.sls可以这样定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; base:
   &#39;*&#39;:
    - apache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一行，声明使用base环境&lt;/li&gt;
  &lt;li&gt;第二行，定义target，这里是匹配所有&lt;/li&gt;
  &lt;li&gt;第三行，声明使用哪些states目录，salt会寻找每个目录下的init.sls文件。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;states&quot;&gt;6.1.2 运行states&lt;/h2&gt;

&lt;p&gt;一旦创建完states并修改完top.sls之后，你可以在master上执行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 salt]# salt &#39;*&#39; state.highstate
sk2:
----------
    State: - pkg
    Name:      httpd
    Function:  installed
        Result:    True
        Comment:   The following packages were installed/updated: httpd.
        Changes:
                   ----------
                   httpd:
                       ----------
                       new:
                           2.2.15-29.el6.centos
                       old:

----------
    State: - service
    Name:      httpd
    Function:  running
        Result:    True
        Comment:   Service httpd has been enabled, and is running
        Changes:
                   ----------
                   httpd:
                       True

Summary
------------
Succeeded: 2
Failed:    0
------------
Total:     2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面命令会触发所有minion从master下载top.sls文件以及其中定一个的states，然后编译、执行。执行完之后，minion会将执行结果的摘要信息汇报给master。&lt;/p&gt;

&lt;h2 id=&quot;grains&quot;&gt;6.2. Grains&lt;/h2&gt;
&lt;p&gt;## Pillar&lt;br /&gt;
## Renderers&lt;/p&gt;

&lt;h1 id=&quot;section-15&quot;&gt;远程执行命令&lt;/h1&gt;

&lt;p&gt;在上面的例子中，test.ping是最简单的一个远程执行的命令，你还可以执行一些更加复杂的命令。&lt;/p&gt;

&lt;p&gt;salt执行命令的格式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt &#39;&amp;lt;target&amp;gt;&#39; &amp;lt;function&amp;gt; [arguments]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;target: 执行salt命令的目标，可以使用正则表达式&lt;/li&gt;
  &lt;li&gt;function： 方法，由module提供&lt;/li&gt;
  &lt;li&gt;arguments：function的参数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;target可以使用正则表达式匹配：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt &#39;*&#39; test.ping
salt &#39;sk2&#39; test.ping
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;function是module提供的方法。通过下面命令可以查看所有的function：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt &#39;*&#39; sys.doc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;function可以接受参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt &#39;*&#39; cmd.run &#39;uname -a&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并且支持关键字参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt &#39;*&#39; cmd.run &#39;uname -a&#39; cwd=/ user=salt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面例子意思是：在所有minion上切换到/目录以salt用户运行&lt;code&gt;uname -a&lt;/code&gt;命令。&lt;/p&gt;

&lt;p&gt;关键字参数可以参考module的&lt;a href=&quot;http://docs.saltstack.com/py-modindex.html&quot;&gt;api&lt;/a&gt;,通过api你可以查看&lt;a href=&quot;http://docs.saltstack.com/ref/states/all/salt.states.cmd.html#salt.states.cmd.run&quot;&gt;cmd.run&lt;/a&gt;的定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt.states.cmd.mod_watch(name, **kwargs)
Execute a cmd function based on a watch call

salt.states.cmd.run(name, onlyif=None, unless=None, cwd=None, user=None, group=None, shell=None, env=(), stateful=False, umask=None, quiet=False, timeout=None, **kwargs)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有module中的方法定义都与上面类似，说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;name： 第一个参数，为执行的命令&lt;/li&gt;
  &lt;li&gt;中间的key=alue为keyword参数，都有默认值&lt;/li&gt;
  &lt;li&gt;最后一个参数为name中命令的输入参数&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;targeting&quot;&gt;TARGETING&lt;/h2&gt;
&lt;p&gt;## Returners&lt;br /&gt;
## Mine&lt;/p&gt;

&lt;h1 id=&quot;section-16&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1]&lt;a href=&quot;http://zhoulg.blog.51cto.com/48455/1140178&quot;&gt;Saltstack服务器集中管理和并行下发命令工具&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2013/11/18/study-note-of-saltstack.html</link>
      <guid>http://blog.javachen.com/2013/11/18/study-note-of-saltstack.html</guid>
      <pubDate>2013-11-18T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用SaltStack安装JBoss</title>
      <description>&lt;p&gt;SaltStack是一个具备puppet与func功能为一身的集中化管理平台，其基于python实现，功能十分强大，各模块融合度及复用性极高。SaltStack 采用 zeromq 消息队列进行通信，和 Puppet/Chef 比起来，SaltStack 速度快得多。&lt;/p&gt;

&lt;p&gt;在开始使用SaltStack之前，首先要对SaltStack的基础进行一系列的学习，这里，强烈推荐官网的&lt;a href=&quot;http://docs.saltstack.com/topics/tutorials/walkthrough.html&quot;&gt;Tutorial&lt;/a&gt;,在完成了整个Tutorial之后，通过Module Index页面，我们能够快速查阅Salt所有模块的功能与用法:&lt;a href=&quot;http://docs.saltstack.com/py-modindex.html&quot;&gt;http://docs.saltstack.com/py-modindex.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;saltstack&quot;&gt;安装saltstack&lt;/h1&gt;

&lt;p&gt;安装过程请参考：&lt;a href=&quot;/linux/2013/11/11/install-saltstack-and-halite/&quot;&gt;安装saltstack和halite&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;pillar&quot;&gt;添加pillar&lt;/h1&gt;

&lt;p&gt;你可以执行下面命令查看minion拥有哪些Pillar数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; pillar.data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;saltstack的默认states目录为&lt;code&gt;/srv/salt&lt;/code&gt;，默认为&lt;code&gt;/srv/pillar&lt;/code&gt;，如果不存在请先创建。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 /]# tree /srv/ -L 3
/srv/
├── pillar
│   ├── jboss
│       ├── params.sls
│   └── top.sls
├── salt
│   ├── jboss
│   ├── _modules
│   └── top.sls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/srv/pillar/&lt;/code&gt;下创建top.sls，该文件引入jboss下的params.sls：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; base:
  &#39;*&#39;:
    - jboss.params
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建jboss目录并添加params.sls如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; jboss_home: /home/jboss/jboss-eap-5.1/jboss-as
 profile_port:
  default1: 
    http_port: ports-default
    jmx_port: 1099
  default2: 
    http_port: ports-01
    jmx_port: 1199
  default3: 
    http_port: ports-02
    jmx_port: 1299
  default4: 
    http_port: ports-03
    jmx_port: 1399

 jmx:
  username: admin
  password: admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该文件定义了如下变量，你可以按需要定义自己的变量：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;jboss_home：mimion机器上jboss的home目录&lt;/li&gt;
  &lt;li&gt;profile_port：定义有多少个profile以及每个profile下的http端口和jmx端口&lt;/li&gt;
  &lt;li&gt;jmx:定义jmx-console用户名和密码&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;定义变量之后，你可以在sates文件中这样引用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pillar[&#39;profile_port&#39;][&#39;default1&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是个复杂的例子，使用了python中的模板引擎语言：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;{% for profile, port in pillar.get(&#39;profile_port&#39;, {}).items() %}
    {{profile}}
{% endfor %}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你还可以在python脚本中或者是saltstack自定义module中这样引用变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__pillar__[&#39;jboss_home&#39;]
__pillar__[&#39;profile_port&#39;][&#39;default1&#39;][&#39;jmx_port&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在master上修改Pilla文件后，需要用以下命令刷新minion上的数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; saltutil.refresh_pillar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果定义好的pillar不生效，建议刷新一下或者重启salt试试。&lt;/p&gt;

&lt;h1 id=&quot;states&quot;&gt;编写states&lt;/h1&gt;

&lt;p&gt;/srv/salt目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 salt]# tree -L 3
.
├── jboss
│   ├── files
│   │   └── jboss-eap-5.1.zip
│   └── init.sls

├── _modules
│   └── jboss.py
├── top.sls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;top.sls为sates入口，定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; base:
  *:
    - jboss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建jboss目录并编写init.sls文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; unzip:
  pkg.installed
 jboss:
  file.managed:
    - name:  /home/jboss/jboss-eap-5.1.zip
    - source: salt://jboss/files/jboss-eap-5.1.zip
    - include_empty: True
    - user: jboss
    - group: jboss
    - mode: 655
  cmd.run:
    - name: &#39;unzip jboss-eap-5.1.zip&#39;
    - cwd: /home/jboss
    - user: jboss
    - unless: &#39;test -e jboss-eap-5.1&#39;
    - require:
      - file.managed: jboss

{% for profile, port in pillar.get(&#39;profile_port&#39;, {}).items() %}
 {{profile}}:
  cmd.run:
    - name: &#39;\cp -r default {{profile}}&#39;
    - cwd:  /home/jboss/jboss-eap-5.1/jboss-as/server
    - user: jboss
    - unless: &#39;test -e {{profile}}&#39;
    - require:
      - file.managed: jboss
{% endfor %}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;p&gt;1.上面文件中创建了jboss ID，其包括两部分：拷贝文件和解压缩，分别对应file.managed和cmd.run。&lt;/p&gt;

&lt;p&gt;2.然后通过脚本语言读取pillar中定义的变量并以依次遍历生成多个ID，ID名称由变量中值定义。本例中，是读取&lt;code&gt;profile_port&lt;/code&gt;的值，然后创建多个profile。&lt;code&gt;profile_port&lt;/code&gt;变量在&lt;code&gt;/srv/pillar/jboss/params.sls&lt;/code&gt;中定义。&lt;/p&gt;

&lt;p&gt;编写完sates文件之后，你可以通过执行以下命令让所有minion执行sates文件中定义的state：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; state.highstate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以单独执行jboss这个states：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; state.sls jboss
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;grainsmodule&quot;&gt;自定义grains_module&lt;/h1&gt;

&lt;p&gt;自定义的&lt;code&gt;grains_module&lt;/code&gt;存放在&lt;code&gt;/srv/salt/_grains&lt;/code&gt;目录，下面定义一个获取&lt;code&gt;max_open_file&lt;/code&gt;的grains：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os,sys,commands
 
def Grains(): 
    grains = {}
    max_open_file=65536 
    try: 
        getulimit=commands.getstatusoutput(&#39;source /etc/profile;ulimit -n&#39;)  
    except Exception,e: 
        pass  
    if getulimit[0]==0: 
        max_open_file=int(getulimit[1])  
    grains[&#39;max_open_file&#39;] = max_open_file 
    return grains
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，同步grains模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ salt &#39;*&#39; saltutil.sync_all
sk2:
    ----------
    grains:
        - grains.max_open_file 
    modules:
    outputters:
    renderers:
    returners:
    states:

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;刷新模块(让minion编译模块)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; sys.reload_modules
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，验证max open file的value：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; grains.item max_open_file
sk2:
  max_open_file: 1024
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;module&quot;&gt;自定义module&lt;/h1&gt;

&lt;p&gt;你通过执行states可以完成bao的安装、配置和部署，如果你想对他们做管理，你可以自定义module来执行一些远程命令。&lt;/p&gt;

&lt;p&gt;自定义的module需要存放在&lt;code&gt;/srv/salt/_modules&lt;/code&gt;目录下，为了对jboss实例进行启动、停止、查看运行状态，编写jboss.py模块：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def _simple_cmd_retcode(cmd):
    home = __pillar__[&#39;jboss_home&#39;]+&#39;/bin&#39;
    out = __salt__[&#39;cmd.retcode&#39;](cmd,cwd=home)
    ret = {}
    ret[&#39;Cmd&#39;]=cmd
    ret[&#39;Msg&#39;]=out
    ret[&#39;Result&#39;]=True
    return ret

def _simple_cmd(cmd):
    home = __pillar__[&#39;jboss_home&#39;]+&#39;/bin&#39;
    out = __salt__[&#39;cmd.run&#39;](cmd,cwd=home).splitlines()
    ret = {}
    ret[&#39;Cmd&#39;]=cmd
    ret[&#39;Msg&#39;]=out
    ret[&#39;Result&#39;]=True
    return ret

def running(profile):
    ret =_simple_cmd(&quot;ps -ef|grep -v grep|grep java|grep &quot;+profile+&quot; |grep &quot;+__pillar__[&#39;profile_port&#39;][profile][&#39;http_port&#39;])
    if ret[&#39;Msg&#39;]==&#39;&#39; or len(ret[&#39;Msg&#39;])==0:
        return False
    return True

def start(profile):
    ip = __grains__[&#39;id&#39;][0]
    port = __pillar__[&#39;profile_port&#39;][profile][&#39;http_port&#39;]
    cmd = &#39;nohup ./run.sh -b &#39;+ip+&#39; -c &#39;+profile+&#39; -Djboss.service.binding.set=&#39;+port+&#39; &amp;amp;&#39;
 
    ret={}
    if running(profile):
        ret[&#39;Cmd&#39;]=cmd
	ret[&#39;Msg&#39;]=profile+&#39; has started&#39;
        ret[&#39;Result&#39;]=False
        return ret
    
    return _simple_cmd_retcode(cmd)

def status(profile):
    ip = __grains__[&#39;id&#39;][0]
    port = __pillar__[&#39;profile_port&#39;][profile][&#39;jmx_port&#39;]
    username = __pillar__[&#39;jmx&#39;][&#39;username&#39;]
    password = __pillar__[&#39;jmx&#39;][&#39;password&#39;]
    cmd=&quot;./twiddle.sh -s &quot;+ip+&quot;:&quot;+str(port)+&quot; -u &quot;+username+&quot; -p &quot;+password+&quot; get jboss.system:type=Server Started&quot;
    
    ret=_simple_cmd(cmd)
    for line in ret[&#39;Msg&#39;]:
        if not line:
            continue
        if &#39;ERROR&#39; in line:
 	    ret[&#39;Result&#39;]=False
	    ret[&#39;Msg&#39;]=ret[&#39;Msg&#39;][0:2]
            break
    return ret
    
def stop(profile):
    ip = __grains__[&#39;id&#39;][0]
    port = __pillar__[&#39;profile_port&#39;][profile][&#39;jmx_port&#39;]
    username = __pillar__[&#39;jmx&#39;][&#39;username&#39;]
    password = __pillar__[&#39;jmx&#39;][&#39;password&#39;]
    cmd=&quot;./shutdown.sh -S -s &quot;+ip+&quot;:&quot;+str(port)+&quot; -u &quot;+username+&quot; -p &quot;+password
    return _simple_cmd(cmd)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上python脚本定义了对单个profile的启动、停止和查看运行状态的方法，你可以修改或者扩增代码，添加更多的方法。&lt;/p&gt;

&lt;p&gt;然后运行下面命令同步所有模块：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; saltutil.sync_all
sk2:
    ----------
    grains:
    modules:
        - modules.jboss
    outputters:
    renderers:
    returners:
    states:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;刷新自定义模块(让minion编译模块)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; sys.reload_modules
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你想启动jboss的default1实例，只需要执行以下方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; jboss.start default1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样，查看状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt &#39;*&#39; jboss.status default1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;jboss为自定义模块的名称，也是jboss.py的名称，start或者status为jboss.py中定义的方法。&lt;/p&gt;

&lt;p&gt;如果运行出错，请查看minion的日志，路径为&lt;code&gt;/var/log/salt/minion&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;同步配置文件&lt;/h1&gt;

&lt;p&gt;现在需要修改jmx-console-users.properties文件中的用户名和密码并将其同步到所有jboss实例中。&lt;/p&gt;

&lt;p&gt;将jmx-console-users.properties文件放置在&lt;code&gt;/srv/salt/jboss/files&lt;/code&gt;目录下，并修改该文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; #A sample users.properties file for use with the UsersRolesLoginModule
 pillar[&#39;jmx&#39;][&#39;username&#39;]= pillar[&#39;jmx&#39;][&#39;password&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面文件中使用了pillar获取变量，你还可以使用模板语言如if、for语句来丰富你的文件内容，saltstack支持的模板引擎有&lt;code&gt;jinja&lt;/code&gt;等。&lt;/p&gt;

&lt;p&gt;然后在init.sls中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; jmx-console-users.properties:
   file.managed:
     - name: /home/jboss/jboss-eap-5.1/jboss-as/server/default/conf/props/jmx-console-users.properties
     - source: salt://jboss/files/jmx-console-users.properties
     - template: jinja
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2013/11/16/install-jboss-with-saltstack.html</link>
      <guid>http://blog.javachen.com/2013/11/16/install-jboss-with-saltstack.html</guid>
      <pubDate>2013-11-16T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>安装SaltStack和Halite</title>
      <description>&lt;p&gt;本文记录安装SaltStack和halite过程。&lt;/p&gt;

&lt;p&gt;首先准备两台rhel或者centos虚拟机sk1和sk2，sk1用于安装master，sk2安装minion。&lt;/p&gt;

&lt;h1 id=&quot;yum&quot;&gt;配置yum源&lt;/h1&gt;

&lt;p&gt;在每个节点上配置yum源：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过下面命令查看epel参考是否安装成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum list #或者查看/etc/yum.repos.d目录下是否有epel.repo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果没有安装成功，则可以手动下载&lt;code&gt;epel-release-6-8.noarch.rpm&lt;/code&gt;，然后打开该rpm找到&lt;code&gt;./etc/yum.repos.d/epel.repo&lt;/code&gt;，将其拷贝到&lt;code&gt;/etc/yum.repos.d&lt;/code&gt;目录&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;安装依赖&lt;/h1&gt;

&lt;p&gt;因为我用jinja2作为SaltStack的渲染引擎，故需要在每个节点上安装&lt;code&gt;python-jinja2&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum install python-jinja2 -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;saltstack&quot;&gt;安装saltstack&lt;/h1&gt;

&lt;p&gt;在sk1上安装master：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum install salt-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在sk1上安装minion：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum install salt-minion
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;关闭防火墙&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ iptables -F
$ setenforce 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;修改配置文件&lt;/h1&gt;

&lt;p&gt;修改master配置文件，使其监听&lt;code&gt;0.0.0.0&lt;/code&gt;地址，并设置自动接受minion的请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/salt/master
 interface: 0.0.0.0 #去掉对该行的注释
 auto_accept: True #去掉对该行的注释,并修改False为True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在所有的minion节点配置master的id和自己的id：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/salt/minion
 master: sk1
 id: sk2
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;启动&lt;/h1&gt;

&lt;p&gt;分别在sk1和sk2上配置开机启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chkconfig salt-master on
$ chkconfig salt-minion on
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分别在sk1和sk2上以service方式启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /etc/init.d/salt-master start
$ /etc/init.d/salt-minion start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以在sk2上以后台运行salt-minion&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt-minion -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者在sk2上debug方式运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ salt-minion -l debug
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;排错&lt;/h1&gt;

&lt;p&gt;如果启动提示如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$  /etc/init.d/salt-master start
Starting salt-master daemon: Traceback (most recent call last):
 File &quot;/usr/bin/salt-master&quot;, line 10, in &amp;lt;module&amp;gt;
   salt_master()
 File &quot;/usr/lib/python2.6/site-packages/salt/scripts.py&quot;, line 20, in salt_master
   master.start()
 File &quot;/usr/lib/python2.6/site-packages/salt/__init__.py&quot;, line 114, in start
   if check_user(self.config[&#39;user&#39;]):
 File &quot;/usr/lib/python2.6/site-packages/salt/utils/verify.py&quot;, line 296, in check_user
   if user in e.gr_mem] + [pwuser.gid])
AttributeError: &#39;pwd.struct_passwd&#39; object has no attribute &#39;gid&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请下载saltstack源码重新编译：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://github.com/saltstack/salt/archive/develop.zip
$ unzip develop
$ cd salt-develop/
$ python2.6 setup.py install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你通过’cmd.run’命令去运行java命令，你会得到这样的结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 salt]# salt &#39;*&#39; cmd.run &#39;java&#39; 
sk2:
    /bin/bash: java: command not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是因为minion在启动过程中并没有加载系统的环境变量，解决这个问题有两种方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;运行java命令前先&lt;code&gt;source&lt;/code&gt;环境变量&lt;/li&gt;
  &lt;li&gt;修改minion启动脚本，添加source命令：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;# Source function library.
if [ -f $DEBIAN_VERSION ]; then
   break
elif [ -f $SUSE_RELEASE -a -r /etc/rc.status ]; then
    . /etc/rc.status
else
    . /etc/rc.d/init.d/functions
    . ~/.bashrc
    . /etc/profile
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;salt-minionmaster&quot;&gt;salt minion和master的认证过程&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;minion在第一次启动时，会在/etc/salt/pki/minion/下自动生成minion.pem(private key), minion.pub(public key)，然后将minion.pub发送给master&lt;/li&gt;
  &lt;li&gt;master在接收到minion的public key后，通过salt-key命令accept minion public key，这样在master的/etc/salt/pki/master/minions下的将会存放以minion id命名的public key, 然后master就能对minion发送指令了&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;master上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 pillar]# salt-key -L
Accepted Keys:
Unaccepted Keys:
Rejected Keys:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接受所有的认证请求：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 pillar]# salt-key -A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次查看：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 pillar]# salt-key -L
Accepted Keys:
sk2
Unaccepted Keys:
Rejected Keys:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;salt-key&lt;/code&gt;更多说明：&lt;a href=&quot;http://docs.saltstack.com/ref/cli/salt-key.html&quot;&gt;http://docs.saltstack.com/ref/cli/salt-key.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;测试运行&lt;/h1&gt;

&lt;p&gt;在master上运行ping：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@sk1 pillar]# salt &#39;*&#39; test.ping
sk2:salt &#39;*&#39; test.ping
    True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;True表明测试成功。&lt;/p&gt;

&lt;h1 id=&quot;halite&quot;&gt;安装halite&lt;/h1&gt;

&lt;h2 id=&quot;section-6&quot;&gt;下载代码&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/saltstack/halite
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;indexhtml&quot;&gt;生成index.html&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ cd halite/halite
$ ./genindex.py -C
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;salt-api&quot;&gt;安装salt-api&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ yum install salt-api
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;salt-master&quot;&gt;配置salt master文件&lt;/h2&gt;

&lt;p&gt;配置salt的master文件，添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;rest_cherrypy:
 host: 0.0.0.0
 port: 8080
 debug: true
 static: /root/halite/halite
 app: /root/halite/halite/index.html
external_auth:
   pam:
     admin:
	 - .*
	 - &#39;@runner&#39;
	 - &#39;@wheel&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启master;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /etc/init.d/salt-master restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-7&quot;&gt;添加登陆用户&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ useradd admin
$ echo admin|passwd –stdin admin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;salt-api-1&quot;&gt;启动 salt-api&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ cd halite/halite
$ python2.6 server_bottle.py -d -C -l debug -s cherrypy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后打开&lt;code&gt;http://ip:8080/app&lt;/code&gt;，通过admin/admin登陆即可。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2013/11/11/install-saltstack-and-halite.html</link>
      <guid>http://blog.javachen.com/2013/11/11/install-saltstack-and-halite.html</guid>
      <pubDate>2013-11-11T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>在Eclipse中调试运行HBase</title>
      <description>&lt;p&gt;这篇文章记录一下如何在eclipse中调试运行hbase。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;下载并编译源代码&lt;/h1&gt;
&lt;p&gt;请参考&lt;a href=&quot;http://blog.javachen.com/hbase/2013/10/28/compile-hbase-source-code-and-apply-patches/&quot;&gt;编译hbase源代码并打补丁&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;修改配置文件&lt;/h1&gt;

&lt;p&gt;修改 &lt;code&gt;conf/hbase-site.xml&lt;/code&gt;文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.defaults.for.version&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;0.94.6-cdh4.4.0&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;file:///home/june/tmp/data&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把conf文件夹加到Classpath中&lt;/p&gt;

&lt;h1 id=&quot;hmaster&quot;&gt;运行HMaster&lt;/h1&gt;

&lt;p&gt;新建一个&lt;code&gt;Debug Configuration&lt;/code&gt;,  main class 是&lt;code&gt;org.apache.hadoop.hbase.master.HMaster&lt;/code&gt;,  参数填start&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;调试运行该类，运行成功之后日志如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;13/10/29 14:38:29 WARN zookeeper.RecoverableZooKeeper: Node /hbase/table/.META. already deleted, and this is not a retry
13/10/29 14:38:29 INFO regionserver.HRegionServer: Received request to open region: .META.,,1.1028785192
13/10/29 14:38:29 INFO regionserver.HRegion: Setting up tabledescriptor config now ...
13/10/29 14:38:29 INFO regionserver.Store: time to purge deletes set to 0ms in store info
13/10/29 14:38:29 INFO regionserver.HRegion: Onlined .META.,,1.1028785192; next sequenceid=1
13/10/29 14:38:29 INFO regionserver.HRegionServer: Post open deploy tasks for region=.META.,,1.1028785192, daughter=false
13/10/29 14:38:29 INFO catalog.MetaEditor: Updated row .META.,,1.1028785192 with server=june-mint,47477,1383028701871
13/10/29 14:38:29 INFO regionserver.HRegionServer: Done with post open deploy task for region=.META.,,1.1028785192, daughter=false
13/10/29 14:38:29 INFO handler.OpenedRegionHandler: Handling OPENED event for .META.,,1.1028785192 from june-mint,47477,1383028701871; deleting unassigned node
13/10/29 14:38:29 INFO master.AssignmentManager: The master has opened the region .META.,,1.1028785192 that was online on june-mint,47477,1383028701871
13/10/29 14:38:29 INFO master.HMaster: .META. assigned=2, rit=false, location=june-mint,47477,1383028701871
13/10/29 14:38:29 INFO catalog.MetaMigrationRemovingHTD: Meta version=0; migrated=true
13/10/29 14:38:29 INFO catalog.MetaMigrationRemovingHTD: ROOT/Meta already up-to date with new HRI.
13/10/29 14:38:29 INFO master.AssignmentManager: Clean cluster startup. Assigning userregions
13/10/29 14:38:29 INFO master.HMaster: Registered HMaster MXBean
13/10/29 14:38:29 INFO master.HMaster: Master has completed initialization
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想修改日志级别，请修改&lt;code&gt;conf/log4j.properties&lt;/code&gt;中级别为INFO:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Logging Threshold
log4j.threshold=INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hregionserver&quot;&gt;运行HRegionServer&lt;/h1&gt;

&lt;p&gt;参考上面的方法，运行HRegionServer，这时候会出现如下日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;13/11/04 11:50:47 INFO util.VersionInfo: HBase 0.94.6-cdh4.4.0
13/11/04 11:50:47 INFO util.VersionInfo: Subversion git://june-mint/chan/workspace/hadoop/hbase -r 979969e1d0d95ce3b8c1d14593f55148da8bc98f
13/11/04 11:50:47 INFO util.VersionInfo: Compiled by june on Tue Oct 29 15:11:51 CST 2013
13/11/04 11:50:47 WARN regionserver.HRegionServerCommandLine: Not starting a distinct region server because hbase.cluster.distributed is false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是因为当&lt;code&gt;hbase.cluster.distributed=false&lt;/code&gt;时，hbase为本地模式，master和regionserver在同一个jvm启动，并且会启动一个最小化的zookeeper集群。请参看：&lt;code&gt;HMasterCommandLine.java&lt;/code&gt;的startMaster()方法。&lt;/p&gt;

&lt;p&gt;如果你把该值设为true，则hbase集群为分布式模式，这时候默认会连接&lt;code&gt;127.0.0.1：2181&lt;/code&gt;对应的zookeeper集群（该集群需要在master启动之前启动）。当然，你可以修改参数让hbase自己维护一个zookeeper集群。&lt;/p&gt;

&lt;h1 id=&quot;hbase-shell&quot;&gt;调试hbase shell&lt;/h1&gt;

&lt;p&gt;新建一个&lt;code&gt;Debug Configuration&lt;/code&gt;,  main class 是&lt;code&gt;org.jruby.Main&lt;/code&gt;，在程序参数中添加&lt;code&gt;bin/hirb.rb&lt;/code&gt;,然后运行即可。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;一些技巧&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;调试java代码的时候, byte[]的变量总是显示成数字,如果要显示对应的字符&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;Window-&amp;gt;Preference-&amp;gt;Java-&amp;gt;Debug-&amp;gt;Primitive Display Options-&amp;gt;Check some of them
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;hbase源码中默认依赖的是hadoop 1.0.x版本，所以mavne依赖中会引入hadoop-core-1.0.4.jar，你可以修改pom.xml文件，将默认的profile修改为你需要的hadoop版本，如2.0版本的hadoop。这样做之后，当你看HMaster的源代码时，你会很方便的关联并浏览ToolRunner类中的源代码。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;默认的profile是hadoop-1.0，配置文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;lt;!-- profile for building against Hadoop 1.0.x: This is the default. --&amp;gt;
    &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;hadoop-1.0&amp;lt;/id&amp;gt;
      &amp;lt;activation&amp;gt;
        &amp;lt;property&amp;gt;
          &amp;lt;name&amp;gt;!hadoop.profile&amp;lt;/name&amp;gt;
        &amp;lt;/property&amp;gt;

   &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;hadoop-2.0&amp;lt;/id&amp;gt;
      &amp;lt;activation&amp;gt;
        &amp;lt;property&amp;gt;
          &amp;lt;name&amp;gt;hadoop.profile&amp;lt;/name&amp;gt;
          &amp;lt;value&amp;gt;2.0&amp;lt;/value&amp;gt;
        &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以将默认的profile改为hadoop-2.0,修改之后的配置文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;hadoop-1.0&amp;lt;/id&amp;gt;
      &amp;lt;activation&amp;gt;
        &amp;lt;property&amp;gt;
          &amp;lt;name&amp;gt;hadoop.profile&amp;lt;/name&amp;gt;
	  &amp;lt;value&amp;gt;1.0&amp;lt;/value&amp;gt;
        &amp;lt;/property&amp;gt;

   &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;hadoop-2.0&amp;lt;/id&amp;gt;
      &amp;lt;activation&amp;gt;
        &amp;lt;property&amp;gt;
          &amp;lt;name&amp;gt;!hadoop.profile&amp;lt;/name&amp;gt;
        &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2013/11/01/debug-hbase-in-eclipse.html</link>
      <guid>http://blog.javachen.com/2013/11/01/debug-hbase-in-eclipse.html</guid>
      <pubDate>2013-11-01T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>编译CDH HBase源代码并打补丁</title>
      <description>&lt;p&gt;写了一篇博客记录编译CDH HBase源代码并打补丁的过程，如有不正确的，欢迎指出！&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;下载源代码&lt;/h1&gt;

&lt;p&gt;从&lt;a href=&quot;https://github.com/cloudera/hbase&quot;&gt;Cloudera github&lt;/a&gt;上下载最新分支源代码，例如：当前最新分支为cdh4-0.94.6_4.4.0&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone git@github.com:cloudera/hbase.git -b cdh4-0.94.6_4.4.0 cdh4-0.94.6_4.4.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;-b 指定下载哪个分支&lt;/li&gt;
  &lt;li&gt;最后一个参数指定下载下来的文件名称&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;snappy&quot;&gt;添加snappy压缩支持&lt;/h1&gt;

&lt;h2 id=&quot;snappy-1&quot;&gt;编译snappy&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ svn checkout http://snappy.googlecode.com/svn/trunk/ snappy
$ cd snappy
$ sh autogen.sh
$ ./configure
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hadoop-snappy&quot;&gt;编译hadoop-snappy&lt;/h2&gt;

&lt;p&gt;降低gcc版本到4.4:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo yum install gcc-4.4
$ rm /usr/bin/gcc
$ ln -s /usr/bin/gcc-4.4 /usr/bin/gcc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;建立libjvm软连接&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo ln -s /usr/java/latest/jre/lib/amd64/server/libjvm.so  /usr/local/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- more --&gt;

&lt;p&gt;下载并编译hadoop-snappy&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ svn checkout http://hadoop-snappy.googlecode.com/svn/trunk/ hadoop-snappy
$ cd hadoop-snappy
$ make package -Dsnappy.prefix=/usr/local/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装jar包到本地仓库&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mvn install:install-file -DgroupId=org.apache.hadoop -DartifactId=hadoop-snappy -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar -Dfile=./target/hadoop-snappy-0.0.1-SNAPSHOT.jar
$ mvn install:install-file -DgroupId=org.apache.hadoop -DartifactId=hadoop-snappy -Dversion=0.0.1-SNAPSHOT -Dclassifier=Linux-amd64-64 -Dpackaging=tar -Dfile=./target/hadoop-snappy-0.0.1-SNAPSHOT-Linux-amd64-64.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;lzo&quot;&gt;添加lzo压缩支持&lt;/h1&gt;

&lt;p&gt;暂不在此列出，请参考网上文章。&lt;/p&gt;

&lt;h1 id=&quot;protobuf&quot;&gt;编译Protobuf&lt;/h1&gt;

&lt;p&gt;注意：目前只能装2.4.1版本的，装最新版本的可能会缺少文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ wget https://protobuf.googlecode.com/files/protobuf-2.4.1.zip
$ unzip protobuf-2.4.1.zip
$ cd protobuf-2.4.1
$ ./configure
$ make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试是否安装成功，如果成功你会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ protoc
Missing input file.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果安装失败，你可能会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ protoc
protoc: error while loading shared libraries: libprotobuf.so.7: cannot open shared object file: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hbase&quot;&gt;编译hbase&lt;/h1&gt;

&lt;p&gt;进入到cdh4-0.94.6_4.4.0 目录，然后运行mvn基本命令。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd cdh4-0.94.6_4.4.0
$ mvn clean install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;忽略测试，请添加如下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-DskipTests
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加MAVEN运行时jvm大小，请在mvn前面添加如下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MAVEN_OPTS=&quot;-Xmx2g&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成javadoc和文档，请添加如下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;javadoc:aggregate site assembly:single
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成release加入security和native包，请添加如下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-Prelease,security,native
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基于hadoop2.0进行编译，请添加如下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-Dhadoop.profile=2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加hadoop-snappy支持，请添加如下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-Prelease,hadoop-snappy -Dhadoop-snappy.version=0.0.1-SNAPSHOT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你添加了一些java代码，在每个文件头没有添加license，则需要添加如下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-Drat.numUnapprovedLicenses=200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;综上，完整命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ MAVEN_OPTS=&quot;-Xmx2g&quot; mvn clean install javadoc:aggregate site assembly:single -DskipTests -Prelease,security,native,hadoop-snappy -Drat.numUnapprovedLicenses=200 -Dhadoop.profile=2.0 -Dhadoop-snappy.version=0.0.1-SNAPSHOT
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;patch&quot;&gt;生成patch&lt;/h1&gt;

&lt;p&gt;修改代码之后，在提交代码之前，运行如下命令生成patch：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git diff &amp;gt;../XXXXX.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你已经将该动文件加入到提交缓存区，即执行了如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git add .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以使用如下代码打补丁：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git diff --staged &amp;gt;../XXXXX.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果在提交之后，想生成patch，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git format-patch -1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;git format-patch&lt;/code&gt; 的详细说明请参考：&lt;a href=&quot;http://devillived.net/forum/home.php?mod=space&amp;amp;uid=2&amp;amp;do=blog&amp;amp;id=211&quot;&gt;git patch操作&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;更多diff的命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git diff &amp;lt;file&amp;gt;   # 比较当前文件和暂存区文件差异
git diff
git diff &amp;lt;$id1&amp;gt; &amp;lt;$id2&amp;gt;    # 比较两次提交之间的差异
git diff &amp;lt;branch1&amp;gt;..&amp;lt;branch2&amp;gt;    # 在两个分支之间比较
git diff --staged   # 比较暂存区和版本库差异
git diff --cached   # 比较暂存区和版本库差异
git diff --stat     # 仅仅比较统计信息
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;patch-1&quot;&gt;打patch&lt;/h1&gt;

&lt;p&gt;打patch：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git apply ../XXXXX.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试patch是否打成功：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git apply --check  ../add-aggregate-in-hbase-shell.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果出现以下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git apply ../XXXXX.patch
fatal: git apply: bad git-diff - expected /dev/null on line 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请安装dos2unix：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install dos2unix -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，执行如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ dos2unix ../add-aggregate-in-hbase-shell.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后再尝试打补丁。&lt;/p&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;请注意，git apply 是一个事务性操作的命令，也就是说，要么所有补丁都打上去，要么全部放弃。&lt;/li&gt;
  &lt;li&gt;对于传统的 diff 命令生成的补丁，则只能用 git apply 处理。对于 format-patch 制作的新式补丁，应当使用 git am 命令。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-1&quot;&gt;升级版本&lt;/h1&gt;

&lt;p&gt;当你fork了&lt;a href=&quot;https://github.com/cloudera/hbase&quot;&gt;Cloudera github&lt;/a&gt;代码之后，cloudera会继续更新代码、发布新的分支，如何将其最新的分支下载到自己的hbase仓库呢？例如，你的仓库中hbase最新分支为&lt;code&gt;cdh4-0.94.6_4.3.0&lt;/code&gt;，而cdh最新分支为&lt;code&gt;cdh4-0.94.6_4.4.0&lt;/code&gt;，现在如何将cdh上的分支下载到自己的参考呢？&lt;/p&gt;

&lt;p&gt;查看远程服务器地址和仓库名称：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git remote -v
origin	git@github.com:javachen/hbase.git (fetch)
origin	git@github.com:javachen/hbase.git (push)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加远程仓库地址：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git remote add cdh git@github.com:cloudera/hbase.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再一次查看远程服务器地址和仓库名称：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git remote -v
cdh	https://github.com/cloudera/hbase (fetch)
cdh	https://github.com/cloudera/hbase (push)
origin	git@github.com:javachen/hbase.git (fetch)
origin	git@github.com:javachen/hbase.git (push)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;抓取远程仓库更新：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git fetch cdh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，再执行下面命令查看远程分支：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git branch -r
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载cdh上的cdh4-0.94.6_4.4.0分支，在本地命名为cdh4-0.94.6_4.4.0：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git checkout -b cdh4-0.94.6_4.4.0 cdh/cdh4-0.94.6_4.4.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将本地的cdh4-0.94.6_4.4.0分支其提交到自己的远程仓库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git push origin cdh4-0.94.6_4.4.0:cdh4-0.94.6_4.4.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;排错&lt;/h1&gt;

&lt;p&gt;如果在启动 hbase 的服务过程中出现如下日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: HBase Unknown
2013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: Subversion Unknown -r Unknown
2013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: Compiled by Unknown on Unknown
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请查看 src/saveVersion.sh 文件的编码及换行符是否和你的操作系统一致。编码应该设置为 UTF-8，如果你使用的是 linux 系统，则换行符应该为 unix/linux 换行符，不应该为 window 换行符。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://robbinfan.com/blog/34/git-common-command&quot;&gt;Git常用命令备忘&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://devillived.net/forum/home.php?mod=space&amp;amp;uid=2&amp;amp;do=blog&amp;amp;id=211&quot;&gt;git patch操作&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://blog.tsnrose.com/blog/2012/04/18/git-fetch/&quot;&gt;Git Fetch拉取他人分支&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[4] &lt;a href=&quot;http://smilejay.com/2012/08/generate-a-patch-from-a-commit/&quot;&gt;git根据commit生成patch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2013/10/28/compile-hbase-source-code-and-apply-patches.html</link>
      <guid>http://blog.javachen.com/2013/10/28/compile-hbase-source-code-and-apply-patches.html</guid>
      <pubDate>2013-10-28T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HiveServer2中使用jdbc客户端用户运行mapreduce</title>
      <description>&lt;p&gt;最近做了个web系统访问hive数据库，类似于官方自带的hwi、安居客的&lt;a href=&quot;https://github.com/anjuke/hwi&quot;&gt;hwi改进版&lt;/a&gt;和大众点评的&lt;a href=&quot;http://blog.csdn.net/lalaguozhe/article/details/9614061&quot;&gt;polestar&lt;/a&gt;(&lt;a href=&quot;https://github.com/dianping/polestar&quot;&gt;github地址&lt;/a&gt;)系统，但是和他们的实现不一样，查询Hive语句走的不是cli而是通过jdbc连接hive-server2。为了实现mapreduce任务中资源按用户调度，需要hive查询自动绑定当前用户、将该用户传到yarn服务端并使mapreduce程序以该用户运行。本文主要是记录实现该功能过程中遇到的一些问题以及解决方法,如果你有更好的方法和建议，欢迎留言发表您的看法！&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;说明&lt;/h1&gt;

&lt;p&gt;集群环境使用的是cdh4.3，没有开启kerberos认证。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;写完这篇文章之后，在微博上收到&lt;a href=&quot;http://weibo.com/shanchao1?from=profile&amp;amp;wvr=5&amp;amp;loc=infdomain&quot;&gt;@单超eric&lt;/a&gt;的&lt;a href=&quot;http://weibo.com/1789178264/AeMItpBRk&quot;&gt;评论&lt;/a&gt;，发现cdh4.3中hive-server2已经实现&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Security-Guide/cdh4sg_topic_9_1.html#topic_9_1_unique_4&quot;&gt;Impersonation&lt;/a&gt;功能，再此对@单超eric的帮助表示感谢。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;so，你可以完全忽略本文后面的内容，直接看cloudera的HiveServer2 Impersonation是怎么做的。&lt;/p&gt;

&lt;h1 id=&quot;hive-server2&quot;&gt;hive-server2的启动&lt;/h1&gt;

&lt;p&gt;先从hive-server2服务的启动开始说起。&lt;/p&gt;

&lt;p&gt;如果你是以服务的方式启动hive-server2进程，则启动hive-server2的用户为hive,运行mapreduce的用户也为hive，启动脚本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/etc/init.d/hive-server2 start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你以命令行方式启动hive-server2进程，则启动hive-server2的用户为root,运行mapreduce的用户也为root，启动脚本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive --service hiveserver2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为什么是上面的结论？这要从hive-server2的启动过程开始说明。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;查看HiveServer2.java的代码可以看到，hive-server2启动时会依次启动&lt;code&gt;cliService&lt;/code&gt;和&lt;code&gt;thriftCLIService&lt;/code&gt;，查看cliService的init()方法，可以看到如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public synchronized void init(HiveConf hiveConf) {
    this.hiveConf = hiveConf;

    sessionManager = new SessionManager();
    addService(sessionManager);
    try {
      HiveAuthFactory.loginFromKeytab(hiveConf);
      serverUserName = ShimLoader.getHadoopShims().
          getShortUserName(ShimLoader.getHadoopShims().getUGIForConf(hiveConf));
    } catch (IOException e) {
      throw new ServiceException(&quot;Unable to login to kerberos with given principal/keytab&quot;, e);
    } catch (LoginException e) {
      throw new ServiceException(&quot;Unable to login to kerberos with given principal/keytab&quot;, e);
    }
    super.init(hiveConf);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的代码可以看到在cliService初始化过程中会做登陆（从kertab中登陆）和获取用户名的操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ShimLoader.getHadoopShims().getUGIForConf(hiveConf)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码最终会调用HadoopShimsSecure类的getUGIForConf方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Override
public UserGroupInformation getUGIForConf(Configuration conf) throws IOException {
  return UserGroupInformation.getCurrentUser();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;UserGroupInformation.getCurrentUser()代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; public synchronized
  static UserGroupInformation getCurrentUser() throws IOException {
    AccessControlContext context = AccessController.getContext();
    Subject subject = Subject.getSubject(context);
    if (subject == null || subject.getPrincipals(User.class).isEmpty()) {
      return getLoginUser();
    } else {
      return new UserGroupInformation(subject);
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为这时候服务刚启动，subject为空，故if分支会调用&lt;code&gt;getLoginUser()&lt;/code&gt;方法，其代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  public synchronized 
  static UserGroupInformation getLoginUser() throws IOException {
    if (loginUser == null) {
      try {
        Subject subject = new Subject();
        LoginContext login;
        if (isSecurityEnabled()) {
          login = newLoginContext(HadoopConfiguration.USER_KERBEROS_CONFIG_NAME,
              subject, new HadoopConfiguration());
        } else {
          login = newLoginContext(HadoopConfiguration.SIMPLE_CONFIG_NAME, 
              subject, new HadoopConfiguration());
        }
        login.login();
        loginUser = new UserGroupInformation(subject);
        loginUser.setLogin(login);
        loginUser.setAuthenticationMethod(isSecurityEnabled() ?
                                          AuthenticationMethod.KERBEROS :
                                          AuthenticationMethod.SIMPLE);
        loginUser = new UserGroupInformation(login.getSubject());
        String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION);
        if (fileLocation != null) {
          // Load the token storage file and put all of the tokens into the
          // user. Don&#39;t use the FileSystem API for reading since it has a lock
          // cycle (HADOOP-9212).
          Credentials cred = Credentials.readTokenStorageFile(
              new File(fileLocation), conf);
          loginUser.addCredentials(cred);
        }
        loginUser.spawnAutoRenewalThreadForUserCreds();
      } catch (LoginException le) {
        LOG.debug(&quot;failure to login&quot;, le);
        throw new IOException(&quot;failure to login&quot;, le);
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug(&quot;UGI loginUser:&quot;+loginUser);
      }
    }
    return loginUser;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为是第一次调用getLoginUser(),故loginUser为空，接下来会创建LoginContext并调用其login方法，login方法最终会调用HadoopLoginModule的commit()方法。&lt;/p&gt;

&lt;p&gt;下图是从hive-server2启动到执行HadoopLoginModule的commit()方法的调用图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/hive-server2-invoke.png&quot; alt=&quot;hive-server2启动过程&quot; /&gt;&lt;/p&gt;

&lt;p&gt;获取登陆用户的关键代码就在commit()，逻辑如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果使用了kerberos，则为kerberos登陆用户。hive-server2中如何使用kerberos登陆，请查看官方文档。&lt;/li&gt;
  &lt;li&gt;如果kerberos用户为空并且没有开启security，则从系统环境变量中取&lt;code&gt;HADOOP_USER_NAME&lt;/code&gt;的值&lt;/li&gt;
  &lt;li&gt;如果环境变量中没有设置&lt;code&gt;HADOOP_USER_NAME&lt;/code&gt;，则使用系统用户，即启动hive-server2进程的用户。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;小结&lt;/h2&gt;

&lt;p&gt;hive-server2启动过程中会做登陆操作并获取到登陆用户，启动之后再次调用&lt;code&gt;UserGroupInformation.getCurrentUser()&lt;/code&gt;取到的用户就为登陆用户了，这样会导致所有请求到hive-server2的hql最后都会以这个用户来运行mapreduce。&lt;/p&gt;

&lt;h1 id=&quot;hive&quot;&gt;提交hive任务&lt;/h1&gt;

&lt;p&gt;现在来看hive任务是怎么提交到yarn服务端然后运行mapreduce的。&lt;/p&gt;

&lt;p&gt;为了调试简单，我在本机eclipse的hive源代码中配置&lt;code&gt;hive-site.xml、core-site.xml、mapred.xml、yarn-site.xml&lt;/code&gt;连接测试集群,添加缺少的yarn依赖并解决hive-builtins中报错的问题，然后运行HiveServer2类的main方法。&lt;em&gt;注意&lt;/em&gt;，我的电脑当前登陆用户为june，故启动hive-server2的用户为june。&lt;/p&gt;

&lt;p&gt;然后，在运行jdbc测试类，运行一个简单的sql语句，大概如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static void test() {
	try {
		Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);

		Connection conn = DriverManager.getConnection(
				&quot;jdbc:hive2://june-mint:10000/default&quot;, &quot;&quot;, &quot;&quot;);

		Statement stmt = conn.createStatement();

		ResultSet rs = stmt.executeQuery(&quot;select count(1) from t&quot;);

		while (rs.next())
			System.out.println(rs.getString(1));

		rs.close();
		stmt.close();
		conn.close();
	} catch (SQLException se) {
		se.printStackTrace();
	} catch (Exception e) {
		e.printStackTrace();
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看yarn监控地址&lt;code&gt;http://192.168.56.101:8088/cluster&lt;/code&gt;，可以看到提交的mapreduce任务由june用户来运行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/20131017-01.png&quot; alt=&quot;yarn cluster monitor page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如何修改mapreduce任务的运行用户呢？如果了解hive提交mapreduce任务的过程的话，就应该知道hive任务会通过&lt;code&gt;org.apache.hadoop.mapred.JobClient&lt;/code&gt;来提交。在JobClient的init方法中有如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  public void init(JobConf conf) throws IOException {
    setConf(conf);
    cluster = new Cluster(conf);
    clientUgi = UserGroupInformation.getCurrentUser();
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;JobClient类中提交mapreduce任务的代码如下，见submitJobInternal方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Job job = clientUgi.doAs(new PrivilegedExceptionAction&amp;lt;Job&amp;gt; () {
	@Override
	public Job run() throws IOException, ClassNotFoundException, 
	  InterruptedException {
	  Job job = Job.getInstance(conf);
	  job.submit();
	  return job;
	}
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从前面知道，hive-server2启动中会进行登陆操作并且登陆用户为june，故clientUgi对应的登陆用户也为june，故提交的mapreduce任务也通过june用户来运行。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;如何修改源代码&lt;/h1&gt;

&lt;p&gt;从上面代码可以知道，修改clientUgi的获取方式就可以改变提交任务的用户。UserGroupInformation中存在如下静态方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  public static UserGroupInformation createRemoteUser(String user) {
    if (user == null || &quot;&quot;.equals(user)) {
      throw new IllegalArgumentException(&quot;Null user&quot;);
    }
    Subject subject = new Subject();
    subject.getPrincipals().add(new User(user));
    UserGroupInformation result = new UserGroupInformation(subject);
    result.setAuthenticationMethod(AuthenticationMethod.SIMPLE);
    return result;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;故可以尝试使用该方法，修改JobClient的init方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; public void init(JobConf conf) throws IOException {
    setConf(conf);
    cluster = new Cluster(conf);
    if(UserGroupInformation.isSecurityEnabled()){
    	clientUgi = UserGroupInformation.getCurrentUser();
    }else{
    	String user = conf.get(&quot;myExecuteName&quot;,&quot;NoName&quot;);
    	clientUgi = UserGroupInformation.createRemoteUser(user);
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码是在没有开启security的情况下，从环境变量（myExecuteName）获取jdbc客户端指定的用户名，然后创建一个远程的UserGroupInformation。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;为什么从环境变量中获取用户名称？&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;在不考虑安全的情况下，可以由客户端任意指定用户。&lt;/li&gt;
  &lt;li&gt;没有使用jdbc连接信息中的用户，是因为这样会导致每次获取jdbc连接的时候都要指定用户名，这样就没法使用已有的连接池。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;编译代码、替换class文件，然后重新运行HiveServer2以及jdbc测试类，查看yarn监控地址&lt;code&gt;http://192.168.56.101:8088/cluster&lt;/code&gt;，截图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/20131017-02.png&quot; alt=&quot;yarn cluster monitor page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这时候mapreduce的运行用户变为NoName，这是因为从JobConf环境变量中找不到myExecuteName变量而使用默认值NoName的原因。&lt;/p&gt;

&lt;p&gt;查看hive-server2运行日志，会发现任务运行失败，关键异常信息如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=NoName, access=WRITE, inode=&quot;/tmp/hive-june/hive_2013-10-18_21-18-12_812_378750610917949668/_tmp.-ext-10001&quot;:june:hadoop:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:224)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:204)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:149)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4705)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4687)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:4661)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameToInternal(FSNamesystem.java:2696)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameToInt(FSNamesystem.java:2663)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameTo(FSNamesystem.java:2642)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rename(NameNodeRpcServer.java:610)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rename
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出现上述异常是因为，mapreduce任务在运行过程中会生成一些临时文件，而NoName用户对临时文件没有写的权限，这些临时文件属于june用户。查看hdfs文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@edh1 lib]# hadoop fs -ls /tmp/
Found 6 items
drwx------   - june hadoop          0 2013-10-15 01:33 /tmp/hadoop-yarn
drwxr-xr-x   - june hadoop          0 2013-10-16 06:52 /tmp/hive-june
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/tmp/hive-june&lt;/code&gt;是hive执行过程中保存在hdfs的路径，由&lt;code&gt;hive.exec.scratchdir&lt;/code&gt;定义，其默认值为&lt;code&gt;/tmp/hive-${user.name}&lt;/code&gt;，而且这个文件是在&lt;code&gt;org.apache.hadoop.hive.ql.Context&lt;/code&gt;类的构造方法中获取并在ExecDriver类的execute(DriverContext driverContext)方法中创建的。&lt;/p&gt;

&lt;p&gt;类似这样的权限问题还会出现在hdfs文件&lt;code&gt;重命名、删除临时目录的时候&lt;/code&gt;。为了避免出现这样的异常，需要修改&lt;code&gt;hive.exec.scratchdir&lt;/code&gt;为当前用户对应的临时目录路径，并使用当前登陆用户创建、重命名、删除临时目录。&lt;/p&gt;

&lt;p&gt;修改获取&lt;code&gt;hive.exec.scratchdir&lt;/code&gt;对应的临时目录代码如下，在Context类的够找方法中修改：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    String user = conf.get(myExecuteName，“”);

    if (user != null &amp;amp;&amp;amp; user.trim().length() &amp;gt; 0) {
      nonLocalScratchPath =
          new Path(&quot;/tmp/hive-&quot; + user, executionId);
    } else {
      nonLocalScratchPath =
          new Path(HiveConf.getVar(conf, HiveConf.ConfVars.SCRATCHDIR),
              executionId);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到这些操作对应的代码似乎太过复杂了，修改的地方也有很多，因为这里是使用的hive-server2，故在对应的jdbc代码中修改似乎会简单很多，例如修改HiveSessionImpl类的以下三个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public OperationHandle executeStatement(String statement, Map&amp;lt;String, String&amp;gt; confOverlay) throws HiveSQLException{}

public void cancelOperation(final OperationHandle opHandle) throws HiveSQLException {}

public void closeOperation(final OperationHandle opHandle) throws HiveSQLException {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个方法是运行sql语句，第二个方法是取消运行，第三个方法是关闭连接。&lt;/p&gt;

&lt;p&gt;executeStatement中所做的修改如下，将&lt;code&gt;operation.run();&lt;/code&gt;改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    if (operation instanceof SQLOperation) {
        try {
          String user = hiveConf.getVar(ConfVars.HIVE_SERVER2_MAPREDUCE_USERNAME);
          ugi = UserGroupInformation.createRemoteUser(user);
          ugi.doAs(new PrivilegedExceptionAction&amp;lt;CommandProcessorResponse&amp;gt;() {
            @Override
            public CommandProcessorResponse run() throws HiveSQLException {
              operation.run();
              return null;
            }
          });
        } catch (IOException e) {
          e.printStackTrace();
        } catch (InterruptedException e) {
          e.printStackTrace();
        }
      } else {
        operation.run();
      }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里添加了判断，当operation操作时，才执行下面代码，这是为了保证从hive环境变量中获取myExecuteName的值不为空时才创建UserGroupInformation。&lt;/p&gt;

&lt;p&gt;myExecuteName是新定义的hive变量，主要是用于jdbc客户端通过set语句设置myExecuteName的值为当前登陆用户名称，然后在执行sql语句。代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Statement stmt = conn.createStatement();

stmt.execute(&quot;set myExecuteName=aaaa&quot;);
ResultSet rs = stmt.executeQuery(&quot;select count(1) from t&quot;);

while (rs.next())
	System.out.println(rs.getString(1));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;小结&lt;/h2&gt;

&lt;p&gt;上面修改的类包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;org.apache.hadoop.mapred.JobClient //从环境变量获取从jdbc客户端传过来的用户，即myExecuteName的值，然后以该值运行mapreduce用户
org.apache.hadoop.hive.ql.Context  //修改hive.exec.scratchdir的地址为从jdbc客户端传过来的用户对应的临时目录
org.apache.hive.service.cli.session.HiveSessionImpl //修改运行sql、取消操作、关闭连接对应的方法
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-5&quot;&gt;测试&lt;/h1&gt;
&lt;p&gt;是用javachen用户测试,hdfs上的临时目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@edh1 lib]# hadoop fs -ls /tmp/
Found 7 items
drwx------   - june         hadoop          0 2013-10-15 01:33 /tmp/hadoop-yarn
drwxr-xr-x   - javachen.com hadoop          0 2013-10-16 07:30 /tmp/hive-javachen.com
drwxr-xr-x   - june         hadoop          0 2013-10-16 06:52 /tmp/hive-june
drwxr-xr-x   - root         hadoop          0 2013-10-15 14:13 /tmp/hive-root
drwxrwxrwt   - yarn         mapred          0 2013-10-16 07:30 /tmp/logs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监控页面截图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/20131017-03.png&quot; alt=&quot;yarn cluster monitor page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;除了简单测试之外，还需要测试修改后的代码是否影响源代码的运行以及hive cli的运行。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;参考文章&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Setting+up+HiveServer2&quot;&gt;HiveServer2 Impersonation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.3.0/CDH4-Security-Guide/cdh4sg_topic_9_1.html&quot;&gt;CDH4 HiveServer2 Security Configuration&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;enjoy-it-&quot;&gt;Enjoy it ！&lt;/h1&gt;

</description>
      <link>http://blog.javachen.com/2013/10/17/run-mapreduce-with-client-user-in-hive-server2.html</link>
      <guid>http://blog.javachen.com/2013/10/17/run-mapreduce-with-client-user-in-hive-server2.html</guid>
      <pubDate>2013-10-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive连接产生笛卡尔集</title>
      <description>&lt;p&gt;在使用hive过程中遇到这样的一个异常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FAILED: ParseException line 1:18 Failed to recognize predicate &#39;a&#39;. Failed rule: &#39;kwInner&#39; in join type specifier
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行的hql语句如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@javachen.com ~]# hive -e &#39;select a.* from t a, t b where a.id=b.id&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从异常信息中很难看出出错原因，hive.log中也没有打印出详细的异常对战信息。改用jdbc连接hive-server2，可以看到hive-server2中提示如下异常信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;13/10/17 09:57:48 ERROR ql.Driver: FAILED: ParseException line 1:18 Failed to recognize predicate &#39;a&#39;. Failed rule: &#39;kwInner&#39; in join type specifier

org.apache.hadoop.hive.ql.parse.ParseException: line 1:18 Failed to recognize predicate &#39;a&#39;. Failed rule: &#39;kwInner&#39; in join type specifier

	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:446)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:349)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:355)
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:95)
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:76)
	at org.apache.hive.service.cli.operation.SQLOperation.run(SQLOperation.java:114)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:194)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:155)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:191)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1193)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.cli.thrift.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:38)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- more --&gt;

&lt;p&gt;从异常信息可以看到是在编译hql语句进行语法解析时出现了错误，到底为什么会出现&lt;code&gt;Failed rule: &#39;kwInner&#39; in join type specifier&lt;/code&gt;这样的异常信息呢？&lt;/p&gt;

&lt;p&gt;在eclipse中查找关键字并没有找到相应代码，在&lt;a href=&quot;http://svn.apache.org/repos/asf/hive/tags/release-0.10.0/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g&quot;&gt;Hive.g&lt;/a&gt; 中查找关键字“kwInner”，可以看到如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;joinToken
@init { msgs.push(&quot;join type specifier&quot;); }
@after { msgs.pop(); }
    :
      KW_JOIN                     -&amp;gt; TOK_JOIN
    | kwInner  KW_JOIN            -&amp;gt; TOK_JOIN
    | KW_CROSS KW_JOIN            -&amp;gt; TOK_CROSSJOIN
    | KW_LEFT  KW_OUTER KW_JOIN   -&amp;gt; TOK_LEFTOUTERJOIN
    | KW_RIGHT KW_OUTER KW_JOIN   -&amp;gt; TOK_RIGHTOUTERJOIN
    | KW_FULL  KW_OUTER KW_JOIN   -&amp;gt; TOK_FULLOUTERJOIN
    | KW_LEFT  KW_SEMI  KW_JOIN   -&amp;gt; TOK_LEFTSEMIJOIN
    ;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面可以看出hive支持的连接包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;join&lt;/li&gt;
  &lt;li&gt;inner join&lt;/li&gt;
  &lt;li&gt;cross join (as of Hive 0.10)&lt;/li&gt;
  &lt;li&gt;left outer join&lt;/li&gt;
  &lt;li&gt;right outer join&lt;/li&gt;
  &lt;li&gt;full outer join&lt;/li&gt;
  &lt;li&gt;left semi join&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;kwInner为什么是小写呢，其含义是什么呢？搜索关键字，找到如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kwInner
:
{input.LT(1).getText().equalsIgnoreCase(&quot;inner&quot;)}? Identifier;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的大概意思是找到输入左边的内容并判断其值在忽略大小写情况下是否等于inner，大概意思是hql语句中缺少inner关键字吧？修改下hql语句如下，然后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@javachen.com ~]#  hive -e &#39;select a.* from t a inner join t b where a.id=b.id&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改后的hql语句能够正常运行，并且变成了内连接。&lt;code&gt;在JION接连查询中没有ON连接key而通过WHERE条件语句会产生笛卡尔集。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Hive本身是不支持笛卡尔集的，不能用&lt;code&gt;select T1.*, T2.* from table1, table2&lt;/code&gt;这种语法。但有时候确实需要用到笛卡尔集的时候，可以用下面的语法来实现同样的效果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select T1.*, T2.* from table1 T1 join table2 T2 where 1=1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意在Hive的Strict模式下不能用这种语法，因为这样会产生笛卡尔集，而这种模式禁止产生笛卡尔集。需要先用&lt;code&gt;set hive.mapred.mode=nonstrict;&lt;/code&gt;设为非strict模式就可以用了，或者将where改为on连接。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select T1.*, T2.* from table1 T1 join table2 T2 on  T1.id=T2.id;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;strict-mode&quot;&gt;关于Strict Mode&lt;/h1&gt;

&lt;p&gt;Hive中的严格模式可以防止用户发出（可以有问题）的查询无意中造成不良的影响。 将&lt;code&gt;hive.mapred.mode&lt;/code&gt;设置成strict可以禁止三种类型的查询：&lt;/p&gt;

&lt;p&gt;1）、在一个分区表上，如果没有在WHERE条件中指明具体的分区，那么这是不允许的，换句话说，不允许在分区表上全表扫描。这种限制的原因是分区表通常会持非常大的数据集并且可能数据增长迅速，对这样的一个大表做全表扫描会消耗大量资源，必须要再WHERE过滤条件中具体指明分区才可以执行成功的查询。&lt;/p&gt;

&lt;p&gt;2）、第二种是禁止执行有ORDER BY的排序要求但没有LIMIT语句的HiveQL查询。因为ORDER BY全局查询会导致有一个单一的reducer对所有的查询结果排序，如果对大数据集做排序，这将导致不可预期的执行时间，必须要加上limit条件才可以执行成功的查询。&lt;/p&gt;

&lt;p&gt;3）、第三种是禁止产生笛卡尔集。在JION接连查询中没有ON连接key而通过WHERE条件语句会产生笛卡尔集，需要改为JOIN…ON语句。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://flyingdutchman.iteye.com/blog/1871983&quot;&gt;深入学习《Programing Hive》：Tuning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://blog.hesey.net/2012/04/hive-tips.html&quot;&gt;Hive Tips&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2013/10/17/cartesian-product-in-hive-inner-join.html</link>
      <guid>http://blog.javachen.com/2013/10/17/cartesian-product-in-hive-inner-join.html</guid>
      <pubDate>2013-10-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>最近的工作</title>
      <description>&lt;p&gt;最近一直在构思这篇博客的内容，到现在还是不知道从何下手。自从将博客从wordpress迁移到github上之后，就很少在博客写一些关于工作和生活的文章，所以想写一篇关于工作的博客，记录最近做过的事情以及一些当时的所思所想。&lt;/p&gt;

&lt;p&gt;最近半年多一直在做hadoop方面的工作，也就是接触hadoop才半年多时间。最开始接触hadoop是去年的11月21日，那天去Intel公司参加了两天的hadoop培训。培训的内容很多干货也有很多枯燥的东西，所以边听边瞌睡的听完了两天的培训内容。培训的ppt打印出来了，时不时地会翻看上面讲述的内容，然后在网上搜索些相关的资料。&lt;/p&gt;

&lt;p&gt;最先接触的hadoop发行版是Intel的IDH，刚开始使用IDH也就是用他的管理界面安装、部署hadoop集群，然后在8节点的集群上作hive两表关联的测试。测试结果不是很满意，但是对IDH倒是印象深刻。IDH的前端使用GWT开发，界面简洁，操作也比较方便，只是同步配置文件有时候非常慢，要等上一杯咖啡的时间。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;IDH的源代码不开源，所以遇到一些问题的时候，只能先自己摸索。我不喜欢闭源也不喜欢许可证以及混淆代码什么的。虽然java代码的混淆了，但是shell和puppet脚本还是能够很容易读懂的。参考IDH的部署安装脚本，我在试着用shell编写一个hadoop的安装部署脚本，这个工作还在慢慢进行中。也许在弄懂puppet的原理和代码之后，我会简化、改进IDH的脚本；也许会使用公司使用的saltstack来完善这个部署脚本。&lt;/p&gt;

&lt;p&gt;IDH实现了hive over hbase，这是一个很不错的特性，其基于hbase的协作器，代码实现并不复杂。IDH中有个hmon用于监控hadoop，我已经把这部分代码反编译了。&lt;/p&gt;

&lt;p&gt;在hadoop版本选择的过程中，体验了Cloudera的CDH。首先是CDH的压缩包手动安装hadoop集群，一点点的修改配置文件，直到最后集群能够成功启动，这种方式安装的hadoop集群不包含本地lib文件；然后又试着解压缩Cloudera-manager.bin文件，尝试在虚拟机中不连接网络的情况下通过Cloudera-manager来安装配置集群，在使用了一段时间之后，发现Cloudera-manager没有使用操作系统的service服务来管理hadoop组件的启动和停止，而是有自己一套实现来管理集群，再加上Cloudera-manager也不开源，故放弃了使用Cloudera-manager来安装集群的方式；最后，最后是使用rpm方式来安装hadoop，安装过程倒是不复杂，只是以后如果自己修改了源代码时候升级不是很方便了，Cloudera的github上有个cdh-package项目，这个其实就是apache的bigtop项目，试过通过这个来编译出hadoop的rpm包，没有成功。&lt;/p&gt;

&lt;p&gt;IDH通过本地文件来保存集群的配置信息，而CDH将这些信息保存到数据库了。CDH的Cloudera-manager的web界面基于bootstrap和jquery插件，ui做的很不错，通过反编译java代码，已经知道了其web界面的实现方式以及编译成功了部分java代码。CDH的Cloudera-manager和IDH-manager也很大不同，我想在这两个的基础上也实现一个hadoop的manager，这是我个人想法，还需要研究、整理出他们的实现思路，然后一点点的构思自己该怎么做。Cloudera-manager免费版简称CMF。&lt;/p&gt;

&lt;p&gt;差不多两个月前，公司想做个hive的查询界面，这个东西其实就是和hwi、hue差不多的个东西。基于Spring，我很快搭好了框架，然后把CMF其中的css和js都迁移过来了。这是我第一次接触bootstrap，稍微修改下代码一个前台框架就弄好了，CMF最主要是使用了require.js使javascript代码模块化，这东西改起来也很简单，我把CMF中大部分基础javascript代码都移到了我搭好的框架中。搭好这个框架没花多少时间，但后来公司不打算使用这一套框架，以后有时间我会继续基于这个框架开发个hadoop的管理界面。&lt;/p&gt;

&lt;p&gt;在使用ganglia监控hadoop的过程中，有时候需要找一个监控指标需要花好长时间，而且一个页面上显示的指标太多的时候，这个页面会反应不过来。hortonworks的HDP发行版中似乎对ganglia做了些修改，具体不知道改了什么。HDP发行版没有使用和研究过，只是下载了1.3和2.0两个版本的VM，然后看了看其中的hue，觉得做的很不错，只可惜是python写的。&lt;/p&gt;

&lt;p&gt;在使用hadoop的过程中，觉得hadoop的门槛对于用户来说还是有点高。用户需要学习hive语法，写出的sql语句通常都不是最优sql，如果有个sql优化器自动帮用户优化sql语句就好了。hbase用于监控业务日志，数据建模和编写代码查询数据对于业务人员来说难度也太大了，如果能够适度封装，让业务人员不用关心hadoop的细节，只需要编写sql语句就能查询数据该多好啊！&lt;/p&gt;

&lt;p&gt;其他工作：hive和mapreduce调优。&lt;/p&gt;

&lt;p&gt;上面是最近在做的一些事情，包括暂停没做的、正在做的以及还未做的。有时候觉得有些事情很简单，但没有时间、没有精力也没有自由一下子做完，有些时候人在江湖，身不由己。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2013/09/08/recent-work.html</link>
      <guid>http://blog.javachen.com/2013/09/08/recent-work.html</guid>
      <pubDate>2013-09-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive中如何确定map数</title>
      <description>&lt;p&gt;Hive 是基于 Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的 sql 查询功能，可以将 sq l语句转换为 MapReduce 任务进行运行。当运行一个 hql 语句的时候，map 数是如何计算出来的呢？有哪些方法可以调整 map 数呢？&lt;/p&gt;

&lt;p&gt;本文测试集群版本：&lt;code&gt;cdh-4.3.0&lt;/code&gt; 。&lt;/p&gt;

&lt;h1 id=&quot;hive--input-format&quot;&gt;hive 默认的 input format&lt;/h1&gt;

&lt;p&gt;在 &lt;code&gt;cdh-4.3.0&lt;/code&gt; 的 hive 中查看 &lt;code&gt;hive.input.format&lt;/code&gt; 值（为什么是&lt;code&gt;hive.input.format&lt;/code&gt;？）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hive&amp;gt; set hive.input.format;
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到默认值为 CombineHiveInputFormat，如果你使用的是 &lt;code&gt;IDH&lt;/code&gt; 的hive，则默认值为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hive&amp;gt; set hive.input.format;
hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CombineHiveInputFormat 类继承自 HiveInputFormat，而 HiveInputFormat 实现了 &lt;code&gt;org.apache.hadoop.mapred.InputFormat&lt;/code&gt; 接口，关于 InputFormat 的分析，可以参考&lt;a href=&quot;http://flyingdutchman.iteye.com/blog/1876400&quot;&gt;Hadoop深入学习：InputFormat组件&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;inputformat-&quot;&gt;InputFormat 接口功能&lt;/h1&gt;

&lt;p&gt;简单来说，InputFormat 主要用于描述输入数据的格式，提供了以下两个功能：&lt;/p&gt;

&lt;p&gt;1)、数据切分，按照某个策略将输入数据且分成若干个 split，以便确定 Map Task 的个数即 Mapper 的个数，在 MapReduce 框架中，一个 split 就意味着需要一个 Map Task;&lt;/p&gt;

&lt;p&gt;2)、为 Mapper 提供输入数据，即给定一个 split(使用其中的 RecordReader 对象)将之解析为一个个的 key/value 键值对。&lt;/p&gt;

&lt;p&gt;该类接口定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public interface InputFormat&amp;lt;K,V&amp;gt;{
	public InputSplit[] getSplits(JobConf job,int numSplits) throws IOException; 
	public RecordReader&amp;lt;K,V&amp;gt; getRecordReader(InputSplit split,JobConf job,Reporter reporter) throws IOException; 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中，&lt;code&gt;getSplit()&lt;/code&gt; 方法主要用于切分数据，每一份数据由，split 只是在逻辑上对数据分片，并不会在磁盘上将数据切分成 split 物理分片，实际上数据在 HDFS 上还是以 block 为基本单位来存储数据的。InputSplit 只记录了 Mapper 要处理的数据的元数据信息，如起始位置、长度和所在的节点。&lt;/p&gt;

&lt;p&gt;MapReduce 自带了一些 InputFormat 的实现类：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://dl2.iteye.com/upload/attachment/0085/0423/fa2e8c9f-f26a-3184-98e7-277c1b56fda1.jpg&quot; alt=&quot;InputFormat实现类&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hive 中有一些 InputFormat 的实现类，如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;AvroContainerInputFormat
RCFileBlockMergeInputFormat
RCFileInputFormat
FlatFileInputFormat
OneNullRowInputFormat
ReworkMapredInputFormat
SymbolicInputFormat
SymlinkTextInputFormat
HiveInputFormat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HiveInputFormat 的子类有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/implement-of-hiveinputformat.png&quot; alt=&quot;HiveInputFormat的子类&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;hiveinputformat&quot;&gt;HiveInputFormat&lt;/h1&gt;

&lt;p&gt;以 HiveInputFormat 为例，看看其getSplit()方法逻辑：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;for (Path dir : dirs) {
  PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir);
  // create a new InputFormat instance if this is the first time to see this
  // class
  Class inputFormatClass = part.getInputFileFormatClass();
  InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);
  Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), newjob);

  // Make filter pushdown information available to getSplits.
  ArrayList&amp;lt;String&amp;gt; aliases =
      mrwork.getPathToAliases().get(dir.toUri().toString());
  if ((aliases != null) &amp;amp;&amp;amp; (aliases.size() == 1)) {
    Operator op = mrwork.getAliasToWork().get(aliases.get(0));
    if ((op != null) &amp;amp;&amp;amp; (op instanceof TableScanOperator)) {
      TableScanOperator tableScan = (TableScanOperator) op;
      pushFilters(newjob, tableScan);
    }
  }

  FileInputFormat.setInputPaths(newjob, dir);
  newjob.setInputFormat(inputFormat.getClass());
  InputSplit[] iss = inputFormat.getSplits(newjob, numSplits / dirs.length);
  for (InputSplit is : iss) {
    result.add(new HiveInputSplit(is, inputFormatClass.getName()));
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码主要过程是：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;遍历每个输入目录，然后获得 PartitionDesc 对象，从该对象调用 getInputFileFormatClass 方法得到实际的 InputFormat 类，并调用其 &lt;code&gt;getSplits(newjob, numSplits / dirs.length)&lt;/code&gt; 方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;按照上面代码逻辑，似乎 hive 中每一个表都应该有一个 InputFormat 实现类。在 hive 中运行下面代码，可以查看建表语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;hive&amp;gt; show create table info; 
OK
CREATE  TABLE info(
  statist_date string, 
  statistics_date string, 
  inner_code string, 
  office_no string, 
  window_no string, 
  ticket_no string, 
  id_kind string, 
  id_no string, 
  id_name string, 
  area_center_code string)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY &#39;\;&#39; 
  LINES TERMINATED BY &#39;\n&#39; 
STORED AS INPUTFORMAT 
  &#39;org.apache.hadoop.mapred.TextInputFormat&#39; 
OUTPUTFORMAT 
  &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;
LOCATION
  &#39;hdfs://node:8020/user/hive/warehouse/info&#39;
TBLPROPERTIES (
  &#39;numPartitions&#39;=&#39;0&#39;, 
  &#39;numFiles&#39;=&#39;1&#39;, 
  &#39;transient_lastDdlTime&#39;=&#39;1378245263&#39;, 
  &#39;numRows&#39;=&#39;0&#39;, 
  &#39;totalSize&#39;=&#39;301240320&#39;, 
  &#39;rawDataSize&#39;=&#39;0&#39;)
Time taken: 0.497 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面可以看到 info 表的 INPUTFORMAT 为&lt;code&gt;org.apache.hadoop.mapred.TextInputFormat&lt;/code&gt;，TextInputFormat 继承自FileInputFormat。FileInputFormat 是一个抽象类，它最重要的功能是为各种 InputFormat 提供统一的 &lt;code&gt;getSplits()&lt;/code&gt;方法，该方法最核心的是文件切分算法和 Host 选择算法。&lt;/p&gt;

&lt;p&gt;算法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;long length = file.getLen();
long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.
FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);

long blockSize = file.getBlockSize();
long splitSize = computeSplitSize(goalSize, minSize, blockSize);
long bytesRemaining = length;
while (((double) bytesRemaining)/splitSize &amp;gt; SPLIT_SLOP) {
String[] splitHosts = getSplitHosts(blkLocations, 
	length-bytesRemaining, splitSize, clusterMap);
	splits.add(makeSplit(path, length-bytesRemaining, splitSize, 
		       splitHosts));
	bytesRemaining -= splitSize;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;华丽的分割线&lt;/code&gt;：以下摘抄自&lt;a href=&quot;http://flyingdutchman.iteye.com/blog/1876400&quot;&gt;Hadoop深入学习：InputFormat组件&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1）文件切分算法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;文件切分算法主要用于确定InputSplit的个数以及每个InputSplit对应的数据段，FileInputSplit以文件为单位切分生成InputSplit。有三个属性值来确定InputSplit的个数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;goalSize&lt;/code&gt;：该值由 &lt;code&gt;totalSize/numSplits&lt;/code&gt; 来确定 InputSplit 的长度，它是根据用户的期望的 InputSplit 个数计算出来的；numSplits 为用户设定的 Map Task 的个数，默认为1。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;minSize&lt;/code&gt;：由配置参数 &lt;code&gt;mapred.min.split.size&lt;/code&gt;（或者 &lt;code&gt;mapreduce.input.fileinputformat.split.minsize&lt;/code&gt;）决定的 InputForma t的最小长度，默认为1。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;blockSize&lt;/code&gt;：HDFS 中的文件存储块block的大小，默认为64MB。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;numSplits=mapred.map.tasks&lt;/code&gt; 或者 &lt;code&gt;mapreduce.job.maps&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这三个参数决定一个 InputFormat 分片的最终的长度，计算方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;splitSize = max{minSize,min{goalSize,blockSize}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;计算出了分片的长度后，也就确定了 InputFormat 的数目。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2）host 选择算法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;InputFormat 的切分方案确定后，接下来就是要确定每一个 InputSplit 的元数据信息。InputSplit 元数据通常包括四部分，&lt;code&gt;&amp;lt;file,start,length,hosts&amp;gt;&lt;/code&gt;其意义为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;file 标识 InputSplit 分片所在的文件；&lt;/li&gt;
  &lt;li&gt;InputSplit 分片在文件中的的起始位置；&lt;/li&gt;
  &lt;li&gt;InputSplit 分片的长度；&lt;/li&gt;
  &lt;li&gt;分片所在的 host 节点的列表。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;InputSplit 的 host 列表的算作策略直接影响到运行作业的本地性。&lt;/p&gt;

&lt;p&gt;我们知道，由于大文件存储在 HDFS上的 block 可能会遍布整个 Hadoop 集群，而一个 InputSplit 分片的划分算法可能会导致一个 split 分片对应多个不在同一个节点上的 blocks，这就会使得在 Map Task 执行过程中会涉及到读其他节点上的属于该 Task 的 block 中的数据，从而不能实现数据本地性，而造成更多的网络传输开销。&lt;/p&gt;

&lt;p&gt;一个 InputSplit 分片对应的 blocks 可能位于多个数据节点地上，但是基于任务调度的效率，通常情况下，不会把一个分片涉及的所有的节点信息都加到其host列表中，而是选择包含该分片的数据总量的最大的前几个节点，作为任务调度时判断是否具有本地性的主要凭证。&lt;/p&gt;

&lt;p&gt;FileInputFormat 使用了一个启发式的 host 选择算法：首先按照 rack 机架包含的数据量对 rack 排序，然后再在 rack 内部按照每个 node 节点包含的数据量对 node 排序，最后选取前 N 个(N 为 block 的副本数)，node 的 host 作为 InputSplit 分片的 host 列表。当任务地调度 Task 作业时，只要将 Task 调度给 host 列表上的节点，就可以认为该 Task 满足了本地性。&lt;/p&gt;

&lt;p&gt;从上面的信息我们可以知道，当 InputSplit 分片的大小大于 block 的大小时，Map Task 并不能完全满足数据的本地性，总有一本分的数据要通过网络从远程节点上读数据，故为了提高 Map Task 的数据本地性，减少网络传输的开销，应尽量是 InputFormat 的大小和 HDFS 的 block 块大小相同。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;combinehiveinputformat&quot;&gt;CombineHiveInputFormat&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;getSplits(JobConf job, int numSplits)&lt;/code&gt; 代码运行过程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;init(job);
CombineFileInputFormatShim combine = ShimLoader.getHadoopShims().getCombineFileInputFormat();
	ShimLoader.loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class);
		Hadoop23Shims
			HadoopShimsSecure.getCombineFileInputFormat()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CombineFileInputFormatShim 继承了&lt;code&gt;org.apache.hadoop.mapred.lib.CombineFileInputFormat&lt;/code&gt;，CombineFileInputFormatShim 的 &lt;code&gt;getSplits&lt;/code&gt; 方法代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException {
  long minSize = job.getLong(&quot;mapred.min.split.size&quot;, 0);

  // For backward compatibility, let the above parameter be used
  if (job.getLong(&quot;mapred.min.split.size.per.node&quot;, 0) == 0) {
    super.setMinSplitSizeNode(minSize);
  }

  if (job.getLong(&quot;mapred.min.split.size.per.rack&quot;, 0) == 0) {
    super.setMinSplitSizeRack(minSize);
  }

  if (job.getLong(&quot;mapred.max.split.size&quot;, 0) == 0) {
    super.setMaxSplitSize(minSize);
  }

  InputSplit[] splits = (InputSplit[]) super.getSplits(job, numSplits);

  InputSplitShim[] isplits = new InputSplitShim[splits.length];
  for (int pos = 0; pos &amp;lt; splits.length; pos++) {
    isplits[pos] = new InputSplitShim((CombineFileSplit)splits[pos]);
  }

  return isplits;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面代码可以看出，如果为 CombineHiveInputFormat，则以下四个参数起作用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;mapred.min.split.size&lt;/code&gt; 或者 &lt;code&gt;mapreduce.input.fileinputformat.split.minsize&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapred.max.split.size&lt;/code&gt; 或者 &lt;code&gt;mapreduce.input.fileinputformat.split.maxsize&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapred.min.split.size.per.rack&lt;/code&gt; 或者 &lt;code&gt;mapreduce.input.fileinputformat.split.minsize.per.rack&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapred.min.split.size.per.node&lt;/code&gt; 或者 &lt;code&gt;mapreduce.input.fileinputformat.split.minsize.per.node&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CombineFileInputFormatShim 的 getSplits 方法最终会调用父类的 getSplits 方法，拆分算法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;long left = locations[i].getLength();
long myOffset = locations[i].getOffset();
long myLength = 0;
do {
	if (maxSize == 0) {
		myLength = left;
	} else {
	if (left &amp;gt; maxSize &amp;amp;&amp;amp; left &amp;lt; 2 * maxSize) {
	  myLength = left / 2;
	} else {
	  myLength = Math.min(maxSize, left);
	}
	}
	OneBlockInfo oneblock = new OneBlockInfo(path, myOffset,
	  myLength, locations[i].getHosts(), locations[i]
	      .getTopologyPaths());
	left -= myLength;
	myOffset += myLength;

	blocksList.add(oneblock);
} while (left &amp;gt; 0);
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hive--map-&quot;&gt;hive 中如何确定 map 数&lt;/h1&gt;

&lt;p&gt;总上总结如下：&lt;/p&gt;

&lt;p&gt;如果 &lt;code&gt;hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat&lt;/code&gt;，则这时候的参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hive&amp;gt; set mapred.min.split.size;
mapred.min.split.size=1
hive&amp;gt; set mapred.map.tasks;
mapred.map.tasks=2
hive&amp;gt; set dfs.blocksize;
dfs.blocksize=134217728
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面参数中 &lt;code&gt;mapred.map.tasks&lt;/code&gt; 为2，&lt;code&gt;dfs.blocksize&lt;/code&gt;（使用的是 cdh-4.3.0 版本的 hadoop，这里 block 和 size 之间没有逗号）为128M。&lt;/p&gt;

&lt;p&gt;假设有一个文件为200M，则按上面 &lt;code&gt;HiveInputFormat&lt;/code&gt; 的 split 算法：&lt;/p&gt;

&lt;p&gt;1、文件总大小为200M，goalSize=200M /2 =100M，minSize=1 ，splitSize = max{1,min{100M,128M}} =100M&lt;/p&gt;

&lt;p&gt;2、200M / 100M &amp;gt;1.1,故第一块大小为100M&lt;/p&gt;

&lt;p&gt;3、剩下文件大小为100M，小于128M，故第二块大小为100M。&lt;/p&gt;

&lt;p&gt;如果 &lt;code&gt;hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat&lt;/code&gt;，则这时候的参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;hive&amp;gt; set mapred.min.split.size;
mapred.min.split.size=1
hive&amp;gt; set mapred.max.split.size;
mapred.max.split.size=67108864
hive&amp;gt; set mapred.min.split.size.per.rack;
mapred.min.split.size.per.rack=1
hive&amp;gt; set mapred.min.split.size.per.node;
mapred.min.split.size.per.node=1
hive&amp;gt; set dfs.blocksize;
dfs.blocksize=134217728
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面参数中 &lt;code&gt;mapred.max.split.size&lt;/code&gt; 为64M，&lt;code&gt;dfs.blocksize&lt;/code&gt; 为128M。&lt;/p&gt;

&lt;p&gt;假设有一个文件为200M，则按上面 &lt;code&gt;CombineHiveInputFormat&lt;/code&gt; 的 split 算法：&lt;/p&gt;

&lt;p&gt;1、128M &amp;lt; 200M &amp;lt;128M X 2，故第一个block大小为128M&lt;/p&gt;

&lt;p&gt;2、剩下文件大小为200M-128M=72M，72M &amp;lt; 128M,故第二块大小为72M&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;网上有一些文章关于 hive 中如何控制 map 数的文章是否考虑的不够全面，没有具体情况具体分析。简而言之，当 InputFormat 的实现类为不同类时，拆分块算法都不一样，相关设置参数也不一样，需要具体分析。&lt;/p&gt;

&lt;h2 id=&quot;map-&quot;&gt;1. map 数不是越多越好&lt;/h2&gt;

&lt;p&gt;如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。&lt;br /&gt;
而且，同时可执行的 map 数是受限的。&lt;/p&gt;

&lt;h2 id=&quot;map--1&quot;&gt;2. 如何适当的增加 map 数？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;将数据导入到 hive 前，手动将大文件拆分为小文件&lt;/li&gt;
  &lt;li&gt;指定 map 数，使用 &lt;code&gt;insert&lt;/code&gt; 或者 &lt;code&gt;create as select&lt;/code&gt; 语句将一个表导入到另一个表，然后对另一张表做查询&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;3. 一些经验&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;合并小文件可以减少 map 数，但是会增加网络 IO。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;尽量使拆分块大小和 hdfs 的块大小接近，避免一个拆分块大小上的多个 hdfs 块位于不同数据节点，从而降低网络 IO。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;根据实际情况，控制 map 数量需要遵循两个原则：&lt;code&gt;使大数据量利用合适的map数&lt;/code&gt;；&lt;code&gt;使单个map任务处理合适的数据量。&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://f.dataguru.cn/thread-149820-1-1.html&quot;&gt;hive的查询注意事项以及优化总结&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://blog.sina.com.cn/s/blog_6ff05a2c010178qd.html&quot;&gt;Hadoop中map数的计算&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3] &lt;a href=&quot;http://blog.sina.com.cn/s/blog_6ff05a2c0101aqvv.html&quot;&gt;[Hive]从一个经典案例看优化mapred.map.tasks的重要性&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[4] &lt;a href=&quot;http://superlxw1234.iteye.com/blog/1582880&quot;&gt;hive优化之——控制hive任务中的map数和reduce数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[5] &lt;a href=&quot;http://www.searchtb.com/2010/12/hadoop-job-tuning.html&quot;&gt;Hadoop Job Tuning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[6] &lt;a href=&quot;http://www.tuicool.com/articles/77f2Af&quot;&gt;Hive配置项的含义详解（2）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[7] &lt;a href=&quot;http://blog.csdn.net/lalaguozhe/article/details/9053645&quot;&gt;Hive小文件合并调研&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[8] &lt;a href=&quot;http://flyingdutchman.iteye.com/blog/1876400&quot;&gt;Hadoop深入学习：InputFormat组件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2013/09/04/how-to-decide-map-number.html</link>
      <guid>http://blog.javachen.com/2013/09/04/how-to-decide-map-number.html</guid>
      <pubDate>2013-09-04T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>我的jekyll配置和修改</title>
      <description>&lt;p&gt;主要记录使用jekyll搭建博客时的一些配置和修改。&lt;/p&gt;

&lt;p&gt;注意：&lt;br /&gt;
&amp;gt;使用时请删除{和%以及{和{之间的空格。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;预览文章&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;source ~/.bash_profile
jekyll server
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;about-me-&quot;&gt;添加about me 边栏&lt;/h1&gt;

&lt;p&gt;参考&lt;a href=&quot;http://www.the5fire.com/&quot;&gt;the5fire的技术博客&lt;/a&gt;在index.html页面加入如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;section&amp;gt;
&amp;lt;h4&amp;gt;About me&amp;lt;/h4&amp;gt;
&amp;lt;div&amp;gt;
 一个Java方案架构师，主要从事hadoop相关工作。&amp;lt;a href=&quot;/about.html&quot;&amp;gt;更多信息&amp;lt;/a&amp;gt; 
&amp;lt;br/&amp;gt;
&amp;lt;br/&amp;gt;
&amp;lt;strong&amp;gt;&amp;lt;font color=&quot;red&quot;&amp;gt;&amp;lt;a href=&quot;/atom.xml&quot; target=&quot;_blank&quot;&amp;gt;订阅本站&amp;lt;/a&amp;gt;&amp;lt;/font&amp;gt;&amp;lt;/strong&amp;gt;
&amp;lt;br/&amp;gt;&amp;lt;br/&amp;gt;
联系博主：javachen.june[a]gmail.com
&amp;lt;/div&amp;gt;
&amp;lt;/section&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;about&quot;&gt;添加about页面&lt;/h1&gt;

&lt;p&gt;在根目录创建about.md并修改，注意：文件开头几行内容如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;title: About
layout: page
group: navigation
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;设置固定链接&lt;/h1&gt;

&lt;p&gt;在 _config.yml 里，找到 permalink，设置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;permalink: /:categories/:year/:month/:day/:title 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;markdownredcarpet&quot;&gt;修改，markdown实现为redcarpet&lt;/h1&gt;

&lt;p&gt;首先通过gem安装redcarpet，然后修改_config.yml：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redcarpet:
  extensions: [&quot;no_intra_emphasis&quot;, &quot;fenced_code_blocks&quot;, &quot;autolink&quot;, &quot;tables&quot;, &quot;strikethrough&quot;, &quot;superscript&quot;, &quot;with_toc_data&quot;, &quot;highlight&quot;, &quot;prettify&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;首页添加最近文章&lt;/h1&gt;

&lt;p&gt;在index.html页面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;section&amp;gt;
&amp;lt;h4&amp;gt;Recent Posts&amp;lt;/h4&amp;gt;
&amp;lt;ul id=&quot;recent_posts&quot;&amp;gt;{ % for rpost in site.posts limit: 15 %}
&amp;lt;li class=&quot;post&quot;&amp;gt;
&amp;lt;a href=&quot;&quot;&amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;/li&amp;gt;{ % endfor %}
&amp;lt;/ul&amp;gt;
&amp;lt;/section&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;首页为每篇文章添加分类、标签、发表日期以及评论连接&lt;/h1&gt;

&lt;p&gt;在index.html页面找到&lt;code&gt;&amp;lt;h3&amp;gt;&amp;lt;a href=&quot;{ { BASE_PATH }}{ { post.url }}&quot;&amp;gt;{ { post.title }}&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;&lt;/code&gt;，在下面添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; &amp;lt;div class=&quot;c9&quot;&amp;gt;
	Categories：
		{ %for cg in post.categories % }
		&amp;lt;a href=&quot;/categories.html#-ref&quot;&amp;gt;&amp;lt;/a&amp;gt;
		{ %if forloop.index &amp;lt; forloop.length % }
		,
		{ %endif%}
		{ %endfor%}
	|
	Tags：
		{ %for cg in post.tags %}
		&amp;lt;a href=&quot;/tags.html#-ref&quot;&amp;gt;&amp;lt;/a&amp;gt;
		{ %if forloop.index &amp;lt; forloop.length %}
		,
		{ %endif%}
		{ %endfor%}
	|
	Time：&amp;lt;time date=&quot;{ { post.date|date: &#39;%Y-%m-%d&#39; }}&quot;&amp;gt;&amp;lt;/time&amp;gt;
	&amp;lt;a href=&#39;#comments&#39; title=&#39;分享文章、查看评论&#39; style=&quot;float:right;margin-right:.5em;&quot;&amp;gt;Comments&amp;lt;/a&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;h1h2&quot;&gt;修改h1、h2等标题字体&lt;/h1&gt;

&lt;p&gt;主要是参考&lt;a href=&quot;http://www.ituring.com.cn/&quot;&gt;图灵社区&lt;/a&gt;的css，在&lt;code&gt;assets/themes/twitter/css/style.css&lt;/code&gt;中添加如下css代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;h1,h2,h3,h4,h5,h6{margin:18px 0 9px;font-family:inherit;font-weight:normal;color:inherit;text-rendering:optimizelegibility;}h1 small,h2 small,h3 small,h4 small,h5 small,h6 small{font-weight:normal;color:#999999;}
h1{font-size:30px;line-height:36px;}h1 small{font-size:18px;}
h2{font-size:24px;line-height:36px;}h2 small{font-size:18px;}
h3{font-size:18px;line-height:27px;}h3 small{font-size:14px;}
h4,h5,h6{line-height:18px;}
h4{font-size:14px;}h4 small{font-size:12px;}
h5{font-size:12px;}
h6{font-size:11px;color:#999999;text-transform:uppercase;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;添加返回顶部功能&lt;/h1&gt;

&lt;p&gt;同样是参考了&lt;a href=&quot;http://www.ituring.com.cn/&quot;&gt;图灵社区&lt;/a&gt;的css和网上的一篇js实现。在&lt;code&gt;assets/themes/twitter/css/style.css&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.backToTop {
    display: block;
    width: 40px;
    height: 32px;
    font-size: 26px;
    line-height: 32px;
    font-family: verdana, arial;
    padding: 5px 0;
    background-color: #000;
    color: #fff;
    text-align: center;
    position: fixed;
    _position: absolute;
    right: 10px;
    bottom: 100px;
    _bottom: &quot;auto&quot;;
    cursor: pointer;
    opacity: .6;
    filter: Alpha(opacity=60);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;assets/themes/twitter/js&lt;/code&gt;添加jquery和main.js，main.js内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jQuery.noConflict();
jQuery(document).ready(function(){
	var backToTopTxt = &quot;▲&quot;, backToTopEle = jQuery(&#39;&amp;lt;div class=&quot;backToTop&quot;&amp;gt;&amp;lt;/div&amp;gt;&#39;).appendTo(jQuery(&quot;body&quot;)).text(backToTopTxt).attr(&quot;title&quot;,&quot;Back top top&quot;).click(function() {
	    jQuery(&quot;html, body&quot;).animate({ scrollTop: 0 }, 120);
	}), backToTopFun = function() {
		var st = jQuery(document).scrollTop(), winh = jQuery(window).height();
		(st &amp;gt; 200)? backToTopEle.show(): backToTopEle.hide();    
		//IE6下的定位
		if (!window.XMLHttpRequest) {
		    backToTopEle.css(&quot;top&quot;, st + winh - 166); 
		}
	};

	backToTopEle.hide(); 
    	jQuery(window).bind(&quot;scroll&quot;, backToTopFun);
	jQuery(&#39;div.main a,div.pic a&#39;).attr(&#39;target&#39;, &#39;_blank&#39;);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-5&quot;&gt;添加文章版权说明&lt;/h1&gt;

&lt;p&gt;在&lt;code&gt;_includes/themes/twitter/post.html&lt;/code&gt;中文章主体下面添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;hr&amp;gt;
&amp;lt;div class=&quot;copyright&quot;&amp;gt;
&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;本文固定链接：&amp;lt;/strong&amp;gt;&amp;lt;a href=&#39;{ {page.url}}&#39;&amp;gt;http://blog.javachen.com/2013/08/31/my-jekyll-config.html&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;原创文章,转载请注明出处：&amp;lt;/strong&amp;gt;&amp;lt;a href=&#39;{ {page.url}}&#39;&amp;gt;JavaChen Blog&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并在&lt;code&gt;assets/themes/twitter/css/style.css&lt;/code&gt;中添加如下css代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.copyright {
margin: 10px 0;
padding: 10px 20px;
line-height: 1;
border-radius: 5px;
background: #f5f5f5;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;read-more&quot;&gt;添加read more功能&lt;/h1&gt;
&lt;p&gt;参考&lt;a href=&quot;http://truongtx.me/2013/05/01/jekyll-read-more-feature-without-any-plugin/&quot;&gt;Jekyll - Read More without plugin&lt;/a&gt;，在index.html找到 ，然后修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ % if post.content contains &quot;&amp;lt;!-- more --&amp;gt;&quot; %}
{ { post.content | split:&quot;&amp;lt;!-- more --&amp;gt;&quot; | first % }}
&amp;lt;h4&amp;gt;&amp;lt;a href=&#39;{ {post.url}}&#39; title=&#39;Read more...&#39;&amp;gt;Read more...&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;
{ % else %}
{ { post.content}}
{ % endif %}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在文章中添加&lt;code&gt;&amp;lt;!-- more --&amp;gt;&lt;/code&gt;即可。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;添加搜索栏&lt;/h1&gt;

&lt;p&gt;参考&lt;a href=&quot;http://truongtx.me/2012/12/28/jekyll-create-simple-search-box/&quot;&gt;Jekyll Bootstrap - Create Simple Search box&lt;/a&gt;，在&lt;code&gt;_includes/themes/twitter/default.html&lt;/code&gt;导航菜单下面添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;form class=&quot;navbar-search pull-left&quot; id=&quot;search-form&quot;&amp;gt;
  &amp;lt;input type=&quot;text&quot; id=&quot;google-search&quot; class=&quot;search-query&quot; placeholder=&quot;Search&quot;&amp;gt;
&amp;lt;/form
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加js：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jQuery(&quot;#search-form&quot;).submit(function(){
	var query = document.getElementById(&quot;google-search&quot;).value;
	window.open(&quot;http://google.com/search?q=&quot; + query+ &quot;%20site:&quot; + &quot;http://blog.javachen.com&quot;);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-7&quot;&gt;其他&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;添加404页面&lt;/li&gt;
  &lt;li&gt;使用多说评论&lt;/li&gt;
  &lt;li&gt;修改博客主体为宽屏模式&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;todo&quot;&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;添加语法高亮，参考&lt;a href=&quot;http://truongtx.me/2012/12/28/jekyll-bootstrap-syntax-highlighting/&quot;&gt;Jekyll - Syntax highlighting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2013/08/31/my-jekyll-config.html</link>
      <guid>http://blog.javachen.com/2013/08/31/my-jekyll-config.html</guid>
      <pubDate>2013-08-31T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用ZooKeeper实现配置同步</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;前言&lt;/h1&gt;

&lt;p&gt;应用项目中都会有一些配置信息，这些配置信息数据量少，一般会保存到内存、文件或者数据库，有时候需要动态更新。当需要在多个应用服务器中修改这些配置文件时，需要做到快速、简单、不停止应用服务器的方式修改并同步配置信息到所有应用中去。本篇文章就是介绍如何使用ZooKeeper来实现配置的动态同步。&lt;/p&gt;

&lt;h1 id=&quot;zookeeper&quot;&gt;ZooKeeper&lt;/h1&gt;

&lt;p&gt;在《&lt;a href=&quot;&quot;&gt;hive Driver类运行过程&lt;/a&gt;》一文中可以看到hive为了支持并发访问引入了ZooKeeper来实现分布式锁。参考《&lt;a href=&quot;http://rdc.taobao.com/team/jm/archives/1232&quot;&gt;ZooKeeper典型应用场景一览&lt;/a&gt;》一文，ZooKeeper还可以用作其他用途，例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据发布与订阅（配置中心）&lt;/li&gt;
  &lt;li&gt;负载均衡&lt;/li&gt;
  &lt;li&gt;命名服务(Naming Service)&lt;/li&gt;
  &lt;li&gt;分布式通知/协调&lt;/li&gt;
  &lt;li&gt;集群管理与Master选举&lt;/li&gt;
  &lt;li&gt;分布式锁&lt;/li&gt;
  &lt;li&gt;分布式队列&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- more --&gt;

&lt;p&gt;一些在线系统在运行中，需要在不停止程序的情况下能够动态调整某一个变量的值并且能够及时生效。特别是当部署了多台应用服务器的时候，需要能够做到在一台机器上修改配置文件，然后在同步到所有应用服务器。这时候使用ZooKeeper来实现就很合适了。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;数据发布与订阅&lt;/h1&gt;

&lt;p&gt;发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。&lt;/p&gt;

&lt;p&gt;使用ZooKeeper的发布与订阅模型，可以将应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。这样的场景适合数据量很小，但是数据更新可能会比较快的需求。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;配置存储方案&lt;/h1&gt;

&lt;p&gt;配置文件通常有如下几种保存方式：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;将配置信息保存在程序代码中&lt;br /&gt;
这种方案简单，但每次修改配置都要重新编译、部署应用程序。显然这种方案很不方便，也不可靠，更无法做到修改的实时生效。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将配置信息保存在xml文件或者属性文件中&lt;br /&gt;
在参数信息保存在xml或者属性文件中，当需要修改参数时，直接修改 xml 文件。这样无需重新编译，只需重新部署修改的文件即可。但然后对所有的应用进行重新部署。这样做的缺点显而易见，要往上百台机器上重新部署应用，简直是一个噩梦。同时该方案还有一个缺点，就是配置修改无法做到实时生效。修改后往往过一段时间才能生效。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将配置信息保存在数据库中&lt;br /&gt;
当需要修改参数时，直接修改数据库，然后重启分布式应用程序，或者刷新分布式应用的缓存。尽管这种做法比以上两种方案简单，但却面临着单点失效问题。如果数据库服务器停机，则分布式应用程序的配置信息将无法更新。另外这种方案的配置修改生效实时性虽然比第二种方案好些，但仍然不能达到某些情况下的要求。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;zookeeper-1&quot;&gt;基于ZooKeeper的配置信息同步方案&lt;/h1&gt;

&lt;p&gt;如果使用ZooKeeper来实现，就可以直接把配置信息保存到ZooKeeper中，或者把属性文件内容保存到ZooKeeper中，当属性文件内容发生变化时，就通知监听者如应用程序去重新读取配置文件。&lt;/p&gt;

&lt;p&gt;在网上搜索了一下，很能找到好用的现成的代码实现。有的基于ZooKeeper来扩张jdk的hashmap来存储配置参数，如：&lt;a href=&quot;http://melin.iteye.com/blog/899435&quot;&gt;使用ZooKeeper实现静态数据中心化配置管理&lt;/a&gt;，也有人直接实现了一个基于java并发框架的工具包，如：&lt;a href=&quot;https://github.com/openUtility/menagerie&quot;&gt;menagerie&lt;/a&gt;。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;code&gt;注意&lt;/code&gt;:以下部分文字和图来自：&lt;a href=&quot;http://www.code365.org/wp-content/uploads/2012/02/%E5%9F%BA%E4%BA%8EZooKeeper%E7%9A%84%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B01.pdf&quot;&gt;基于ZooKeeper的配置信息存储方案的设计与实现1.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;基于ZooKeeper的特性,借助ZooKeeper可以实现一个可靠的、简单的、修改配置能够实时生效的配置信息存储方案,整体的设计方案如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/zookeeper-01.jpg&quot; alt=&quot;基于zookeeper的方案&quot; /&gt;&lt;/p&gt;

&lt;p&gt;整个配置信息存储方案由三部分组成:ZooKeeper服务器集群、配置管理程序、分布式应用程序。&lt;/p&gt;

&lt;p&gt;ZooKeeper服务器集群存储配置信息,在服务器上创建一个保存数据的节点(创建节点操作);配置管理程序提供一个配置管理的UI界面或者命令行方式,用户通过配置界面修改ZooKeeper服务器节点上配置信息(设置节点数据操作);分布式应用连接到ZooKeeper集群上(创建ZooKeeper客户端操作),监听配置信息的变化(使用获取节点数据操作,并注册一个watcher)。&lt;/p&gt;

&lt;p&gt;当配置信息发生变化时，分布式应用会更新程序中使用配置信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/zookeeper-02.jpg&quot; alt=&quot;修改配置的时许图&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;源代码&lt;/h1&gt;

&lt;p&gt;找到一个淘宝工程师写的实现方式， 代码见：&lt;a href=&quot;https://github.com/javachen/learning-hadoop/tree/master/zkpublisher&quot;&gt;zkpublisher&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;优点&lt;/h1&gt;

&lt;p&gt;借助 ZooKeeper我们实现的配置信息存储方案具有的优点如下:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;简单。尽管前期搭建ZooKeeper服务器集群较为麻烦,但是实现该方案后,修改配置整个过程变得简单很多。用户只要修改配置,无需进行其他任何操作,配置自动生效。&lt;/li&gt;
  &lt;li&gt;可靠。ZooKeeper服务集群具有无单点失效的特性,使整个系统更加可靠。即使ZooKeeper 集群中的一台机器失效,也不会影响整体服务,更不会影响分布式应用配置信息的更新。&lt;/li&gt;
  &lt;li&gt;实时。ZooKeeper的数据更新通知机制,可以在数据发生变化后,立即通知给分布式应用程序,具有很强的变化响应能力。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;section-5&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;本文参考了网上的一些文章，给出了基于ZooKeeper的配置信息同步方案,解决了传统配置信息同步方案的缺点如实时性差、可靠性差、复杂等。&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;参考文章&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://rdc.taobao.com/team/jm/archives/1232&quot;&gt;ZooKeeper典型应用场景一览&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://melin.iteye.com/blog/899435&quot;&gt;使用ZooKeeper实现静态数据中心化配置管理&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/openUtility/menagerie&quot;&gt;menagerie&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.code365.org/wp-content/uploads/2012/02/%E5%9F%BA%E4%BA%8EZooKeeper%E7%9A%84%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B01.pdf&quot;&gt;基于ZooKeeper的配置信息存储方案的设计与实现1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2013/08/23/publish-proerties-using-zookeeper.html</link>
      <guid>http://blog.javachen.com/2013/08/23/publish-proerties-using-zookeeper.html</guid>
      <pubDate>2013-08-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive源码分析：Driver类运行过程</title>
      <description>&lt;p&gt;说明：&lt;/p&gt;

&lt;p&gt;本文的源码分析基于hive-0.12.0-cdh5.0.1。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;概括&lt;/h1&gt;

&lt;p&gt;从《&lt;a href=&quot;/2013/08/21/hive-CliDriver.html&quot;&gt;hive cli的入口类&lt;/a&gt;》中可以知道hive中处理hive命令的处理器一共有以下几种：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（1）set       SetProcessor，设置修改参数,设置到SessionState的HiveConf里。 
（2）dfs       DfsProcessor，使用hadoop的FsShell运行hadoop的命令。 
（3）add       AddResourceProcessor，添加到SessionState的resource_map里，运行提交job的时候会写入Hadoop的Distributed Cache。 
（4）delete    DeleteResourceProcessor，从SessionState的resource_map里删除。
（5）reset     RestResourceProcessor，重置终端输出
（6）其他命令   Driver 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Driver类的主要作用是用来编译并执行hive命令，然后返回执行结果。这里主要分析Driver类的运行逻辑，其时序图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/Hive-Driver-sequence.jpg&quot; alt=&quot;hive-driver&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从时序图上可以看出有以下步骤：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;run方法调用内部方法runInternal&lt;/li&gt;
  &lt;li&gt;在runInternal方法内部先，调用HiveDriverRunHookContext的preDriverRun方法&lt;/li&gt;
  &lt;li&gt;调用compileInternal方法&lt;/li&gt;
  &lt;li&gt;compileInternal方法内部调用compile方法&lt;/li&gt;
  &lt;li&gt;compile方法内，先调用HiveSemanticAnalyzerHookContext的preAnalyze方法&lt;/li&gt;
  &lt;li&gt;再进行语法分析，调用BaseSemanticAnalyzer的analyze方法&lt;/li&gt;
  &lt;li&gt;调用HiveSemanticAnalyzerHookContext的postAnalyze方法&lt;/li&gt;
  &lt;li&gt;再进行语法校验，调用BaseSemanticAnalyzer的validate方法&lt;/li&gt;
  &lt;li&gt;compileInternal方法运行完成之后，调用checkConcurrency方法&lt;/li&gt;
  &lt;li&gt;再来运行execute方法，该方法用于运行任务&lt;/li&gt;
  &lt;li&gt;最后，调用HiveDriverRunHookContext的postDriverRun方法&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;driver&quot;&gt;Driver初始化&lt;/h1&gt;

&lt;p&gt;在继续分析之前，需要弄清楚Driver类初始化时做了什么事情。&lt;/p&gt;

&lt;p&gt;在CliDriver的&lt;code&gt;processCmd(String cmd)&lt;/code&gt;方法中可以看到proc是在CommandProcessorFactory类中new出来的并调用了init方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;} else { // local mode
      CommandProcessor proc = CommandProcessorFactory.get(tokens[0], (HiveConf) conf);
      ret = processLocalCmd(cmd, proc, ss);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CommandProcessorFactory.get方法代码片段：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;if (conf == null) {
return new Driver();
}

Driver drv = mapDrivers.get(conf);
if (drv == null) {
drv = new Driver();
mapDrivers.put(conf, drv);
}
drv.init();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;init方法和构造方法代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public void init() {
	Operator.resetId();
}

public Driver() {
    if (SessionState.get() != null) {
      conf = SessionState.get().getConf();
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上可以看出仅仅是初始化了conf属性和重置了Operator的id。&lt;/p&gt;

&lt;h2 id=&quot;run&quot;&gt;run方法过程&lt;/h2&gt;

&lt;p&gt;1、调用runInternal方法，根据该方法返回值判断是否出错。&lt;/p&gt;

&lt;p&gt;2、runInternal方法内，运行HiveDriverRunHook的前置方法preDriverRun&lt;/p&gt;

&lt;p&gt;3、判断是否需要编译，如果需要，则运行&lt;code&gt;compileInternal(command)&lt;/code&gt;方法，并根据返回值判断是否该释放Hive锁。hive中可以配置&lt;code&gt;hive.support.concurrency&lt;/code&gt;值为true并设置zookeeper的服务器地址和端口，基于zookeeper实现分布式锁以支持hive的多并发访问。这部分内容不是本文重点故不做介绍。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code&gt;compileInternal(command)&lt;/code&gt;方法内部代码说明见下文。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;4、判断是否需要对Task加锁。如果需要，则调用checkConcurrency方法。&lt;/p&gt;

&lt;p&gt;5、调用execute()方法执行任务。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;执行计划开始：&lt;code&gt;plan.setStarted();&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;先运行ExecuteWithHookContext的前置hook方法，ExecuteWithHookContext类型有三种：前置、运行失败、后置。&lt;/li&gt;
  &lt;li&gt;然后创建DriverContext用于维护正在运行的task任务，正在运行的task任务会添加到队列runnable中去。&lt;/li&gt;
  &lt;li&gt;其次，在while循环中遍历队列中的任务，然后启动任务让其执行，并且轮训任务执行结果，如果任务运行完成，则将其从running中删除并将当前任务的子任务加入队列中；如果运行失败，则会启动备份的任务，并运行失败的hook。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;while (runnable.peek() != null &amp;amp;&amp;amp; running.size() &amp;lt; maxthreads) {
  Task&amp;lt;? extends Serializable&amp;gt; tsk = runnable.remove();
  launchTask(tsk, queryId, noName, running, jobname, jobs, driverCxt);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;在launchTask方法中，先判断是否支持并发执行，如果支持则调用TaskRunner的start()方法，否则调用&lt;code&gt;tskRun.runSequential()&lt;/code&gt;方法顺序执行，只有当是MapReduce任务时，才执行并发执行：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;	public void launchTask(Task&amp;lt;? extends Serializable&amp;gt; tsk, String queryId, boolean noName,
      Map&amp;lt;TaskResult, TaskRunner&amp;gt; running, String jobname, int jobs, DriverContext cxt) {

    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().startTask(queryId, tsk, tsk.getClass().getName());
    }
    if (tsk.isMapRedTask() &amp;amp;&amp;amp; !(tsk instanceof ConditionalTask)) {
      if (noName) {
        conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME, jobname + &quot;(&quot; + tsk.getId() + &quot;)&quot;);
      }
      cxt.incCurJobNo(1);
      console.printInfo(&quot;Launching Job &quot; + cxt.getCurJobNo() + &quot; out of &quot; + jobs);
    }
    tsk.initialize(conf, plan, cxt);
    TaskResult tskRes = new TaskResult();
    TaskRunner tskRun = new TaskRunner(tsk, tskRes);

    // Launch Task
    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.EXECPARALLEL) &amp;amp;&amp;amp; tsk.isMapRedTask()) {
      // Launch it in the parallel mode, as a separate thread only for MR tasks
      tskRun.start();
    } else {
      tskRun.runSequential();
    }
    running.put(tskRes, tskRun);
    return;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后任务的执行情况，就要看具体的&lt;code&gt;Task&amp;lt;? extends Serializable&amp;gt;&lt;/code&gt;的实现类的逻辑了。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;执行计划完成：&lt;code&gt;plan.setDone();&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;6、运行HiveDriverRunHook的后置方法postDriverRun&lt;/p&gt;

&lt;h2 id=&quot;compileinternal&quot;&gt;compileInternal方法过程&lt;/h2&gt;

&lt;p&gt;1、保存当前查询状态&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;QueryState queryState = new QueryState();

if (plan != null) {
  close();
  plan = null;
}

if (resetTaskIds) {
  TaskFactory.resetId();
}
saveSession(queryState);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;QueryState中保存了HiveOperation以及当前查询语句或者命令。&lt;/p&gt;

&lt;p&gt;2、创建Context上下文&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;command = new VariableSubstitution().substitute(conf,command);
ctx = new Context(conf);
ctx.setTryCount(getTryCount());
ctx.setCmd(command);
ctx.setHDFSCleanup(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、创建ParseDriver对象，然后解析命令、生成AST树。语法和词法分析内容，不是本文重点故不做介绍。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;ParseDriver pd = new ParseDriver();
ASTNode tree = pd.parse(command, ctx);
tree = ParseUtils.findRootNonNullToken(tree);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单归纳来说，解析程包括如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;词法分析，生成AST树，ParseDriver完成。&lt;/li&gt;
  &lt;li&gt;分析AST树，AST拆分成查询子块，信息记录在QB，这个QB在下面几个阶段都需要用到，SemanticAnalyzer.doPhase1完成。&lt;/li&gt;
  &lt;li&gt;从metastore中获取表的信息，SemanticAnalyzer.getMetaData完成。&lt;/li&gt;
  &lt;li&gt;生成逻辑执行计划，SemanticAnalyzer.genPlan完成。&lt;/li&gt;
  &lt;li&gt;优化逻辑执行计划，Optimizer完成，ParseContext作为上下文信息进行传递。&lt;/li&gt;
  &lt;li&gt;生成物理执行计划，SemanticAnalyzer.genMapRedTasks完成。&lt;/li&gt;
  &lt;li&gt;物理计划优化，PhysicalOptimizer完成，PhysicalContext作为上下文信息进行传递。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、读取环境变量，如果配置了语法分析的hook，参数为：&lt;code&gt;hive.semantic.analyzer.hook&lt;/code&gt;，则:先用反射得到&lt;code&gt;AbstractSemanticAnalyzerHook&lt;/code&gt;的集合，调用&lt;code&gt;hook.preAnalyze(hookCtx, tree)&lt;/code&gt;方法,然后再调用&lt;code&gt;sem.analyze(tree, ctx)&lt;/code&gt;方法，该方法才是用来作语法分析的,最后再调用&lt;code&gt;hook.postAnalyze(hookCtx, tree)&lt;/code&gt;方法执行一些用户定义的后置操作；&lt;/p&gt;

&lt;p&gt;否则，直接调用&lt;code&gt;sem.analyze(tree, ctx)&lt;/code&gt;进行语法分析。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;  BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, tree);
  List&amp;lt;AbstractSemanticAnalyzerHook&amp;gt; saHooks =
      getHooks(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK,
               AbstractSemanticAnalyzerHook.class);

  // Do semantic analysis and plan generation
  if (saHooks != null) {
    HiveSemanticAnalyzerHookContext hookCtx = new HiveSemanticAnalyzerHookContextImpl();
    hookCtx.setConf(conf);
    hookCtx.setUserName(userName);
    for (AbstractSemanticAnalyzerHook hook : saHooks) {
      tree = hook.preAnalyze(hookCtx, tree);
    }
    sem.analyze(tree, ctx);
    hookCtx.update(sem);
    for (AbstractSemanticAnalyzerHook hook : saHooks) {
      hook.postAnalyze(hookCtx, sem.getRootTasks());
    }
  } else {
    sem.analyze(tree, ctx);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、校验执行计划：&lt;code&gt;sem.validate()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;6、创建查询计划QueryPlan。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;plan = new QueryPlan(command, sem, perfLogger.getStartTime(PerfLogger.DRIVER_RUN),
           SessionState.get().getCommandType());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、初始化FetchTask。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;if (plan.getFetchTask() != null) {
   plan.getFetchTask().initialize(conf, plan, null);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、得到schema&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;schema = getSchema(sem, conf);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;9、授权校验工作。&lt;/p&gt;

&lt;h2 id=&quot;hivehook&quot;&gt;hive中支持的hook&lt;/h2&gt;

&lt;p&gt;上面分析中，提到了hive的hook机制，hive中一共存在以下几种hook。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive.semantic.analyzer.hook
hive.exec.filter.hook
hive.exec.driver.run.hooks
hive.server2.session.hook
hive.exec.pre.hooks
hive.exec.post.hooks
hive.exec.failure.hooks
hive.client.stats.publishers
hive.metastore.ds.connection.url.hook
hive.metastore.init.hooks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过hook机制，可以在运行前后做一些用户想做的事情。如：你可以在语法分析的hook中对hive的操作做一些超级管理员级别的权限判断；你可以对hive-server2做一些session级别的控制。&lt;/p&gt;

&lt;p&gt;cloudera的github仓库&lt;a href=&quot;https://github.com/cloudera/access&quot;&gt;access&lt;/a&gt;中关于hive的访问控制就是使用了hive的hook机制。&lt;/p&gt;

&lt;p&gt;twitter的mapreduce可视化项目监控项目&lt;a href=&quot;https://github.com/twitter/ambrose&quot;&gt;ambrose&lt;/a&gt;也利用了hive的hook机制，有兴趣的话，你可以去看看其是如何使用hive的hook并且你也可以扩增hook做些自己想做的事情。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;本文主要介绍了hive运行过程，包括hive语法词法解析以及hook机制，任务的最后运行过程取决于具体的&lt;code&gt;Task&amp;lt;? extends Serializable&amp;gt;&lt;/code&gt;的实现类的逻辑。关于hive语法词法解析，这一部分没有做详细的解释。&lt;/p&gt;

&lt;p&gt;hive Driver类的执行过程如下（该图是根据hive-0.11版本画出来的）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/hive-driver.jpg&quot; alt=&quot;hive-driver&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考文章&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/end/archive/2012/12/19/2825320.html&quot;&gt;hive 初始化运行流程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/access&quot;&gt;Cloudera access&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/twitter/ambrose&quot;&gt;twitter ambrose&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
      <link>http://blog.javachen.com/2013/08/22/hive-Driver.html</link>
      <guid>http://blog.javachen.com/2013/08/22/hive-Driver.html</guid>
      <pubDate>2013-08-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hive源码分析：CLI入口类</title>
      <description>&lt;p&gt;说明：&lt;/p&gt;

&lt;p&gt;本文的源码分析基于hive-0.10.0-cdh4.3.0。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;启动脚本&lt;/h1&gt;

&lt;p&gt;从shell脚本&lt;code&gt;/usr/lib/hive/bin/ext/cli.sh&lt;/code&gt;可以看到hive cli的入口类为&lt;code&gt;org.apache.hadoop.hive.cli.CliDriver&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;	cli () {
	  CLASS=org.apache.hadoop.hive.cli.CliDriver
	  execHiveCmd $CLASS &quot;$@&quot;
	}
	cli_help () {
	  CLASS=org.apache.hadoop.hive.cli.CliDriver
	  execHiveCmd $CLASS &quot;--help&quot;
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;入口类&lt;/h1&gt;
&lt;p&gt;java中的类如果有main方法就能运行，故直接查找&lt;code&gt;org.apache.hadoop.hive.cli.CliDriver&lt;/code&gt;中的main方法即可。&lt;/p&gt;

&lt;p&gt;CliDriver类中的方法有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/Hive-CliDriver-method.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;main方法代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;	public static void main(String[] args) throws Exception {
	    int ret = run(args);
	    System.exit(ret);
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;阅读run函数可以看到，主要做了以下几件事情：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;读取main方法的参数&lt;/li&gt;
  &lt;li&gt;重置默认的log4j配置并为hive重新初始化log4j，注意，在这里是读取hive-log4j.properties来初始化log4j。&lt;/li&gt;
  &lt;li&gt;创建CliSessionState，并初始化in、out、info、error等stream流。CliSessionState是一次命令行操作的session会话，其继承了SessionState。&lt;/li&gt;
  &lt;li&gt;重命令行参数中读取参数并设置到CliSessionState中。&lt;/li&gt;
  &lt;li&gt;启动SessionState并连接到hive server&lt;/li&gt;
  &lt;li&gt;如果cli是本地模式运行，则加载&lt;code&gt;hive.aux.jars.path&lt;/code&gt;参数配置的jar包到classpath&lt;/li&gt;
  &lt;li&gt;创建一个CliDriver对象，并设置当前选择的数据库。可以在命令行参数添加&lt;code&gt;-database database&lt;/code&gt;来选择连接那个数据库，默认为default数据库。&lt;/li&gt;
  &lt;li&gt;加载初始化文件&lt;code&gt;.hiverc&lt;/code&gt;，该文件位于当前用户主目录下，读取该文件内容后，然后调用processFile方法处理文件内容。&lt;/li&gt;
  &lt;li&gt;如果命令行中有-e参数，则运行指定的sql语句；如果有-f参数，则读取该文件内容并运行。注意：不能同时指定这两个参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;	hive -e &#39;show tables&#39;
	hive -f /root/hive.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;如果没有指定上面两个参数，则从当前用户主目录读取&lt;code&gt;.hivehistory&lt;/code&gt;文件，如果不存在则创建。该文件保存了当前用户所有运行的hive命令。&lt;/li&gt;
  &lt;li&gt;在while循环里不断读取控制台的输入内容，每次读取一行，如果行末有分号，则调用CliDriver的processLine方法运行读取到的内容。&lt;/li&gt;
  &lt;li&gt;每次调用processLine方法时，都会创建SignalHandler用于捕捉用户的输入，当用户输入Ctrl+C时，会kill当前正在运行的任务以及kill掉当前进程。kill当前正在运行的job的代码如下.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;	HadoopJobExecHelper.killRunningJobs();
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;处理hive命令。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hive&quot;&gt;处理hive命令过程&lt;/h2&gt;
&lt;p&gt;如果输入的是quit或者exit,则程序退出。&lt;/p&gt;

&lt;p&gt;如果命令开头是source，则会读取source 后面文件内容，然后执行该文件内容。通过这种方式，你可以在hive命令行模式运行一个文件中的hive命令。&lt;/p&gt;

&lt;p&gt;如果命令开头是感叹号，执行操作系统命令（如&lt;code&gt;!ls&lt;/code&gt;，列出当前目录的文件信息）。通过以下代码来运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;	Process executor = Runtime.getRuntime().exec(shell_cmd);
	StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, ss.out);
	StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, ss.err);

	outPrinter.start();
	errPrinter.start();

	ret = executor.waitFor();
	if (ret != 0) {
	  console.printError(&quot;Command failed with exit code = &quot; + ret);
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;shell_cmd的内容大概如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;	shell_cmd = &quot;/bin/bash -c \&#39;&quot; + shell_cmd + &quot;\&#39;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果命令开头是list，列出jar/file/archive&lt;/p&gt;

&lt;p&gt;如果是远程模式运行命令行，则通过HiveClient来运行命令；否则，调用processLocalCmd方法运行本地命令。&lt;/p&gt;

&lt;p&gt;以本地模式运行时，会通过CommandProcessorFactory工厂解析输入的语句来获得一个CommandProcessor，CommandProcessor接口的实现类见下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/CommandProcessor-implements.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上图可以看到指定的命令(&lt;code&gt;set/dfs/add/delete/reset&lt;/code&gt;)交给指定的CommandProcessor处理，其余的(指hql语句)交给Driver类来处理。&lt;/p&gt;

&lt;p&gt;故，&lt;code&gt;org.apache.hadoop.hive.ql.Driver&lt;/code&gt;类是hql查询的起点，而run()方法会先后调用compile()和execute()两个函数来完成查询，所以一个command的查询分为compile和execute两个阶段。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;作为尝试，第一次使用思维导图分析代码逻辑，简单整理了一下CliDriver类的运行逻辑，如下图。以后还需要加强画图和表达能力。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/hive-cli-clidriver.jpg&quot; alt=&quot;hive-cli-clidriver&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/end/archive/2012/12/19/2825320.html&quot;&gt;hive 初始化运行流程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2013/08/21/hive-CliDriver.html</link>
      <guid>http://blog.javachen.com/2013/08/21/hive-CliDriver.html</guid>
      <pubDate>2013-08-21T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Hadoop中遇到的一些问题</title>
      <description>&lt;p&gt;本文主要记录安装hadoop过程需要注意的一些细节以及使用hadoop过程中发现的一些问题以及对应解决办法，有些地方描述的不是很清楚可能还会不准确，之后会重现问题然后修改完善这篇文章。&lt;/p&gt;

&lt;h1 id=&quot;hadoop&quot;&gt;安装hadoop过程中需要注意以下几点：&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;每个节点配置hosts&lt;/li&gt;
  &lt;li&gt;每个节点配置时钟同步&lt;/li&gt;
  &lt;li&gt;如果没有特殊要求，关闭防火墙&lt;/li&gt;
  &lt;li&gt;hadoop需要在&lt;code&gt;/tmp&lt;/code&gt;目录下存放一些日志和临时文件，要求&lt;code&gt;/tmp&lt;/code&gt;目录权限必须为&lt;code&gt;1777&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;!-- more --&gt;

&lt;h1 id=&quot;intelhadoopidh&quot;&gt;使用intel的hadoop发行版IDH过程遇到问题：&lt;/h1&gt;

&lt;p&gt;1、 IDH集群中需要配置管理节点到集群各节点的无密码登录，公钥文件存放路径为&lt;code&gt;/etc/intelcloud&lt;/code&gt;目录下，文件名称为&lt;code&gt;idh-id_rsa&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;如果在管理界面发现不能启动/停止hadoop组件的进程，请检查ssh无密码登录是否有问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh -i /etc/intelcloud/idh-id_rsa nodeX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果存在问题，请重新配置无密码登录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scp -i /etc/intelcloud/idh-id_rsa nodeX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、 IDH使用puppt和shell脚本来管理hadoop集群，shell脚本中有一处调用puppt的地方存在问题，详细说明待整理！！&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;cdh430hadooprpm&quot;&gt;使用CDH4.3.0的hadoop（通过rpm安装）过程中发现如下问题：&lt;/h1&gt;

&lt;h2 id=&quot;cdhhadoop&quot;&gt;说明：以下问题不局限于CDH的hadoop版本。&lt;/h2&gt;

&lt;p&gt;1、 在hive运行过程中会打印如下日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Starting Job = job_1374551537478_0001, Tracking URL = http://june-fedora:8088/proxy/application_1374551537478_0001/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1374551537478_0001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过上面的&lt;code&gt;kill command&lt;/code&gt;可以killjob，但是运行过程中发现提示错误，错误原因：&lt;code&gt;HADOOP_LIBEXEC_DIR&lt;/code&gt;未做设置&lt;/p&gt;

&lt;p&gt;解决方法：在hadoop-env.sh中添加如下代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export HADOOP_LIBEXEC_DIR=$HADOOP_COMMON_HOME/libexec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、 查看java进程中发现，JVM参数中-Xmx重复出现&lt;/p&gt;

&lt;p&gt;解决办法：&lt;code&gt;/etc/hadoop/conf/hadoop-env.sh&lt;/code&gt;去掉第二行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export HADOOP_OPTS=&quot;-Djava.net.preferIPv4Stack=true $HADOOP_OPTS&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、 hive中mapreduce运行为本地模式，而不是远程模式&lt;/p&gt;

&lt;p&gt;解决办法：&lt;code&gt;/etc/hadoop/conf/hadoop-env.sh&lt;/code&gt;设置&lt;code&gt;HADOOP_MAPRED_HOME&lt;/code&gt;变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、 如何设置hive的jvm启动参数&lt;/p&gt;

&lt;p&gt;hive脚本运行顺序：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive--&amp;gt;hive-config.sh--&amp;gt;hive-env.sh--&amp;gt;hadoop-config.sh--&amp;gt;hadoop-env.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;故如果hadoop-env.sh中设置了&lt;code&gt;HADOOP_HEAPSIZE&lt;/code&gt;，则hive-env.sh中设置的无效&lt;/p&gt;

&lt;p&gt;5、如何设置JOB_HISTORYSERVER的jvm参数&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;/etc/hadoop/conf/hadoop-env.sh&lt;/code&gt;添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=256
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2013/08/17/some-problems-about-hadoop.html</link>
      <guid>http://blog.javachen.com/2013/08/17/some-problems-about-hadoop.html</guid>
      <pubDate>2013-08-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hadoop自动化安装shell脚本</title>
      <description>&lt;p&gt;之前写过一些如何安装Cloudera Hadoop的文章，安装hadoop过程中，最开始是手动安装apache版本的hadoop，其次是使用Intel的IDH管理界面安装IDH的hadoop，再然后分别手动和通过cloudera manager安装hadoop，也使用bigtop-util yum方式安装过apache的hadoop。&lt;/p&gt;

&lt;p&gt;安装过程中参考了很多网上的文章，解压缩过cloudera的&lt;code&gt;cloudera-manager-installer.bin&lt;/code&gt;，发现并修复了IDH shell脚本中关于puppt的自认为是bug的一个bug，最后整理出了一个自动安装hadoop的shell脚本，脚本托管在github上面: &lt;a href=&quot;https://github.com/javachen/hadoop-install&quot;&gt;hadoop-install&lt;/a&gt;。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h1 id=&quot;hadoop&quot;&gt;hadoop安装文章&lt;/h1&gt;
&lt;p&gt;博客中所有关于安装hadoop的文章列出如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2013/03/08/note-about-installing-hadoop-cluster.html&quot;&gt;【笔记】Hadoop安装部署&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2013/03/24/manual-install-Cloudera-hive-CDH.html&quot;&gt;手动安装Cloudera Hive CDH&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2013/03/24/manual-install-Cloudera-hbase-CDH.html&quot;&gt;手动安装Cloudera HBase CDH&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html&quot;&gt;手动安装Cloudera Hadoop CDH&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2013/03/29/install-impala.html&quot;&gt;安装impala过程&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;从yum安装Cloudera CDH集群&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2013/06/24/install-cdh-by-cloudera-manager.html&quot;&gt;通过Cloudera Manager安装CDH&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;hadoop-install&quot;&gt;hadoop-install&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/javachen/hadoop-install&quot;&gt;hadoop-install&lt;/a&gt;上脚本，all-in-one-install.sh是在一个节点上安装hdfs、hive、yarn、zookeeper和hbase，编写该脚本是为了在本机（fedora19系统）上调试mapreduce、hive和hbase；cluster-install.sh是在多个节点上安装hadoop集群，同样目前完成了hdfs、hive、yarn、zookeeper和hbase的自动安装。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;脚本片段&lt;/h1&gt;
&lt;p&gt;IDH安装脚本中有一些写的比较好的shell代码片段，摘出如下，供大家学习。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;检测操作系统版本&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;( grep -i &quot;CentOS&quot; /etc/issue &amp;gt; /dev/null ) &amp;amp;&amp;amp; OS_DISTRIBUTOR=centos
( grep -i &quot;Red[[:blank:]]*Hat[[:blank:]]*Enterprise[[:blank:]]*Linux&quot; /etc/issue &amp;gt; /dev/null ) &amp;amp;&amp;amp; OS_DISTRIBUTOR=rhel
( grep -i &quot;Oracle[[:blank:]]*Linux&quot; /etc/issue &amp;gt; /dev/null ) &amp;amp;&amp;amp; OS_DISTRIBUTOR=oel
( grep -i &quot;Asianux[[:blank:]]*Server&quot; /etc/issue &amp;gt; /dev/null ) &amp;amp;&amp;amp; OS_DISTRIBUTOR=an
( grep -i &quot;SUSE[[:blank:]]*Linux[[:blank:]]*Enterprise[[:blank:]]*Server&quot; /etc/issue &amp;gt; /dev/null ) &amp;amp;&amp;amp; OS_DISTRIBUTOR=sles
( grep -i &quot;Fedora&quot; /etc/issue &amp;gt; /dev/null ) &amp;amp;&amp;amp; OS_DISTRIBUTOR=fedora

major_revision=`grep -oP &#39;\d+&#39; /etc/issue | sed -n &quot;1,1p&quot;`
minor_revision=`grep -oP &#39;\d+&#39; /etc/issue | sed -n &quot;2,2p&quot;`
OS_RELEASE=&quot;$major_revision.$minor_revision&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;root&quot;&gt;修改root密码&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;redhat&#39;|passwd root --stdin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;dns&quot;&gt;修改dns&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# Set up nameservers.
# http://ithelpblog.com/os/linux/redhat/centos-redhat/howto-fix-couldnt-resolve-host-on-centos-redhat-rhel-fedora/
# http://stackoverflow.com/a/850731/1486325
echo &quot;nameserver 8.8.8.8&quot; | tee -a /etc/resolv.conf
echo &quot;nameserver 8.8.4.4&quot; | tee -a /etc/resolv.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;修改操作系统时区&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hosts&quot;&gt;修改hosts文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF
127.0.0.1       localhost

192.168.56.121 cdh1
192.168.56.122 cdh2
192.168.56.123 cdh3
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ba&quot;&gt;去掉b文件中包括a文件的内容&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;grep -vf a b &amp;gt;result.log
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;file-max&quot;&gt;修改file-max&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;echo -e &quot;Global file limit ...&quot;
rst=`grep &quot;^fs.file-max&quot; /etc/sysctl.conf`
if [ &quot;x$rst&quot; = &quot;x&quot; ] ; then
	echo &quot;fs.file-max = 727680&quot; &amp;gt;&amp;gt; /etc/sysctl.conf || exit $?
else
	sed -i &quot;s:^fs.file-max.*:fs.file-max = 727680:g&quot; /etc/sysctl.conf
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ssh&quot;&gt;生成ssh公要&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[ ! -d ~/.ssh ] &amp;amp;&amp;amp; ( mkdir ~/.ssh ) &amp;amp;&amp;amp; ( chmod 600 ~/.ssh )
yes|ssh-keygen -f ~/.ssh/id_rsa -t rsa -N &quot;&quot; &amp;amp;&amp;amp; ( chmod 600 ~/.ssh/id_rsa.pub )
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ssh-1&quot;&gt;ssh设置无密码登陆&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;set timeout 20

set host [lindex $argv 0]
set password [lindex $argv 1]
set pubkey [exec cat /root/.ssh/id_rsa.pub]
set localsh [exec cat ./config_ssh_local.sh]

#spawn ssh-copy-id -i /root/.ssh/id_rsa.pub root@$host
spawn ssh root@$host &quot;
umask 022
mkdir -p  /root/.ssh
echo \&#39;$pubkey\&#39; &amp;gt; /root/.ssh/authorized_keys
echo \&#39;$localsh\&#39; &amp;gt;  /root/.ssh/config_ssh_local.sh
cd /root/.ssh/; sh config_ssh_local.sh
&quot;
expect {
	timeout exit
	yes/no  {send &quot;yes\r&quot;;exp_continue}
	assword {send &quot;$password\r&quot;}
}
expect eof
#interact
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;javahome&quot;&gt;配置JAVA_HOME&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;### JAVA_HOME ###
if [ -f ~/.bashrc ] ; then
    sed -i &#39;/^export[[:space:]]\{1,\}JAVA_HOME[[:space:]]\{0,\}=/d&#39; ~/.bashrc
    sed -i &#39;/^export[[:space:]]\{1,\}CLASSPATH[[:space:]]\{0,\}=/d&#39; ~/.bashrc
    sed -i &#39;/^export[[:space:]]\{1,\}PATH[[:space:]]\{0,\}=/d&#39; ~/.bashrc
fi
echo &quot;&quot; &amp;gt;&amp;gt;~/.bashrc
echo &quot;export JAVA_HOME=/usr/java/latest&quot; &amp;gt;&amp;gt;~/.bashrc
echo &quot;export CLASSPATH=.:\$JAVA_HOME/lib/tools.jar:\$JAVA_HOME/lib/dt.jar&quot;&amp;gt;&amp;gt;~/.bashrc
echo &quot;export PATH=\$JAVA_HOME/bin:\$PATH&quot; &amp;gt;&amp;gt; ~/.bashrc

alternatives --install /usr/bin/java java /usr/java/latest 5
alternatives --set java /usr/java/latest 
source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;格式化集群&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;su -s /bin/bash hdfs -c &#39;yes Y | hadoop namenode -format &amp;gt;&amp;gt; /tmp/format.log 2&amp;gt;&amp;amp;1&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hadoop-1&quot;&gt;创建hadoop目录&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;su -s /bin/bash hdfs -c &quot;hadoop fs -chmod a+rw /&quot;
while read dir user group perm
do
     su -s /bin/bash hdfs -c &quot;hadoop fs -mkdir -R $dir &amp;amp;&amp;amp; hadoop fs -chmod -R $perm $dir &amp;amp;&amp;amp; hadoop fs -chown -R $user:$group $dir&quot;
     echo &quot;.&quot;
done &amp;lt;&amp;lt; EOF
/tmp hdfs hadoop 1777 
/tmp/hadoop-yarn mapred mapred 777
/var hdfs hadoop 755 
/var/log yarn mapred 1775 
/var/log/hadoop-yarn/apps yarn mapred 1777
/hbase hbase hadoop 755
/user hdfs hadoop 777
/user/history mapred hadoop 1777
/user/root root hadoop 777
/user/hive hive hadoop 777
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hivepostgresql&quot;&gt;hive中安装并初始化postgresql&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;yum install postgresql-server postgresql-jdbc -y &amp;gt;/dev/null
chkconfig postgresql on
rm -rf /var/lib/pgsql/data
rm -rf /var/run/postgresql/.s.PGSQL.5432
service postgresql initdb

sed -i &quot;s/max_connections = 100/max_connections = 600/&quot; /var/lib/pgsql/data/postgresql.conf
sed -i &quot;s/#listen_addresses = &#39;localhost&#39;/listen_addresses = &#39;*&#39;/&quot; /var/lib/pgsql/data/postgresql.conf
sed -i &quot;s/shared_buffers = 32MB/shared_buffers = 256MB/&quot; /var/lib/pgsql/data/postgresql.conf
sed -i &quot;s/127.0.0.1\/32/0.0.0.0\/0/&quot; /var/lib/pgsql/data/pg_hba.conf

sudo cat /var/lib/pgsql/data/postgresql.conf | grep -e listen -e standard_conforming_strings

rm -rf /usr/lib/hive/lib/postgresql-jdbc.jar
ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jar

su -c &quot;cd ; /usr/bin/pg_ctl start -w -m fast -D /var/lib/pgsql/data&quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;create user hiveuser with password &#39;redhat&#39;; \&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;CREATE DATABASE metastore owner=hiveuser;\&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;GRANT ALL privileges ON DATABASE metastore TO hiveuser;\&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql -U hiveuser -d metastore -f /usr/lib/hive/scripts/metastore/upgrade/postgres/hive-schema-0.10.0.postgres.sql&quot; postgres
su -c &quot;cd ; /usr/bin/pg_ctl restart -w -m fast -D /var/lib/pgsql/data&quot; postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;更多脚本，请关注github：&lt;a href=&quot;https://github.com/javachen/hadoop-install&quot;&gt;hadoop-install&lt;/a&gt;，你可以下载、使用并修改其中代码！&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2013/08/02/hadoop-install-script.html</link>
      <guid>http://blog.javachen.com/2013/08/02/hadoop-install-script.html</guid>
      <pubDate>2013-08-02T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>远程调试Hadoop各组件</title>
      <description>&lt;p&gt;远程调试对应用程序开发十分有用。例如，为不能托管开发平台的低端机器开发程序，或在专用的机器上（比如服务不能中断的 Web 服务器）调试程序。其他情况包括：运行在内存小或 CUP 性能低的设备上的 Java 应用程序（比如移动设备），或者开发人员想要将应用程序和开发环境分开，等等。&lt;/p&gt;

&lt;p&gt;为了进行远程调试，必须使用 Java Virtual Machine (JVM) V5.0 或更新版本。&lt;/p&gt;

&lt;h1 id=&quot;jpda-&quot;&gt;JPDA 简介&lt;/h1&gt;

&lt;p&gt;Sun Microsystem 的 Java Platform Debugger Architecture (JPDA) 技术是一个多层架构，使您能够在各种环境中轻松调试 Java 应用程序。JPDA 由两个接口（分别是 JVM Tool Interface 和 JDI）、一个协议（Java Debug Wire Protocol）和两个用于合并它们的软件组件（后端和前端）组成。它的设计目的是让调试人员在任何环境中都可以进行调试。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;更详细的介绍，您可以参考&lt;a href=&quot;http://www.ibm.com/developerworks/cn/opensource/os-eclipse-javadebug/&quot;&gt;使用 Eclipse 远程调试 Java 应用程序&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;jdwp-&quot;&gt;JDWP 设置&lt;/h1&gt;

&lt;p&gt;JVM本身就支持远程调试，Eclipse也支持JDWP，只需要在各模块的JVM启动时加载以下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-Xdebug -Xrunjdwp:transport=dt_socket, address=8000,server=y,suspend=y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;各参数的含义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-Xdebug
启用调试特性
-Xrunjdwp
启用JDWP实现，包含若干子选项：
transport=dt_socket
JPDA front-end和back-end之间的传输方法。dt_socket表示使用套接字传输。
address=8000
JVM在8000端口上监听请求，这个设定为一个不冲突的端口即可。
server=y
y表示启动的JVM是被调试者。如果为n，则表示启动的JVM是调试器。
suspend=y
y表示启动的JVM会暂停等待，直到调试器连接上才继续执行。suspend=n，则JVM不会暂停等待。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hbase&quot;&gt;配置hbase远程调试&lt;/h1&gt;

&lt;p&gt;打开&lt;code&gt;/etc/hbase/conf/hbase-env.sh&lt;/code&gt;，找到以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Enable remote JDWP debugging of major HBase processes. Meant for Core Developers
# export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070&quot;
# export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8071&quot;
# export HBASE_THRIFT_OPTS=&quot;$HBASE_THRIFT_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8072&quot;
# export HBASE_ZOOKEEPER_OPTS=&quot;$HBASE_ZOOKEEPER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8073&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想远程调式hbase-master进程，请去掉对&lt;code&gt;HBASE_MASTER_OPTS&lt;/code&gt;的注释，其他依次类推。注意，我这里使用的是cdh-4.3.0中的hbase。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 注意（20130817更新）：&lt;/p&gt;

&lt;p&gt;如果启动hbase时提示&lt;code&gt; check your java command line for duplicate jdwp options&lt;/code&gt;，请把上面参数加到/usr/lib/hbase/bin/hbase中if else对应分支中去。&lt;/p&gt;

&lt;p&gt;例如，如果你想调试regionserver，请把下面代码加到&lt;code&gt;elif [ &quot;$COMMAND&quot; = &quot;regionserver&quot; ] ; then&lt;/code&gt;中去：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8071&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hive&quot;&gt;配置hive远程调试&lt;/h1&gt;

&lt;p&gt;停止hive-server2进程，然后以下面命令启动hive-server2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	hive --service hiveserver --debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进程会监听在8000端口等待调试连接。如果想更改监听端口，可以修改配置文件:&lt;code&gt;${HIVE_HOME}bin/ext/debug.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如果Hadoop是0.23以上版本，debug模式启动Cli会报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ERROR: Cannot load this JVM TI agent twice, check your java command line for duplicate jdwp options.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开&lt;code&gt;${Hadoop_HOME}/bin/hadoop&lt;/code&gt;，注释掉以下代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Always respect HADOOP_OPTS and HADOOP_CLIENT_OPTS
HADOOP_OPTS=&quot;$HADOOP_OPTS $HADOOP_CLIENT_OPTS&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;yarn&quot;&gt;配置yarn远程调试&lt;/h1&gt;

&lt;p&gt;请在以下代码添加调试参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if [ &quot;$COMMAND&quot; = &quot;classpath&quot; ] ; then
if $cygwin; then
CLASSPATH=`cygpath -p -w &quot;$CLASSPATH&quot;`
fi
echo $CLASSPATH
exit
elif [ &quot;$COMMAND&quot; = &quot;rmadmin&quot; ] ; then
CLASS=&#39;org.apache.hadoop.yarn.client.RMAdmin&#39;
YARN_OPTS=&quot;$YARN_OPTS $YARN_CLIENT_OPTS&quot;
elif [ &quot;$COMMAND&quot; = &quot;application&quot; ] ; then
class=&quot;org&quot;.apache.hadoop.yarn.client.cli.ApplicationCLI
YARN_OPTS=&quot;$YARN_OPTS $YARN_CLIENT_OPTS&quot;
elif [ &quot;$COMMAND&quot; = &quot;node&quot; ] ; then
class=&quot;org&quot;.apache.hadoop.yarn.client.cli.NodeCLI
YARN_OPTS=&quot;$YARN_OPTS $YARN_CLIENT_OPTS&quot;
elif [ &quot;$COMMAND&quot; = &quot;resourcemanager&quot; ] ; then
CLASSPATH=${CLASSPATH}:$YARN_CONF_DIR/rm-config/log4j.properties
CLASS=&#39;org.apache.hadoop.yarn.server.resourcemanager.ResourceManager&#39;
YARN_OPTS=&quot;$YARN_OPTS $YARN_RESOURCEMANAGER_OPTS&quot;
if [ &quot;$YARN_RESOURCEMANAGER_HEAPSIZE&quot; != &quot;&quot; ]; then
JAVA_HEAP_MAX=&quot;-Xmx&quot;&quot;$YARN_RESOURCEMANAGER_HEAPSIZE&quot;&quot;m&quot;
fi
elif [ &quot;$COMMAND&quot; = &quot;nodemanager&quot; ] ; then
CLASSPATH=${CLASSPATH}:$YARN_CONF_DIR/nm-config/log4j.properties
CLASS=&#39;org.apache.hadoop.yarn.server.nodemanager.NodeManager&#39;
YARN_OPTS=&quot;$YARN_OPTS -server $YARN_NODEMANAGER_OPTS&quot;
if [ &quot;$YARN_NODEMANAGER_HEAPSIZE&quot; != &quot;&quot; ]; then
JAVA_HEAP_MAX=&quot;-Xmx&quot;&quot;$YARN_NODEMANAGER_HEAPSIZE&quot;&quot;m&quot;
fi
elif [ &quot;$COMMAND&quot; = &quot;proxyserver&quot; ] ; then
CLASS=&#39;org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer&#39;
YARN_OPTS=&quot;$YARN_OPTS $YARN_PROXYSERVER_OPTS&quot;
if [ &quot;$YARN_PROXYSERVER_HEAPSIZE&quot; != &quot;&quot; ]; then
JAVA_HEAP_MAX=&quot;-Xmx&quot;&quot;$YARN_PROXYSERVER_HEAPSIZE&quot;&quot;m&quot;
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如：&lt;br /&gt;
如果你想调试resourcemanager代码，请在&lt;code&gt; elif [ &quot;$COMMAND&quot; = &quot;resourcemanager&quot; ]&lt;/code&gt; 分支内添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;YARN_RESOURCEMANAGER_OPTS=&quot;$YARN_RESOURCEMANAGER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6001&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他进程，参照上面即可。&lt;/p&gt;

&lt;p&gt;注意：端口不要冲突。&lt;/p&gt;

&lt;h1 id=&quot;mapreduce&quot;&gt;配置mapreduce远程调试&lt;/h1&gt;
&lt;p&gt;如果想要调试Map 或Reduce Task，则修改&lt;code&gt;bin/hadoop&lt;/code&gt;已经没用了，因为&lt;code&gt;bin/hadoop&lt;/code&gt;中没有Map Task的启动参数。&lt;/p&gt;

&lt;p&gt;此时需要修改mapred-site.xml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;mapred.child.java.opts&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;-Xmx800m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000&amp;lt;/value&amp;gt;
&amp;lt;/property
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在一个TaskTracker上，只能启动一个Map Task或一个Reduce Task，否则启动时会有端口冲突。因此要修改所有TaskTracker上的&lt;code&gt;conf/hadoop-site.xml&lt;/code&gt;中的配置项：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;mapred.tasktracker.map.tasks.maximum&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;mapred.tasktracker.reduce.tasks.maximum&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;0&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;eclipse&quot;&gt;在Eclipse中使用方法：&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;打开eclipse，找到&lt;code&gt;Debug Configurations...&lt;/code&gt;，添加一个Remout Java Application:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在source中可以关联到hive的源代码,然后，单击Debug按钮进入远程debug模式。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;编写个jdbc的测试类，运行代码，这时候因为hive-server2端没有设置端点，故程序可以正常运行直到结束。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在hive代码中设置一个断点，如&lt;code&gt;ExecDriver.java&lt;/code&gt;的&lt;code&gt;execute&lt;/code&gt;方法中设置断点，然后再运行jdbc测试类。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;section&quot;&gt;参考文章&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://zhangjie.me/eclipse-debug-hadoop/&quot;&gt;在Eclipse中远程调试Hadoop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://long-xie.iteye.com/blog/1779072&quot;&gt;hive远程调试&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ibm.com/developerworks/cn/opensource/os-eclipse-javadebug/&quot;&gt;使用 Eclipse 远程调试 Java 应用程序&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      <link>http://blog.javachen.com/2013/08/01/remote-debug-hadoop.html</link>
      <guid>http://blog.javachen.com/2013/08/01/remote-debug-hadoop.html</guid>
      <pubDate>2013-08-01T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>安装RHadoop</title>
      <description>&lt;h1 id=&quot;r-language-install&quot;&gt;1. R Language Install&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;安装相关依赖&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;yum install -y perl* pcre-devel tcl-devel zlib-devel bzip2-devel libX11-devel tk-devel tetex-latex *gfortran*  compat-readline5
yum install libRmath-*
rpm -Uvh --force --nodeps  R-core-2.10.0-2.el5.x86_64.rpm
rpm -Uvh R-2.10.0-2.el5.x86_64.rpm R-devel-2.10.0-2.el5.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;r-301&quot;&gt;编译安装：R-3.0.1&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf R-3.0.1 
./configure
make 
make install #R运行
export HADOOP_CMD=/usr/bin/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;排错&lt;/h2&gt;

&lt;p&gt;1、错误1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;error: --with-readline=yes (default) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装readline&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install readline*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、错误2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;error: No F77 compiler found 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装gfortran&lt;/p&gt;

&lt;p&gt;3、错误3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;error: –with-x=yes (default) and X11 headers/libs are not available 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install libXt*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、错误4&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;error: C++ preprocessor &quot;/lib/cpp&quot; fails sanity check 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装g++或build-essential（redhat6.2安装gcc-c++和glibc-headers）&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;验证是否安装成功&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 bin]# R
R version 3.0.1 (2013-05-16) -- &quot;Good Sport&quot;
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R是自由软件，不带任何担保。
在某些条件下你可以将其自由散布。
用&#39;license()&#39;或&#39;licence()&#39;来看散布的详细条件。

R是个合作计划，有许多人为之做出了贡献.
用&#39;contributors()&#39;来看合作者的详细情况
用&#39;citation()&#39;会告诉你如何在出版物中正确地引用R或R程序包。

用&#39;demo()&#39;来看一些示范程序，用&#39;help()&#39;来阅读在线帮助文件，或
用&#39;help.start()&#39;通过HTML浏览器来看帮助文件。
用&#39;q()&#39;退出R.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;rhadoop&quot;&gt;2. 安装Rhadoop&lt;/h1&gt;

&lt;h2 id=&quot;rhdfsrmr2&quot;&gt;安装rhdfs，rmr2&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;cd Rhadoop/
R CMD javareconf
R CMD INSTALL &#39;plyr_1.8.tar.gz&#39;
R CMD INSTALL &#39;stringr_0.6.2.tar.gz&#39;
R CMD INSTALL &#39;reshape2_1.2.2.tar.gz&#39;
R CMD INSTALL &#39;digest_0.6.3.tar.gz&#39;
R CMD INSTALL &#39;functional_0.4.tar.gz&#39;
R CMD INSTALL &#39;iterators_1.0.6.tar.gz&#39;
R CMD INSTALL &#39;itertools_0.1-1.tar.gz&#39;
R CMD INSTALL &#39;Rcpp_0.10.3.tar.gz&#39;
R CMD INSTALL &#39;rJava_0.9-4.tar.gz&#39;
R CMD INSTALL &#39;RJSONIO_1.0-3.tar.gz&#39;
R CMD INSTALL &#39;reshape2_1.2.2.tar.gz&#39;
R CMD INSTALL &#39;rhdfs_1.0.5.tar.gz&#39;
R CMD INSTALL &#39;rmr2_2.2.0.tar.gz&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R library(rhdfs)检查是否能正常工作&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;验证测试&lt;/h2&gt;

&lt;p&gt;Rmr测试命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; train.mr&amp;lt;-mapreduce( + train.hdfs, + map = function(k, v) { + keyval(k,v$item) + } + ,reduce=function(k,v){ + m&amp;lt;-merge(v,v) + keyval(m$x,m$y) + } + )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出现如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;packageJobJar: [/tmp/RtmpCuhs7d/rmr-local-env18916b6f86b3, /tmp/RtmpCuhs7d/rmr-global-env18913824c681, /tmp/RtmpCuhs7d/rmr-streaming-map18912d6c2b1c, /tmp/RtmpCuhs7d/rmr-streaming-reduce1891179bb645, /tmp/hadoop-root/hadoop-unjar4575094085541826184/] [] /tmp/streamjob2910108622786868147.jar tmpDir=null 13/06/05 18:22:28
WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same. 13/06/05 18:22:28 INFO mapred.FileInputFormat: Total input paths to process : 1 13/06/05 18:22:29
INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-root/mapred/local] 13/06/05 18:22:29 INFO streaming.StreamJob: Running job: job_201306050931_0004 13/06/05 18:22:29
INFO streaming.StreamJob: To kill this job, run: 13/06/05 18:22:29
INFO streaming.StreamJob: /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=cdh1:8021 -kill job_201306050931_0004 13/06/05 18:22:29 INFO streaming.StreamJob: Tracking URL: http://cdh1:50030/jobdetails.jsp?jobid=job_201306050931_0004 13/06/05 18:22:30 
INFO streaming.StreamJob:  map 0%  reduce 0% 13/06/05 18:22:56
INFO streaming.StreamJob:  map 100%  reduce 100% 13/06/05 18:22:56
INFO streaming.StreamJob: To kill this job, run: 13/06/05 18:22:56
INFO streaming.StreamJob: /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=cdh1:8021 -kill job_201306050931_0004 13/06/05 18:22:56
INFO streaming.StreamJob: Tracking URL: http://cdh1:50030/jobdetails.jsp?jobid=job_201306050931_0004 13/06/05 18:22:56 
ERROR streaming.StreamJob: Job not successful. Error: NA 13/06/05 18:22:56
INFO streaming.StreamJob: killJob... Streaming Command Failed! Error in mr(map = map, reduce = reduce, combine = combine, vectorized.reduce,  :   hadoop streaming failed with error code 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;错误解决方法： 通过查看日志，hadoop没有在&lt;code&gt;/usr/bin&lt;/code&gt;下找到Rscript,于是从R的安装目录&lt;code&gt;/usr/local/bin&lt;/code&gt;下做R和Rscript的符号链接到&lt;code&gt;/usr/bin&lt;/code&gt;下，再次执行即可解决次错。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#ln -s /usr/loca/bin/R  /usr/bin
#ln -s /usr/local/bin/Rscript  /usr/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;rhbase&quot;&gt;3. 安装rhbase&lt;/h1&gt;
&lt;p&gt;## 安装依赖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#yum install boost*
#yum install openssl*
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;thrift&quot;&gt;安装thrift&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;#tar -zxvf thrift-0.9.0.tar.gz
#mv thrift-0.9.0/lib/cpp/src/thrift/qt/moc_TQTcpServer.cpp  thrift-0.9.0/lib/cpp/src/thrift/qt/moc_TQTcpServer.cpp.bak
#cd thrift-0.9.0
#./configure --with-boost=/usr/include/boost JAVAC=/usr/java/jdk1.6.0_31/bin/javac
#make
#make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果报错：error: “Error: libcrypto required.”&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#yum install openssl*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;src/thrift/qt/moc_TQTcpServer.cpp:14:2: error: #error &quot;This file was generated using the moc from 4.8.1. It&quot;
src/thrift/qt/moc_TQTcpServer.cpp:15:2: error: #error &quot;cannot be used with the include files from this version of Qt.&quot;
src/thrift/qt/moc_TQTcpServer.cpp:16:2: error: #error &quot;(The moc has changed too much.)&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则运行下面命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#mv thrift-0.9.0/lib/cpp/src/thrift/qt/moc_TQTcpServer.cpp  thrift-0.9.0/lib/cpp/src/thrift/qt/moc_TQTcpServer.cpp.bak
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;pkgconfigpath&quot;&gt;配置PKG_CONFIG_PATH&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig/
 	pkg-config --cflags thrift    #返回：-I/usr/local/include/thrift为正确
 	cp /usr/local/lib/libthrift-0.9.0.so /usr/lib/
 	cp /usr/local/lib/libthrift-0.9.0.so /usr/lib64/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动hbase：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/lib/hbase/bin/hbase-daemon.sh  start  thrift 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用jps查看thrift进程&lt;/p&gt;

&lt;h2 id=&quot;rhbase-1&quot;&gt;安装rhbase&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;R CMD INSTALL &#39;rhbase_1.1.1.tar.gz&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;验证并测试&lt;/h2&gt;

&lt;p&gt;在R命令行中输入library(rmr2)、library(rhdfs)、library(rhbase)，载入成功即表示安装成功&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@desktop27 hadoop]# R
R version 3.0.1 (2013-05-16) -- &quot;Good Sport&quot;
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)
R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type &#39;license()&#39; or &#39;licence()&#39; for distribution details.
Natural language support but running in an English locale
R is a collaborative project with many contributors.
Type &#39;contributors()&#39; for more information and
&#39;citation()&#39; on how to cite R or R packages in publications.
Type &#39;demo()&#39; for some demos, &#39;help()&#39; for on-line help, or
&#39;help.start()&#39; for an HTML browser interface to help.
Type &#39;q()&#39; to quit R.
&amp;gt; library(rhdfs)
Loading required package: rJava
HADOOP_CMD=/usr/bin/hadoop
Be sure to run hdfs.init()
&amp;gt; library(rmr2)
Loading required package: Rcpp
Loading required package: RJSONIO
Loading required package: digest
Loading required package: functional
Loading required package: stringr
Loading required package: plyr
Loading required package: reshape2
&amp;gt; library(rhbase)
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;rhive&quot;&gt;4. 装RHive&lt;/h1&gt;

&lt;h2 id=&quot;section-5&quot;&gt;环境变量&lt;/h2&gt;
&lt;p&gt;设置环境变量 &lt;code&gt;vim /etc/profile&lt;/code&gt;,末行添加如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export HADOOP_CMD=/usr/bin/hadoop
export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig/
export HADOOP_STREAMING=/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.2.1.jar
export HADOOP_HOME=/usr/lib/hadoop
export RHIVE_DATA=/hadoop/dfs/rhive/data
export HIVE_HOME=/usr/lib/hive
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;rserve&quot;&gt;安装Rserve：&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;#R CMD INSTALL &#39;Rserve_1.7-1.tar.gz&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在安装Rsever用户下，创建一目录，并创建Rserv.conf文件，写入``remote enable’‘保存并退出。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cd /usr/local/lib64/R/
#echo remote enable &amp;gt; Rserv.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动Rserve：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#R CMD Rserve --RS-conf /usr/local/lib64/R/Rserv.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查Rserve启动是否正常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#telnet localhost 6311
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示 Rsrv0103QAP1 则表示连接成功&lt;/p&gt;

&lt;h2 id=&quot;rhive-1&quot;&gt;安装RHive&lt;/h2&gt;
&lt;p&gt;创建数据目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#R CMD INSTALL RHive_0.0-7.tar.gz
#cd /usr/local/lib64/R/
mkdir -p rhive/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上传rhive_udf.jar到hdfs上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoop fs -mkdir /rhive/lib
cd /usr/local/lib64/R/library/RHive/java
hadoop fs -put rhive_udf.jar /rhive/lib
hadoop fs -chmod a+rw /rhive/lib/rhive_udf.jar
cd /usr/lib/hadoop
ln -s /etc/hadoop/conf conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试RHive安装是否成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;R
library（RHive）
rhive.connect(&#39;192.168.0.27&#39;)【hive的地址】
rhive.env()
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2013/07/20/install-rhadoop.html</link>
      <guid>http://blog.javachen.com/2013/07/20/install-rhadoop.html</guid>
      <pubDate>2013-07-20T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>通过Cloudera Manager安装CDH</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;1 方法一&lt;/h1&gt;

&lt;p&gt;你可以从&lt;a href=&quot;https://ccp.cloudera.com/display/SUPPORT/Downloads&quot;&gt;https://ccp.cloudera.com/display/SUPPORT/Downloads&lt;/a&gt;下载&lt;code&gt;cloudera-manager-installer.bin&lt;/code&gt;，然后修改执行权限并执行该脚本。&lt;/p&gt;

&lt;p&gt;该脚本中配置的rhel6的yum源为：&lt;a href=&quot;http://archive.cloudera.com/cm4/redhat/6/x86_64/cm/4/&quot;&gt;http://archive.cloudera.com/cm4/redhat/6/x86_64/cm/4/&lt;/a&gt;，下载的过程必须连网并且rpm的过程会非常慢，这种方法对虚拟机或者是无法连网的内网机器来说根本无法使用。&lt;/p&gt;

&lt;p&gt;因为知道所有的rpm都在上面网址可以下载到，故你可以手动下载这些rpm然后手动安装，详细过程请参考：&lt;a href=&quot;http://dreamyue.com/post/41090075449/cloudera-manager-hadoop&quot;&gt;通过cloudera-manager来安装hadoop&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2 方法二&lt;/h1&gt;

&lt;p&gt;这里还有一种方法，就是手动下载&lt;code&gt;Cloudera Manager&lt;/code&gt;的yum tar包，在虚拟机中搭建一个本地yum源，然后修改hosts文件，使&lt;code&gt;archive.cloudera.com&lt;/code&gt;域名映射到本地ip。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3 方法三&lt;/h1&gt;
&lt;p&gt;出于好奇，想破解&lt;code&gt;cloudera-manager-installer.bin&lt;/code&gt;，然后看看其中做了哪些操作。通过以下脚本即可解压该文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mv cloudera-manager-installer.bin cloudera-manager-installer.zip
$ unzip cloudera-manager-installer.zip 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解压之后的目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ll
总用量 512
-rwxrwxr-x. 1 june june 501698 5月  25 09:53 cloudera-manager-installer.zip
drwxr-xr-x. 2 june june   4096 5月  23 03:05 data
drwxr-xr-x. 2 june june   4096 5月  22 21:48 guis
drwxr-xr-x. 2 june june   4096 5月  22 21:48 meta
drwxr-xr-x. 2 june june   4096 5月  22 21:48 scripts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看解压之后的文件可以看到安装脚本是用lua编写并用MojoSetup编译的，从&lt;code&gt;scripts/config.lua&lt;/code&gt;脚本中大概可以看出安装脚本的执行过程。&lt;/p&gt;

&lt;p&gt;整理下该脚本逻辑，主要是做了以下操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install -y jdk.x86_64 
$ yum install -y cloudera-manager-server 
$ yum install -y cloudera-manager-server-db
$ /etc/init.d/cloudera-scm-server start
$ /etc/init.d/cloudera-scm-server-db start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;知道了上面这点之后，就可以在本地的cloudera-manager yum中，执行以上操作完成cloudera-manager的安装，安装成功之后查看7180端口是否打开：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ netstat -na| grep 7180
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过浏览器访问&lt;code&gt;http://IP:7180&lt;/code&gt;登录cloudera manager界面：用户名/密码：&lt;code&gt;admin/admin&lt;/code&gt;,按照界面提示完成hadoop集群安装。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;卸载&lt;/h1&gt;

&lt;p&gt;执行以下命令，可以卸载并删除相应的文件和目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;yum remove -y hadoop hbase hive zookeeper hue oozie sqoop flume
yum remove cloudera*

rm -rf /usr/lib/{hadoop,hbase,oozie}
rm -rf /etc/{hadoop,hadoop-httpfs,hbase,hive,zookeeper}
rm -rf /etc/{hadoop,hadoop-httpfs,hbase,hive,zookeeper,sqoop,oozie,flume}
rm -rf /etc/cloudera-scm-agent/
rm -rf /tmp/hadoop-hive/ /usr/share/hue/  /var/log/hive/
rm -rf /var/log/hadoop-*
rm -rf /var/log/hbase/
rm -rf /var/log/zookeeper/
rm -rf /var/log/cloudera-scm-*
rm -rf /var/log/flume-ng/
rm -rf /var/log/hue
rm -rf /usr/share/doc/oozie*
rm -rf /etc/hue/ /etc/rc.d/init.d/hue
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5 排错&lt;/h1&gt;
&lt;p&gt;在执行下面一个命令时候可能会出现如下异常&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /etc/init.d/cloudera-scm-server-db start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;异常信息如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[root@cdh1 cloudera-scm-server-db]# /etc/init.d/cloudera-scm-server-db start
属于此数据库系统的文件宿主为用户 &quot;cloudera-scm&quot;.
此用户也必须为服务器进程的宿主.
数据库簇将带有 locale en_US.UTF8 初始化.
缺省的文本搜索配置将会被设置到&quot;english&quot;

修复已存在目录 /var/lib/cloudera-scm-server-db/data 的权限 ... initdb: 无法改变目录 &quot;/var/lib/cloudera-scm-server-db/data&quot; 的权限: 权限不够
Could not initialize database server.
  This usually means that your PostgreSQL installation failed or isn&#39;t working properly.
  PostgreSQL is installed using the set of repositories found on this machine. Please
  ensure that PostgreSQL can be installed. Please also uninstall any other instances of
  PostgreSQL and then try again., giving up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候，请执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ su -s /bin/bash cloudera-scm -c &quot;touch /var/log/cloudera-scm-server/db.log; /usr/share/cmf/bin/initialize_embedded_db.sh /var/lib/cloudera-scm-server-db/data /var/log/cloudera-scm-server/db.log&quot;
$ su -s /bin/bash cloudera-scm -c &quot;pg_ctl start -w -D /var/lib/cloudera-scm-server-db/data -l /var/log/cloudera-scm-server/db.log&quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2013/06/24/install-cdh-by-cloudera-manager.html</link>
      <guid>http://blog.javachen.com/2013/06/24/install-cdh-by-cloudera-manager.html</guid>
      <pubDate>2013-06-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>HBase笔记：存储结构</title>
      <description>&lt;p&gt;从HBase的架构图上可以看出，HBase中的存储包括HMaster、HRegionServer、HRegion、Store、MemStore、StoreFile、HFile、HLog等，本篇文章统一介绍他们的作用即存储结构。&lt;/p&gt;

&lt;p&gt;以下是网络上流传的HBase存储架构图:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/hbase-structure.jpg&quot; alt=&quot;hbase-structure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;HBase中的每张表都通过行键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，这个过程由HRegionServer管理，而HRegion的分配由HMaster管理。&lt;/p&gt;

&lt;h1 id=&quot;hmaster&quot;&gt;HMaster的作用：&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;为Region server分配region&lt;/li&gt;
  &lt;li&gt;负责Region server的负载均衡&lt;/li&gt;
  &lt;li&gt;发现失效的Region server并重新分配其上的region&lt;/li&gt;
  &lt;li&gt;HDFS上的垃圾文件回收&lt;/li&gt;
  &lt;li&gt;处理schema更新请求&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hregionserver&quot;&gt;HRegionServer作用：&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;维护master分配给他的region，处理对这些region的io请求&lt;/li&gt;
  &lt;li&gt;负责切分正在运行过程中变的过大的region&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以看到，client访问hbase上的数据并不需要master参与（寻址访问zookeeper和region server，数据读写访问region server），master仅仅维护table和region的元数据信息（table的元数据信息保存在zookeeper上），负载很低。&lt;/p&gt;

&lt;p&gt;HRegionServer存取一个子表时，会创建一个HRegion对象，然后对表的每个列族创建一个Store实例，每个Store都会有一个MemStore和0个或多个StoreFile与之对应，每个StoreFile都会对应一个HFile， HFile就是实际的存储文件。因此，一个HRegion有多少个列族就有多少个Store。&lt;/p&gt;

&lt;p&gt;一个HRegionServer会有多个HRegion和一个HLog。&lt;/p&gt;

&lt;h1 id=&quot;hregion&quot;&gt;HRegion&lt;/h1&gt;

&lt;p&gt;table在行的方向上分隔为多个Region。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。&lt;/p&gt;

&lt;p&gt;Region按大小分隔，每个表一行是只有一个region。随着数据不断插入表，region不断增大，&lt;strong&gt;当region的某个列族达到一个阈值&lt;/strong&gt;（默认256M）时就会分成两个新的region。&lt;/p&gt;

&lt;p&gt;每个region由以下信息标识：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&amp;lt;表名,startRowkey,创建时间&amp;gt;&lt;/li&gt;
  &lt;li&gt;由目录表(-ROOT-和.META.)可值该region的endRowkey&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HRegion定位：&lt;/p&gt;

&lt;p&gt;Region被分配给哪个Region Server是完全动态的，所以需要机制来定位Region具体在哪个region server。&lt;/p&gt;

&lt;p&gt;HBase使用三层结构来定位region：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、 通过zk里的文件/hbase/rs得到-ROOT-表的位置。-ROOT-表只有一个region。&lt;/li&gt;
  &lt;li&gt;2、通过-ROOT-表查找.META.表的第一个表中相应的region的位置。其实-ROOT-表是.META.表的第一个region；.META.表中的每一个region在-ROOT-表中都是一行记录。&lt;/li&gt;
  &lt;li&gt;3、通过.META.表找到所要的用户表region的位置。用户表中的每个region在.META.表中都是一行记录。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnblogs.com/cnblogs_com/chenli0513/image0030.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;-ROOT-表永远不会被分隔为多个region，保证了最多需要三次跳转，就能定位到任意的region。client会讲查询的位置信息保存缓存起来，缓存不会主动失效，因此如果client上的缓存全部失效，则需要进行6次网络来回，才能定位到正确的region，其中蚕丝用来发现缓存失效，另外三次用来获取位置信息。&lt;/p&gt;

&lt;h1 id=&quot;store&quot;&gt;Store&lt;/h1&gt;

&lt;p&gt;每一个region有一个或多个store组成，至少是一个store，hbase会把一起访问的数据放在一个store里面，即为每个ColumnFamily建一个store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个memStore和0或者多个StoreFile组成。&lt;/p&gt;

&lt;p&gt;HBase以store的大小来判断是否需要切分region。&lt;/p&gt;

&lt;h1 id=&quot;memstore&quot;&gt;MemStore&lt;/h1&gt;

&lt;p&gt;memStore 是放在内存里的。保存修改的数据即keyValues。当memStore的大小达到一个阀值（默认64MB）时，memStore会被flush到文件，即生成一个快照。目前hbase 会有一个线程来负责memStore的flush操作。&lt;/p&gt;

&lt;h1 id=&quot;storefile&quot;&gt;StoreFile&lt;/h1&gt;

&lt;p&gt;memStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。&lt;/p&gt;

&lt;h1 id=&quot;hfile&quot;&gt;HFile&lt;/h1&gt;

&lt;p&gt;HBase中KeyValue数据的存储格式，是hadoop的二进制格式文件。&lt;/p&gt;

&lt;p&gt;首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。Trailer中又指针指向其他数据块的起始点，FileInfo记录了文件的一些meta信息。&lt;/p&gt;

&lt;p&gt;Data Block是hbase io的基本单元，为了提高效率，HRegionServer中又基于LRU的block cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定（默认块大小64KB），大号的Block有利于顺序Scan，小号的Block利于随机查询。每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成，Magic内容就是一些随机数字，目的是烦着数据损坏，结构如下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/hfile-keyvalue-structure.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;HFile结构图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/hfile-structure.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Data Block段用来保存表中的数据，这部分可以被压缩。&lt;/p&gt;

&lt;p&gt;Meta Block段（可选的）用来保存用户自定义的kv段，可以被压缩。&lt;/p&gt;

&lt;p&gt;FileInfo段用来保存HFile的元信息，本能被压缩，用户也可以在这一部分添加自己的元信息。&lt;/p&gt;

&lt;p&gt;Data Block Index段（可选的）用来保存Meta Blcok的索引。&lt;/p&gt;

&lt;p&gt;Trailer这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰。&lt;/p&gt;

&lt;p&gt;HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。目标HFile的压缩支持两种方式：gzip、lzo。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/hfile-data-storeage.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另外，针对目前针对现有HFile的两个主要缺陷：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a) 暂用过多内存&lt;/li&gt;
  &lt;li&gt;b) 启动加载时间缓慢&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;提出了HFile Version2设计：&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12478329/hfile_format_v2_design_draft_0.1.pdf&quot;&gt;https://issues.apache.org/jira/secure/attachment/12478329/hfile_format_v2_design_draft_0.1.pdf&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;hlog&quot;&gt;HLog&lt;/h1&gt;

&lt;p&gt;其实HLog文件就是一个普通的Hadoop Sequence File，&lt;br /&gt;
Sequence File的value是key时HLogKey对象，其中记录了写入数据的归属信息，除了table和region名字外，还同时包括sequence number和timestamp，timestamp是写入时间，equence number的起始值为0，或者是最近一次存入文件系统中的equence number。&lt;/p&gt;

&lt;p&gt;Sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/hlog-structure.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/hbase-write-hlog-process.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LogFlusher&lt;/p&gt;

&lt;p&gt;前面提到，数据以KeyValue形式到达HRegionServer，将写入WAL，之后，写入一个SequenceFile。看过去没问题，但是因为数据流在写入文件系统时，经常会缓存以提高性能。这样，有些本以为在日志文件中的数据实际在内存中。这里，我们提供了一个LogFlusher的类。它调用HLog.optionalSync(),后者根据 &lt;code&gt;hbase.regionserver.optionallogflushinterval&lt;/code&gt; (默认是10秒)，定期调用Hlog.sync()。另外，HLog.doWrite()也会根据 &lt;code&gt;hbase.regionserver.flushlogentries&lt;/code&gt; (默认100秒)定期调用Hlog.sync()。Sync() 本身调用HLog.Writer.sync()，它由SequenceFileLogWriter实现。&lt;/p&gt;

&lt;p&gt;LogRoller&lt;/p&gt;

&lt;p&gt;Log的大小通过$HBASE_HOME/conf/hbase-site.xml 的 &lt;code&gt;hbase.regionserver.logroll.period&lt;/code&gt; 限制，默认是一个小时。所以每60分钟，会打开一个新的log文件。久而久之，会有一大堆的文件需要维护。首先，LogRoller调用HLog.rollWriter()，定时滚动日志，之后，利用HLog.cleanOldLogs()可以清除旧的日志。它首先取得存储文件中的最大的sequence number，之后检查是否存在一个log所有的条目的“sequence number”均低于这个值，如果存在，将删除这个log。&lt;/p&gt;

&lt;p&gt;每个region server维护一个HLog，而不是每一个region一个，这样不同region（来自不同的table）的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数，因此可以提高table的写性能。带来麻烦的时，如果一个region server下线，为了恢复其上的region，需要讲region server上的log进行拆分，然后分发到其他region server上进行恢复。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2013/06/15/hbase-note-about-data-structure.html</link>
      <guid>http://blog.javachen.com/2013/06/15/hbase-note-about-data-structure.html</guid>
      <pubDate>2013-06-15T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Java笔记：单例模式</title>
      <description>&lt;p&gt;什么是单例模式呢？就是在整个系统中，只有一个唯一存在的实例。使用Singleton的好处还在于可以节省内存，因为它限制了实例的个数，有利于Java垃圾回收。&lt;/p&gt;

&lt;p&gt;单例模式主要有3个特点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1、单例类确保自己只有一个实例。&lt;/li&gt;
  &lt;li&gt;2、单例类必须自己创建自己的实例。&lt;/li&gt;
  &lt;li&gt;3、单例类必须为其他对象提供唯一的实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;单例模式的实现方式有五种方法：懒汉，恶汉，双重校验锁，枚举和静态内部类。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;懒汉模式：&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class Singleton {
    private static Singleton instance;
    private Singleton (){}

    public static synchronized Singleton getInstance() {
		if (instance == null) {
		    instance = new Singleton();
		}
		return instance;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种写法能够在多线程中很好的工作，而且看起来它也具备很好的&lt;code&gt;lazy loading&lt;/code&gt;，但是，遗憾的是，效率很低，99%情况下不需要同步。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;恶汉模式：&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class Singleton {
    private static Singleton instance = new Singleton();
    private Singleton (){}

    public static Singleton getInstance() {
		return instance;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式基于classloder机制避免了多线程的同步问题，但是没有达到&lt;code&gt;lazy loading&lt;/code&gt;的效果。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;双重检测：&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class Singleton {
    public static final Singleton singleton = null;
    private Singleton(){}
    public static Singleton getInstance(){
        if(singleton == null){ //如果singleton为空，表明未实例化
           synchronize (Singleton.class){
               if( singleton == null ) { // double check 进来判断后再实例化。
                   singleton = new Singleton();
               }
        }
        return singleton;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当两个线程执行完第一个 &lt;code&gt;singleton == null&lt;/code&gt; 后等待锁， 其中一个线程获得锁并进入synchronize后，实例化了，然后退出释放锁，另外一个线程获得锁，进入又想实例化，会判断是否进行实例化了，如果存在，就不进行实例化了。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;静态内部类（懒汉模式）&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//一个延迟实例化的内部类的单例模式
public final class Singleton {
 
    //一个内部类的容器，调用getInstance时，JVM加载这个类
    private static final class SingletonHolder {
        static final Singleton singleton =  new Singleton();
    }
 
    private Singleton() {}
 
    public static Singleton getInstance() {
        return SingletonHolder.singleton;
    }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先，其他类在引用这个Singleton的类时，只是新建了一个引用，并没有开辟一个的堆空间存放（对象所在的内存空间）。接着，当使用&lt;code&gt;Singleton.getInstance()&lt;/code&gt;方法后，Java虚拟机（JVM）会加载&lt;code&gt;SingletonHolder.class&lt;/code&gt;（JLS规定每个class对象只能被初始化一次），并实例化一个Singleton对象。&lt;/p&gt;

&lt;p&gt;缺点：需要在Java的另外一个内存空间（Java PermGen 永久代内存，这块内存是虚拟机加载class文件存放的位置）占用一个大块的空间。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;枚举&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public enum Singleton {  
    INSTANCE;  
    public void whateverMethod() {  
    }  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式是Effective Java作者Josh Bloch 提倡的方式，它不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象。&lt;/p&gt;

&lt;p&gt;通过这种方式，不能通过反射和序列化来获取一个实例，因为所有的枚举类都继承自java.lang.Enum类, 而不是Object类：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;protected final Object clone() throws CloneNotSupportedException {  
    throw new CloneNotSupportedException();  
} 

private void readObject(ObjectInputStream in) throws IOException,  
        ClassNotFoundException {  
            throw new InvalidObjectException(&quot;can&#39;t deserialize enum&quot;);  
}  
  
private void readObjectNoData() throws ObjectStreamException {  
        throw new InvalidObjectException(&quot;can&#39;t deserialize enum&quot;);  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他需要注意的事项：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不同的CLASSLOADER加载SINGLETON生成的实例不一样，这样，需要自己写classloader，保证Singleton.class的加载唯一。参考文章&lt;a href=&quot;http://www.oschina.net/question/9709_102019&quot;&gt;http://www.oschina.net/question/9709_102019&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;如果Singleton实现了&lt;code&gt;java.io.Serializable&lt;/code&gt;接口，使用反序列化可以生成一个新的对象，这样需要重写readResolve方法，返回静态的单个实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;单例模式在 Java 标准库中的使用：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;java.lang.Runtime#getRuntime()&lt;/code&gt;是 Java 标准库中常用的方法，它返回与当前Java应用关联的运行时对象。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2013/06/09/note-about-java-singleton-model.html</link>
      <guid>http://blog.javachen.com/2013/06/09/note-about-java-singleton-model.html</guid>
      <pubDate>2013-06-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Java笔记：IO</title>
      <description>&lt;p&gt;说明，本文内容来源于&lt;a href=&quot;http://www.cnblogs.com/skywang12345/p/io_01.html&quot;&gt;java io系列01之 “目录”&lt;/a&gt;，做了一些删减。&lt;/p&gt;

&lt;p&gt;Java库的IO分为输入/输出两部分。&lt;/p&gt;

&lt;p&gt;早期的Java 1.0版本的输入系统是InputStream及其子类，输出系统是OutputStream及其子类。&lt;/p&gt;

&lt;p&gt;后来的Java 1.1版本对IO系统进行了重新设计。输入系统是Reader及其子类，输出系统是Writer及其子类。&lt;/p&gt;

&lt;p&gt;Java1.1之所以要重新设计，主要是为了添加国际化支持(即添加了对16位Unicode码的支持)。具体表现为Java 1.0的IO系统是字节流，而Java 1.1的IO系统是字符流。&lt;/p&gt;

&lt;p&gt;字节流，就是数据流中最小的数据单元是8位的字节。&lt;/p&gt;

&lt;p&gt;字符流，就是数据流中最小的数据单元是16位的字符。&lt;/p&gt;

&lt;p&gt;字节流在操作的时候，不会用到缓冲；而字符流会用到缓冲。所以，字符流的效率会更高一些。&lt;/p&gt;

&lt;p&gt;至于为什么用到缓冲会效率更高一些呢？那是因为，缓冲本质上是一段内存区域；而文件大多是存储在硬盘或者Nand Flash上面。读写内存的速度比读写硬盘或Nand Flash上文件的速度快很多！&lt;/p&gt;

&lt;p&gt;目前，文件大多以字节的方式存储的。所以在开发中，字节流使用较为广泛。&lt;/p&gt;

&lt;h1 id=&quot;java-10java-11-io&quot;&gt;Java 1.0和Java 1.1 的IO类的比较&lt;/h1&gt;

&lt;p&gt;基本类对比表&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Java 1.0 IO基本类(字节流)&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Java 1.1 IO基本类(字符流)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;InputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Reader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OutputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Writer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FileInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FileReader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FileOutputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FileWriter&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;StringBufferInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;StringReader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;无&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;StringWriter&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ByteArrayInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CharArrayReader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ByteArrayOutputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CharArrayWriter&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PipedInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PipedReader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PipedOutputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PipedWriter&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;装饰器对比表&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Java 1.0 IO装饰器(字节流)&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Java 1.1 IO装饰器(字符流)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;FilterInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FilterReader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FilterOutputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FilterWriter（没有子类的抽象类&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BufferedInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;BufferedReader（也有 readLine()）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BufferedOutputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;BufferedWriter&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DataInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;无&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PrintStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PrintWriter&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LineNumberInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;LineNumberReader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;StreamTokenizer&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;无&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PushBackInputStream&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PushBackReader&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;io&quot;&gt;io框架&lt;/h1&gt;

&lt;p&gt;以字节为单位的输入流的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234201-95f7519c9a174cbbb8b3c6e0a076a56d.jpg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;是以字节为单位的输出流的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234231-929b2961bb604a05922c9a6ce1348110.jpg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以字节为单位的输入流和输出流关联的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234245-b708d62c6397495db7915d8fee6616f7.jpg&quot; alt=&quot;&quot; width=&quot;740&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以字符为单位的输入流的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234317-f9f030ae18904626b08b8d464e87eed1.jpg&quot; alt=&quot;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以字符为单位的输出流的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234330-d18eb674e6ba44beb6d65b05c602f065.jpg&quot; alt=&quot;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以字符为单位的输入流和输出流关联的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234410-c986ccb259594865ae75f14f19e1179f.jpg&quot; alt=&quot;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;字节转换为字符流的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234430-bb419718ff01462c8d94fc2ac3e1aeb6.jpg&quot; alt=&quot;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;字节和字符的输入流对应关系：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234451-97f7312056a642ccb58ca02a2803dbb4.jpg&quot; alt=&quot;&quot; width=&quot;740&quot; /&gt;&lt;/p&gt;

&lt;p&gt;字节和字符的输出流对应关系：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.cnitblog.com/blog/497634/201310/20234541-d488f6a75e524979acfe8a77ff14ec78.jpg&quot; alt=&quot;&quot; width=&quot;740&quot; /&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2013/06/09/note-about-java-io.html</link>
      <guid>http://blog.javachen.com/2013/06/09/note-about-java-io.html</guid>
      <pubDate>2013-06-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Java笔记：工厂模式</title>
      <description>&lt;p&gt;工厂模式主要是为创建对象提供接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。&lt;/p&gt;

&lt;p&gt;工厂模式在《Java与模式》中分为三类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1)简单工厂模式(Simple Factory)：不利于产生系列产品。&lt;/li&gt;
  &lt;li&gt;2)工厂方法模式(Factory Method)：又称为多形性工厂。&lt;/li&gt;
  &lt;li&gt;3)抽象工厂模式(Abstract Factory)：又称为工具箱，产生产品族，但不利于产生新的产品。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GOF在《设计模式》一书中将工厂模式分为两类：工厂方法模式(Factory Method)与抽象工厂模式(Abstract Factory)。将简单工厂模式(Simple Factory)看为工厂方法模式的一种特例，两者归为一类。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;简单工厂模式&lt;/h1&gt;

&lt;p&gt;在简单工厂模式中，可以根据自变量的不同返回不同类的实例。简单工厂模式专门定义一个类来负责创建其他类的实例，被创建的实例通常都具有共同的父类。&lt;/p&gt;

&lt;p&gt;简单工厂模式角色：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;工厂类&lt;/li&gt;
  &lt;li&gt;抽象产品&lt;/li&gt;
  &lt;li&gt;具体产品&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单工厂模式的优点如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;工厂类含有必要的判断逻辑，可以决定在什么时候创建哪一个产品类的实例，客户端可以免除直接创建产品对象的责任，而仅仅“消费”产品；简单工厂模式通过这种做法实现了对责任的分割，它提供了专门的工厂类用于创建对象。&lt;/li&gt;
  &lt;li&gt;客户端无需知道所创建的具体产品类的类名，只需要知道具体产品类所对应的参数即可，对于一些复杂的类名，通过简单工厂模式可以减少使用者的记忆量。&lt;/li&gt;
  &lt;li&gt;通过引入配置文件，可以在不修改任何客户端代码的情况下更换和增加新的具体产品类，在一定程度上提高了系统的灵活性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单工厂模式的缺点如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;工厂类中包括了创建产品类的业务逻辑，一旦工厂类不能正常工作，整个系统都要受到影响。&lt;/li&gt;
  &lt;li&gt;系统扩展困难，一旦添加新产品就需要修改工厂逻辑，在产品类型较多时，有可能造成工厂逻辑过于复杂，不利于系统的扩展和维护。&lt;/li&gt;
  &lt;li&gt;简单工厂模式由于使用了静态工厂方法，造成工厂角色无法形成基于继承的等级结构。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt; //抽象产品角色
public interface Car{
    public void drive();
}
//具体产品角色
public class Benz implements Car{
    public void drive() {
        System.out.println(&quot;Driving Benz &quot;);
    }
}
public class Bmw implements Car{
    public void drive() {
        System.out.println(&quot;Driving Bmw &quot;);
    }
}
//工厂类角色
public class CarFactory{
    //工厂方法.注意 返回类型为抽象产品角色
    public static Car create(String s)throws Exception{
        if(s.equalsIgnoreCase(&quot;Benz&quot;))
            return new Benz();
        else if(s.equalsIgnoreCase(&quot;Bmw&quot;))
            return new Bmw();
        else throw new Exception();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;工厂方法模式&lt;/h1&gt;

&lt;p&gt;在工厂方法模式中，核心的工厂类不再负责所有的产品的创建，而是将具体创建的工作交给子类去做。&lt;/p&gt;

&lt;p&gt;工厂方法模式角色:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;抽象工厂：是具体工厂角色必须实现的接口或者必须继承的父类。&lt;/li&gt;
  &lt;li&gt;具体工厂：它含有和具体业务逻辑有关的代码。由应用程序调用以创建对应的具体产品的对象。&lt;/li&gt;
  &lt;li&gt;抽象产品：它是具体产品继承的父类或者是实现的接口。&lt;/li&gt;
  &lt;li&gt;具体产品：具体工厂角色所创建的对象就是此角色的实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt; //抽象产品角色
public interface Car{
    public void drive();
}
//具体产品角色
public class Benz implements Car{
    public void drive() {
        System.out.println(&quot;Driving Benz &quot;);
    }
}
public class Bmw implements Car{
    public void drive() {
        System.out.println(&quot;Driving Bmw &quot;);
    }
}
//抽象工厂类角色
public abstract class CarFactory{
    public abstract Car create();
}

public class BenzCarFactory extends CarFactory{
    public Car create(){
        return new Benz();
    }
}

public class BmwCarFactory extends CarFactory{
    public Car create(){
        return new Bmw();
    }
}

//测试类
public class Test {
    public static void main(String[] args) {
        CarFactory factory = new BenzCarFactory();
        Car m = factory.create();
        m.drive();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;抽象工厂模式&lt;/h1&gt;

&lt;p&gt;抽象工厂模式提供一个创建一系列或相互依赖的对象的接口，而无需指定它们具体的类。它针对的是有多个产品的等级结构。而工厂方法模式针对的是一个产品的等级结构。&lt;/p&gt;

&lt;p&gt;举例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//抽象工厂类
public abstract class AbstractFactory {
    public abstract Vehicle createVehicle();
    public abstract Weapon createWeapon();
    public abstract Food createFood();
}
//具体工厂类，其中Food,Vehicle，Weapon是抽象类
public class DefaultFactory extends AbstractFactory{
    @Override
    public Food createFood() {
        return new Apple();
    }
    @Override
    public Vehicle createVehicle() {
        return new Car();
    }
    @Override
    public Weapon createWeapon() {
        return new AK47();
    }
}
//测试类
public class Test {
    public static void main(String[] args) {
        AbstractFactory f = new DefaultFactory();
        Vehicle v = f.createVehicle();
        Weapon w = f.createWeapon();
        Food a = f.createFood();
    }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2013/06/09/note-about-java-factory-model.html</link>
      <guid>http://blog.javachen.com/2013/06/09/note-about-java-factory-model.html</guid>
      <pubDate>2013-06-09T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Java笔记：多线程</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;一些概念&lt;/h1&gt;

&lt;p&gt;现在的操作系统是多任务操作系统。多线程是实现多任务的一种方式。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;进程&lt;/code&gt; 是指一个内存中运行的应用程序，每个进程都有自己独立的一块内存空间，一个进程中可以启动多个线程。比如在 Windows 系统中，一个运行的 exe 就是一个进程。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;线程&lt;/code&gt; 是指进程中的一个执行流程，一个进程中可以运行多个线程。比如 &lt;code&gt;java.exe&lt;/code&gt; 进程中可以运行很多线程。&lt;code&gt;线程总是属于某个进程，进程中的多个线程共享进程的内存&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;同时&lt;/code&gt; 执行是人的感觉，&lt;strong&gt;在线程之间实际上轮换执行&lt;/strong&gt;。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;多线程间堆空间共享，栈空间独立。堆存的是地址，栈存的是变量（如：局部变量）。这部分内容结合 &lt;a href=&quot;/2014/04/09/note-about-jvm-memery-model.html&quot;&gt;Java 内存模型&lt;/a&gt; 来理解。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;创建线程两种方式：&lt;code&gt;继承Thread类&lt;/code&gt; 或 &lt;code&gt;实现Runnable接口&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;Thread 对象代表一个线程，一个 Thread 类实例只是一个对象，像 Java 中的任何其他对象一样，具有变量和方法，生死于堆上。&lt;/p&gt;

&lt;p&gt;Java 中，每个线程都有一个调用栈，即使不在程序中创建任何新的线程，线程也在后台运行着。&lt;/p&gt;

&lt;p&gt;一个 Java 应用总是从 &lt;code&gt;main()&lt;/code&gt; 方法开始运行，&lt;code&gt;mian()&lt;/code&gt; 方法运行在一个线程内，它被称为主线程。&lt;/p&gt;

&lt;p&gt;一旦创建一个新的线程，就产生一个新的调用栈。&lt;/p&gt;

&lt;p&gt;多线程共同访问的同一个对象（临界资源），如果破坏了不可分割的操作（原子操作），就会造成数据不一致的情况。&lt;/p&gt;

&lt;p&gt;线程总体分两类：&lt;code&gt;用户线程&lt;/code&gt; 和&lt;code&gt;守候线程&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;当所有用户线程执行完毕的时候，JVM自动关闭。但是守候线程却不独立于JVM，守候线程一般是由操作系统或者用户自己创建的。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;线程状态图&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/thread-state.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;说明：&lt;br /&gt;
线程共包括以下5种状态。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;新建状态(New)&lt;/code&gt;： 线程对象被创建后，就进入了新建状态。例如，&lt;code&gt;Thread thread = new Thread()&lt;/code&gt;。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;就绪状态(Runnable)&lt;/code&gt;： 也被称为“可执行状态”。线程对象被创建后，其它线程调用了该对象的 &lt;code&gt;start()&lt;/code&gt; 方法，从而来启动该线程。例如，&lt;code&gt;thread.start()&lt;/code&gt;。运行中的线程调用 &lt;code&gt;yield()&lt;/code&gt; 之后也会进入就绪状态。处于就绪状态的线程，随时可能被 CPU 调度执行。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;运行状态(Running)&lt;/code&gt;： 线程获取CPU权限进行执行。需要注意的是，线程只能从就绪状态进入到运行状态。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;阻塞状态(Blocked)&lt;/code&gt;： 阻塞状态是线程因为某种原因放弃 CPU 使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种：
        &lt;ul&gt;
          &lt;li&gt;a) 等待阻塞 – 通过调用线程的 &lt;code&gt;wait()&lt;/code&gt; 方法，让线程等待某工作的完成。&lt;/li&gt;
          &lt;li&gt;b) 同步阻塞 – 线程在获取 synchronized 同步锁失败(因为锁被其它线程所占用)，它会进入同步阻塞状态。&lt;/li&gt;
          &lt;li&gt;b) 其他阻塞 – 通过调用线程的 &lt;code&gt;sleep()&lt;/code&gt; 或 &lt;code&gt;join()&lt;/code&gt; 或发出了 I/O 请求时，线程会进入到阻塞状态。当 &lt;code&gt;sleep()&lt;/code&gt; 状态超时、&lt;code&gt;join()&lt;/code&gt; 等待线程终止或者超时、或者 I/O 处理完毕时，线程重新转入就绪状态。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code&gt;死亡状态(Dead)&lt;/code&gt;：线程执行完了或者因异常退出了 &lt;code&gt;run()&lt;/code&gt; 方法，该线程结束生命周期。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;thread--runnable&quot;&gt;Thread 和 Runnable&lt;/h1&gt;

&lt;p&gt;Runnable 是一个接口，该接口中只包含了一个 &lt;code&gt;run()&lt;/code&gt; 方法。它的定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public interface Runnable {
    public abstract void run();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以定义一个类 A 实现 Runnable 接口；然后，通过 &lt;code&gt;new Thread(new A())&lt;/code&gt; 等方式新建线程。&lt;/p&gt;

&lt;p&gt;Thread 是一个类。Thread 本身就实现了 Runnable 接口。它的声明如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class Thread implements Runnable {
	public Thread() {}
	public Thread(Runnable target) {}
	public Thread(ThreadGroup group, Runnable target){}
	public Thread(String name){}
	public Thread(ThreadGroup group, String name){}
	public Thread(Runnable target, String name){}
	public Thread(ThreadGroup group, Runnable target, String name){}
	public Thread(ThreadGroup group, Runnable target, String name,long stackSize){}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;相同点&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;都是“多线程的实现方式”。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;不同点&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;Thread 是类，而 Runnable 是接口；Thread 本身是实现了 Runnable 接口的类。我们知道“一个类只能有一个父类，但是却能实现多个接口”，因此 Runnable 具有更好的扩展性。&lt;/p&gt;

&lt;p&gt;此外，Runnable 还可以用于“资源的共享”。即，&lt;strong&gt;多个线程都是基于某一个Runnable对象建立的&lt;/strong&gt;，它们会共享这个Runnable对象上的资源。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;创建和运行线程的两种方法：&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//测试Runnable类实现的多线程程序 
public class DoSomething implements Runnable { 
    private String name; 

    public DoSomething(String name) { 
        this.name = name; 
    } 

    public void run() { 
        for (int i = 0; i &amp;lt; 5; i++) { 
            for (long k = 0; k &amp;lt; 100000000; k++) ; 
            System.out.println(name + &quot;: &quot; + i); 
        } 
    } 
}

public class TestRunnable { 
    public static void main(String[] args) { 
        DoSomething ds1 = new DoSomething(&quot;javachen&quot;); 
        DoSomething ds2 = new DoSomething(&quot;blog&quot;); 

        Thread t1 = new Thread(ds1); 
        Thread t2 = new Thread(ds2); 

        t1.start(); 
        t2.start(); 
    } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//测试扩展Thread类实现的多线程程序 
public class TestThread extends Thread{ 
    public TestThread(String name) { 
        super(name); 
    } 

    public void run() { 
        for(int i = 0;i&amp;lt;5;i++){ 
            for(long k= 0; k &amp;lt;100000000;k++); 
            System.out.println(this.getName()+&quot; :&quot;+i); 
        } 
    } 

    public static void main(String[] args) { 
        Thread t1 = new TestThread(&quot;javachen&quot;); 
        Thread t2 = new TestThread(&quot;blog&quot;); 
        t1.start(); 
        t2.start(); 
    } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;start--run&quot;&gt;start() 和 run()&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;start()&lt;/code&gt;：它的作用是启动一个新线程，新线程会执行相应的 &lt;code&gt;run()&lt;/code&gt;方法。&lt;code&gt;start()&lt;/code&gt; 不能被重复调用。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;run()&lt;/code&gt;：和普通的成员方法一样，可以被重复调用。单独调用 &lt;code&gt;run()&lt;/code&gt; 的话，会在当前线程中执行 &lt;code&gt;run()&lt;/code&gt;，而并不会启动新线程！&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在调用 &lt;code&gt;start()&lt;/code&gt;方法之前，线程处于新状态中，新状态指有一个 Thread 对象，但还没有一个真正的线程。&lt;/p&gt;

&lt;p&gt;在调用 &lt;code&gt;start()&lt;/code&gt;方法之后，发生了一系列复杂的事情：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;启动新的执行线程（具有新的调用栈）；&lt;/li&gt;
  &lt;li&gt;该线程从新状态转移到可运行状态；&lt;/li&gt;
  &lt;li&gt;当该线程获得机会执行时，其目标 &lt;code&gt;run()&lt;/code&gt;方法将运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;wait-notify-notifyall&quot;&gt;wait(), notify(), notifyAll()&lt;/h2&gt;

&lt;p&gt;在 Object.java 中，定义了 &lt;code&gt;wait()&lt;/code&gt;, &lt;code&gt;notify()&lt;/code&gt; 和 &lt;code&gt;notifyAll()&lt;/code&gt; 等接口。&lt;code&gt;wait()&lt;/code&gt; 的作用是让&lt;strong&gt;当前线程&lt;/strong&gt;进入等待状态，同时，&lt;code&gt;wait()&lt;/code&gt; 也会让当前线程释放它所持有的锁。而 &lt;code&gt;notify()&lt;/code&gt; 和 &lt;code&gt;notifyAll()&lt;/code&gt; 的作用，则是唤醒当前对象上的等待线程；&lt;code&gt;notify()&lt;/code&gt; 是唤醒单个线程，而 &lt;code&gt;notifyAll()&lt;/code&gt;是唤醒所有的线程。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;notify()&lt;/code&gt;,&lt;code&gt; wait()&lt;/code&gt; 依赖于“同步锁”，而“同步锁”是对象锁持有，并且每个对象有且仅有一个！&lt;/p&gt;

&lt;p&gt;在 java 中，任何对象都有一个锁池，用来存放等待该对象锁标记的线程，线程阻塞在对象锁池中时，不会释放其所拥有的其它对象的锁标记。&lt;/p&gt;

&lt;p&gt;在 java 中，任何对象都有一个等待队列，用来存放线程，线程 t1对（让）o调用 wait 方法,必须放在对 o 加锁的同步代码块中!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;t1 会释放其所拥有的所有锁标记;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;t1会进入 o 的等待队列&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;t2 对（让）o调用 notify/notifyAll 方法,也必须放在对 o 加锁的同步代码块中! 会从 o 的等待队列中释放一个/全部线程，对 t2 毫无影响，t2 继续执行。&lt;/p&gt;

&lt;h2 id=&quot;yield&quot;&gt;yield()&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Thread.yield()&lt;/code&gt; 方法作用是：暂停当前正在执行的线程对象，并执行其他线程。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;yield()&lt;/code&gt; 应该做的是让当前运行线程回到可运行状态，以允许具有相同优先级的其他线程获得运行机会。因此，使用 &lt;code&gt;yield()&lt;/code&gt; 的目的是让相同优先级的线程之间能适当的轮转执行。&lt;/p&gt;

&lt;p&gt;但是，实际中无法保证 &lt;code&gt;yield()&lt;/code&gt; 达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。&lt;/p&gt;

&lt;p&gt;结论：&lt;code&gt;yield()&lt;/code&gt;从未导致线程转到等待/睡眠/阻塞状态。在大多数情况下，&lt;code&gt;yield()&lt;/code&gt; 将导致线程从运行状态转到可运行状态，但有可能没有效果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;wait()是会线程释放它所持有对象的同步锁，而yield()方法不会释放锁。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;sleep&quot;&gt;sleep()&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;sleep()&lt;/code&gt; 的作用是让当前线程休眠，即当前线程会从 &lt;code&gt;运行状态&lt;/code&gt; 进入到 &lt;code&gt;休眠(阻塞)状态&lt;/code&gt; 。&lt;code&gt;sleep()&lt;/code&gt; 会指定休眠时间，线程休眠的时间会大于/等于该休眠时间；在线程重新被唤醒时，它会由 &lt;code&gt;阻塞状态&lt;/code&gt; 变成 &lt;code&gt;就绪状态&lt;/code&gt;，从而等待 cpu 的调度执行。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;wait()会释放对象的同步锁，而sleep()则不会释放锁。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;join&quot;&gt;join()&lt;/h2&gt;

&lt;p&gt;Thread的非静态方法 &lt;code&gt;join()&lt;/code&gt; 让一个线程 B “加入” 到另外一个线程 A 的尾部。在 A 执行完毕之前，B 不能工作。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Thread t = new MyThread();
t.start();
t.join();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，&lt;code&gt;join()&lt;/code&gt; 方法还有带超时限制的重载版本。 例如 &lt;code&gt;t.join(5000);&lt;/code&gt; 让线程等待5000毫秒，如果超过这个时间，则停止等待，变为可运行状态。&lt;/p&gt;

&lt;h2 id=&quot;interrupt&quot;&gt;interrupt()&lt;/h2&gt;

&lt;p&gt;interrupt()的作用是中断本线程。&lt;/p&gt;

&lt;p&gt;本线程中断自己是被允许的；其它线程调用本线程的 interrupt() 方法时，会通过 checkAccess() 检查权限。这有可能抛出 SecurityException 异常。&lt;/p&gt;

&lt;p&gt;如果本线程是处于阻塞状态：调用线程的wait(), wait(long)或 wait(long, int)会让它进入等待(阻塞)状态，或者调用线程的join(), join(long), join(long, int), sleep(long), sleep(long, int) 也会让它进入阻塞状态。若线程在阻塞状态时，调用了它的 &lt;code&gt;interrupt()&lt;/code&gt;方法，那么它的“中断状态”会被清除并且会收到一个 InterruptedException 异常。&lt;/p&gt;

&lt;p&gt;例如，线程通过 wait() 进入阻塞状态，此时通过 interrupt() 中断该线程；调用 interrupt() 会立即将线程的中断标记设为“true”，但是由于线程处于阻塞状态，所以该“中断标记”会立即被清除为“false”，同时，会产生一个 InterruptedException 的异常。&lt;/p&gt;

&lt;p&gt;如果线程被阻塞在一个 Selector 选择器中，那么通过 interrupt() 中断它时；线程的中断标记会被设置为 true，并且它会立即从选择操作中返回。&lt;/p&gt;

&lt;p&gt;如果不属于前面所说的情况，那么通过 interrupt() 中断线程时，它的中断标记会被设置为“true”。中断一个“已终止的线程”不会产生任何操作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;interrupt()常常被用来终止“阻塞状态”线程。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interrupted() 和 isInterrupted()都能够用于检测对象的“中断标记”。&lt;/p&gt;

&lt;p&gt;区别是，interrupted()除了返回中断标记之外，它还会清除中断标记(即将中断标记设为false)；而isInterrupted()仅仅返回中断标记。&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;线程的同步与锁&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;线程的同步&lt;/strong&gt; 是为了防止多个线程访问一个数据对象时，对数据造成的破坏。&lt;/p&gt;

&lt;p&gt;Java中每个对象都有一个内置锁。当程序运行到非静态的 synchronized 同步方法上时，自动获得与正在执行代码类的当前实例有关的锁。获得一个对象的锁也称为获取锁、锁定对象、在对象上锁定或在对象上同步。&lt;/p&gt;

&lt;p&gt;当程序运行到 synchronized 同步方法或代码块时才该对象锁才起作用。&lt;/p&gt;

&lt;p&gt;一个对象只有一个锁。所以，如果一个线程获得该锁，就没有其他线程可以获得锁，直到第一个线程释放（或返回）锁。这也意味着任何其他线程都不能进入该对象上的 synchronized 方法或代码块，直到该锁被释放。&lt;code&gt;释放锁&lt;/code&gt; 是指持锁线程退出了synchronized同步方法或代码块。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;关于锁和同步，有一下几个要点&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）、只能同步方法，而不能同步变量和类；&lt;/li&gt;
  &lt;li&gt;2）、每个对象只有一个锁；当提到同步时，应该清楚在什么上同步？也就是说，在哪个对象上同步？&lt;/li&gt;
  &lt;li&gt;3）、不必同步类中所有的方法，类可以同时拥有同步和非同步方法。&lt;/li&gt;
  &lt;li&gt;4）、如果两个线程要执行一个类中的 synchronized 方法，并且两个线程使用相同的实例来调用方法，那么一次只能有一个线程能够执行方法，另一个需要等待，直到锁被释放。也就是说：如果一个线程在对象上获得一个锁，就没有任何其他线程可以进入（该对象的）类中的任何一个同步方法。&lt;/li&gt;
  &lt;li&gt;5）、如果线程拥有同步和非同步方法，则非同步方法可以被多个线程自由访问而不受锁的限制。&lt;/li&gt;
  &lt;li&gt;6）、线程睡眠时，它所持的任何锁都不会释放。&lt;/li&gt;
  &lt;li&gt;7）、线程可以获得多个锁。比如，在一个对象的同步方法里面调用另外一个对象的同步方法，则获取了两个对象的同步锁。&lt;/li&gt;
  &lt;li&gt;8）、同步损害并发性，应该尽可能缩小同步范围。同步不但可以同步整个方法，还可以同步方法中一部分代码块。&lt;/li&gt;
  &lt;li&gt;9）、在使用同步代码块时候，应该指定在哪个对象上同步，也就是说要获取哪个对象的锁。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;//对方法同步
public synchronized int getX() {
    return x++;
}

//对代码块同步
public int getX() {
    synchronized (this) {
        return x;
    }
}    

//对静态方法同步
public static synchronized int setName(String name){
    Xxx.name = name;
}

//对静态方法中的代码块同步
public static int setName(String name){
    synchronized(Xxx.class){
        Xxx.name = name;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://lavasoft.blog.51cto.com/62575/27069&quot;&gt;Java多线程编程总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2013/06/08/note-about-java-thread.html</link>
      <guid>http://blog.javachen.com/2013/06/08/note-about-java-thread.html</guid>
      <pubDate>2013-06-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Java笔记：集合框架实现原理</title>
      <description>&lt;blockquote&gt;
  &lt;p&gt;这篇文章是对&lt;a href=&quot;http://www.cnblogs.com/skywang12345/category/455711.html&quot;&gt;http://www.cnblogs.com/skywang12345/category/455711.html&lt;/a&gt;中java集合框架相关文章的一个总结，在此对&lt;a href=&quot;http://www.cnblogs.com/skywang12345&quot;&gt;原作者&lt;/a&gt;的辛勤整理表示感谢。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Java集合是java提供的工具包，包含了常用的数据结构：集合、链表、队列、栈、数组、映射等。Java集合工具包位置是&lt;code&gt;java.util.*&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Java集合主要可以划分为4个部分：List列表、Set集合、Map映射、工具类(Iterator迭代器、Enumeration枚举类、Arrays和Collections)。&lt;/p&gt;

&lt;p&gt;Java集合工具包框架图(如下)：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://images.cnitblog.com/blog/497634/201309/08171028-a5e372741b18431591bb577b1e1c95e6.jpg&quot;&gt;&lt;img style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; src=&quot;http://images.cnitblog.com/blog/497634/201309/08171028-a5e372741b18431591bb577b1e1c95e6.jpg&quot; alt=&quot;&quot; width=&quot;900&quot; height=&quot;360&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Collection是一个接口，是以对象为单位来管理元素的。有两个子接口：List接口和Set接口：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;List接口：元素是有顺序（下标），可以重复。有点像数组，可以用迭代器（Iterator）和数组遍历集合。List的实现类有LinkedList、ArrayList、Vector、Stack。&lt;/li&gt;
  &lt;li&gt;Set接口：无顺序，不可以重复（内容不重复，而非地址不重复）。只能用迭代器（Iterator）遍历集合。Set接口里本身无方法，方法都是从父接口Collection里继承的。Set的实现类有HastSet和TreeSet。HashSet依赖于HashMap，它实际上是通过HashMap实现的；TreeSet依赖于TreeMap，它实际上是通过TreeMap实现的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Map:对应一个键对象和一个值对象，可以根据key找value。AbstractMap是个抽象类，它实现了Map接口中的大部分API。而HashMap，TreeMap，WeakHashMap都是继承于AbstractMap。Hashtable虽然继承于Dictionary，但它实现了Map接口。&lt;/p&gt;

&lt;p&gt;Iterator用于遍历集合，Enumeration是JDK 1.0引入的抽象类，只能在Hashtable、Vector、Stack中使用。&lt;/p&gt;

&lt;p&gt;Arrays和Collections是两个操作数组和集合的工具类，提供一些静态方法。&lt;/p&gt;

&lt;h1 id=&quot;list&quot;&gt;1. List&lt;/h1&gt;
&lt;p&gt;## 1.1 ArrayList&lt;/p&gt;

&lt;p&gt;ArrayList 是一个数组列表，相当于动态数组。与Java中的数组相比，它的容量能动态增长。它继承于AbstractList，实现了List、RandomAccess、Cloneable、java.io.Serializable这些接口。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;通过数组保存数据，数组初始大小为10，也可以通过构造函数修改数组初始大小。&lt;/p&gt;

&lt;p&gt;当ArrayList容量不足以容纳全部元素时，ArrayList会重新设置容量：新的容量=“(原始容量x3)/2 + 1”。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;访问数据&lt;/h3&gt;

&lt;p&gt;当添加和删除元素时，需要移动数组中得元素；当获取元素时，只需要通过数组的索引返回元素，故&lt;code&gt;查询快，增删慢&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;ArrayList中的方法没有加同步锁，故&lt;code&gt;线程不安全&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;遍历方式&lt;/h3&gt;

&lt;p&gt;1）通过Iterator去遍历&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Integer value = null;
Iterator iter = list.iterator();
while (iter.hasNext()) {
    value = (Integer)iter.next();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）随机访问，通过索引值去遍历&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Integer value = null;
int size = list.size();
for(int i=0; i&amp;lt;size; i++){
 value = (Integer)list.get(i);        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3）for循环遍历&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Integer value = null;
for (Integer integ:list) {
    value = integ;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;性能比较：&lt;/p&gt;

&lt;h2 id=&quot;vector&quot;&gt;1.2 Vector&lt;/h2&gt;

&lt;p&gt;Vector 是矢量列表，它是JDK1.0版本添加的类。继承于AbstractList，实现了List, RandomAccess, Cloneable这些接口。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;通过数组保存数据，数组初始大小为10，容量增加系数为0，也可以通过构造函数修改数组初始大小。&lt;/p&gt;

&lt;p&gt;当容量不足以容纳全部元素时，若容量增加系数大于0，则将容量的值增加“容量增加系数”；否则，将容量大小增加一倍。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;访问数据&lt;/h3&gt;

&lt;p&gt;当添加和删除元素时，需要移动数组中得元素；当获取元素时，只需要通过数组的索引返回元素，故&lt;code&gt;查询快，增删慢&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;方法没有加同步锁，故&lt;code&gt;线程不安全&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;遍历方式&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;1）通过Iterator去遍历&lt;/li&gt;
  &lt;li&gt;2）随机访问，通过索引值去遍历&lt;/li&gt;
  &lt;li&gt;3）foreach循环遍历&lt;/li&gt;
  &lt;li&gt;4）Enumeration遍历&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;性能比较：&lt;/p&gt;

&lt;h2 id=&quot;stack&quot;&gt;1.3 Stack&lt;/h2&gt;

&lt;p&gt;Stack是栈。它的特性是：先进后出(FILO, First In Last Out)。&lt;/p&gt;

&lt;p&gt;java工具包中的Stack是继承于Vector的，由于Vector是通过数组实现的，这就意味着，Stack也是通过数组实现的，而非链表。&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;和 Vector 一致，Stack的push、peek、和pull方法都是对数组末尾的元素进行操作。&lt;/p&gt;

&lt;h2 id=&quot;linkedlist&quot;&gt;1.4 LinkedList&lt;/h2&gt;

&lt;p&gt;LinkedList 是一个继承于AbstractSequentialList的双向链表。它也可以被当作堆栈、队列或双端队列进行操作。它实现了List、Deque、Cloneable、java.io.Serializable接口。&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;LinkedList的本质是双向链表。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LinkedList继承于AbstractSequentialList，并且实现了Dequeue接口。&lt;/li&gt;
  &lt;li&gt;LinkedList包含两个重要的成员：header 和 size。&lt;/li&gt;
  &lt;li&gt;header是双向链表的表头，它是双向链表节点所对应的类Entry的实例。Entry中包含成员变量： previous, next, element。其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值。&lt;/li&gt;
  &lt;li&gt;size是双向链表中节点的个数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LinkedList不存在容量不足的问题。&lt;/p&gt;

&lt;h3 id=&quot;section-8&quot;&gt;访问数据&lt;/h3&gt;

&lt;p&gt;当添加和删除元素时，只需要将新的元素插入指定位置之前，然后修改链表的指向；当获取元素时（&lt;code&gt;get(int location)&lt;/code&gt;），会比较“location”和“双向链表长度的1/2”；若前者大，则从链表头开始往后查找，直到location位置；否则，从链表末尾开始先前查找，直到location位置。故&lt;code&gt;查询慢，增删快&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;方法没有加同步锁，故&lt;code&gt;线程不安全&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;section-9&quot;&gt;遍历方式&lt;/h3&gt;

&lt;p&gt;LinkedList支持多种遍历方式。建议不要采用随机访问的方式去遍历LinkedList，而采用foreach逐个遍历的方式。&lt;/p&gt;

&lt;h2 id=&quot;section-10&quot;&gt;1.5 总结&lt;/h2&gt;

&lt;h3 id=&quot;section-11&quot;&gt;应用场景&lt;/h3&gt;

&lt;p&gt;如果涉及到“栈”、“队列”、“链表”等操作，应该考虑用List，具体的选择哪个List，根据下面的标准来取舍。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于需要快速插入，删除元素，应该使用LinkedList。&lt;/li&gt;
  &lt;li&gt;对于需要快速随机访问元素，应该使用ArrayList。&lt;/li&gt;
  &lt;li&gt;对于“单线程环境” 或者 “多线程环境，但List仅仅只会被单个线程操作”，此时应该使用非同步的类(如ArrayList)；对于“多线程环境，且List可能同时被多个线程操作”，此时，应该使用同步的类(如Vector)。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vectorarraylist&quot;&gt;Vector和ArrayList&lt;/h3&gt;

&lt;p&gt;相同点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;都继承于AbstractList，并且实现List接口&lt;/li&gt;
  &lt;li&gt;都实现了RandomAccess和Cloneable接口&lt;/li&gt;
  &lt;li&gt;都是通过数组实现的，本质上都是动态数组，默认数组容量是10&lt;/li&gt;
  &lt;li&gt;都支持Iterator和listIterator遍历&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不同点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ArrayList是非线程安全，而Vector是线程安全的&lt;/li&gt;
  &lt;li&gt;ArrayList支持序列化，而Vector不支持&lt;/li&gt;
  &lt;li&gt;容量增加方式不同，Vector默认增长为原来一培，而ArrayList却是原来的一半+1&lt;/li&gt;
  &lt;li&gt;Vector支持通过Enumeration去遍历，而List不支持&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;map&quot;&gt;2. Map&lt;/h1&gt;

&lt;p&gt;Map 是映射接口，Map中存储的内容是键值对(key-value)。&lt;/p&gt;

&lt;p&gt;AbstractMap 是继承于Map的抽象类，它实现了Map中的大部分API。其它Map的实现类可以通过继承AbstractMap来减少重复编码。&lt;/p&gt;

&lt;p&gt;SortedMap 是继承于Map的接口。SortedMap中的内容是排序的键值对，排序的方法是通过比较器(Comparator)。&lt;/p&gt;

&lt;p&gt;NavigableMap 是继承于SortedMap的接口。相比于SortedMap，NavigableMap有一系列的导航方法；如”获取大于/等于某对象的键值对”、“获取小于/等于某对象的键值对”等等。&lt;/p&gt;

&lt;p&gt;TreeMap 继承于AbstractMap，且实现了NavigableMap接口；因此，TreeMap中的内容是“有序的键值对”&lt;/p&gt;

&lt;p&gt;HashMap 继承于AbstractMap，但没实现NavigableMap接口；因此，HashMap的内容是“键值对，但不保证次序”&lt;/p&gt;

&lt;p&gt;Hashtable 虽然不是继承于AbstractMap，但它继承于Dictionary(Dictionary也是键值对的接口)，而且也实现Map接口；因此，Hashtable的内容也是“键值对，也不保证次序”。但和HashMap相比，Hashtable是线程安全的，而且它支持通过Enumeration去遍历。&lt;/p&gt;

&lt;p&gt;WeakHashMap 继承于AbstractMap。它和HashMap的键类型不同，WeakHashMap的键是“弱键”。&lt;/p&gt;

&lt;h2 id=&quot;hashmap&quot;&gt;2.1 HashMap&lt;/h2&gt;

&lt;p&gt;HashMap 是一个散列表，它存储的内容是键值对(key-value)映射。&lt;/p&gt;

&lt;p&gt;HashMap 继承于AbstractMap，实现了Map、Cloneable、java.io.Serializable接口。&lt;/p&gt;

&lt;p&gt;HashMap 的实现不是同步的，这意味着它不是线程安全的。它的key、value都可以为null。此外，HashMap中的映射不是有序的。&lt;/p&gt;

&lt;h3 id=&quot;section-12&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;HashMap是通过”拉链法”实现的哈希表。它包括几个重要的成员变量：table、size、threshold、loadFactor、modCount。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;table是一个Entry[]数组类型，而Entry实际上就是一个单向链表。哈希表的”key-value键值对”都是存储在Entry数组中的。&lt;/li&gt;
  &lt;li&gt;size是HashMap的大小，它是HashMap保存的键值对的数量。&lt;/li&gt;
  &lt;li&gt;threshold是HashMap的阈值，用于判断是否需要调整HashMap的容量。threshold的值=”容量*加载因子”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。&lt;br /&gt;
-loadFactor就是加载因子。&lt;/li&gt;
  &lt;li&gt;modCount是用来实现fail-fast机制的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HashMap中的key-value都是存储在 Entry ç数组中的，而 Entry 实际上就是一个单向链表。&lt;/p&gt;

&lt;p&gt;影响HashMap性能的有两个参数：初始容量(initialCapacity) 和加载因子(loadFactor)。&lt;/p&gt;

&lt;p&gt;容量是哈希表中桶的数量，初始容量只是哈希表在创建时的容量，默认值为16。&lt;/p&gt;

&lt;p&gt;加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度。当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 rehash 操作（即重建内部数据结构），从而哈希表将具有大约两倍的桶数。&lt;/p&gt;

&lt;p&gt;通常，默认加载因子是 0.75, 这是在时间和空间成本上寻求一种折衷。加载因子过高虽然减少了空间开销，但同时也增加了查询成本（在大多数 HashMap 类的操作中，包括 get 和 put 操作，都反映了这一点）。在设置初始容量时应该考虑到映射中所需的条目数及其加载因子，以便最大限度地减少 rehash 操作次数。如果初始容量大于最大条目数除以加载因子，则不会发生 rehash 操作。&lt;/p&gt;

&lt;h3 id=&quot;section-13&quot;&gt;访问数据&lt;/h3&gt;

&lt;p&gt;当get（&lt;code&gt;get(Object key)&lt;/code&gt;）数据时，先通过key得到一个hash值，然后调用&lt;code&gt;indexFor(hash, table.length)&lt;/code&gt;方法得到table数组中得索引值，其次再通过索引得到数组中得 Entry 对象，最后遍历 Entry 对象，判断key和hash是否相等，如果相等则返回该Entry的value属性值。&lt;/p&gt;

&lt;p&gt;当put（&lt;code&gt;put(K key, V value)&lt;/code&gt;）数据时，若“key为null”，则将该键值对添加到table[0]中；若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中； 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出；若“该key”对应的键值对不存在，则将“key-value”添加到table中&lt;/p&gt;

&lt;h3 id=&quot;section-14&quot;&gt;遍历方式&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;遍历HashMap的键值对&lt;/li&gt;
  &lt;li&gt;遍历HashMap的键&lt;/li&gt;
  &lt;li&gt;遍历HashMap的值&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hashtable&quot;&gt;2.2 Hashtable&lt;/h2&gt;

&lt;p&gt;和HashMap一样，Hashtable 也是一个散列表，它存储的内容是键值对(key-value)映射。&lt;br /&gt;
Hashtable 继承于Dictionary，实现了Map、Cloneable、java.io.Serializable接口。&lt;br /&gt;
Hashtable 的函数都是同步的，这意味着它是线程安全的。它的key、value都不可以为null。此外，Hashtable中的映射不是有序的。&lt;/p&gt;

&lt;p&gt;Hashtable 的实例有两个参数影响其性能：初始容量 和 加载因子。&lt;/p&gt;

&lt;p&gt;容量是哈希表中桶 的数量，初始容量就是哈希表创建时的容量，默认值为11。、&lt;/p&gt;

&lt;p&gt;加载因子 是对哈希表在其容量自动增加之前可以达到多满的一个尺度。初始容量和加载因子这两个参数只是对该实现的提示。关于何时以及是否调用 rehash 方法的具体细节则依赖于该实现。&lt;/p&gt;

&lt;p&gt;通常，默认加载因子是 0.75,这是在时间和空间成本上寻求一种折衷。加载因子过高虽然减少了空间开销，但同时也增加了查找某个条目的时间（在大多数 Hashtable 操作中，包括 get 和 put 操作，都反映了这一点）。&lt;/p&gt;

&lt;h3 id=&quot;section-15&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;和HashMap一样，Hashtable也是一个散列表，它也是通过“拉链法”解决哈希冲突的。&lt;/p&gt;

&lt;p&gt;Hashtable里的方法是同步的，故其是线程安全的。&lt;/p&gt;

&lt;h2 id=&quot;treemap&quot;&gt;2.3 TreeMap&lt;/h2&gt;

&lt;p&gt;TreeMap 是一个有序的key-value集合，基于红黑树（Red-Black tree）实现。该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。&lt;/p&gt;

&lt;p&gt;TreeMap的基本操作 containsKey、get、put 和 remove 的时间复杂度是 log(n) 。&lt;/p&gt;

&lt;p&gt;另外，TreeMap是非同步的。 它的iterator 方法返回的迭代器是fail-fastl的。&lt;/p&gt;

&lt;h3 id=&quot;section-16&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;TreeMap的本质是R-B Tree(红黑树)，它包含几个重要的成员变量：root、size、comparator。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;root 是红黑数的根节点。它是Entry类型，Entry是红黑数的节点，它包含了红黑数的6个基本组成成分：key(键)、value(值)、left(左孩子)、right(右孩子)、parent(父节点)、color(颜色)。Entry节点根据key进行排序，Entry节点包含的内容为value。&lt;/li&gt;
  &lt;li&gt;红黑数排序时，根据Entry中的key进行排序；Entry中的key比较大小是根据比较器comparator来进行判断的。&lt;/li&gt;
  &lt;li&gt;size是红黑数中节点的个数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要彻底理解TreeMap，需要先理解红黑树。参考文章：&lt;a href=&quot;http://www.cnblogs.com/skywang12345/p/3245399.html&quot;&gt;红黑树(一)之 原理和算法详细介绍&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-17&quot;&gt;遍历方式&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;遍历HashMap的键值对&lt;/li&gt;
  &lt;li&gt;遍历HashMap的键&lt;/li&gt;
  &lt;li&gt;遍历HashMap的值&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;weakhashmap&quot;&gt;2.4 WeakHashMap&lt;/h2&gt;

&lt;p&gt;WeakHashMap 继承于AbstractMap，实现了Map接口。和HashMap一样，WeakHashMap 也是一个散列表，它存储的内容也是键值对(key-value)映射，而且键和值都可以是null。&lt;/p&gt;

&lt;p&gt;不过WeakHashMap的键是“弱键”。在 WeakHashMap 中，当某个键不再正常使用时，会被从WeakHashMap中被自动移除。更精确地说，对于一个给定的键，其映射的存在并不阻止垃圾回收器对该键的丢弃，这就使该键成为可终止的，被终止，然后被回收。某个键被终止时，它对应的键值对也就从映射中有效地移除了。&lt;/p&gt;

&lt;p&gt;这个“弱键”的原理呢？大致上就是，通过WeakReference和ReferenceQueue实现的。 WeakHashMap的key是“弱键”，即是WeakReference类型的；ReferenceQueue是一个队列，它会保存被GC回收的“弱键”。实现步骤是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(01) 新建WeakHashMap，将“键值对”添加到WeakHashMap中。实际上，WeakHashMap是通过数组table保存Entry(键值对)；每一个Entry实际上是一个单向链表，即Entry是键值对链表。&lt;/li&gt;
  &lt;li&gt;(02) 当某“弱键”不再被其它对象引用，并被GC回收时。在GC回收该“弱键”时，这个“弱键”也同时会被添加到ReferenceQueue(queue)队列中。&lt;/li&gt;
  &lt;li&gt;(03) 当下一次我们需要操作WeakHashMap时，会先同步table和queue。table中保存了全部的键值对，而queue中保存被GC回收的键值对；同步它们，就是删除table中被GC回收的键值对。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这就是“弱键”如何被自动从WeakHashMap中删除的步骤了。&lt;/p&gt;

&lt;h3 id=&quot;section-18&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;WeakHashMap和HashMap都是通过”拉链法“实现的散列表。它们的源码绝大部分内容都一样，这里就只是对它们不同的部分就是说明。&lt;/p&gt;

&lt;p&gt;WeakReference是“弱键”实现的哈希表。它这个“弱键”的目的就是：实现对“键值对”的动态回收。当“弱键”不再被使用到时，GC会回收它，WeakReference也会将“弱键”对应的键值对删除。&lt;/p&gt;

&lt;p&gt;“弱键”是一个“弱引用(WeakReference)”，在Java中，WeakReference和ReferenceQueue 是联合使用的。在WeakHashMap中亦是如此：如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 接着，WeakHashMap会根据“引用队列”，来删除“WeakHashMap中已被GC回收的‘弱键’对应的键值对”。&lt;/p&gt;

&lt;h2 id=&quot;section-19&quot;&gt;2.5 总结&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;HashMap 是基于“拉链法”实现的散列表。一般用于单线程程序中。&lt;/li&gt;
  &lt;li&gt;Hashtable 也是基于“拉链法”实现的散列表。它一般用于多线程程序中。&lt;/li&gt;
  &lt;li&gt;WeakHashMap 也是基于“拉链法”实现的散列表，它一般也用于单线程程序中。相比HashMap，WeakHashMap中的键是“弱键”，当“弱键”被GC回收时，它对应的键值对也会被从WeakHashMap中删除；而HashMap中的键是强键。&lt;/li&gt;
  &lt;li&gt;TreeMap 是有序的散列表，它是通过红黑树实现的。它一般用于单线程中存储有序的映射。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hashmaphashtable&quot;&gt;HashMap和Hashtable异同&lt;/h3&gt;

&lt;p&gt;相同点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;都是存储“键值对(key-value)”的散列表，而且都是采用拉链法实现的。存储的思想都是：通过table数组存储，数组的每一个元素都是一个Entry；而一个Entry就是一个单向链表，Entry链表中的每一个节点就保存了key-value键值对数据。&lt;/li&gt;
  &lt;li&gt;添加key-value键值对：首先，根据key值计算出哈希值，再计算出数组索引(即，该key-value在table中的索引)。然后，根据数组索引找到Entry(即，单向链表)，再遍历单向链表，将key和链表中的每一个节点的key进行对比。若key已经存在Entry链表中，则用该value值取代旧的value值；若key不存在Entry链表中，则新建一个key-value节点，并将该节点插入Entry链表的表头位置。&lt;/li&gt;
  &lt;li&gt;删除key-value键值对：删除键值对，相比于“添加键值对”来说，简单很多。首先，还是根据key计算出哈希值，再计算出数组索引(即，该key-value在table中的索引)。然后，根据索引找出Entry(即，单向链表)。若节点key-value存在与链表Entry中，则删除链表中的节点即可。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不同点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;继承和实现方式不同。HashMap 继承于AbstractMap，实现了Map、Cloneable、java.io.Serializable接口；Hashtable 继承于Dictionary，实现了Map、Cloneable、java.io.Serializable接口。&lt;/li&gt;
  &lt;li&gt;线程安全不同。Hashtable的几乎所有函数都是同步的，即它是线程安全的，支持多线程，而HashMap的函数则是非同步的，它不是线程安全的。&lt;/li&gt;
  &lt;li&gt;对null值的处理不同。HashMap的key、value都可以为null，Hashtable的key、value都不可以为null。&lt;/li&gt;
  &lt;li&gt;支持的遍历方式不同。HashMap只支持Iterator(迭代器)遍历，而Hashtable支持Iterator(迭代器)和Enumeration(枚举器)两种方式遍历。&lt;/li&gt;
  &lt;li&gt;通过Iterator迭代器遍历时，遍历的顺序不同。HashMap是“从前向后”的遍历数组；再对数组具体某一项对应的链表，从表头开始进行遍历，Hashtabl是“从后往前”的遍历数组；再对数组具体某一项对应的链表，从表头开始进行遍历。&lt;/li&gt;
  &lt;li&gt;容量的初始值 和 增加方式都不一样。HashMap默认的容量大小是16；增加容量时，每次将容量变为“原始容量x2”，Hashtable默认的容量大小是11；增加容量时，每次将容量变为“原始容量x2 + 1”。&lt;/li&gt;
  &lt;li&gt;添加key-value时的hash值算法不同。HashMap添加元素时，是使用自定义的哈希算法，Hashtable没有自定义哈希算法，而直接采用的key的hashCode()。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;set&quot;&gt;3. Set&lt;/h1&gt;

&lt;h2 id=&quot;hashset&quot;&gt;3.1 HashSet&lt;/h2&gt;

&lt;p&gt;HashSet 是一个没有重复元素的集合。它是由HashMap实现的，不保证元素的顺序，而且HashSet允许使用 null 元素。HashSet是非同步的。&lt;br /&gt;
HashSet通过iterator()返回的迭代器是fail-fast的。&lt;/p&gt;

&lt;h3 id=&quot;section-20&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;HashSet中含有一个HashMap类型的成员变量map，HashSet的操作函数，实际上都是通过map实现的。&lt;/p&gt;

&lt;h2 id=&quot;treeset&quot;&gt;3.2 TreeSet&lt;/h2&gt;

&lt;p&gt;TreeSet 是一个有序的集合，它的作用是提供有序的Set集合。它继承于AbstractSet抽象类，实现了NavigableSet&lt;e&gt;, Cloneable, java.io.Serializable接口。&lt;/e&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-21&quot;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;TreeSet的本质是一个”有序的，并且没有重复元素”的集合，它是通过TreeMap实现的。TreeSet中含有一个”NavigableMap类型的成员变量”m，而m实际上是”TreeMap的实例”。&lt;/p&gt;

&lt;h1 id=&quot;section-22&quot;&gt;4. 参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://www.cnblogs.com/skywang12345/p/3308498.html&quot;&gt;Java 集合系列01之 总体框架&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://blog.hongtium.com/java-map-skiplist/&quot;&gt;Java里多个Map的性能比较（TreeMap、HashMap、ConcurrentSkipListMap）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2013/06/08/java-collection-framework.html</link>
      <guid>http://blog.javachen.com/2013/06/08/java-collection-framework.html</guid>
      <pubDate>2013-06-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Kettle访问IDH2.3中的HBase</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;摘要&lt;/h1&gt;

&lt;p&gt;Kettle是一款国外开源的ETL工具，纯java编写，可以在Window、Linux、Unix上运行，绿色无需安装，数据抽取高效稳定。&lt;a href=&quot;https://github.com/pentaho/big-data-plugin&quot;&gt;big-data-plugin&lt;/a&gt;是kettle中用于访问bigdata，包括hadoop、cassandra、mongodb等nosql数据库的一个插件。&lt;/p&gt;

&lt;p&gt;截至目前，kettle的版本为4.4.1，&lt;code&gt;big-data-plugin&lt;/code&gt;插件支持&lt;code&gt;cloudera CDH3u4、CDH4.1&lt;/code&gt;，暂不支持Intel的hadoop发行版本IDH。&lt;/p&gt;

&lt;p&gt;本文主要介绍如何让kettle支持IDH的hadoop版本。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;方法&lt;/h1&gt;

&lt;p&gt;假设你已经安装好IDH-2.3的集群，并已经拷贝出&lt;code&gt;/usr/lib/&lt;/code&gt;下的hadoop、hbase、zookeeper目录。&lt;/p&gt;

&lt;p&gt;首先，下载一个kettle版本，如社区版data-integration，然后进入&lt;code&gt;data-integration/plugins/pentaho-big-data-plugin&lt;/code&gt;目录，修改plugin.properties文件中的&lt;code&gt;active.hadoop.configuration&lt;/code&gt;属性，将其值改为cdh4&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;active.hadoop.configuration=cdh4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改kettle的log4j日志等级，并启动kettle，检查启动过程中是否报错，如有错误，请修正错误。&lt;/p&gt;

&lt;p&gt;进入hadoop-configurations目录，copy and paste cdh3u4并命名为idh2.3。&lt;/p&gt;

&lt;p&gt;因为IDH和CDH的hadoop版本不一致，故需要替换hadoop和hbase、zookeeper为IDH的版本，涉及到需要替换、增加的jar有，这些jar文件从IDH安装后的目录中拷贝即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/hbase-0.94.1-Intel.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/protobuf-java-2.4.0a.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/zookeeper-3.4.5-Intel.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-ant-1.0.3-Intel.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-core-1.0.3-Intel.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-examples-1.0.3-Intel.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-test-1.0.3-Intel.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-tools-1.0.3-Intel.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/libthrift-0.8.0.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他依赖包可以尝试添加，并删除多版本的jar文件。&lt;/p&gt;

&lt;p&gt;需要删除CDH的jar有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/hbase-0.90.6-cdh3u4.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/zookeeper-3.3.5-cdh3u4.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-client-0.20.2-cdh3u4.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-core-0.20.2-cdh3u4.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/libfb303-0.5.0-cdh.jar
data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/libthrift-0.5.0-cdh.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改plugin.properties文件中的active.hadoop.configuration属性，将其值改为idh2.3。重起kettle，观察启动过程中是否报错。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;验证&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;打开hbase output组件，配置zookeeper的host和port&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/hbase-output-setup-for-idh-2.3.png&quot; alt=&quot;hbase-output-setup-for-idh-2.3&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在&lt;code&gt;Create/Edit mappings&lt;/code&gt; tab页点击&lt;code&gt;Get table names&lt;/code&gt;，发现该组件卡住，kettle控制台提示异常则需要检查客户端jar版本和服务端是否一致：&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 0 of 10 failed; 
retrying after sleep of 1000
java.io.IOException: Call to OS-GZP2308-04/192.168.40.84:60000 failed on local exception: java.io.EOFException
at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:1110)
at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1079)
at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
at $Proxy5.getProtocolVersion(Unknown Source)
at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:183)
at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:335)
at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:312)
at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:364)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:710)
at org.apache.hadoop.hbase.client.HBaseAdmin.&amp;lt; init&amp;gt;(HBaseAdmin.java:141)
at com.intel.hbase.test.createtable.TableBuilder.main(TableBuilder.java:48)
Caused by: java.io.EOFException
at java.io.DataInputStream.readInt(DataInputStream.java:375)
at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:605)
at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:538)
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2013/04/17/access-idh-2.3-hbase-in-kettle.html</link>
      <guid>http://blog.javachen.com/2013/04/17/access-idh-2.3-hbase-in-kettle.html</guid>
      <pubDate>2013-04-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Kettle中添加一个参数字段到输出</title>
      <description>&lt;p&gt;kettle可以将输入流中的字段输出到输出流中，输入输出流可以为数据库、文件或其他，通常情况下输入流中字段为已知确定的，如果我想在输出流中添加一个来自转换的命令行参数的一个字段，该如何操作？&lt;/p&gt;

&lt;p&gt;上述问题可以拆分为两个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;从命令行接受一个参数作为一个字段&lt;/li&gt;
  &lt;li&gt;合并输入流和这个字段&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section&quot;&gt;问题1&lt;/h2&gt;

&lt;p&gt;第一个问题可以使用kettle中&lt;code&gt;获取系统信息&lt;/code&gt;组件，定义一个变量，该值来自命令行参数，见下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/get-a-field-from-paramter.png&quot; alt=&quot;get-a-field-from-paramter&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;问题2&lt;/h2&gt;
&lt;p&gt;第二个问题可以使用kettle中&lt;code&gt;记录关联 (笛卡尔输出)&lt;/code&gt;组件将两个组件关联起来，输出一个笛卡尔结果集，关联条件设定恒为true，在运行前设置第一个参数的值，然后运行即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/run-kettle-for-join-two-inputs.png&quot; alt=&quot;run-kettle-for-join-two-inputs&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;下载脚本&lt;/h2&gt;
&lt;p&gt;最后，kettle转换文件下载地址：&lt;a href=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2013/join-a-paramter-to-input-in-kettle.zip&quot;&gt;在这里&lt;/a&gt;。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2013/04/07/add-a-field-from-paramter-to-output.html</link>
      <guid>http://blog.javachen.com/2013/04/07/add-a-field-from-paramter-to-output.html</guid>
      <pubDate>2013-04-07T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用yum源安装CDH Hadoop集群</title>
      <description>&lt;p&gt;本文主要是记录使用yum安装CDH Hadoop集群的过程，包括HDFS、Yarn、Hive和HBase。&lt;code&gt;本文使用CDH5.4版本进行安装，故下文中的过程都是针对CDH5.4版本的&lt;/code&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;0. 环境说明&lt;/h1&gt;

&lt;p&gt;系统环境：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：CentOs 6.6&lt;/li&gt;
  &lt;li&gt;Hadoop版本：&lt;code&gt;CDH5.4&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;JDK版本：&lt;code&gt;1.7.0_71&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;运行用户：root&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群各节点角色规划为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        cdh2     DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server
192.168.56.123        cdh3     DataNode、HBase、NodeManager、Hive Server2、Impala Server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cdh1作为master节点，其他节点作为slave节点。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;1. 准备工作&lt;/h1&gt;

&lt;p&gt;安装 Hadoop 集群前先做好下面的准备工作，在修改配置文件的时候，建议在一个节点上修改，然后同步到其他节点，例如：对于 hdfs 和 yarn ，在 NameNode 节点上修改然后再同步，对于 HBase，选择一个节点再同步。因为要同步配置文件和在多个节点启动服务，建议配置 ssh 无密码登陆。&lt;/p&gt;

&lt;h2 id=&quot;hosts&quot;&gt;1.1 配置hosts&lt;/h2&gt;

&lt;p&gt;CDH 要求使用 IPv4，IPv6 不支持，&lt;strong&gt;禁用IPv6方法：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ vim /etc/sysctl.conf
#disable ipv6
net.ipv6.conf.all.disable_ipv6=1
net.ipv6.conf.default.disable_ipv6=1
net.ipv6.conf.lo.disable_ipv6=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使其生效：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sysctl -p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后确认是否已禁用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /proc/sys/net/ipv6/conf/all/disable_ipv6
1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、设置hostname，以cdh1为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hostname cdh1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、确保&lt;code&gt;/etc/hosts&lt;/code&gt;中包含ip和FQDN，如果你在使用DNS，保存这些信息到&lt;code&gt;/etc/hosts&lt;/code&gt;不是必要的，却是最佳实践。&lt;/p&gt;

&lt;p&gt;3、确保&lt;code&gt;/etc/sysconfig/network&lt;/code&gt;中包含&lt;code&gt;hostname=cdh1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;4、检查网络，运行下面命令检查是否配置了hostname以及其对应的ip是否正确。&lt;/p&gt;

&lt;p&gt;运行&lt;code&gt;uname -a&lt;/code&gt;查看hostname是否匹配&lt;code&gt;hostname&lt;/code&gt;命令运行的结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ uname -a
Linux cdh1 2.6.32-358.23.2.el6.x86_64 #1 SMP Wed Oct 16 18:37:12 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
$ hostname
cdh1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行&lt;code&gt;/sbin/ifconfig&lt;/code&gt;查看ip:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ifconfig
eth1      Link encap:Ethernet  HWaddr 08:00:27:75:E0:95  
          inet addr:192.168.56.121  Bcast:192.168.56.255  Mask:255.255.255.0
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先安装bind-utils，才能运行host命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install bind-utils -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行下面命令查看hostname和ip是否匹配:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ host -v -t A `hostname`
Trying &quot;cdh1&quot;
...
;; ANSWER SECTION:
cdh1. 60 IN	A	192.168.56.121
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、hadoop的所有配置文件中配置节点名称时，请使用hostname和不是ip&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;1.2 关闭防火墙&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ setenforce 0
$ vim /etc/sysconfig/selinux #修改SELINUX=disabled

#清空iptables
$ iptables -F
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;1.3 时钟同步&lt;/h2&gt;

&lt;h2 id=&quot;section-4&quot;&gt;搭建时钟同步服务器&lt;/h2&gt;

&lt;p&gt;这里选择 cdh1 节点为时钟同步服务器，其他节点为客户端同步时间到该节点。安装ntp:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install ntp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 cdh1 上的配置文件 &lt;code&gt;/etc/ntp.conf&lt;/code&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;restrict default ignore   //默认不允许修改或者查询ntp,并且不接收特殊封包
restrict 127.0.0.1        //给于本机所有权限
restrict 192.168.56.0 mask 255.255.255.0 notrap nomodify  //给于局域网机的机器有同步时间的权限
server  192.168.56.121     # local clock
driftfile /var/lib/ntp/drift
fudge   127.127.1.0 stratum 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动 ntp：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#设置开机启动
$ chkconfig ntpd on

$ service ntpd start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ntpq用来监视ntpd操作，使用标准的NTP模式6控制消息模式，并与NTP服务器通信。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ntpq -p&lt;/code&gt; 查询网络中的NTP服务器，同时显示客户端和每个服务器的关系。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ntpq -p
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
*LOCAL(1)        .LOCL.           5 l    6   64    1    0.000    0.000   0.000
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;”* “：响应的NTP服务器和最精确的服务器。&lt;/li&gt;
  &lt;li&gt;”+”：响应这个查询请求的NTP服务器。&lt;/li&gt;
  &lt;li&gt;“blank（空格）”：没有响应的NTP服务器。&lt;/li&gt;
  &lt;li&gt;“remote” ：响应这个请求的NTP服务器的名称。&lt;/li&gt;
  &lt;li&gt;“refid “：NTP服务器使用的更高一级服务器的名称。&lt;/li&gt;
  &lt;li&gt;“st”：正在响应请求的NTP服务器的级别。&lt;/li&gt;
  &lt;li&gt;“when”：上一次成功请求之后到现在的秒数。&lt;/li&gt;
  &lt;li&gt;“poll”：当前的请求的时钟间隔的秒数。&lt;/li&gt;
  &lt;li&gt;“offset”：主机通过NTP时钟同步与所同步时间源的时间偏移量，单位为毫秒（ms）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;客户端的配置&lt;/h2&gt;

&lt;p&gt;在cdh2和cdh3节点上执行下面操作：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ntpdate cdh1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ntpd启动的时候通常需要一段时间大概5分钟进行时间同步，所以在ntpd刚刚启动的时候还不能正常提供时钟服务，报错”no server suitable for synchronization found”。启动时候需要等待5分钟。&lt;/p&gt;

&lt;p&gt;如果想定时进行时间校准，可以使用crond服务来定时执行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 每天 1:00 Linux 系统就会自动的进行网络时间校准
00 1 * * * root /usr/sbin/ntpdate 192.168.56.121 &amp;gt;&amp;gt; /root/ntpdate.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;jdk&quot;&gt;1.4 安装jdk&lt;/h2&gt;

&lt;p&gt;CDH5.4要求使用JDK1.7，JDK的安装过程请参考网上文章。&lt;/p&gt;

&lt;h2 id=&quot;yum&quot;&gt;1.5 设置本地yum源&lt;/h2&gt;

&lt;p&gt;CDH官方的yum源地址在 http://archive.cloudera.com/cdh4/redhat/6/x86_64/cdh/cloudera-cdh4.repo 或 http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo ，请根据你安装的cdh版本修改该文件中baseurl的路径。&lt;/p&gt;

&lt;p&gt;你可以从&lt;a href=&quot;http://archive.cloudera.com/cdh4/repo-as-tarball/&quot;&gt;这里&lt;/a&gt;下载 cdh4 的仓库压缩包，或者从&lt;a href=&quot;http://archive.cloudera.com/cdh5/repo-as-tarball/&quot;&gt;这里&lt;/a&gt; 下载 cdh5 的仓库压缩包。&lt;/p&gt;

&lt;p&gt;因为我是使用的centos操作系统，故我这里下载的是cdh5的centos6压缩包，将其下载之后解压到ftp服务的路径下，然后配置cdh的本地yum源：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[hadoop]
name=hadoop
baseurl=ftp://cdh1/cdh/5/
enabled=1
gpgcheck=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;操作系统的yum源，建议你通过下载 centos 的 dvd 然后配置一个本地的 yum 源。&lt;/p&gt;

&lt;h1 id=&quot;hdfs&quot;&gt;2. 安装和配置HDFS&lt;/h1&gt;

&lt;p&gt;根据文章开头的节点规划，cdh1 为NameNode节点，cdh2为SecondaryNameNode节点，cdh2 和 cdh3 为DataNode节点&lt;/p&gt;

&lt;p&gt;在 cdh1 节点安装 hadoop-hdfs-namenode：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hadoop hadoop-hdfs hadoop-client hadoop-doc hadoop-debuginfo hadoop-hdfs-namenode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh2 节点安装 hadoop-hdfs-secondarynamenode&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hadoop-hdfs-secondarynamenode -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh2、cdh3节点安装 hadoop-hdfs-datanode&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hadoop hadoop-hdfs hadoop-client hadoop-doc hadoop-debuginfo hadoop-hdfs-datanode -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NameNode HA 的配置过程请参考&lt;a href=&quot;/2014/07/18/install-hdfs-ha-in-cdh.html&quot;&gt;CDH中配置HDFS HA&lt;/a&gt;，建议暂时不用配置。&lt;/p&gt;

&lt;h2 id=&quot;hadoop&quot;&gt;2.1 修改hadoop配置文件&lt;/h2&gt;

&lt;p&gt;在&lt;code&gt;/etc/hadoop/conf/core-site.xml&lt;/code&gt;中设置&lt;code&gt;fs.defaultFS&lt;/code&gt;属性值，该属性指定NameNode是哪一个节点以及使用的文件系统是file还是hdfs，格式：&lt;code&gt;hdfs://&amp;lt;namenode host&amp;gt;:&amp;lt;namenode port&amp;gt;/&lt;/code&gt;，默认的文件系统是&lt;code&gt;file:///&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;hdfs://cdh1:8020&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/hadoop/conf/hdfs-site.xml&lt;/code&gt;中设置&lt;code&gt;dfs.permissions.superusergroup&lt;/code&gt;属性，该属性指定hdfs的超级用户，默认为hdfs，你可以修改为hadoop：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;dfs.permissions.superusergroup&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;hadoop&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;更多的配置信息说明，请参考 &lt;a href=&quot;http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html&quot;&gt;Apache Cluster Setup&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-6&quot;&gt;2.2 指定本地文件目录&lt;/h2&gt;

&lt;p&gt;在hadoop中默认的文件路径以及权限要求如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;目录									所有者		权限		默认路径
hadoop.tmp.dir						hdfs:hdfs	drwx------	/var/hadoop
dfs.namenode.name.dir				hdfs:hdfs	drwx------	file://${hadoop.tmp.dir}/dfs/name
dfs.datanode.data.dir				hdfs:hdfs	drwx------	file://${hadoop.tmp.dir}/dfs/data
dfs.namenode.checkpoint.dir			hdfs:hdfs	drwx------	file://${hadoop.tmp.dir}/dfs/namesecondary
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明你可以在 hdfs-site.xm l中只配置&lt;code&gt;hadoop.tmp.dir&lt;/code&gt;，也可以分别配置上面的路径。这里使用分别配置的方式，hdfs-site.xml中配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;dfs.namenode.name.dir&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;file:///data/dfs/nn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;dfs.datanode.data.dir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;file:///data/dfs/dn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;strong&gt;NameNode&lt;/strong&gt;上手动创建 &lt;code&gt;dfs.name.dir&lt;/code&gt; 或 &lt;code&gt;dfs.namenode.name.dir&lt;/code&gt; 的本地目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p /data/dfs/nn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;strong&gt;DataNode&lt;/strong&gt;上手动创建 &lt;code&gt;dfs.data.dir&lt;/code&gt; 或 &lt;code&gt;dfs.datanode.data.dir&lt;/code&gt; 的本地目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p /data/dfs/dn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改上面目录所有者：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chown -R hdfs:hdfs /data/dfs/nn /data/dfs/dn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hadoop的进程会自动设置 &lt;code&gt;dfs.data.dir&lt;/code&gt; 或 &lt;code&gt;dfs.datanode.data.dir&lt;/code&gt;，但是 &lt;code&gt;dfs.name.dir&lt;/code&gt; 或 &lt;code&gt;dfs.namenode.name.dir&lt;/code&gt; 的权限默认为755，需要手动设置为700：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ chmod 700 /data/dfs/nn

# 或者
$ chmod go-rx /data/dfs/nn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：DataNode的本地目录可以设置多个，你可以设置 &lt;code&gt;dfs.datanode.failed.volumes.tolerated&lt;/code&gt; 参数的值，表示能够容忍不超过该个数的目录失败。&lt;/p&gt;

&lt;h2 id=&quot;secondarynamenode&quot;&gt;2.3 配置 SecondaryNameNode&lt;/h2&gt;

&lt;p&gt;配置 SecondaryNameNode 需要在 &lt;code&gt;/etc/hadoop/conf/hdfs-site.xml&lt;/code&gt; 中添加以下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;dfs.namenode.checkpoint.check.period
dfs.namenode.checkpoint.txns
dfs.namenode.checkpoint.dir
dfs.namenode.checkpoint.edits.dir
dfs.namenode.num.checkpoints.retained
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;/etc/hadoop/conf/hdfs-site.xml&lt;/code&gt; 中加入如下配置，将cdh2设置为 SecondaryNameNode：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.secondary.http.address&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;cdh2:50090&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置多个secondarynamenode，请参考&lt;a href=&quot;http://blog.cloudera.com/blog/2009/02/multi-host-secondarynamenode-configuration/&quot;&gt;multi-host-secondarynamenode-configuration&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;2.4 开启回收站功能&lt;/h2&gt;

&lt;p&gt;回收站功能默认是关闭的，建议打开。在 &lt;code&gt;/etc/hadoop/conf/core-site.xml&lt;/code&gt; 中添加如下两个参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;fs.trash.interval&lt;/code&gt;,该参数值为时间间隔，单位为分钟，默认为0，表示回收站功能关闭。该值表示回收站中文件保存多长时间，如果服务端配置了该参数，则忽略客户端的配置；如果服务端关闭了该参数，则检查客户端是否有配置该参数；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;fs.trash.checkpoint.interval&lt;/code&gt;，该参数值为时间间隔，单位为分钟，默认为0。该值表示检查回收站时间间隔，该值要小于&lt;code&gt;fs.trash.interval&lt;/code&gt;，该值在服务端配置。如果该值设置为0，则使用 &lt;code&gt;fs.trash.interval&lt;/code&gt; 的值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datanode&quot;&gt;2.5 (可选)配置DataNode存储的负载均衡&lt;/h2&gt;

&lt;p&gt;在 &lt;code&gt;/etc/hadoop/conf/hdfs-site.xml&lt;/code&gt; 中配置以下三个参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;dfs.datanode.fsdataset. volume.choosing.policy&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细说明，请参考 &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_hdfs_cluster_deploy.html#concept_ncq_nnk_ck_unique_1&quot;&gt;Optionally configure DataNode storage balancing&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;webhdfs&quot;&gt;2.6 开启WebHDFS&lt;/h2&gt;

&lt;p&gt;在NameNode节点上安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hadoop-httpfs -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后修改 /etc/hadoop/conf/core-site.xml配置代理用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;  
&amp;lt;name&amp;gt;hadoop.proxyuser.httpfs.hosts&amp;lt;/name&amp;gt;  
&amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
&amp;lt;name&amp;gt;hadoop.proxyuser.httpfs.groups&amp;lt;/name&amp;gt;  
&amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;lzo&quot;&gt;2.7 配置LZO&lt;/h2&gt;

&lt;p&gt;下载repo文件到 &lt;code&gt;/etc/yum.repos.d/&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果你安装的是 CDH4，请下载&lt;a href=&quot;http://archive.cloudera.com/gplextras/redhat/6/x86_64/gplextras/cloudera-gplextras4.repo&quot;&gt;Red Hat/CentOS 6&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;如果你安装的是 CDH5，请下载&lt;a href=&quot;http://archive.cloudera.com/gplextras5/redhat/6/x86_64/gplextras/cloudera-gplextras5.repo&quot;&gt;Red Hat/CentOS 6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后，安装lzo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hadoop-lzo* impala-lzo  -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，在 &lt;code&gt;/etc/hadoop/conf/core-site.xml&lt;/code&gt; 中添加如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;io.compression.codecs&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;io.compression.codec.lzo.class&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;com.hadoop.compression.lzo.LzoCodec&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多关于LZO信息，请参考：&lt;a href=&quot;http://wiki.apache.org/hadoop/UsingLzoCompression&quot;&gt;Using LZO Compression&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;snappy&quot;&gt;2.8 (可选)配置Snappy&lt;/h2&gt;

&lt;p&gt;cdh 的 rpm 源中默认已经包含了 snappy ，直接在每个节点安装Snappy：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install snappy snappy-devel  -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在 &lt;code&gt;core-site.xml&lt;/code&gt; 中修改&lt;code&gt;io.compression.codecs&lt;/code&gt;的值，添加 &lt;code&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/code&gt; 。&lt;/p&gt;

&lt;p&gt;使 snappy 对 hadoop 可用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ln -sf /usr/lib64/libsnappy.so /usr/lib/hadoop/lib/native/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hdfs-1&quot;&gt;2.9 启动HDFS&lt;/h2&gt;

&lt;p&gt;将cdh1上的配置文件同步到每一个节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp -r /etc/hadoop/conf root@cdh2:/etc/hadoop/
$ scp -r /etc/hadoop/conf root@cdh3:/etc/hadoop/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在cdh1节点格式化NameNode：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop namenode -format
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在每个节点运行下面命令启动hdfs：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ for x in `ls /etc/init.d/|grep  hadoop-hdfs` ; do service $x start ; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 hdfs 运行之后，创建 &lt;code&gt;/tmp&lt;/code&gt; 临时目录，并设置权限为 &lt;code&gt;1777&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir /tmp
$ sudo -u hdfs hadoop fs -chmod -R 1777 /tmp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果安装了HttpFS，则启动 HttpFS 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service hadoop-httpfs start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-8&quot;&gt;2.10 测试&lt;/h2&gt;

&lt;p&gt;通过 &lt;a href=&quot;http://cdh1:50070/&quot;&gt;http://cdh1:50070/&lt;/a&gt; 可以访问 NameNode 页面。使用 curl 运行下面命令，可以测试 webhdfs 并查看执行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ curl &quot;http://localhost:14000/webhdfs/v1?op=gethomedirectory&amp;amp;user.name=hdfs&quot;
{&quot;Path&quot;:&quot;\/user\/hdfs&quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多的 API，请参考 &lt;a href=&quot;http://archive.cloudera.com/cdh5/cdh/5/hadoop/hadoop-project-dist/hadoop-hdfs/WebHDFS.html&quot;&gt;WebHDFS REST API&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;yarn&quot;&gt;3. 安装和配置YARN&lt;/h1&gt;

&lt;p&gt;根据文章开头的节点规划，cdh1 为resourcemanager节点，cdh2 和 cdh3 为nodemanager节点，为了简单，historyserver 也装在 cdh1 节点上。&lt;/p&gt;

&lt;h2 id=&quot;section-9&quot;&gt;3.1 安装服务&lt;/h2&gt;

&lt;p&gt;在 cdh1 节点安装:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hadoop-yarn hadoop-yarn-resourcemanager -y

#安装 historyserver
$ yum install hadoop-mapreduce-historyserver hadoop-yarn-proxyserver -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh2、cdh3 节点安装:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hadoop-yarn hadoop-yarn-nodemanager hadoop-mapreduce -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-10&quot;&gt;3.2 修改配置参数&lt;/h2&gt;

&lt;p&gt;要想使用YARN，需要在 &lt;code&gt;/etc/hadoop/conf/mapred-site.xml&lt;/code&gt; 中做如下配置:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改/etc/hadoop/conf/yarn-site.xml，配置resourcemanager的节点名称以及一些服务的端口号：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.resource-tracker.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;cdh1:8031&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;cdh1:8032&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.scheduler.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;cdh1:8030&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.admin.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;cdh1:8033&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.webapp.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;cdh1:8088&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;/etc/hadoop/conf/yarn-site.xml&lt;/code&gt; 中添加如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.hadoop.mapred.ShuffleHandler&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.log-aggregation-enable&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.application.classpath&amp;lt;/name&amp;gt;
   &amp;lt;value&amp;gt;
    $HADOOP_CONF_DIR,
    $HADOOP_COMMON_HOME/*,
    $HADOOP_COMMON_HOME/lib/*,
    $HADOOP_HDFS_HOME/*,
    $HADOOP_HDFS_HOME/lib/*,
    $HADOOP_MAPRED_HOME/*,
    $HADOOP_MAPRED_HOME/lib/*,
    $HADOOP_YARN_HOME/*,
    $HADOOP_YARN_HOME/lib/*
    &amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;yarn.log.aggregation.enable&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;yarn.nodemanager.aux-services&lt;/code&gt; 的值在 cdh4 中应该为 &lt;code&gt;mapreduce.shuffle&lt;/code&gt;，并配置参数&lt;code&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/code&gt;值为 &lt;code&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/code&gt; ，在cdh5中为&lt;code&gt;mapreduce_shuffle&lt;/code&gt;，这时候请配置&lt;code&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/code&gt;参数&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;这里配置了 &lt;code&gt;yarn.application.classpath&lt;/code&gt; ，需要设置一些喜欢环境变量：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export HADOOP_HOME=/usr/lib/hadoop
export HIVE_HOME=/usr/lib/hive
export HBASE_HOME=/usr/lib/hbase
export HADOOP_HDFS_HOME=/usr/lib/hadoop-hdfs
export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=/usr/lib/hadoop-hdfs
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HDFS_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_YARN_HOME=/usr/lib/hadoop-yarn
export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在hadoop中默认的文件路径以及权限要求如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;目录									                   所有者		 权限		        默认路径
yarn.nodemanager.local-dirs			      yarn:yarn	  drwxr-xr-x    ${hadoop.tmp.dir}/nm-local-dir
yarn.nodemanager.log-dirs			        yarn:yarn	  drwxr-xr-x	  ${yarn.log.dir}/userlogs
yarn.nodemanager.remote-app-log-dir							                hdfs://cdh1:8020/var/log/hadoop-yarn/apps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;故在 &lt;code&gt;/etc/hadoop/conf/yarn-site.xml&lt;/code&gt; 文件中添加如下配置:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.local-dirs&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/data/yarn/local&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.log-dirs&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/data/yarn/logs&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.remote-app-log-dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/yarn/apps&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 &lt;code&gt;yarn.nodemanager.local-dirs&lt;/code&gt; 和 &lt;code&gt;yarn.nodemanager.log-dirs&lt;/code&gt; 参数对应的目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p /data/yarn/{local,logs}
$ chown -R yarn:yarn /data/yarn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 hdfs 上创建 &lt;code&gt;yarn.nodemanager.remote-app-log-dir&lt;/code&gt; 对应的目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir -p /yarn/apps
$ sudo -u hdfs hadoop fs -chown yarn:mapred /yarn/apps
$ sudo -u hdfs hadoop fs -chmod 1777 /yarn/apps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;/etc/hadoop/conf/mapred-site.xml&lt;/code&gt; 中配置 MapReduce History Server：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapreduce.jobhistory.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;cdh1:10020&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapreduce.jobhistory.webapp.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;cdh1:19888&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外，确保 mapred、yarn 用户能够使用代理，在 &lt;code&gt;/etc/hadoop/conf/core-site.xml&lt;/code&gt; 中添加如下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.proxyuser.mapred.groups&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.proxyuser.mapred.hosts&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.proxyuser.yarn.groups&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.proxyuser.yarn.hosts&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置 Staging 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.app.mapreduce.am.staging-dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/user&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并在 hdfs 上创建相应的目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir -p /user
$ sudo -u hdfs hadoop fs -chmod 777 /user
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可选的，你可以在 &lt;code&gt;/etc/hadoop/conf/mapred-site.xml&lt;/code&gt; 设置以下两个参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/code&gt;，该目录权限应该为1777，默认值为 &lt;code&gt;${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;mapreduce.jobhistory.done-dir&lt;/code&gt;，该目录权限应该为750，默认值为 &lt;code&gt;${yarn.app.mapreduce.am.staging-dir}/history/done&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后，在 hdfs 上创建目录并设置权限：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir -p /user/history
$ sudo -u hdfs hadoop fs -chmod -R 1777 /user/history
$ sudo -u hdfs hadoop fs -chown mapred:hadoop /user/history
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置 &lt;code&gt;HADOOP_MAPRED_HOME&lt;/code&gt;，或者把其加入到 hadoop 的配置文件中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hdfs-&quot;&gt;3.4 验证 HDFS 结构：&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -ls -R /
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你应该看到如下结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;drwxrwxrwt   - hdfs hadoop          0 2014-04-19 14:21 /tmp
drwxrwxrwx   - hdfs hadoop          0 2014-04-19 14:26 /user
drwxrwxrwt   - mapred hadoop        0 2014-04-19 14:31 /user/history
drwxr-x---   - mapred hadoop        0 2014-04-19 14:38 /user/history/done
drwxrwxrwt   - mapred hadoop        0 2014-04-19 14:48 /user/history/done_intermediate
drwxr-xr-x   - hdfs   hadoop        0 2014-04-19 15:31 /yarn
drwxrwxrwt   - yarn   mapred        0 2014-04-19 15:31 /yarn/apps
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-11&quot;&gt;3.5 同步配置文件&lt;/h2&gt;

&lt;p&gt;同步配置文件到整个集群:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp -r /etc/hadoop/conf root@cdh2:/etc/hadoop/
$ scp -r /etc/hadoop/conf root@cdh3:/etc/hadoop/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-12&quot;&gt;3.6 启动服务&lt;/h2&gt;

&lt;p&gt;在每个节点启动 YARN :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ for x in `ls /etc/init.d/|grep hadoop-yarn` ; do service $x start ; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh1 节点启动 mapred-historyserver :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /etc/init.d/hadoop-mapreduce-historyserver start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为每个 MapReduce 用户创建主目录，比如说 hive 用户或者当前用户：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir /user/$USER
$ sudo -u hdfs hadoop fs -chown $USER /user/$USER
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-13&quot;&gt;3.7 测试&lt;/h2&gt;

&lt;p&gt;通过 &lt;a href=&quot;http://cdh1:8088/&quot;&gt;http://cdh1:8088/&lt;/a&gt; 可以访问 Yarn 的管理页面，通过 &lt;a href=&quot;http://cdh1:19888/&quot;&gt;http://cdh1:19888/&lt;/a&gt; 可以访问 JobHistory 的管理页面，查看在线的节点：&lt;a href=&quot;http://cdh1:8088/cluster/nodes&quot;&gt;http://cdh1:8088/cluster/nodes&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;运行下面的测试程序，看是否报错：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Find how many jars name ending with examples you have inside location /usr/lib/
$ find /usr/lib/ -name &quot;*hadoop*examples*.jar&quot;

# To list all the class name inside jar
$ find /usr/lib/ -name &quot;hadoop-examples.jar&quot; | xargs -0 -I &#39;{}&#39; sh -c &#39;jar tf {}&#39;

# To search for specific class name inside jar
$ find /usr/lib/ -name &quot;hadoop-examples.jar&quot; | xargs -0 -I &#39;{}&#39; sh -c &#39;jar tf {}&#39; | grep -i wordcount.class

# 运行 randomwriter 例子
$ sudo -u hdfs hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomwriter out
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;zookeeper&quot;&gt;4. 安装 Zookeeper&lt;/h1&gt;

&lt;p&gt;Zookeeper 至少需要3个节点，并且节点数要求是基数，这里在所有节点上都安装 Zookeeper。&lt;/p&gt;

&lt;h2 id=&quot;section-14&quot;&gt;4.1 安装&lt;/h2&gt;

&lt;p&gt;在每个节点上安装zookeeper：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install zookeeper* -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-15&quot;&gt;4.2 修改配置文件&lt;/h2&gt;

&lt;p&gt;设置 zookeeper 配置 &lt;code&gt;/etc/zookeeper/conf/zoo.cfg&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-properties&quot;&gt;maxClientCnxns=50
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/var/lib/zookeeper
clientPort=2181
server.1=cdh1:2888:3888
server.2=cdh3:2888:3888
server.3=cdh3:2888:3888
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;##4.3  同步配置文件&lt;/p&gt;

&lt;p&gt;将配置文件同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp -r /etc/zookeeper/conf root@cdh2:/etc/zookeeper/
$ scp -r /etc/zookeeper/conf root@cdh3:/etc/zookeeper/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-16&quot;&gt;4.4 初始化并启动服务&lt;/h2&gt;

&lt;p&gt;在每个节点上初始化并启动 zookeeper，注意 n 的值需要和 zoo.cfg 中的编号一致。&lt;/p&gt;

&lt;p&gt;在 cdh1 节点运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service zookeeper-server init --myid=1
$ service zookeeper-server start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh2 节点运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service zookeeper-server init --myid=2
$ service zookeeper-server start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 cdh3 节点运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ service zookeeper-server init --myid=3
$ service zookeeper-server start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-17&quot;&gt;4.5 测试&lt;/h2&gt;

&lt;p&gt;通过下面命令测试是否启动成功：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ zookeeper-client -server cdh1:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hbase&quot;&gt;5. 安装 HBase&lt;/h1&gt;

&lt;p&gt;HBase 依赖 ntp 服务，故需要提前安装好 ntp。&lt;/p&gt;

&lt;h2 id=&quot;section-18&quot;&gt;5.1 安装前设置&lt;/h2&gt;

&lt;p&gt;1）修改系统 ulimit 参数，在 &lt;code&gt;/etc/security/limits.conf&lt;/code&gt; 中添加下面两行并使其生效：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hdfs  -       nofile  32768
hbase -       nofile  32768
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2）修改 &lt;code&gt;dfs.datanode.max.xcievers&lt;/code&gt;，在 &lt;code&gt;hdfs-site.xml&lt;/code&gt; 中修改该参数值，将该值调整到较大的值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.datanode.max.xcievers&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;8192&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-19&quot;&gt;5.2 安装&lt;/h2&gt;

&lt;p&gt;在每个节点上安装 master 和 regionserver，如果需要你可以安装 hbase-rest、hbase-solr-indexer、hbase-thrift&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hbase hbase-master hbase-regionserver -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-20&quot;&gt;5.3 修改配置文件&lt;/h2&gt;

&lt;p&gt;修改 &lt;code&gt;hbase-site.xml&lt;/code&gt;文件，关键几个参数及含义如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;hbase.distributed&lt;/code&gt;：是否为分布式模式&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.rootdir&lt;/code&gt;：HBase在hdfs上的目录路径&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.tmp.dir&lt;/code&gt;：本地临时目录&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.zookeeper.quorum&lt;/code&gt;：zookeeper集群地址，逗号分隔&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.hregion.max.filesize&lt;/code&gt;：hregion文件最大大小&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hbase.hregion.memstore.flush.size&lt;/code&gt;：memstore文件最大大小&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外，在CDH5中建议&lt;code&gt;关掉Checksums&lt;/code&gt;（见&lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_hbase_upgrade.html&quot;&gt;Upgrading HBase&lt;/a&gt;）以提高性能，最后的配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;hdfs://cdh1:8020/hbase&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hbase.tmp.dir&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;/data/hbase&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;cdh1,cdh2,cdh3&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.hregion.max.filesize&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;536870912&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.hregion.memstore.flush.size&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;67108864&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.regionserver.lease.period&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;600000&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.client.retries.number&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;3&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.regionserver.handler.count&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;100&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.hstore.compactionThreshold&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;10&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.hstore.blockingStoreFiles&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;30&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.regionserver.checksum.verify&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.hstore.checksum.algorithm&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;NULL&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 hdfs 中创建 &lt;code&gt;/hbase&lt;/code&gt; 目录&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir /hbase
$ sudo -u hdfs hadoop fs -chown hbase:hbase /hbase
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置 crontab 定时删除日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ crontab -e
* 10 * * * cd /var/log/hbase/; rm -rf `ls /var/log/hbase/|grep -P &#39;hbase\-hbase\-.+\.log\.[0-9]&#39;\`&amp;gt;&amp;gt; /dev/null &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-21&quot;&gt;5.4 同步配置文件&lt;/h2&gt;

&lt;p&gt;将配置文件同步到其他节点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ scp -r /etc/hbase/conf root@cdh2:/etc/hbase/
$ scp -r /etc/hbase/conf root@cdh3:/etc/hbase/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-22&quot;&gt;5.5 创建本地目录&lt;/h2&gt;

&lt;p&gt;在 hbase-site.xml 配置文件中配置了 &lt;code&gt;hbase.tmp.dir&lt;/code&gt; 值为 &lt;code&gt;/data/hbase&lt;/code&gt;，现在需要在每个 hbase 节点创建该目录并设置权限：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir /data/hbase
$ chown -R hbase:hbase /data/hbase/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hbase-1&quot;&gt;5.6 启动HBase&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ for x in `ls /etc/init.d/|grep hbase` ; do service $x start ; done
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-23&quot;&gt;5.7 测试&lt;/h2&gt;

&lt;p&gt;通过 &lt;a href=&quot;http://cdh1:60030/&quot;&gt;http://cdh1:60030/&lt;/a&gt; 可以访问 RegionServer 页面，然后通过该页面可以知道哪个节点为 Master，然后再通过 60010 端口访问 Master 管理界面。&lt;/p&gt;

&lt;h1 id=&quot;hive&quot;&gt;6. 安装hive&lt;/h1&gt;

&lt;p&gt;在一个 NameNode 节点上安装 hive：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hive hive-metastore hive-server2 hive-jdbc hive-hbase  -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在其他 DataNode 上安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hive hive-server2 hive-jdbc hive-hbase -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;postgresql&quot;&gt;安装postgresql&lt;/h2&gt;

&lt;p&gt;这里使用 postgresq l数据库来存储元数据，如果你想使用 mysql 数据库，请参考下文。手动安装、配置 postgresql 数据库，请参考 &lt;a href=&quot;/hadoop/2013/03/24/manual-install-Cloudera-hive-CDH.html&quot;&gt;手动安装Cloudera Hive CDH&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;yum 方式安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum install postgresql-server postgresql-jdbc -y

$ ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置开启启动，并初始化数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ chkconfig postgresql on
$ service postgresql initdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置文件postgresql.conf，修改完后内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cat /var/lib/pgsql/data/postgresql.conf  | grep -e listen -e standard_conforming_strings
	listen_addresses = &#39;*&#39;
	standard_conforming_strings = off
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 /var/lib/pgsql/data/pg_hba.conf，添加以下一行内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	host    all         all         0.0.0.0/0                     trust
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建数据库和用户，设置密码为hive：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;su -c &quot;cd ; /usr/bin/pg_ctl start -w -m fast -D /var/lib/pgsql/data&quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;create user hive with password &#39;hive&#39;; \&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;drop database hive;\&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;CREATE DATABASE sentry owner=hive;\&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/psql --command \&quot;GRANT ALL privileges ON DATABASE hive TO hive;\&quot; &quot; postgres
su -c &quot;cd ; /usr/bin/pg_ctl restart -w -m fast -D /var/lib/pgsql/data&quot; postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候的hive-site.xml文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;configuration&amp;gt;
	    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;jdbc:postgresql://localhost/hive&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;org.postgresql.Driver&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;datanucleus.autoCreateSchema&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.resource-tracker.address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;cdh1:8031&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.auto.convert.join&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.schema.verification&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.warehouse.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/user/hive/warehouse&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.warehouse.subdir.inherit.perms&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.uris&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;thrift://cdh1:9083&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.client.socket.timeout&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;36000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.support.concurrency&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.zookeeper.quorum&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;cdh1,cdh2,cdh3&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.thrift.min.worker.threads&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;5&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.thrift.max.worker.threads&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;100&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，hive-server和 hive-server2 的 thrift 端口都为10000，如果要修改 hive-server2 thrift 端口，请修改 &lt;code&gt;hive.server2.thrift.port&lt;/code&gt; 参数的值。&lt;/p&gt;

&lt;p&gt;如果要设置运行 hive 的用户为连接的用户而不是启动用户，则添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hive.server2.enable.impersonation&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并在 core-site.xml 中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.hive.hosts&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.proxyuser.hive.groups&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;mysql&quot;&gt;安装mysql&lt;/h2&gt;

&lt;p&gt;yum方式安装mysql以及jdbc驱动：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install mysql mysql-devel mysql-server mysql-libs -y

$ yum install mysql-connector-java
$ ln -s /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建数据库和用户，并设置密码为hive：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mysql -e &quot;
	CREATE DATABASE hive;
	USE hive;
	CREATE USER &#39;hive&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;hive&#39;;
	GRANT ALL PRIVILEGES ON metastore.* TO &#39;hive&#39;@&#39;localhost&#39;;
	GRANT ALL PRIVILEGES ON metastore.* TO &#39;hive&#39;@&#39;cdh1&#39;;
	FLUSH PRIVILEGES;
&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是第一次安装，则初始化 hive 的元数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /usr/lib/hive/bin/schematool --dbType mysql --initSchema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是更新，则执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ /usr/lib/hive/bin/schematool --dbType mysql --upgradeSchema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置开启启动并启动数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ chkconfig mysqld on
$ service mysqld start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 hive-site.xml 文件中以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;	&amp;lt;property&amp;gt;
	  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
	  &amp;lt;value&amp;gt;jdbc:mysql://cdh1:3306/hive?useUnicode=true&amp;amp;amp;characterEncoding=UTF-8&amp;lt;/value&amp;gt;
	&amp;lt;/property&amp;gt;
	&amp;lt;property&amp;gt;
	  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
	  &amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
	&amp;lt;/property&amp;gt;
            &amp;lt;property&amp;gt;
                &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
                &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;
            &amp;lt;/property&amp;gt;

            &amp;lt;property&amp;gt;
                &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
                &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;
            &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hive-1&quot;&gt;配置hive&lt;/h2&gt;

&lt;p&gt;修改&lt;code&gt;/etc/hadoop/conf/hadoop-env.sh&lt;/code&gt;，添加环境变量 &lt;code&gt;HADOOP_MAPRED_HOME&lt;/code&gt;，如果不添加，则当你使用 yarn 运行 mapreduce 时候会出现 &lt;code&gt;UNKOWN RPC TYPE&lt;/code&gt; 的异常&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 hdfs 中创建 hive 数据仓库目录:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hive 的数据仓库在 hdfs 中默认为 &lt;code&gt;/user/hive/warehouse&lt;/code&gt;,建议修改其访问权限为 &lt;code&gt;1777&lt;/code&gt;，以便其他所有用户都可以创建、访问表，但不能删除不属于他的表。&lt;/li&gt;
  &lt;li&gt;每一个查询 hive 的用户都必须有一个 hdfs 的 home 目录( &lt;code&gt;/user&lt;/code&gt; 目录下，如 root 用户的为 &lt;code&gt;/user/root&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;hive 所在节点的 &lt;code&gt;/tmp&lt;/code&gt; 必须是 world-writable 权限的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;创建目录并设置权限：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo -u hdfs hadoop fs -mkdir /user/hive
$ sudo -u hdfs hadoop fs -chown hive /user/hive

$ sudo -u hdfs hadoop fs -mkdir /user/hive/warehouse
$ sudo -u hdfs hadoop fs -chmod 1777 /user/hive/warehouse
$ sudo -u hdfs hadoop fs -chown hive /user/hive/warehouse
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动hive-server和metastore:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service hive-metastore start
$ service hive-server start
$ service hive-server2 start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-24&quot;&gt;测试&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ hive -e &#39;create table t(id int);&#39;
$ hive -e &#39;select * from t limit 2;&#39;
$ hive -e &#39;select id from t;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问beeline:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ beeline
beeline&amp;gt; !connect jdbc:hive2://localhost:10000 hive hive org.apache.hive.jdbc.HiveDriver
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hbase-2&quot;&gt;与hbase集成&lt;/h2&gt;

&lt;p&gt;先安装 hive-hbase:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ yum install hive-hbase -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你是使用的 cdh4，则需要在 hive shell 里执行以下命令添加 jar：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ADD JAR /usr/lib/hive/lib/zookeeper.jar;
$ ADD JAR /usr/lib/hive/lib/hbase.jar;
$ ADD JAR /usr/lib/hive/lib/hive-hbase-handler-&amp;lt;hive_version&amp;gt;.jar
# guava 包的版本以实际版本为准。
$ ADD JAR /usr/lib/hive/lib/guava-11.0.2.jar;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你是使用的 cdh5，则需要在 hive shell 里执行以下命令添加 jar：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ADD JAR /usr/lib/hive/lib/zookeeper.jar;
ADD JAR /usr/lib/hive/lib/hive-hbase-handler.jar;
ADD JAR /usr/lib/hbase/lib/guava-12.0.1.jar;
ADD JAR /usr/lib/hbase/hbase-client.jar;
ADD JAR /usr/lib/hbase/hbase-common.jar;
ADD JAR /usr/lib/hbase/hbase-hadoop-compat.jar;
ADD JAR /usr/lib/hbase/hbase-hadoop2-compat.jar;
ADD JAR /usr/lib/hbase/hbase-protocol.jar;
ADD JAR /usr/lib/hbase/hbase-server.jar;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上你也可以在 hive-site.xml 中通过 &lt;code&gt;hive.aux.jars.path&lt;/code&gt; 参数来配置，或者你也可以在 hive-env.sh 中通过 &lt;code&gt;export HIVE_AUX_JARS_PATH=&lt;/code&gt; 来设置。&lt;/p&gt;

&lt;h1 id=&quot;section-25&quot;&gt;7. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/CDH5-Installation-Guide.html&quot;&gt;CDH5-Installation-Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;http://roserouge.iteye.com/blog/1558498&quot;&gt;hadoop cdh 安装笔记&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2013/04/06/install-cloudera-cdh-by-yum.html</link>
      <guid>http://blog.javachen.com/2013/04/06/install-cloudera-cdh-by-yum.html</guid>
      <pubDate>2013-04-06T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>安装Impala过程</title>
      <description>&lt;p&gt;与Hive类似，Impala也可以直接与HDFS和HBase库直接交互。只不过Hive和其它建立在MapReduce上的框架适合需要长时间运行的批处理任务。例如：那些批量提取，转化，加载（ETL）类型的Job，而Impala主要用于实时查询。&lt;/p&gt;

&lt;p&gt;Hadoop集群各节点的环境设置及安装过程见 &lt;a href=&quot;/2013/04/06/install-cloudera-cdh-by-yum.html&quot;&gt;使用yum安装CDH Hadoop集群&lt;/a&gt;，参考这篇文章。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1. 环境&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;CentOS 6.4 x86_64&lt;/li&gt;
  &lt;li&gt;CDH 5.0.1&lt;/li&gt;
  &lt;li&gt;jdk1.6.0_31&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群规划为3个节点，每个节点的ip、主机名和部署的组件分配如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.121        cdh1     NameNode、Hive、ResourceManager、HBase、impala
192.168.56.122        cdh2     DataNode、SSNameNode、NodeManager、HBase、impala
192.168.56.123        cdh3     DataNode、HBase、NodeManager、impala
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2. 安装&lt;/h1&gt;

&lt;p&gt;目前，CDH 5.0.1中 impala 版本为&lt;code&gt;1.4.0&lt;/code&gt;，下载repo文件到 /etc/yum.repos.d/:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果你安装的是 CDH4，请下载 &lt;a href=&quot;http://archive.cloudera.com/impala/redhat/6/x86_64/impala/1.3.1/&quot;&gt;Red Hat/CentOS 6&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;如果你安装的是 CDH5，请下载 &lt;a href=&quot;http://archive.cloudera.com/impala/redhat/6/x86_64/impala/1.4.0/&quot;&gt;Red Hat/CentOS 6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后，可以执行下面的命令安装所有的 impala 组件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sudo yum install impala impala-server impala-state-store impala-catalog impala-shell -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是，通常只是在需要的节点上安装对应的服务：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 hive metastore 所在节点安装impala-state-store和impala-catalog&lt;/li&gt;
  &lt;li&gt;在 DataNode 所在节点安装 impala-server 和 impala-shell&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3. 配置&lt;/h1&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3.1 修改配置文件&lt;/h2&gt;

&lt;p&gt;查看安装路径：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ find / -name impala
	/var/run/impala
	/var/lib/alternatives/impala
	/var/log/impala
	/usr/lib/impala
	/etc/alternatives/impala
	/etc/default/impala
	/etc/impala
	/etc/default/impala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;impalad的配置文件路径由环境变量&lt;code&gt;IMPALA_CONF_DIR&lt;/code&gt;指定，默认为&lt;code&gt;/usr/lib/impala/conf&lt;/code&gt;，impala 的默认配置在/etc/default/impala，修改该文件中的 &lt;code&gt;IMPALA_CATALOG_SERVICE_HOST&lt;/code&gt; 和 &lt;code&gt;IMPALA_STATE_STORE_HOST&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;IMPALA_CATALOG_SERVICE_HOST=cdh1
IMPALA_STATE_STORE_HOST=cdh1
IMPALA_STATE_STORE_PORT=24000
IMPALA_BACKEND_PORT=22000
IMPALA_LOG_DIR=/var/log/impala

IMPALA_CATALOG_ARGS=&quot; -log_dir=${IMPALA_LOG_DIR} &quot;
IMPALA_STATE_STORE_ARGS=&quot; -log_dir=${IMPALA_LOG_DIR} -state_store_port=${IMPALA_STATE_STORE_PORT}&quot;
IMPALA_SERVER_ARGS=&quot; \
    -log_dir=${IMPALA_LOG_DIR} \
    -catalog_service_host=${IMPALA_CATALOG_SERVICE_HOST} \
    -state_store_port=${IMPALA_STATE_STORE_PORT} \
    -use_statestore \
    -state_store_host=${IMPALA_STATE_STORE_HOST} \
    -be_port=${IMPALA_BACKEND_PORT}&quot;

ENABLE_CORE_DUMPS=false

# LIBHDFS_OPTS=-Djava.library.path=/usr/lib/impala/lib
# MYSQL_CONNECTOR_JAR=/usr/share/java/mysql-connector-java.jar
# IMPALA_BIN=/usr/lib/impala/sbin
# IMPALA_HOME=/usr/lib/impala
# HIVE_HOME=/usr/lib/hive
# HBASE_HOME=/usr/lib/hbase
# IMPALA_CONF_DIR=/etc/impala/conf
# HADOOP_CONF_DIR=/etc/impala/conf
# HIVE_CONF_DIR=/etc/impala/conf
# HBASE_CONF_DIR=/etc/impala/conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置 impala 可以使用的最大内存：在上面的 &lt;code&gt;IMPALA_SERVER_ARGS&lt;/code&gt; 参数值后面添加 &lt;code&gt;-mem_limit=70%&lt;/code&gt; 即可。&lt;/p&gt;

&lt;p&gt;如果需要设置 impala 中每一个队列的最大请求数，需要在上面的 &lt;code&gt;IMPALA_SERVER_ARGS&lt;/code&gt; 参数值后面添加 &lt;code&gt;-default_pool_max_requests=-1&lt;/code&gt; ，该参数设置每一个队列的最大请求数，如果为-1，则表示不做限制。&lt;/p&gt;

&lt;p&gt;在节点cdh1上拷贝&lt;code&gt;hive-site.xml&lt;/code&gt;、&lt;code&gt;core-site.xml&lt;/code&gt;、&lt;code&gt;hdfs-site.xml&lt;/code&gt;至&lt;code&gt;/usr/lib/impala/conf&lt;/code&gt;目录并作下面修改在&lt;code&gt;hdfs-site.xml&lt;/code&gt;文件中添加如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.client.read.shortcircuit&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
 
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.domain.socket.path&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/var/run/hadoop-hdfs/dn._PORT&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.datanode.hdfs-blocks-metadata.enabled&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同步以上文件到其他节点。&lt;/p&gt;

&lt;h2 id=&quot;socket-path&quot;&gt;3.2 创建socket path&lt;/h2&gt;

&lt;p&gt;在每个节点上创建/var/run/hadoop-hdfs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir -p /var/run/hadoop-hdfs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拷贝postgres jdbc jar：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/impala/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;3.3 用户要求&lt;/h2&gt;

&lt;p&gt;impala 安装过程中会创建名为 impala 的用户和组，不要删除该用户和组。&lt;/p&gt;

&lt;p&gt;如果想要 impala 和 YARN 和 Llama 合作，需要把 impala 用户加入 hdfs 组。&lt;/p&gt;

&lt;p&gt;impala 在执行 DROP TABLE 操作时，需要把文件移到到 hdfs 的回收站，所以你需要创建一个 hdfs 的目录 /user/impala，并将其设置为impala 用户可写。同样的，impala 需要读取 hive 数据仓库下的数据，故需要把 impala 用户加入 hive 组。&lt;/p&gt;

&lt;p&gt;impala 不能以 root 用户运行，因为 root 用户不允许直接读。&lt;/p&gt;

&lt;p&gt;创建 impala 用户家目录并设置权限：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo -u hdfs hadoop fs -mkdir /user/impala
sudo -u hdfs hadoop fs -chown impala /user/impala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 impala 用户所属的组：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ groups impala
impala : impala hadoop hdfs hive
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由上可知，impala 用户是属于 imapal、hadoop、hdfs、hive 用户组的&lt;/p&gt;

&lt;h1 id=&quot;section-5&quot;&gt;4. 启动服务&lt;/h1&gt;

&lt;p&gt;在 cdh1节点启动：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ service impala-state-store start
$ service impala-catalog start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果impalad正常启动，可以在&lt;code&gt;/tmp/ impalad.INFO&lt;/code&gt;查看。如果出现异常，可以查看&lt;code&gt;/tmp/impalad.ERROR&lt;/code&gt;定位错误信息。&lt;/p&gt;

&lt;h1 id=&quot;shell&quot;&gt;5. 使用shell&lt;/h1&gt;

&lt;p&gt;使用&lt;code&gt;impala-shell&lt;/code&gt;启动Impala Shell，连接 cdh1，并刷新元数据&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;&amp;gt;impala-shell
[Not connected] &amp;gt;connect cdh1
[cdh1:21000] &amp;gt;invalidate metadata
[cdh2:21000] &amp;gt;connect cdh2
[cdh2:21000] &amp;gt;select * from t
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当在 Hive 中创建表之后，第一次启动 impala-shell 时，请先执行 &lt;code&gt;INVALIDATE METADATA&lt;/code&gt; 语句以便 Impala 识别出新创建的表(在 Impala 1.2 及以上版本，你只需要在一个节点上运行 &lt;code&gt;INVALIDATE METADATA&lt;/code&gt; ，而不是在所有的 Impala 节点上运行)。&lt;/p&gt;

&lt;p&gt;你也可以添加一些其他参数，查看有哪些参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# impala-shell -h
Usage: impala_shell.py [options]

Options:
  -h, --help            show this help message and exit
  -i IMPALAD, --impalad=IMPALAD
                        &amp;lt;host:port&amp;gt; of impalad to connect to
  -q QUERY, --query=QUERY
                        Execute a query without the shell
  -f QUERY_FILE, --query_file=QUERY_FILE
                        Execute the queries in the query file, delimited by ;
  -k, --kerberos        Connect to a kerberized impalad
  -o OUTPUT_FILE, --output_file=OUTPUT_FILE
                        If set, query results will written to the given file.
                        Results from multiple semicolon-terminated queries
                        will be appended to the same file
  -B, --delimited       Output rows in delimited mode
  --print_header        Print column names in delimited mode, true by default
                        when pretty-printed.
  --output_delimiter=OUTPUT_DELIMITER
                        Field delimiter to use for output in delimited mode
  -s KERBEROS_SERVICE_NAME, --kerberos_service_name=KERBEROS_SERVICE_NAME
                        Service name of a kerberized impalad, default is
                        &#39;impala&#39;
  -V, --verbose         Enable verbose output
  -p, --show_profiles   Always display query profiles after execution
  --quiet               Disable verbose output
  -v, --version         Print version information
  -c, --ignore_query_failure
                        Continue on query failure
  -r, --refresh_after_connect
                        Refresh Impala catalog after connecting
  -d DEFAULT_DB, --database=DEFAULT_DB
                        Issue a use database command on startup.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如，你可以在连接时候字段刷新：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ impala-shell -r
Starting Impala Shell in unsecure mode
Connected to 192.168.56.121:21000
Server version: impalad version 1.1.1 RELEASE (build 83d5868f005966883a918a819a449f636a5b3d5f)
Invalidating Metadata
Welcome to the Impala shell. Press TAB twice to see a list of available commands.

Copyright (c) 2012 Cloudera, Inc. All rights reserved.

(Shell build version: Impala Shell v1.1.1 (83d5868) built on Fri Aug 23 17:28:05 PDT 2013)
Query: invalidate metadata
Query finished, fetching results ...

Returned 0 row(s) in 5.13s
[192.168.56.121:21000] &amp;gt;                  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 impala 导出数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ impala-shell -i &#39;192.168.56.121:21000&#39; -r -q &quot;select * from test&quot; -B --output_delimiter=&quot;\t&quot; -o result.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-6&quot;&gt;6. 参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://yuntai.1kapp.com/?p=904&quot;&gt;Impala安装文档完整版&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tech.uc.cn/?p=817&quot;&gt;Impala入门笔记&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ccp.cloudera.com/display/IMPALA10BETADOC/Installing+and+Using+Cloudera+Impala&quot;&gt;Installing and Using Cloudera Impala&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2013/03/29/install-impala.html</link>
      <guid>http://blog.javachen.com/2013/03/29/install-impala.html</guid>
      <pubDate>2013-03-29T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>手动安装Cloudera Hive CDH</title>
      <description>&lt;p&gt;本文主要记录手动安装Cloudera Hive集群过程，环境设置及Hadoop安装过程见&lt;a href=&quot;/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html&quot;&gt;手动安装Cloudera Hadoop CDH&lt;/a&gt;,参考这篇文章，hadoop各个组件和jdk版本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	hadoop-2.0.0-cdh4.6.0
	hbase-0.94.15-cdh4.6.0
	hive-0.10.0-cdh4.6.0
	jdk1.6.0_38
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hadoop各组件可以在&lt;a href=&quot;http://archive.cloudera.com/cdh4/cdh/4/&quot;&gt;这里&lt;/a&gt;下载。&lt;/p&gt;

&lt;p&gt;集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	192.168.0.1        desktop1     NameNode、Hive、ResourceManager、impala
	192.168.0.2        desktop2     SSNameNode
	192.168.0.3        desktop3     DataNode、HBase、NodeManager、impala
	192.168.0.4        desktop4     DataNode、HBase、NodeManager、impala
	192.168.0.5        desktop5     DataNode、HBase、NodeManager、impala
	192.168.0.6        desktop6     DataNode、HBase、NodeManager、impala
	192.168.0.7        desktop7     DataNode、HBase、NodeManager、impala
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hive&quot;&gt;安装hive&lt;/h1&gt;

&lt;p&gt;hive安装在desktop1上，&lt;strong&gt;注意&lt;/strong&gt;：hive默认是使用derby数据库保存元数据，这里替换为postgresql，下面会提到postgresql的安装说明，并且需要拷贝postgres的jdbc jar文件导hive的lib目录下。&lt;/p&gt;

&lt;p&gt;上传&lt;code&gt;hive-0.10.0-cdh4.6.0.tar&lt;/code&gt;到desktop1的&lt;code&gt;/opt&lt;/code&gt;，并解压缩。&lt;/p&gt;

&lt;h1 id=&quot;postgres&quot;&gt;安装postgres&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;创建数据库&lt;/h2&gt;

&lt;p&gt;这里创建数据库metastore并创建hiveuser用户，其密码为redhat。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	bash# sudo -u postgres psql
	bash$ psql
	postgres=# CREATE USER hiveuser WITH PASSWORD &#39;redhat&#39;;
	postgres=# CREATE DATABASE metastore owner=hiveuser;
	postgres=# GRANT ALL privileges ON DATABASE metastore TO hiveuser;
	postgres=# \q;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;初始化数据库&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;psql  -U hiveuser -d metastore
 \i /opt/hive-0.10.0-cdh4.6.0/scripts/metastore/upgrade/postgres/hive-schema-0.10.0.postgres.sql 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑postgresql配置文件(&lt;code&gt;/opt/PostgreSQL/9.1/data/pg_hba.conf&lt;/code&gt;)，修改访问权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;host    all             all             0.0.0.0/0            md5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改postgresql.conf&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	standard_conforming_strings = of
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;postgres-1&quot;&gt;重起postgres&lt;/h2&gt;

&lt;p&gt;拷贝postgres的jdbc驱动到&lt;code&gt;/opt/hive-0.10.0-cdh4.6.0/lib&lt;/code&gt;目录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;su -c &#39;/opt/PostgreSQL/9.1/bin/pg_ctl -D /opt/PostgreSQL/9.1/data restart&#39; postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;修改配置文件&lt;/h1&gt;

&lt;h2 id=&quot;hive-sitexml&quot;&gt;hive-site.xml&lt;/h2&gt;

&lt;p&gt;注意修改下面配置文件中postgres数据库的密码，注意配置&lt;code&gt;hive.aux.jars.path&lt;/code&gt;，在hive集成hbase时候需要从该路径家在hbase的一些jar文件。&lt;/p&gt;

&lt;p&gt;hive-site.xml文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;configuration&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;jdbc:postgresql://127.0.0.1/metastore&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;org.postgresql.Driver&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hiveuser&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;redhat&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;mapred.job.tracker&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;desktop1:8031&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.aux.jars.path&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;file:///opt/hive-0.10.0-cdh4.6.0/lib/zookeeper-3.4.5-cdh4.6.0.jar,
	file:///opt/hive-0.10.0-cdh4.6.0/lib/hive-hbase-handler-0.10.0-cdh4.6.0.jar,
	file:///opt/hive-0.10.0-cdh4.6.0/lib/hbase-0.94.15-cdh4.6.0.jar,
	file:///opt/hive-0.10.0-cdh4.6.0/lib/guava-11.0.2.jar&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.metastore.warehouse.dir&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/opt/data/warehouse-${user.name}&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.exec.scratchdir&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/opt/data/hive-${user.name}&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.querylog.location&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/opt/data/querylog-${user.name}&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.support.concurrency&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.zookeeper.quorum&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;desktop3,desktop4,desktop5,desktop6,desktop7&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.hwi.listen.host&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;desktop1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.hwi.listen.port&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;9999&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.hwi.war.file&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;lib/hive-hwi-0.10.0-cdh4.6.0.war&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;环境变量&lt;/h2&gt;

&lt;p&gt;参考&lt;a href=&quot;/hadoop/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html&quot;&gt;手动安装Cloudera Hadoop CDH&lt;/a&gt;中环境变量的设置。&lt;/p&gt;

&lt;h2 id=&quot;hive-1&quot;&gt;启动hive&lt;/h2&gt;

&lt;p&gt;在启动完之后，执行一些sql语句可能会提示错误，如何解决错误可以参考&lt;a href=&quot;http://kicklinux.com/hive-deploy/&quot;&gt;Hive安装与配置&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;hivehbase&quot;&gt;hive与hbase集成&lt;/h2&gt;

&lt;p&gt;在&lt;code&gt;hive-site.xml&lt;/code&gt;中配置&lt;code&gt;hive.aux.jars.path&lt;/code&gt;,在环境变量中配置hadoop、mapreduce的环境变量&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;异常说明&lt;/h1&gt;

&lt;h2 id=&quot;section-5&quot;&gt;异常1：&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;FAILED: Error in metadata: MetaException(message:org.apache.hadoop.hbase.ZooKeeperConnectionException: An error is preventing HBase from connecting to ZooKeeper
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：hadoop配置文件没有zk&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;异常2&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.hive.metastore.api.MetaException javax.jdo.JDODataStoreException: Error executing JDOQL query &quot;SELECT &quot;THIS&quot;.&quot;TBL_NAME&quot; AS NUCORDER0 FROM &quot;TBLS&quot; &quot;THIS&quot; LEFT OUTER JOIN &quot;DBS&quot; &quot;THIS_DATABASE_NAME&quot; ON &quot;THIS&quot;.&quot;DB_ID&quot; = &quot;THIS_DATABASE_NAME&quot;.&quot;DB_ID&quot; WHERE &quot;THIS_DATABASE_NAME&quot;.&quot;NAME&quot; = ? AND (LOWER(&quot;THIS&quot;.&quot;TBL_NAME&quot;) LIKE ? ESCAPE &#39;\\&#39; ) ORDER BY NUCORDER0 &quot; : ERROR: invalid escape string 建议：Escape string must be empty or one character..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考：https://issues.apache.org/jira/browse/HIVE-3994&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;异常3，以下语句没反应&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;select count(*) from hive_userinfo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;## 异常4&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(966)) - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (无法定位登录配置)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：hive中没有设置zk&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;异常5&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;hbase 中提示：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：cloudera hadoop lib中没有hadoop的native jar&lt;/p&gt;

&lt;h2 id=&quot;section-9&quot;&gt;异常6&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/v2/app/MRAppMaster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因：classpath没有配置正确，检查环境变量以及yarn的classpath&lt;/p&gt;

&lt;h1 id=&quot;section-10&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kicklinux.com/hive-deploy/&quot;&gt;Hive安装与配置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ccp.cloudera.com/display/CDH4DOC/Hive+Installation&quot;&gt;Hive Installation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2013/03/24/manual-install-Cloudera-hive-CDH.html</link>
      <guid>http://blog.javachen.com/2013/03/24/manual-install-Cloudera-hive-CDH.html</guid>
      <pubDate>2013-03-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>手动安装Cloudera HBase CDH</title>
      <description>&lt;p&gt;本文主要记录手动安装Cloudera HBase集群过程，环境设置及Hadoop安装过程见&lt;a href=&quot;/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html&quot;&gt;手动安装Cloudera Hadoop CDH&lt;/a&gt;,参考这篇文章，hadoop各个组件和jdk版本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	hadoop-2.0.0-cdh4.6.0
	hbase-0.94.15-cdh4.6.0
	hive-0.10.0-cdh4.6.0
	jdk1.6.0_38
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hadoop各组件可以在&lt;a href=&quot;http://archive.cloudera.com/cdh4/cdh/4/&quot;&gt;这里&lt;/a&gt;下载。&lt;/p&gt;

&lt;p&gt;集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	192.168.0.1        desktop1     NameNode、Hive、ResourceManager、impala
	192.168.0.2        desktop2     SSNameNode
	192.168.0.3        desktop3     DataNode、HBase、NodeManager、impala
	192.168.0.4        desktop4     DataNode、HBase、NodeManager、impala
	192.168.0.5        desktop5     DataNode、HBase、NodeManager、impala
	192.168.0.6        desktop6     DataNode、HBase、NodeManager、impala
	192.168.0.7        desktop7     DataNode、HBase、NodeManager、impala
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hbase&quot;&gt;安装HBase&lt;/h1&gt;

&lt;p&gt;HBase安装在desktop3、desktop4、desktop5、desktop6、desktop7节点上。&lt;/p&gt;

&lt;p&gt;上传hbase压缩包(hbase-0.94.15-cdh4.6.0.tar.gz)到desktop3上的/opt目录，先在desktop3上修改好配置文件，在同步到其他机器上。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;hbase-site.xml&lt;/code&gt;内容修改如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;	&amp;lt;configuration&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;hdfs://desktop1/hbase-${user.name}&amp;lt;/value&amp;gt;
	&amp;lt;/property&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
	&amp;lt;/property&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;hbase.tmp.dir&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;/opt/data/hbase-${user.name}&amp;lt;/value&amp;gt;
	&amp;lt;/property&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;desktop3,desktop4,desktop6,desktop7,desktop8&amp;lt;/value&amp;gt;
	&amp;lt;/property&amp;gt;
	&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;regionservers内容修改如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	desktop3
	desktop4
	desktop5
	desktop6
	desktop7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来将上面几个文件同步到其他各个节点（desktop4、desktop5、desktop6、desktop7）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop3 ~]# scp /opt/hbase-0.94.15-cdh4.6.0/conf/ desktop4:/opt/hbase-0.94.15-cdh4.6.0/conf/
	[root@desktop3 ~]# scp /opt/hbase-0.94.15-cdh4.6.0/conf/ desktop5:/opt/hbase-0.94.15-cdh4.6.0/conf/
	[root@desktop3 ~]# scp /opt/hbase-0.94.15-cdh4.6.0/conf/ desktop6:/opt/hbase-0.94.15-cdh4.6.0/conf/
	[root@desktop3 ~]# scp /opt/hbase-0.94.15-cdh4.6.0/conf/ desktop7:/opt/hbase-0.94.15-cdh4.6.0/conf/
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section&quot;&gt;环境变量&lt;/h1&gt;

&lt;p&gt;参考&lt;a href=&quot;/hadoop/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html&quot;&gt;手动安装Cloudera Hadoop CDH&lt;/a&gt;中环境变量的设置。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;启动脚本&lt;/h1&gt;

&lt;p&gt;在desktop3、desktop4、desktop5、desktop6、desktop7节点上分别启动hbase：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	start-hbase.sh 
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2013/03/24/manual-install-Cloudera-hbase-CDH.html</link>
      <guid>http://blog.javachen.com/2013/03/24/manual-install-Cloudera-hbase-CDH.html</guid>
      <pubDate>2013-03-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>手动安装Cloudera Hadoop CDH</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;安装版本&lt;/h1&gt;

&lt;p&gt;hadoop各个组件和jdk版本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	hadoop-2.0.0-cdh4.6.0
	hbase-0.94.15-cdh4.6.0
	hive-0.10.0-cdh4.6.0
	jdk1.6.0_38
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hadoop各组件可以在&lt;a href=&quot;http://archive.cloudera.com/cdh4/cdh/4/&quot;&gt;这里&lt;/a&gt;下载。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;安装前说明&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;确定安装目录为/opt&lt;/li&gt;
  &lt;li&gt;检查hosts文件是否设置集群各节点的hostname和ip映射&lt;/li&gt;
  &lt;li&gt;关闭每个节点的防火墙&lt;/li&gt;
  &lt;li&gt;设置每个节点时钟同步&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;规划&lt;/h1&gt;

&lt;p&gt;集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	192.168.0.1        desktop1     NameNode、Hive、ResourceManager、impala
	192.168.0.2        desktop2     SSNameNode
	192.168.0.3        desktop3     DataNode、HBase、NodeManager、impala
	192.168.0.4        desktop4     DataNode、HBase、NodeManager、impala
	192.168.0.5        desktop5     DataNode、HBase、NodeManager、impala
	192.168.0.6        desktop6     DataNode、HBase、NodeManager、impala
	192.168.0.7        desktop7     DataNode、HBase、NodeManager、impala
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;部署过程&lt;/h1&gt;
&lt;p&gt;## 系统和网络配置&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;修改每个节点的主机名称&lt;/p&gt;

    &lt;p&gt;例如在desktop1节点上做如下修改：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop1 ~]# cat /etc/sysconfig/network
	NETWORKING=yes
	HOSTNAME=desktop1
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;在每个节点上修改&lt;code&gt;/etc/hosts&lt;/code&gt;增加以下内容:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop1 ~]# cat /etc/hosts
	127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
	::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
	192.168.0.1		desktop1
	192.168.0.2		desktop2
	192.168.0.3		desktop3
	192.168.0.4		desktop4
	192.168.0.5		desktop5
	192.168.0.6		desktop6
	192.168.0.7		desktop7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改一台机器之后，可以使用scp同步到其他机器。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;配置ssh无密码登陆&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以下是设置desktop1上可以无密码登陆到其他机器上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop1 ~]# ssh-keygen
	[root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop2
	[root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop3
	[root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop4
	[root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop5
	[root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop6
	[root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop7
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;每台机器上关闭防火墙：&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop1 ~]# service iptables stop
	[root@desktop1 ~]# ssh desktop2 &#39;service iptables stop&#39;
	[root@desktop1 ~]# ssh desktop3 &#39;service iptables stop&#39;
	[root@desktop1 ~]# ssh desktop4 &#39;service iptables stop&#39;
	[root@desktop1 ~]# ssh desktop5 &#39;service iptables stop&#39;
	[root@desktop1 ~]# ssh desktop6 &#39;service iptables stop&#39;
	[root@desktop1 ~]# ssh desktop7 &#39;service iptables stop&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;hadoop&quot;&gt;安装Hadoop&lt;/h1&gt;
&lt;p&gt;## 配置Hadoop&lt;/p&gt;

&lt;p&gt;将jdk1.6.0_38.zip上传到/opt，并解压缩。&lt;/p&gt;

&lt;p&gt;将hadoop-2.0.0-cdh4.2.0.zip上传到/opt，并解压缩。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在NameNode上需要修改以下文件&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;core-site.xml fs.defaultFS指定NameNode文件系统，开启回收站功能。&lt;/li&gt;
  &lt;li&gt;hdfs-site.xml&lt;/li&gt;
  &lt;li&gt;dfs.namenode.name.dir指定NameNode存储meta和editlog的目录，&lt;/li&gt;
  &lt;li&gt;dfs.datanode.data.dir指定DataNode存储blocks的目录，&lt;/li&gt;
  &lt;li&gt;dfs.namenode.secondary.http-address指定Secondary NameNode地址。&lt;/li&gt;
  &lt;li&gt;开启WebHDFS。&lt;/li&gt;
  &lt;li&gt;slaves 添加DataNode节点主机&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：在desktop1节点上修改如下几个文件的内容：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;core-site.xml&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在该文件中修改fs.defaultFS指向desktop1节点，即配置desktop1为NameNode节点。&lt;/p&gt;

&lt;p&gt;修改后的core-site.xml(&lt;code&gt;/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/core-site.xml&lt;/code&gt;)目录如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
&amp;lt;!--fs.default.name for MRV1 ,fs.defaultFS for MRV2(yarn) --&amp;gt;
&amp;lt;property&amp;gt;
     &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
         &amp;lt;!--这个地方的值要和hdfs-site.xml文件中的dfs.federation.nameservices一致--&amp;gt;
     &amp;lt;value&amp;gt;hdfs://desktop1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;fs.trash.interval&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;10080&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;fs.trash.checkpoint.interval&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;10080&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;hdfs-site.xml&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;该文件主要设置数据副本保存份数，以及namenode、datanode数据保存路径(&lt;code&gt;/opt/data/hadoop-${user.name}&lt;/code&gt;)以及http-address。&lt;/p&gt;

&lt;p&gt;修改后的hdfs-site.xml(&lt;code&gt;/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/hdfs-site.xml&lt;/code&gt;)文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/opt/data/hadoop-${user.name}&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;dfs.namenode.http-address&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;desktop1:50070&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;dfs.namenode.secondary.http-address&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;desktop2:50090&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;dfs.webhdfs.enabled&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;masters&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置namenode和secondary namenode节点。&lt;/p&gt;

&lt;p&gt;修改后的masters(&lt;code&gt;/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/masters&lt;/code&gt;)文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;desktop1
desktop2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一行为namenode，第二行为secondary namenode。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;slaves&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置哪些机器上安装datanode节点。&lt;br /&gt;
修改后的slaves(&lt;code&gt;/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/slaves&lt;/code&gt;)文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;desktop3
desktop4
desktop5
desktop6
desktop7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来将上面几个文件同步到其他各个节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop2:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop3:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop4:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop5:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop6:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop7:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;mapreduce&quot;&gt;配置MapReduce&lt;/h2&gt;

&lt;p&gt;接下来还是在desktop1节点上修改以下几个文件：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;mapred-site.xml&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;配置使用yarn计算框架，以及jobhistory的地址。&lt;/p&gt;

&lt;p&gt;修改后的mapred-site.xml(&lt;code&gt;/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/mapred-site.xml&lt;/code&gt;)文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;mapreduce.jobhistory.address&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;desktop1:10020&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;mapreduce.jobhistory.webapp.address&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;desktop1:19888&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;yarn-site.xml&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;主要配置resourcemanager地址以及&lt;code&gt;yarn.application.classpath&lt;/code&gt;（这个路径很重要，要不然集成hive时候会提示找不到class）&lt;/p&gt;

&lt;p&gt;修改后的yarn-site.xml(&lt;code&gt;/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/yarn-site.xml&lt;/code&gt;)文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.resource-tracker.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;desktop1:8031&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;desktop1:8032&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.scheduler.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;desktop1:8030&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.admin.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;desktop1:8033&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.webapp.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;desktop1:8088&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;description&amp;gt;Classpath for typical applications.&amp;lt;/description&amp;gt;
    &amp;lt;name&amp;gt;yarn.application.classpath&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,
	$HADOOP_COMMON_HOME/share/hadoop/common/lib/*,
	$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,
	$YARN_HOME/share/hadoop/yarn/*,$YARN_HOME/share/hadoop/yarn/lib/*,
	$YARN_HOME/share/hadoop/mapreduce/*,$YARN_HOME/share/hadoop/mapreduce/lib/*&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce.shuffle&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.hadoop.mapred.ShuffleHandler&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.local-dirs&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/opt/data/yarn/local&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.log-dirs&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/opt/data/yarn/logs&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;description&amp;gt;Where to aggregate logs&amp;lt;/description&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.remote-app-log-dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/opt/data/yarn/logs&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.app.mapreduce.am.staging-dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/user&amp;lt;/value&amp;gt;
 &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样地，将上面2个文件同步到其他各个节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop2:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop3:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop4:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop5:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop6:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
	[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop7:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;修改环境变量&lt;/h2&gt;

&lt;p&gt;修改&lt;code&gt;/root/.bashrc&lt;/code&gt;环境变量，并将其同步到其他几台机器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@desktop1 ~] # cat .bashrc 
# .bashrc
alias rm=&#39;rm -i&#39;
alias cp=&#39;cp -i&#39;
alias mv=&#39;mv -i&#39;
# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
# User specific environment and startup programs
export LANG=zh_CN.utf8
export JAVA_HOME=/opt/jdk1.6.0_38
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=./:$JAVA_HOME/lib:$JRE_HOME/lib:$JRE_HOME/lib/tools.jar
export HADOOP_HOME=/opt/hadoop-2.0.0-cdh4.2.0
export HIVE_HOME=/opt/hive-0.10.0-cdh4.2.0
export HBASE_HOME=/opt/hbase-0.94.2-cdh4.2.0
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export HADOOP_YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HDFS_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置文件之后，使其生效。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@desktop1 ~]# source .bashrc 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将该文件同步到其他各个节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop1 ~]# scp /root/.bashrc desktop2:/root
	[root@desktop1 ~]# scp /root/.bashrc desktop3:/root
	[root@desktop1 ~]# scp /root/.bashrc desktop4:/root
	[root@desktop1 ~]# scp /root/.bashrc desktop5:/root
	[root@desktop1 ~]# scp /root/.bashrc desktop6:/root
	[root@desktop1 ~]# scp /root/.bashrc desktop7:/root
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并且使各个节点的环境变量生效：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	[root@desktop1 ~]# ssh desktop2 &#39;source .bashrc&#39;
	[root@desktop1 ~]# ssh desktop3 &#39;source .bashrc&#39;
	[root@desktop1 ~]# ssh desktop4 &#39;source .bashrc&#39;
	[root@desktop1 ~]# ssh desktop5 &#39;source .bashrc&#39;
	[root@desktop1 ~]# ssh desktop6 &#39;source .bashrc&#39;
	[root@desktop1 ~]# ssh desktop7 &#39;source .bashrc&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;启动脚本&lt;/h2&gt;

&lt;p&gt;第一次启动hadoop需要先格式化NameNode，该操作只做一次。当修改了配置文件时，需要重新格式化&lt;/p&gt;

&lt;p&gt;在desktop1上格式化：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@desktop1 hadoop]hadoop namenode -format
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在desktop1上启动hdfs：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@desktop1 hadoop]#start-dfs.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在desktop1上启动mapreduce：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@desktop1 hadoop]#start-yarn.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在desktop1上启动historyserver：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@desktop1 hadoop]#mr-jobhistory-daemon.sh start historyserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看MapReduce：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://desktop1:8088/cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://desktop2:8042/
http://desktop2:8042/node
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;检查集群进程&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;[root@desktop1 ~]# jps
5389 NameNode
5980 Jps
5710 ResourceManager
7032 JobHistoryServer
[root@desktop2 ~]# jps
3187 Jps
3124 SecondaryNameNode
[root@desktop3 ~]# jps
3187 Jps
3124 DataNode
5711 NodeManager
&lt;/code&gt;&lt;/pre&gt;
</description>
      <link>http://blog.javachen.com/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html</link>
      <guid>http://blog.javachen.com/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html</guid>
      <pubDate>2013-03-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>【笔记】Hadoop安装部署</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;安装虚拟机&lt;/h1&gt;

&lt;p&gt;使用VirtualBox安装rhel6.3，存储为30G，内存为1G，并使用复制克隆出两个新的虚拟机，这样就存在3台虚拟机，设置三台虚拟机的主机名称，如：rhel-june、rhel-june-1、rhel-june-2&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;配置网络&lt;/h1&gt;

&lt;p&gt;a. VirtualBox全局设定-网络中添加一个新的连接：vboxnet0&lt;/p&gt;

&lt;p&gt;b. 设置每一个虚拟机的网络为Host-Only&lt;/p&gt;

&lt;p&gt;c.分别修改每个虚拟机的ip，DHCP或手动设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim /etc/sysconfig/network-scripts/ifcfg-eth0
vim /etc/udev/rules.d/70-persistent-net.rules  #删掉第一个，修改第二个名字为eth0
start_udev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;d.修改主机名&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim /etc/sysconfig/network
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;e.每个虚拟机中修改hosts：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.56.100 rhel-june
192.168.56.101 rhel-june-1
192.168.56.102 rhel-june-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后机器列表为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rhel-june:   192.168.56.100
rhel-june-1: 192.168.56.101
rhel-june-2: 192.168.56.102
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;机群规划&lt;/h1&gt;

&lt;p&gt;版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoop:1.1.1
JDK:1.6.0_38
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群各节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NameNode:192.168.56.100
NameSecondary:192.168.56.100
DataNode:192.168.56.101
DataNode:192.168.56.102
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;安装过程&lt;/h1&gt;

&lt;p&gt;a.将hadoop压缩包解压缩到/opt目录&lt;/p&gt;

&lt;p&gt;b.修改以下配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;core-site.xml
hdfs-site.sml
mapred-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c.设置master、slaves文件&lt;/p&gt;

&lt;p&gt;master中内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	192.168.56.100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;slaves中内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	192.168.56.101
	192.168.56.102
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;d.设置环境变量&lt;/p&gt;

&lt;p&gt;方便执行java命令及hadoop命令. 使用root登录，&lt;code&gt;vi ~/.bash_profile&lt;/code&gt;追加下列信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_HOME=/opt/jdk1.6.0_38
export HADOOP_INSTALL=/opt/hadoop-1.1.1
export PATH=$PATH:$HADOOP_INSTALL/bin:$JAVA_HOME/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;e.修改hadoop脚本中&lt;code&gt;JAVA_HOME&lt;/code&gt;：&lt;code&gt;/opt/hadoop-1.1.1/conf/hadoop-env.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;f.格式化namenode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoop namenode -format
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;g.启动hdfs集群&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sh /opt/hadoop-1.1.1/bin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;h.查看节点进程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jps
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;查看状态&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;http://rhel-june:50030/
http://rhel-june:50070/
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2013/03/08/note-about-installing-hadoop-cluster.html</link>
      <guid>http://blog.javachen.com/2013/03/08/note-about-installing-hadoop-cluster.html</guid>
      <pubDate>2013-03-08T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>2012年度总结</title>
      <description>&lt;p&gt;2012年是在公司工作的第二年，在总结2012年的得与失的时候，有必要和《2011的度年终总结》相比较，在比较中审视自己在2012年是否有改进2011年存在的不足、是否有实现2011年定下的2012年工作计划。&lt;br /&gt;
以下是2012年相对于2011年的一些变化。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2011年，在调研云计算产品过程中，深刻的意识到自身在linux方面存在的不足；2012年，熟悉了基本的linux命令，能够读懂并编写简单shell脚本；&lt;/li&gt;
  &lt;li&gt;2011年，工作环境是win7+fedora；2012年，一直使用fedora操作系统工作、编码；&lt;/li&gt;
  &lt;li&gt;2011年，较多的时间花在编写代码、完成开发任务上；2012年，更多的时间花在学习架构的设计、系统的运维、项目的管理上，视野不再局限于开发、精力不再局限于编码。&lt;/li&gt;
  &lt;li&gt;2011年，在工作中没有及时提交项目周报，没有及时的跟踪、检查分配下去的任务完成情况，对新人的指导不够；2012年，没有写过项目周报，做到了及是跟踪、检查分配下去的任务完成情况；&lt;/li&gt;
  &lt;li&gt;2011年，博客文章篇数较少，平时的总结与分享不够积极；2012年，很少有时间写技术方面的博客；&lt;/li&gt;
  &lt;li&gt;2011年，在与客户的交流中底气不足、表达能力不够；2012年，还是发现自己与客户交流中胆怯、没有底气；&lt;/li&gt;
  &lt;li&gt;2011年，希望能够将Pentaho的咨询服务工作更多交给其他人完成；2012年，发现大部分的工作还是落在自己身上一个人去完成，没有发挥其他人员的作用；&lt;/li&gt;
  &lt;li&gt;2011年，希望2012年能够深入理解Spring、Jboss、Pentaho、缓存、云计算、架构等技术；2012年，了解gemfire、infinispan、jboss cache、cassandra等分布式缓存的实现及原理，但每一个方面都没有时间去深入研究和学习；&lt;/li&gt;
  &lt;li&gt;2011年，公司在代码复查方面做的不够；2012年，这方面还是做的不够；&lt;/li&gt;
  &lt;li&gt;2011年，项目开发方面没有形成一套成型的开发框架；2012年，还是没有看到一个成熟、易用、简单的开发框架以及相配套开发文档；&lt;/li&gt;
  &lt;li&gt;2011年，项目于项目之间在一些同时使用的相关技术上面的沟通于交流做的不够；2012年，团队在项目上还是缺少沟通交流，尤其体现在XXXX项目网站开发上。&lt;/li&gt;
  &lt;li&gt;2011年，花了一些时间在Pentaho上，并希望2012年能够创建一个Pentaho社区、一个QQ分享群；2012年，Pentaho方面基本上没有投入；&lt;/li&gt;
  &lt;li&gt;2011年，编写文档时候，没有可参考的模版，导致文档编写不规范；2012年，每次写文档时都要去找文档模版；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2012年参与了XXXXX项目、cassandra项目，。。。。。。此处省略314.15926个字。&lt;/p&gt;

&lt;p&gt;2012年，公司也存在一些不足：上级对下级、项目经理对团队人员了解不足，不知道其工作上、生活上的内心想法以及遇到何种困难；多数情况下，团队自我要求低，积极性不高，没有生机与活力；对新人能力审核不够，对新人培养不够重视，对新人的存在感不够关注；在各个项目的人员安排及使用上、任务分配和工作计划上不合理，导致经常被动加班、熬夜等等。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;2013年工作计划：&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;通过CE考试，熟练掌握shell编程；&lt;/li&gt;
  &lt;li&gt;做好项目管理者的角色，培养新人，提高团队人员编码、处理问题的能力；&lt;/li&gt;
  &lt;li&gt;深入理解、学习cassandra源码、原理以及cassandra的运维；&lt;/li&gt;
  &lt;li&gt;学习hadoop的安装、部署、原理、开发及运维，掌握kettle和nosql的集成，希望积累几个hadoop项目经验；&lt;/li&gt;
  &lt;li&gt;学习分布式缓存理论知识，阅读源代码，完善缓存系统的监控及运维&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在经历了2011年和2012年之后，2013年希望自己能够专注细节，深入理解，在技术、管理、交际方面有所成长；希望公司能够重视对团队的培养，能够规范各种规章制度，能够更上一层楼！&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2013/02/20/summary-of-the-work-in-2012.html</link>
      <guid>http://blog.javachen.com/2013/02/20/summary-of-the-work-in-2012.html</guid>
      <pubDate>2013-02-20T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Octopress将博客从wordpress迁移到GitHub</title>
      <description>&lt;h1 id=&quot;step1---octopress&quot;&gt;Step1 - 在本机安装Octopress&lt;/h1&gt;

&lt;p&gt;首先，必须先在本机安装配置&lt;a href=&quot;http://git-scm.com/&quot;&gt;Git&lt;/a&gt;和&lt;a href=&quot;https://rvm.beginrescueend.com/rvm/install/&quot;&gt;Ruby&lt;/a&gt;,Octopress需要Ruby版本至少为1.9.2。你可以使用&lt;a href=&quot;http://rvm.beginrescueend.com/&quot;&gt;RVM&lt;/a&gt;或&lt;a href=&quot;https://github.com/sstephenson/rbenv&quot;&gt;rbenv&lt;/a&gt;安装ruby，安装方法见Octopress官方文档：&lt;a href=&quot;http://octopress.org/docs/setup/&quot;&gt;http://octopress.org/docs/setup/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;我使用rvm安装：&lt;br /&gt;
    rvm install 1.9.2 &amp;amp;&amp;amp; rvm use 1.9.2&lt;br /&gt;
安装完之后可以查看ruby版本：&lt;br /&gt;
    ruby –version&lt;br /&gt;
结果为：&lt;br /&gt;
    ruby 1.9.2p320 (2012-04-20 revision 35421) [x86_64-linux]&lt;/p&gt;

&lt;p&gt;然后需要从github下载Octopress：&lt;br /&gt;
    git clone git://github.com/imathis/octopress.git octopress&lt;/p&gt;

&lt;p&gt;因为我fork了Octopress，并在配置文件上做了一些修改，故我从我的仓库地址下载Octopress，命令如下：&lt;br /&gt;
    git clone git@github.com:javachen/octopress.git&lt;br /&gt;
运行上面的代码后，你会看到：&lt;br /&gt;
	Cloning into ‘octopress’…&lt;br /&gt;
	remote: Counting objects: 6579, done.&lt;br /&gt;
	remote: Compressing objects: 100% (2361/2361), done.&lt;br /&gt;
	remote: Total 6579 (delta 3773), reused 6193 (delta 3610)&lt;br /&gt;
	Receiving objects: 100% (6579/6579), 1.34 MiB | 35 KiB/s, done.&lt;br /&gt;
	Resolving deltas: 100% (3773/3773), done.&lt;/p&gt;

&lt;p&gt;接下来进入octopress：&lt;br /&gt;
	cd octopress&lt;/p&gt;

&lt;p&gt;接下来安装依赖：&lt;br /&gt;
	gem install bundler&lt;br /&gt;
	rbenv rehash    # If you use rbenv, rehash to be able to run the bundle command&lt;br /&gt;
	bundle install&lt;/p&gt;

&lt;p&gt;安装Octopress默认的主题：&lt;br /&gt;
	rake install&lt;/p&gt;

&lt;p&gt;你也可以安装自定义的主题，blog为主题名称：&lt;br /&gt;
	rake install[‘blog’]&lt;/p&gt;

&lt;p&gt;至此，Octopress所需的环境已经搭建成功。&lt;/p&gt;

&lt;h1 id=&quot;step2---github-pages&quot;&gt;Step2 - 连接GitHub Pages&lt;/h1&gt;
&lt;p&gt;首先，你得有一个GitHub的帐号，并且已经创建了一个新的Repository。如果你准备用自己的域名的话，Repository的名称可以随便取，不过正常人在正常情况下，一般都是以域名取名的。如果你没有自己的域名，GitHub是提供二级域名使用的，但是你得把Repository取名为&lt;code&gt;你的帐号.github.com&lt;/code&gt;，并且，部署的时候会占用你的master分支。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tips：&lt;/em&gt;&lt;br /&gt;
如果用自己的一级域名，记得把source/CNAME文件内的域名改成你的一级域名，还有在dns管理中把域名的A Record指向IP：207.97.227.245；&lt;br /&gt;
如果用自己的二级域名，记得把source/CNAME文件内的域名改成你的二级域名，还有在dns管理中把域名的CNAME Record指向网址：charlie.github.com；&lt;br /&gt;
	echo ‘your-domain.com’ » source/CNAME&lt;br /&gt;
如果用GitHub提供的二级域名，记得把source/CNAME删掉。&lt;/p&gt;

&lt;p&gt;完成上述准备工作后，运行：&lt;br /&gt;
	rake setup_github_pages&lt;br /&gt;
它会提示你输入有读写权限的Repository Url，这个在GitHub上可以找到。Url形如：https://github.com/javachen/javachen.github.com.git，javachen.github.com是我的Repository的名称。&lt;/p&gt;

&lt;h1 id=&quot;step3---&quot;&gt;Step3 - 配置你的博客&lt;/h1&gt;
&lt;p&gt;需要配置博客url、名称、作者、rss等信息。&lt;br /&gt;
	url: http://javachen.github.com&lt;br /&gt;
	title: JavaChen on Java&lt;br /&gt;
	subtitle: Just some random thoughts about technology,Java and life.&lt;br /&gt;
	author: javachen&lt;br /&gt;
	simple_search: http://google.com/search&lt;br /&gt;
	description:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;date_format: &quot;%Y年%m月%d日&quot;

subscribe_rss: /atom.xml
subscribe_email:
email:

# 如果你使用的是一个子目录，如http://site.com/project，则设置为&#39;root: /project&#39;
root: /
# 文章标题格式
permalink: /:year/:month/:day/:title/
source: source
destination: public
plugins: plugins
code_dir: downloads/code
# 分类存放路径
category_dir: categories
markdown: rdiscount
pygments: false # default python pygments have been replaced by pygments.rb
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step4---&quot;&gt;Step4 - 部署&lt;/h1&gt;

&lt;p&gt;先把整个项目静态化，然后再部署到GitHub：&lt;br /&gt;
	rake generate&lt;br /&gt;
	rake deploy&lt;br /&gt;
当你看到“Github Pages deploy complete”后，就表示你大功已成。Enjoy!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tips：&lt;/em&gt;&lt;br /&gt;
Octopress提供的所有rake方法，可以运行&lt;code&gt;rake -T&lt;/code&gt;查看。&lt;br /&gt;
如果在执行上述命令中ruby报错，则需要一一修复错误，这一步是没有接触过ruby的人比较苦恼的。&lt;/p&gt;

&lt;h1 id=&quot;step5---wordpressoctopress&quot;&gt;Step5 - 从Wordpress迁移到Octopress&lt;/h1&gt;
&lt;p&gt;## 备份&lt;br /&gt;
### 备份评论内容&lt;br /&gt;
Octopress由于是纯静态，所以没有办法存储用户评论了，我们可以使用DISQUS提供的“云评论”服务。首先安装DISQUS的WordPress插件，在插件设置中我们可以将现有的评论内容导入到DISQUS中。DISQUS处理导入数据的时间比较长，往往需要24小时甚至以上的时间。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;备份文章内容&lt;/h3&gt;
&lt;p&gt;在WordPress后台我们可以将整站数据备份成一个.xml文件下载下来。同时，我原先文章中的图片都是直接在Wordpress后台上传的，所以要把服务器上&lt;code&gt;wp-content/uploads&lt;/code&gt;下的所有文件备份下来。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;迁移&lt;/h2&gt;
&lt;p&gt;### 迁移文章&lt;br /&gt;
jekyll本身提供了一个从WordPress迁移文章的工具，不过对中文实在是不太友好。这里我使用了YORKXIN的修改版本。将上面备份的wordpress.xml放到Octopress根目录，把脚本放到新建的utils目录中，然后运行：&lt;br /&gt;
	ruby -r “./utils/wordpressdotcom.rb” -e “Jekyll::WordpressDotCom.process”&lt;br /&gt;
于是转换好的文章都放进source目录了。&lt;/p&gt;

&lt;h3 id=&quot;url&quot;&gt;迁移URL&lt;/h3&gt;
&lt;p&gt;迁移URL，便是要保证以前的文章链接能够自动重定向到新的链接上。这样既能保证搜索引擎的索引不受影响，也是一项对读者负责任的行为是吧。不过这是一项挺麻烦的事情。&lt;/p&gt;

&lt;p&gt;幸好我当初建立WordPress的时候就留下了后路。原先网站的链接是这样的：&lt;br /&gt;
	http://XXXXXXXXX.com/[year]/[month]/[the-long-long-title].html&lt;br /&gt;
	http://XXXXXXXXX.com/page/xx/&lt;br /&gt;
	http://XXXXXXXXX.com/category/[category-name]/&lt;br /&gt;
这样的格式是比较容易迁移的。如果原先的文章URL是带有数字ID的话，只能说声抱歉了。到_config.yml里面设置一下新站点的文章链接格式，跟原先的格式保持一致：&lt;br /&gt;
	permalink: /:year/:month/:title/&lt;br /&gt;
	category_dir: category&lt;br /&gt;
	pagination_dir:  # 留空&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;迁移评论&lt;/h3&gt;
&lt;p&gt;既然做好了301，那么迁移评论就显得非常简单了。登录DISQUS后台，进入站点管理后台的“Migrate Threads”栏目，那里有一个“Redirect Crawler”的功能，便是自动跟随301重定向，将评论指向新的网址。点一下那个按钮就大功告成。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;迁移图片&lt;/h3&gt;
&lt;p&gt;可以参考&lt;a href=&quot;http://log4d.com/2012/05/image-host/&quot;&gt;使用独立图床子域名&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;step6---&quot;&gt;Step6 - 再次部署&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;rake generate
rake deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-4&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Octopress Setup： http://octopress.org/docs/setup/&lt;/li&gt;
  &lt;li&gt;Octopress Deploying：http://octopress.org/docs/deploying/&lt;/li&gt;
  &lt;li&gt;Blog = GitHub + Octopress：http://mrzhang.me/blog/blog-equals-github-plus-octopress.html&lt;/li&gt;
  &lt;li&gt;从Wordpress迁移到Octopress：http://blog.dayanjia.com/2012/04/migration-to-octopress-from-wordpress/&lt;/li&gt;
  &lt;li&gt;使用独立图床子域名：http://log4d.com/2012/05/image-host/ http://log4d.com/2012/05/image-host/&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress.html</link>
      <guid>http://blog.javachen.com/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress.html</guid>
      <pubDate>2012-06-03T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Kettle dependency management</title>
      <description>&lt;p&gt;pentaho的项目使用了ant和ivy解决项目依赖,所以必须编译源码需要ivy工具.直接使用ivy编译pentaho的bi server项目,一直没有编译成功.&lt;br /&gt;
使用ivy编译kettle的源代码却是非常容易的事情.&lt;/p&gt;

&lt;p&gt;该篇文章翻译并参考了Will Gorman在pentaho的wiki上添加的&lt;a href=&quot;http://wiki.pentaho.com/display/EAI/Kettle+dependency+management&quot; target=&quot;_blank&quot;&gt;Kettle dependency management&lt;/a&gt;,文章标题没作修改.&lt;br /&gt;
编写此文,是为了记录编译kettle源码的方法和过程.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;以下是对原文的一个简单翻译.&lt;/strong&gt;
将kettle作为一个产品发行是一个很有趣的事情.有很多来自于pentaho其他项目(其中有一些有依赖于kettle)的jar包被导入到kettle.这些jar包必须在发行的时候构建并且加入到kettle中.如果一个核心的库被更新了,我们必须将其导入到kettle中(如果有必要).bi服务器,pentaho报表以及pentaho元数据编辑器都将kettle作为一个服务/引擎资源而被构建的.自从我们已经将这些jar导入到我们的源码仓库,这些项目必须使用ivy明确列出kettle以及他的依赖.当kettle的依赖变化的时候,我们必须审查libext文件是否需要更新.&lt;/p&gt;

&lt;p&gt;pentaho创建了一系列的脚本来自动化的安装ivy,解决jar(或者是artifacts),构建并发行artifacts.kettle已经升级使用subfloor(简单的意味着build.xml继承自subfloor的构建脚本).subfloor使用ivy从pentaho仓库()或者ibiblio maven2仓库来获取跟新jar.ibiblio仓库用于大多数第三方的jar文件(如apache-commons).pentaho仓库用于在线的pentaho项目或者一些比在ibiblio的三方库.为了解决kettle的依赖,我们不得不在ivy.xml里创建一个清单.这个文件明确地列出每一个没有传递依赖的jar文件.这意味着libext文件的映射在ivy.xml中是一对一的.
&lt;!--more--&gt;
&lt;strong&gt;关于Ivy&lt;/strong&gt;
&lt;a href=&quot;http://ant.apache.org/ivy/&quot; target=&quot;_blank&quot;&gt;Apache Ivy™&lt;/a&gt;是一个流行的致力于灵活性和简单性的依赖管理工具.更多的参考:&lt;a href=&quot;http://ant.apache.org/ivy/features.html&quot; target=&quot;_blank&quot;&gt;enterprise features&lt;/a&gt;, &lt;a href=&quot;http://ant.apache.org/ivy/testimonials.html&quot; target=&quot;_blank&quot;&gt;what people say about it&lt;/a&gt;, 以及 &lt;a href=&quot;http://ant.apache.org/ivy/history/latest-milestone/index.html&quot; target=&quot;_blank&quot;&gt;how it can improve your build system&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在kettle中使用ivyIDE&lt;/strong&gt;
首先,从svn上下载kettle的源代码:
&lt;pre&gt;
svn://source.pentaho.org/svnkettleroot/Kettle/trunk
&lt;/pre&gt;
如果你想在Eclipse上使用&lt;a href=&quot;http://ant.apache.org/ivy/ivyde/download.cgi&quot; target=&quot;_blank&quot;&gt;ivyde plugin&lt;/a&gt;.&lt;br /&gt;
请参考相关文章安装该插件.&lt;/p&gt;

&lt;p&gt;如果你不想使用ivyde,你可以简单快速并且容易的开始并编译代码.&lt;br /&gt;
1.执行&lt;code&gt;ant resolve&lt;/code&gt;,这个命令将会创建一个叫做resolved-libs的文件夹.&lt;br /&gt;
2.使用下面命令更新classpath &lt;br /&gt;
  a.手动的添加这些jar文件到你的ide的classpath&lt;br /&gt;
  b.执行ant create-dot-classpath,将会修改你的.classpath文件(注意刷新项目以使改变生效)&lt;br /&gt;
注意:kettle项目中的构建脚本会自动安装ivy插件.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;构建Kettle&lt;/strong&gt;
你可以下载kettle源代码然后立即执行&lt;code&gt;ant distrib&lt;/code&gt;命令&lt;br /&gt;
或者你可以在ide中导入下载的kettle工程,然后按照你的操作系统(默认的是Windows 32-bit)版本修改依赖的swt.jar文件.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ivy中未完成的&lt;/strong&gt;
&lt;strong&gt;pentaho-database-&lt;/strong&gt;这是一个依赖kettle-db的常用项目,但又被kettle-ui使用.这样会导致循环依赖,将来可能会将其引入到kettle项目或是从该项目中去掉对kettle的依赖.
&lt;strong&gt;swt-&lt;/strong&gt;swt文件目前没有包括在ivy.xml文件中
&lt;strong&gt;library configurations-&lt;/strong&gt;每一个kettle库(kettle-db,kettle-core等等)应该在ivy.xml中有他自己的依赖.这些库应该继承一些特定的依赖,而取代继承整个kettle依赖.
&lt;strong&gt;checked-in plugins-&lt;/strong&gt;当前引入的插件如;DummyJob, DummyPlugin, S3CsvInput, ShapeFileReader3,versioncheck应该都移到ivy的plugin配置中.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考文章&lt;/strong&gt;
&lt;a href=&quot;http://wiki.pentaho.com/display/EAI/Kettle+dependency+management&quot; target=&quot;_blank&quot;&gt;Kettle dependency management&lt;/a&gt;
&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2012/04/13/kettle-dependency-management.html</link>
      <guid>http://blog.javachen.com/2012/04/13/kettle-dependency-management.html</guid>
      <pubDate>2012-04-13T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>哈希表</title>
      <description>&lt;h1 id=&quot;section&quot;&gt;定义&lt;/h1&gt;

&lt;p&gt;一般的线性表、树，数据在结构中的相对位置是随机的，即和记录的关键字之间不存在确定的关系，因此，在结构中查找记录时需进行一系列和关键字的比较。这一类查找方法建立在“比较“的基础上，查找的效率依赖于查找过程中所进行的比较次数。 若想能直接找到需要的记录，必须在记录的存储位置和它的关键字之间建立一个确定的对应关系f，使每个关键字和结构中一个唯一的存储位置相对应，这就是哈希表。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;哈希表&lt;/code&gt;又称散列表。&lt;em&gt;哈希表存储的基本思想是&lt;/em&gt;：以数据表中的每个记录的关键字 k为自变量，通过一种函数H(k)计算出函数值。把这个值解释为一块连续存储空间（即&lt;code&gt;数组空间&lt;/code&gt;）的单元地址（即&lt;code&gt;下标&lt;/code&gt;），将该记录存储到这个单元中。在此称该函数H为哈希函数或散列函数。按这种方法建立的表称为&lt;code&gt;哈希表&lt;/code&gt;或&lt;code&gt;散列表&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;哈希表是一种数据结构，它可以提供快速的插入操作和查找操作。&lt;/p&gt;

&lt;p&gt;哈希表是基于数组结构实现的，所以它也存在一些&lt;strong&gt;缺点&lt;/strong&gt;： 数组创建后难于扩展，某些哈希表被基本填满时，性能下降得非常严重。 这个问题是哈希表不可避免的，即冲突现象：对不同的关键字可能得到同一哈希地址。 所以在以下情况下可以优先考虑使用哈希表： &lt;strong&gt;不需要有序遍历数据，并且可以提前预测数据量的大小&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;冲突&lt;/h1&gt;

&lt;p&gt;理想情况下，哈希函数在关键字和地址之间建立了一个一一对应关系，从而使得查找只需一次计算即可完成。由于关键字值的某种随机性，使得这种一一对应关系难以发现或构造。因而可能会出现不同的关键字对应一个存储地址。即k1≠k2，但H(k1)=H(k2)，这种现象称为冲突。&lt;br /&gt;
把这种具有不同关键字值而具有相同哈希地址的对象称&lt;code&gt;同义词&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在大多数情况下，冲突是不能完全避免的。这是因为所有可能的关键字的集合可能比较大，而对应的地址数则可能比较少。&lt;/p&gt;

&lt;p&gt;对于哈希技术，主要研究两个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;（1）如何设计哈希函数以使冲突尽可能少地发生。&lt;/li&gt;
  &lt;li&gt;（2）发生冲突后如何解决。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;哈希函数的构造方法&lt;/h1&gt;

&lt;p&gt;构造好的哈希函数的方法，应能使冲突尽可能地少，因而应具有较好的随机性。这样可使一组关键字的散列地址均匀地分布在整个地址空间。根据关键字的结构和分布的不同，可构造出许多不同的哈希函数。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;1）．直接定址法&lt;/h2&gt;

&lt;p&gt;直接定址法是以关键字k本身或关键字加上某个数值常量c作为哈希地址的方法。&lt;/p&gt;

&lt;p&gt;该哈希函数H(k)为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;H(k)=k+c (c≥0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种哈希函数计算简单，并且不可能有冲突发生。当关键字的分布基本连续时，可使用直接定址法的哈希函数。否则，若关键字分布不连续将造成内存单元的大量浪费&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;2）．除留余数法&lt;/h2&gt;

&lt;p&gt;取关键字k除以哈希表长度m所得余数作为哈希函数地址的方法。即：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;H(k)=k％m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一种较简单、也是较常见的构造方法。&lt;/p&gt;

&lt;p&gt;这种方法的关键是选择好哈希表的长度m。使得数据集合中的每一个关键字通过该函数转化后映射到哈希表的任意地址上的概率相等。&lt;br /&gt;
理论研究表明，在m取值为素数（质数）时，冲突可能性相对较少。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;3）．平方取中法&lt;/h2&gt;

&lt;p&gt;取关键字平方后的中间几位作为哈希函数地址（若超出范围时，可再取模）。&lt;/p&gt;

&lt;p&gt;设有一组关键字ABC，BCD,CDE，DEF，……其对应的机内码如表所示。假定地址空间的大小为1000，编号为0-999。现按平方取中法构造哈希函数，则可取关键字机内码平方后的中间三位作为存储位置。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;4）．折叠法&lt;/h2&gt;

&lt;p&gt;这种方法适合在关键字的位数较多，而地址区间较小的情况。&lt;/p&gt;

&lt;p&gt;将关键字分隔成位数相同的几部分。然后将这几部分的叠加和作为哈希地址（若超出范围，可再取模）。&lt;/p&gt;

&lt;p&gt;例如，假设关键字为某人身份证号码430104681015355，则可以用4位为一组进行叠加。即有5355+8101+1046+430=14932，舍去高位。 则有H(430104681015355)=4932 为该身份证关键字的哈希函数地址。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;5）．数值分析法&lt;/h2&gt;

&lt;p&gt;若事先知道所有可能的关键字的取值时，可通过对这些关键字进行分析，发现其变化规律，构造出相应的哈希函数。&lt;/p&gt;

&lt;p&gt;例：对如下一组关键字通过分析可知：&lt;/p&gt;

&lt;p&gt;每个关键字从左到右的第l，2，3位和第6位取值较集中，不宜作哈希地址。 剩余的第4，5，7和8位取值较分散，可根据实际需要取其中的若干位作为哈希地址。&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;6）. 随机数法&lt;/h2&gt;

&lt;p&gt;选择一个随机函数，取关键字的随机函数值为它的哈希地址，即&lt;code&gt;H(key)＝random(key)&lt;/code&gt;，其中random为随机函数。&lt;/p&gt;

&lt;h2 id=&quot;fibonacci&quot;&gt;7）. 斐波那契（Fibonacci）散列法&lt;/h2&gt;

&lt;p&gt;平方散列法的缺点是显而易见的，所以我们能不能找出一个理想的乘数，而不是拿value本身当作乘数呢？答案是肯定的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1，对于16位整数而言，这个乘数是40503&lt;/li&gt;
  &lt;li&gt;2，对于32位整数而言，这个乘数是2654435769&lt;/li&gt;
  &lt;li&gt;3，对于64位整数而言，这个乘数是11400714819323198485&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这几个“理想乘数”是如何得出来的呢？这跟一个法则有关，叫黄金分割法则，而描述黄金分割法则的最经典表达式无疑就是著名的斐波那契数列，如果你还有兴趣，就到网上查找一下“斐波那契数列”等关键字，我数学水平有限，不知道怎么描述清楚为什么，另外斐波那契数列的值居然和太阳系八大行星的轨道半径的比例出奇吻合，很神奇，对么？&lt;/p&gt;

&lt;p&gt;对我们常见的32位整数而言，公式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;index = (value * 2654435769) &amp;gt;&amp;gt; 28
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果用这种斐波那契散列法的话，那我上面的图就变成这样了：&lt;/p&gt;

&lt;h1 id=&quot;section-9&quot;&gt;冲突的解决方法&lt;/h1&gt;

&lt;p&gt;假设哈希表的地址范围为&lt;code&gt;0～m-l&lt;/code&gt;，当对给定的关键字k，由哈希函数&lt;code&gt;H(k)&lt;/code&gt;算出的哈希地址为&lt;code&gt;i（0≤i≤m-1）&lt;/code&gt;的位置上已存有记录，这种情况就是冲突现象。 处理冲突就是为该关键字的记录找到另一个“空”的哈希地址。即通过一个新的哈希函数得到一个新的哈希地址。如果仍然发生冲突，则再求下一个，依次类推。直至新的哈希地址不再发生冲突为止。&lt;/p&gt;

&lt;p&gt;常用的处理冲突的方法有开放地址法、链地址法两大类&lt;/p&gt;

&lt;h2 id=&quot;section-10&quot;&gt;1）．开放定址法&lt;/h2&gt;

&lt;p&gt;用开放定址法处理冲突就是当冲突发生时，形成一个地址序列。沿着这个序列逐个探测，直到找出一个“空”的开放地址。将发生冲突的关键字值存放到该地址中去。&lt;br /&gt;
如 Hi=(H(k)+d（i）) % m, i=1，2，…k (k 其中H(k)为哈希函数，m为哈希表长，d为增量函数，d(i)=dl，d2…dn-l。&lt;/p&gt;

&lt;p&gt;增量序列的取法不同，可得到不同的开放地址处理冲突探测方法。&lt;/p&gt;

&lt;h3 id=&quot;a&quot;&gt;a）线性探测法&lt;/h3&gt;

&lt;p&gt;线性探测法是从发生冲突的地址（设为d）开始，依次探查d+l，d+2，…m-1（当达到表尾m-1时，又从0开始探查）等地址，直到找到一个空闲位置来存放冲突处的关键字。&lt;/p&gt;

&lt;p&gt;若整个地址都找遍仍无空地址，则产生溢出。&lt;/p&gt;

&lt;p&gt;线性探查法的数学递推描述公式为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d0=H(k)
di=(di-1+1)% m (1≤i≤m-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;【例】已知哈希表地址区间为0～10，给定关键字序列（20，30，70，15，8，12，18，63，19）。哈希函数为H(k)=k％ll，采用线性探测法处理冲突，则将以上关键字依次存储到哈希表中。试构造出该哈希表，并求出等概率情况下的平均查找长度。&lt;/p&gt;

&lt;p&gt;假设数组为A, 本题中各元素的存放过程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;H(20)=9，可直接存放到A[9]中去。
H(30)=8，可直接存放到A[8]中去。
H(70)=4，可直接存放到A[4]中去。
H(15)=4，冲突；
d0=4
d1=(4+1)%11=5，将15放入到A[5]中。
H(8)=8，冲突；
d0=8
d1=(8+1)%11=9，仍冲突；
d2=(8+2)%11=10，将8放入到A[10]中。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在等概率情况下成功的平均查找长度为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（1*5+2+3+4+6）/9 =20/9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用线性探查法处理冲突容易造成关键字的堆积问题。这是因为当连续n个单元被占用后，再散列到这些单元上的关键字和直接散列到后面一个空闲单元上的关键字都要占用这个空闲单元，致使该空闲单元很容易被占用，从而发生非同义冲突。造成平均查找长度的增加。&lt;br /&gt;
为了克服堆积现象的发生，可以用下面的方法替代线性探查法。&lt;/p&gt;

&lt;h3 id=&quot;b&quot;&gt;b）平方探查法&lt;/h3&gt;

&lt;p&gt;设发生冲突的地址为d，则平方探查法的探查序列为：d+12，d+22，…直到找到一个空闲位置为止。&lt;/p&gt;

&lt;p&gt;平方探查法的数学描述公式为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d0=H(k)
di=(d0+i2) % m (1≤i≤m-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在等概率情况下成功的平均查找长度为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（1*4+2*2+3+4+6）/9 =21/9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;平方探查法是一种较好的处理冲突的方法，可以避免出现堆积问题。它的缺点是不能探查到哈希表上的所有单元，但至少能探查到一半单元。&lt;/p&gt;

&lt;p&gt;例如，若表长m=13，假设在第3个位置发生冲突，则后面探查的位置依次为4、7、12、6、2、0，即可以探查到一半单元。&lt;/p&gt;

&lt;p&gt;若解决冲突时，探查到一半单元仍找不到一个空闲单元。则表明此哈希表太满，需重新建立哈希表。&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;2）．链地址法&lt;/h2&gt;

&lt;p&gt;用链地址法解决冲突的方法是：&lt;br /&gt;
把所有关键字为同义词的记录存储在一个线性链表中，这个链表称为同义词链表。并将这些链表的表头指针放在数组中（下标从0到m-1）。这类似于图中的邻接表和树中孩子链表的结构。&lt;/p&gt;

&lt;p&gt;由于在各链表中的第一个元素的查找长度为l，第二个元素的查找长度为2，依此类推。因此，在等概率情况下成功的平均查找长度为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(1*5+2*2+3*l+4*1)／9=16／9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;虽然链地址法要多费一些存储空间，但是彻底解决了“堆积”问题，大大提高了查找效率。&lt;/p&gt;

&lt;h2 id=&quot;section-12&quot;&gt;3）. 再哈希法：&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Hi=R Hi(key)&lt;/code&gt;，R和Hi均是不同的哈希函数，即在同义词产生地址冲突时计算另一个哈希函数地址，直到冲突不再发生。这种方法不易产生聚集，但增加了计算的时间。&lt;/p&gt;

&lt;h2 id=&quot;section-13&quot;&gt;4）.建立一个公共溢出区&lt;/h2&gt;

&lt;p&gt;这也是处理冲突的一种方法。&lt;/p&gt;

&lt;p&gt;假设哈希函数的值域为[0，m-1]，则设向量HashTable[0…m-1]为基本表，每个分量存放一个记录，另设立向量OverTable[0．．v]为溢出表。所有关键字和基本表中关键字为同义词的记录，不管它们由哈希函数得到的哈希地址是什么，一旦发生冲突，都填入溢出表。&lt;/p&gt;

&lt;h1 id=&quot;section-14&quot;&gt;哈希表的查找及性能分析&lt;/h1&gt;

&lt;p&gt;哈希法是利用关键字进行计算后直接求出存储地址的。当哈希函数能得到均匀的地址分布时，不需要进行任何比较就可以直接找到所要查的记录。但实际上不可能完全避免冲突，因此查找时还需要进行探测比较。&lt;/p&gt;

&lt;p&gt;在哈希表中，虽然冲突很难避免，但发生冲突的可能性却有大有小。这主要与三个因素有关。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一:与装填因子有关&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所谓装填因子是指哈希表中己存入的元素个数n与哈希表的大小m的比值，即f=n/m。&lt;br /&gt;
当f越小时，发生冲突的可能性越小，越大（最大为1）时，发生冲突的可能性就越大。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第二:与所构造的哈希函数有关&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;若哈希函数选择得当，就可使哈希地址尽可能均匀地分布在哈希地址空间上，从而减少冲突的发生。否则，若哈希函数选择不当，就可能使哈希地址集中于某些区域，从而加大冲突的发生。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第三:与解决冲突的哈希冲突函数有关&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;哈希冲突函数选择的好坏也将减少或增加发生冲突的可能性。&lt;/p&gt;

&lt;h1 id=&quot;java-&quot;&gt;java 哈希表实现&lt;/h1&gt;

&lt;p&gt;java中哈希表的实现有多个，比如hashtable，hashmap，currenthashmap，也有其他公司实现的，如apache的FashHashmap,google的mapmarker,high-lib的NonBlockingHashMap,其中差别是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hastable:线程同步，比较慢&lt;/li&gt;
  &lt;li&gt;hashmap：线程不同步，不同步时候读写最快（但是不能保证读到最新数据），加同步修饰的时候， 读写比较慢&lt;/li&gt;
  &lt;li&gt;currenthashmap:线程同步，默认分成16块，写入的时候只锁要写入的快，读取一般不锁块，只有读到空的时候，才锁块，性能比较高，处于hashmap同步和不同步之间。&lt;/li&gt;
  &lt;li&gt;fashhashmap:apache collection 将HashMap封装，读取的时候copy一个新的，写入比较慢（尤其是存入比较多对象每写一次都要复制一个对象，超级慢），读取快&lt;/li&gt;
  &lt;li&gt;NoBlockingHashMap： high_scale_lib实现写入慢，读取较快&lt;br /&gt;
MiltigetHashMap，MapMaker google collection，和CurrentHashMap性能相当，功能比较全，可以设置超时，重复的可以保存成list&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-15&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://course.onlinesjtu.com/mod/page/view.php?id=423&quot;&gt;哈希表&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/bigshuai/articles/2398116.html&quot;&gt;哈希表（Hash Table）及散列法（Hashing）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/11454/&quot;&gt;Hash碰撞的拒绝式服务攻击&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.webzone8.com/article/560.html&quot;&gt;Berkeley DB Hash、Btree、Queue、Recno选择&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://javapapers.com/core-java/java-hashtable/#&amp;amp;slider1=1&quot;&gt;Java Hashtable &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kantery.iteye.com/blog/441755&quot;&gt;Java Hashtable分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2012/03/26/hash-and-hash-functions.html</link>
      <guid>http://blog.javachen.com/2012/03/26/hash-and-hash-functions.html</guid>
      <pubDate>2012-03-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>如何在Kettle4.2上面实现cassandra的输入与输出</title>
      <description>&lt;p&gt;这是在QQ群里有人问到的一个问题。&lt;/p&gt;

&lt;p&gt;如何在pdi-ce-4.2.X-stable上面实现cassandra的输入与输出,或是实现hadoop,hbase,mapreduce,mongondb的输入输出?&lt;/p&gt;

&lt;p&gt;在kettle中实现cassandra的输入与输出有以下两种方式:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一种方式:自己编写cassandra输入输出组件&lt;/li&gt;
  &lt;li&gt;第二种方式:使用别人编写好的插件,将其集成进来&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然还有第三种方法,直接使用4.3版本的pdi.&lt;/p&gt;

&lt;p&gt;第一种方法需要对cassandra很熟悉编写插件才可以做到,第二种方法可以通过拷贝pdi-ce-big-data-4.3.0-preview中的文件来完成.&lt;/p&gt;

&lt;p&gt;在pdi-ce-big-data-4.3.0-preview&lt;a href=&quot;http://ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/&quot; target=&quot;_blank&quot;&gt;(下载页面&lt;/a&gt;)版本中可以看到kettle开始支持cassandra的输入和输出.&lt;/p&gt;

&lt;p&gt;故我们可以将4.3版本中的cassandra相关文件拷贝到4.2.1中.我使用的是pdi-ce-4.2.1-stable.&lt;/p&gt;

&lt;p&gt;在pdi-ce-big-data-4.3.0-preview/plugins目录下有以下目录或文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
|-- databases
|-- hour-partitioner.jar
|-- jobentries
|-- kettle-gpload-plugin
|-- kettle-hl7-plugin
|-- kettle-palo-plugin
|-- pentaho-big-data-plugin
|-- repositories
|-- spoon
|-- steps
`-- versioncheck
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pentaho-big-data-plugin目录是kettle对大数据的集成与支持,我们只需要将该目录拷贝到pdi-ce-4.2.1-stable/plugins目录下即可.最后的结构如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
|-- databases
|-- hour-partitioner.jar
|-- jobentries
|   `-- DummyJob
|       |-- DPL.png
|       |-- dummyjob.jar
|       `-- plugin.xml
|-- pentaho-big-data-plugin
|   |-- lib
|   |   |-- apache-cassandra-1.0.0.jar
|   |   |-- apache-cassandra-thrift-1.0.0.jar
|   |   |-- aws-java-sdk-1.0.008.jar
|   |   |-- commons-cli-1.2.jar
|   |   |-- guava-r08.jar
|   |   |-- hbase-comparators-TRUNK-SNAPSHOT.jar
|   |   |-- jline-0.9.94.jar
|   |   |-- libthrift-0.6.jar
|   |   |-- mongo-java-driver-2.7.2.jar
|   |   |-- pig-0.8.1.jar
|   |   |-- xpp3_min-1.1.4c.jar
|   |   `-- xstream-1.3.1.jar
|   `-- pentaho-big-data-plugin-TRUNK-SNAPSHOT.jar
|-- repositories
|-- spoon
|-- steps
|   |-- DummyPlugin
|   |   |-- DPL.png
|   |   |-- dummy.jar
|   |   `-- plugin.xml
|   |-- S3CsvInput
|   |   |-- jets3t-0.7.0.jar
|   |   |-- plugin.xml
|   |   |-- S3CIN.png
|   |   `-- s3csvinput.jar
|   `-- ShapeFileReader3
|       |-- plugin.xml
|       |-- SFR.png
|       `-- shapefilereader3.jar
`-- versioncheck
    |-- kettle-version-checker-0.2.0.jar
    `-- lib
	`-- pentaho-versionchecker.jar

13 directories, 29 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动pdi-ce-4.2.1-stable之后,打开一个转换,在核心对象窗口就可以看到Big Data步骤目录了.&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dr9zaa66nbj.jpg&quot; target=&quot;_blank&quot;&gt;
&lt;img alt=&quot;&quot; src=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dr9zaa66nbj.jpg&quot; title=&quot;pdi big data plugin in kette 4.2&quot; class=&quot;aligncenter&quot; width=&quot;600&quot; height=&quot;375&quot; /&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;获取pentaho-big-data-plugin源码&lt;/strong&gt;&lt;br /&gt;
如果想在eclipse中查看或修改pentaho-big-data-plugin源码,该怎么做呢?&lt;br /&gt;
你可以从&lt;a href=&quot;http://ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/pentaho-big-data-plugin-TRUNK-SNAPSHOT-sources.zip&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;下载到源码,然后将src下的文件拷贝到你的pdi-ce-4.2.1-stable源码工程中.&lt;/p&gt;

&lt;p&gt;然后,需要在kettle-steps.xml中注册步骤节点&lt;br /&gt;
例如,下面是MongoDbInput步骤的注册方法,请针对不同插件的不同类路径加以修改.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;step id=&quot;MongoDbInput&quot;&amp;gt;
&amp;lt;description&amp;gt;i18n:org.pentaho.di.trans.step:BaseStep.TypeLongDesc.MongoDbInput
&amp;lt;classname&amp;gt;org.pentaho.di.trans.steps.mongodbinput.MongoDbInputMeta
&amp;lt;category&amp;gt;i18n:org.pentaho.di.trans.step:BaseStep.Category.Input
&amp;lt;tooltip&amp;gt;i18n:org.pentaho.di.trans.step:BaseStep.TypeTooltipDesc.MongoDbInput
&amp;lt;iconfile&amp;gt;ui/images/mongodb-input.png
&amp;lt;/iconfile&amp;gt;&amp;lt;/tooltip&amp;gt;
&amp;lt;/category&amp;gt;
&amp;lt;/classname&amp;gt;
&amp;lt;/description&amp;gt;
&amp;lt;/step&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;note&quot;&gt;
&lt;h&gt;注意:
由于pdi-ce-4.2.1-stable中存在hive组件,故添加pentaho-big-data-plugin插件之后有可能会出现找不到类的情况,这是由于jar重复版本不一致导致的,按照异常信息,找到重复的jar并按情况删除一个jar包即可.
&lt;/h&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;扩展阅读:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pentaho Big Data Plugin &lt;a href=&quot;http://wiki.pentaho.com/display/BAD/Getting+Started+for+Java+Developers&quot; target=&quot;_blank&quot;&gt;http://wiki.pentaho.com/display/BAD/Getting+Started+for+Java+Developers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;pentaho-big-data-plugin ci&lt;br /&gt;
&lt;a href=&quot;http://ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/&quot; target=&quot;_blank&quot;&gt;http://- - ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Pentaho Community Edition (CE) downloads &lt;a href=&quot;http://wiki.pentaho.com/display/BAD/Downloads&quot; target=&quot;_blank&quot;&gt;http://wiki.pentaho.com/display/BAD/Downloads&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2.html</link>
      <guid>http://blog.javachen.com/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2.html</guid>
      <pubDate>2012-03-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>2011年度年终总结</title>
      <description>&lt;p&gt;2011年工作总结，只能算是七个月的工作总结，在这七个月里学到了许多、收获了许多、感悟了许多。以下是对这六个月的一个回顾与总结。&lt;/p&gt;

&lt;p&gt;来公司最初的两个月主要负责云计算相关产品的调研工作，相关云计算产品有：Eucalyptus、OpenNebula、OpenStack。在这两个月里，对云计算的原理、服务、架构以及安装和部署有了初步的了解于实践，并积累了一些文档。在一次又一次的安装部署过程中，体验到了失败的痛苦、无助的迷茫、成功的喜悦，深刻的意识到自身在linux方面存在的不足；强烈的感觉到有必要学习一些语言，如Shell、Python、Groovy、Ruby等等。2012年，打算将工作环境切换到ubuntu，并向叶凌学习ubuntu相关方面的知识；打算学习rhl6相关课程，遇到不懂的问题向同事提问、学习。&lt;/p&gt;

&lt;p&gt;随后在公司里参与了报表项目的开发，在这次开发中较快的熟悉了Spring+JPA框架、掌握了dhtmlx的使用、接触到了postgresql数据库，顺利的完成了各项开发工作。通过这个项目，熟悉了公司的项目管理方式、版本控制、代码开发规范等等。总体来说，公司在代码复查方面做的不够，项目开发方面没有形成一套成型的开发框架，并且，项目于项目之间在一些同时使用的相关技术上面的沟通于交流做的不够。希望，公司以后能够制定一些编码规范、引入代码审查、形成一套成型的可重复利用的开发框架或是基础开发包。&lt;/p&gt;

&lt;p&gt;剩下的时间接触了Pentaho BI，在整体上了解了Pentaho相关组件之后，开始了阅读其代码、特别是阅读了其分析报表和交互式报表的实现方式以及一些底层基础代码。在阅读代码过程中，深切的体会到有必要深入的学习LDAP以及数据挖掘相关的理论知识。当然，更多的时间花在了kettle的学习和使用以及修改、扩展代码上。2012年，LDAP和kettle将会是一个工作重点。2011年，做了三次Pentaho的服务，通过这三次服务意识到有必要加强自己的沟通交流能力、储备足够的专业知识、灵活应对客户的要求和需求。&lt;/p&gt;

&lt;p&gt;2011年，工作上存在一些不足。在调研云计算过程中没有整理、形成足够详细的文档，自己所做的工作没有及时分享；在工作中没有及时提交项目周报；没有及时的跟踪、检查分配下去的任务完成情况，对新人的指导不够；编写文档时候，没有可参考的模版，导致文档编写不规范；在与客户的交流中底气不足、表达能力不够；下班之后惰性较强，缺少自主学习性；博客文章篇数较少，平时的总结与分享不够积极等等。&lt;/p&gt;

&lt;p&gt;2012年，将会从以下方面指导自己的工作：以CE为目标提高linux水平，多参加云计算相关活动、多学习云计算开源产品；更加深入的掌握、理解Kettle源码及使用，掌握数据挖掘相关理论知识，争取创建一个Pentaho社区、一个QQ分享群，培养新人，希望能够将Pentaho的咨询服务工作更多交给其他人完成；在项目管理上有所进步；及时总结自己存在的不足，发现问题，不断进步，通过博客、微博及时记录、分享一些技术心得。2012年，是职业生涯的第三年，在这一年希望能够深入理解Spring、Jboss、Pentaho、缓存、云计算、架构等技术，希望自己在技术方面不断成长的同时，在项目管理方面能有所进步；希望自己在工作上、生活里能够有更多的自主性、创造性。&lt;/p&gt;

&lt;p&gt;2012年，我们有所希望、有所期待。希望公司不断完善公司规则制度、注重团队培养、关心每一个同事的成长；希望公司能够多分享资源、多交流心得；希望每一个同事都能忙碌着、进步着并快乐着，大家一起努力，一起进步；期待公司在开源方面能够走的更远，公司各方面能够更上一层楼！&lt;/p&gt;

&lt;p&gt;2012年，期望明天会更好！&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2012/02/26/summary-of-the-work-in-2011.html</link>
      <guid>http://blog.javachen.com/2012/02/26/summary-of-the-work-in-2011.html</guid>
      <pubDate>2012-02-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Seam的启动过程</title>
      <description>&lt;p&gt;了解seam2的人知道，seam是通过在web. xml中配置监听器启动的。注意，本文中的seam是指的seam2，不是seam3.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;listener&amp;gt;
	&amp;lt;listenerclass&amp;gt;org. jboss. seam. servlet. SeamListener&amp;lt;/listenerclass&amp;gt;
&amp;lt;/listener&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该监听器会做哪些事情呢？看看Gavin King对SeamListener类的描述。&lt;/p&gt;

&lt;blockquote&gt;Drives certain Seam functionality such as initialization and cleanup of application and session contexts from the web application lifecycle. &lt;/blockquote&gt;

&lt;p&gt;从描述中可以知道SeamListener主要完成应用以及web应用生命周期中的session上下文的初始化和清理工作。&lt;/p&gt;

&lt;p&gt;该类实现了ServletContextListener接口，在contextInitialized(ServletContextEvent event)方法内主要初始化生命周期并完成应用的初始化，在contextDestroyed(ServletContextEvent event)方法内结束应用的生命周期。&lt;/p&gt;

&lt;p&gt;该类实现了HttpSessionListener接口，主要是用于在生命周期中开始和结束session。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一步&lt;/strong&gt;，构造方法里从ServletContext获取一些路径信息：warRoot、warClassesDirectory、warLibDirectory、hotDeployDirectory。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二步&lt;/strong&gt;，扫描配置文件完成seam组件的初始化（Initialization的create方法）。&lt;br /&gt;
其中包括：添加命名空间、初始化组件、初始化Properties、初始化jndi信息。这一步，其实主要是读取一些配置文件,加载seam组件。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;添加命名空间&lt;/li&gt;
  &lt;li&gt;从/WEBINF/components. xml加载组件&lt;/li&gt;
  &lt;li&gt;从/WEBINF/events. xml加载组件&lt;/li&gt;
  &lt;li&gt;从METAINF/components. xml加载组件&lt;/li&gt;
  &lt;li&gt;从ServletContext初始化Properties&lt;/li&gt;
  &lt;li&gt;从/seam. properties初始化Properties&lt;/li&gt;
  &lt;li&gt;初始化jndi Properties&lt;/li&gt;
  &lt;li&gt;从system加载Properties&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;第三步&lt;/strong&gt;，seam初始化过程（Initialization的init方法）。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ServletLifecycle开始初始化&lt;/li&gt;
  &lt;li&gt;设置Application上下文&lt;/li&gt;
  &lt;li&gt;添加Init组件&lt;/li&gt;
  &lt;li&gt;通过standardDeploymentStrategy的注解和xml组件扫描组件&lt;/li&gt;
  &lt;li&gt;判断jbpm是否安装&lt;/li&gt;
  &lt;li&gt;检查默认拦截器&lt;/li&gt;
  &lt;li&gt;添加特别组件&lt;/li&gt;
  &lt;li&gt;添加war root部署、热部署&lt;/li&gt;
  &lt;li&gt;安装组件&lt;/li&gt;
  &lt;li&gt;导入命名空间&lt;/li&gt;
  &lt;li&gt;ServletLifecycle结束初始化。启动生命周期为APPLICATION的组件。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果组件标注为startup，则会构造其实例进行初始化。例如seam于Hibernate的集成，就可以通过此方法初始化Hibernate，对应的组件类为org. jboss. seam. persistence. HibernateSessionFactory。&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2012/02/23/the-process-of-seam-initiation.html</link>
      <guid>http://blog.javachen.com/2012/02/23/the-process-of-seam-initiation.html</guid>
      <pubDate>2012-02-23T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Kettle运行作业之前的初始化过程</title>
      <description>&lt;p&gt;本文主要描述Kettle是如何通过GUI调用代码启动线程执行作业的。&lt;/p&gt;

&lt;p&gt;之前用英文写了一篇文章《&lt;a href=&quot;/2012/02/the-execution-process-of-kettles-job/&quot; target=&quot;_blank&quot;&gt;The execution process of kettle’s job&lt;/a&gt;》 ，这篇文章只是用于英语写技术博客的一个尝试。由于很久没有使用英语写作了，故那篇文章只是简单的通过UML的序列图描述kettle运行job的一个java类调用过程。将上篇文章的序列图和这篇文章联系起来，会更加容易理解本文。&lt;/p&gt;

&lt;p&gt;在Spoon界面点击运行按钮，Spoon GUI会调用Spoon.runFile()方法，这可以从xul文件（ui/menubar.xul）中的描述看出来。关于kettle中的xul的使用，不是本文重点故不在此说明。&lt;/p&gt;

&lt;pre lang=&quot;java&quot;&gt;
public void runFile() {
	executeFile(true, false, false, false, false, null, false);
}

public void executeFile(boolean local, boolean remote, boolean cluster,
		boolean preview, boolean debug, Date replayDate, boolean safe) {
	TransMeta transMeta = getActiveTransformation();
	if (transMeta != null)
		executeTransformation(transMeta, local, remote, cluster, preview,
				debug, replayDate, safe);

	JobMeta jobMeta = getActiveJob();
	if (jobMeta != null)
		executeJob(jobMeta, local, remote, replayDate, safe, null, 0);
}

public void executeJob(JobMeta jobMeta, boolean local, boolean remote,
		Date replayDate, boolean safe, String startCopyName, int startCopyNr) {
	try {
		delegates.jobs.executeJob(jobMeta, local, remote, replayDate, safe,
				startCopyName, startCopyNr);
	} catch (Exception e) {
		new ErrorDialog(shell, &quot;Execute job&quot;,
				&quot;There was an error during job execution&quot;, e);
	}
}
&lt;/pre&gt;

&lt;p&gt;runFile()方法内部调用executeFile()方法，executeFile方法有以下几个参数：&lt;br /&gt;
- local：是否本地运行&lt;br /&gt;
- remote：是否远程运行&lt;br /&gt;
- cluster：是否集群环境运行&lt;br /&gt;
- preview：是否预览&lt;br /&gt;
- debug：是否调试&lt;br /&gt;
- replayDate：回放时间&lt;br /&gt;
- safe：是否安全模式&lt;/p&gt;

&lt;p&gt;executeFile方法会先获取当前激活的转换，如果获取结果不为空，则执行该转换；否则获取当前激活的作业，执行该作业。 本文主要讨论作业的执行过程，关于转换的执行过程，之后单独一篇文章进行讨论。&lt;/p&gt;

&lt;p&gt;executeJob委托SpoonJobDelegate执行其内部的executeJob方法，注意，其将JobMeta传递给了executeJob方法。SpoonJobDelegate还保存着对Spoon的引用。&lt;/p&gt;

&lt;p&gt;SpoonJobDelegate的executeJob方法主要完成以下操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.设置Spoon的执行配置JobExecutionConfiguration类，该类设置变量、仓库、是否执行安全模式、日志等级等等。&lt;/li&gt;
  &lt;li&gt;2.获得当前Job对应的图形类JobGraph。&lt;/li&gt;
  &lt;li&gt;3.将执行配置类JobExecutionConfiguration的变量、参数、命令行参数设置给jobMeta。&lt;/li&gt;
  &lt;li&gt;4.如果本地执行，则调用jobGraph.startJob(executionConfiguration)，如果远程执行，则委托给SpoonSlaveDelegate执行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JobExecutionConfiguration类是保存job执行过程中的一些配置，该类会在Spoon、JobGraph类之间传递。&lt;/p&gt;

&lt;p&gt;本文只讨论本地执行的情况，故往下查看jobGraph.startJob(executionConfiguration)方法。该方法被synchronized关键字修饰。&lt;/p&gt;

&lt;p&gt;JobGraph类包含当前Spoon类的引用、以及对Job的引用。初始情况，Job的引用应该为null。该类会做以下操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.如果job为空或者没有运行或者没有激活，则先保存，然后往下执行作业。&lt;/li&gt;
  &lt;li&gt;2.在仓库不为空的时候，通过仓库加载Job获得一个运行时的JobMeta对象，名称为runJobMeta；否则，通过文件名称直接new一个JobMeta对象，名称也为runJobMeta。&lt;/li&gt;
  &lt;li&gt;3.通过仓库和runJobMeta对象构建一个Job对象，并将jobMeta对象（此对象通过JobGraph构造方法传入）的变量、参数共享给Job对象。&lt;/li&gt;
  &lt;li&gt;4.Job对象添加JobEntry监听器、Job监听器。&lt;/li&gt;
  &lt;li&gt;5.调用Job的start方法，启动线程开始执行一个job。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Job继承自Thread类，该类的run方法内部会递归执行该作业内部的作业项，限于篇幅，本文不做深究。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2012/02/22/the-init-process-before-job-execution.html</link>
      <guid>http://blog.javachen.com/2012/02/22/the-init-process-before-job-execution.html</guid>
      <pubDate>2012-02-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>The execution process of kettle’s job</title>
      <description>&lt;p&gt;How to execute a kettle job in Spoon GUI or command line after we create a job in Spoon GUI? In Spoon GUI,the main class is &quot;org.pentaho.di.ui.spoon.Spoon.java&quot;.This class handles the main window of the Spoon graphical transformation editor.Many operations about a job or transformation such as run,debug,preview,zoomIn,etc,are all in this class.This post just writes about the code execution process.&lt;/p&gt;

&lt;p&gt;When we start a job or transformation,Spoon invokes the method runFile(),and then is distributed to executeTransformation() or executeJob().At now,we mainly study about executeJob() method.&lt;/p&gt;

&lt;p&gt;This is a simple sequence diagram below.It contains several classes for Starting to execute a job using execute(int nr, Result result) in Job.java.We can see the relation of these classes from it.&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;pic&quot;&gt;
&lt;a href=&quot;http://www.javachen.com/wp-content/uploads/2012/02/spoon-execute-sequence.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;http://www.javachen.com/wp-content/uploads/2012/02/spoon-execute-sequence.jpg&quot; alt=&quot;&quot; title=&quot;spoon execute sequence&quot; width=&quot;300&quot; height=&quot;180&quot; class=&quot;aligncenter size-medium wp-image-2511&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;What is the detail process of job execution? You should look into the Job.run() method for detail information.&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2012/02/21/the-execution-process-of-kettles-job.html</link>
      <guid>http://blog.javachen.com/2012/02/21/the-execution-process-of-kettles-job.html</guid>
      <pubDate>2012-02-21T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Kettle中定义错误处理</title>
      <description>&lt;p&gt;在kettle执行的过程中，如果遇到错误，kettle会停止运行。在某些时候，并不希望kettle停止运行，这时候可以使用错误处理（Step Error Handling）。错误处理允许你配置一个步骤来取代出现错误时停止运行一个转换，出现错误的记录行将会传递给另一个步骤。在Step error handling settings对话框里，需要设置启用错误处理。&lt;/p&gt;

&lt;p&gt;下面例子中读取postgres数据库中的a0表数据，然后输出到a1表：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img alt=&quot;&quot; src=&quot;http://ww2.sinaimg.cn/mw600/48e24b4cjw1dq56wck3m7j.jpg&quot; class=&quot;alignnone&quot; width=&quot;600&quot; height=&quot;172&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;a1表结构如下：&lt;/p&gt;
&lt;pre lang=&quot;sql&quot;&gt;
CREATE TABLE a1
(
  a double precision,
  id integer NOT NULL,
  CONSTRAINT id_pk PRIMARY KEY (id ),
  CONSTRAINT id_unin UNIQUE (id )
)
&lt;/pre&gt;

&lt;p&gt;从表结构可以看出，a1表中id为主键、唯一。&lt;/p&gt;

&lt;p&gt;a0表数据预览：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img alt=&quot;&quot; src=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dq56wcr6c2j.jpg&quot; class=&quot;alignnone&quot; width=&quot;553&quot; height=&quot;403&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;现在a1表数据为空，执行上面的转换，执行成功之后，a1表数据和a0表数据一致。&lt;br /&gt;
再次执行，上面的转换会报错，程序停止运行，会报主键重复的异常。&lt;/p&gt;

&lt;p&gt;现在，我想报错之后，程序继续往下执行，并记录错误的记录的相关信息，这时候可以使用“定义错误处理”的功能。&lt;br /&gt;
在“表输出”的步骤上右键选择“定义错误处理”，弹出如下对话框。&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://ww3.sinaimg.cn/mw600/48e24b4cjw1dq56wd5ckwj.jpg&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;相关字段说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;目标步骤：指定处理错误的步骤&lt;/li&gt;
  &lt;li&gt;启用错误处理？：设置是否启用错误处理&lt;/li&gt;
  &lt;li&gt;错误数列名：出错的记录个数&lt;/li&gt;
  &lt;li&gt;错误描述列名：描述错误信息的列名称&lt;/li&gt;
  &lt;li&gt;错误列的列名：出错列的名称&lt;/li&gt;
  &lt;li&gt;错误编码列名：描述错误的代码的列名&lt;/li&gt;
  &lt;li&gt;允许的最大错误数：允许的最大错误数，超过此数，不在处理错误&lt;/li&gt;
  &lt;li&gt;允许的最大错误百分比：&lt;/li&gt;
  &lt;li&gt;在计算百分百前最少要读入的行数：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;添加错误处理后的转换如下：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dq56wdntipj.jpg&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;记录错误信息的字段列表如下，可以看出，errorNum、errorDesc、errorName、errorCode都是在定义错误处理时候填入的列名称，a、id来自于输入的记录的列。&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://ww2.sinaimg.cn/mw600/48e24b4cjw1dq56wdvk6uj.jpg&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;记录的错误信息如下：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://ww4.sinaimg.cn/mw600/48e24b4cjw1dq56we2sn2j.jpg&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;分析&lt;/strong&gt;&lt;br /&gt;
可以看到,错误日志里只是记录了出错的行里面的信息，并没有记录当前行所在的表名称以及执行时间等等，如果能够对此进行扩展，则该错误日志表才能更有实际意义。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;&lt;br /&gt;
1.错误日志的错误码含义（如：TOP001）含义见参考文章2.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考文章&lt;/strong&gt;&lt;/p&gt;
&lt;li&gt;&lt;a href=&quot;http://wiki.pentaho.com/display/EAI/.09+Transformation+Steps#.09TransformationSteps-StepErrorHandling&quot; target=&quot;_blank&quot;&gt;Step Error Handling&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://wiki.pentaho.com/display/COM/Step+error+handling+codes&quot; target=&quot;_blank&quot;&gt;Step error handling codes&lt;/a&gt;
&lt;/li&gt;
</description>
      <link>http://blog.javachen.com/2012/02/17/step-error-handling-in-kettle.html</link>
      <guid>http://blog.javachen.com/2012/02/17/step-error-handling-in-kettle.html</guid>
      <pubDate>2012-02-17T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>JSF中EL表达式之this扩展</title>
      <description>&lt;p&gt;本篇文章来自以前公司的一套jsf+seam+Hibernate的一套框架，其对jsf进行了一些改进，其中包括:EL表达式中添加this，通过jsf的渲染实现权限控制到按钮等等。JSF表达式中添加this，主要是为了在facelets页面使用this关键字引用（JSF自动查找）到当前页面对应的pojo类，详细说明见下午。因为，本文的文章是公司同事整理的，本文作者仅仅是将其分享出来，供大家参考思路，如果有什么不妥的话，请告知。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EL表达式this扩展&lt;/strong&gt;&lt;br /&gt;
在业务系统中，大量页面具有大量区域是相似或者相同的，或者可能根据某些局部特征的变化具有一定的变化，jsf中通过facelet模板功能可以达到一定程度的页面重用，从而减轻开发人员编辑和拷贝一些页面代码，达到重用的目的。然而，她们具有如下限制：&lt;br /&gt;
1.Java语言作为一种典型的OO语言，通过抽象、继承等功能，可以大量重用已经实现或者在父类中已经存在的属性和方法等。模板技术作为一种静态加载和内容替换，无法充分利用面向对象的继承功能&lt;br /&gt;
2.由于Jsf/jsp框架采用视图和动作分离的模型，多个相似功能在不同的页面实现中由于页面对应点动作类不同因而必须使用复制的方法；&lt;br /&gt;
3.模板中使用EL表达式与后台动作类交互，这种交互是基于绝对名称的，不同的网页对应的动作类是完全不同的，因此很难重用和利用面向对象的特征。&lt;/p&gt;

&lt;p&gt;我们需要一种新的功能，实现：&lt;br /&gt;
1.模板的应用特种可以参照OO的继承特种，即模板的对模板的引用可以看成一种继承，这种继承可以和java的OO是一致的&lt;br /&gt;
2.多个页面和多个独立java后台程序相同部分完全可以抽离出来，不依赖它们是否继承关系、只需保证他们具有相同的属性或者方法&lt;br /&gt;
3.动态映射功能，即在满足上述基础上可以实现页面和后台实现类的属性和方法的自动映射&lt;br /&gt;
4.兼容标准的EL表达式&lt;/p&gt;

&lt;p&gt;我们将上述功能处理为“this”表达式。其功能模型为：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2012/02/this-expression-of-el.jpg&quot; alt=&quot;&quot; title=&quot;this expression of el&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;页面A和页面B分别引用了通用功能T,内含this相关的El表达式，通过分析处理，分别映射到对应的页面动作类的属性A.name和B.name。A和B可以从相同的基类C派生而来，只需C类实现了name属性即可，A类和B类也可以毫不相关，但是它们具有相同的属性name。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;动作类和页面的一致性保证&lt;/strong&gt;&lt;br /&gt;
为了有效实现this表达式，我们实现如下映射规则：&lt;br /&gt;
1.名称为小写方式，不管页面如何命名，对应的后台类的jsf标识符都转换为小写&lt;br /&gt;
2.页面和相应的后台类以相同命名方式，页面的目录转化为后台类的包名，名称通过点分隔包名，如根目录的a.xhtml对应的后台类名称为A.java，其唯一jsf标识名称为“a”，test/b.xhtml的后台类为test/B.java，其唯一jsf标识为“test.b”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“this”EL表达式算法&lt;/strong&gt;&lt;br /&gt;
算法流程如下图：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2012/02/this-expression-flow-of-el.jpg&quot; alt=&quot;&quot; title=&quot;this expression flow of el&quot; /&gt;
&lt;/div&gt;
</description>
      <link>http://blog.javachen.com/2012/02/14/this-expression-of-jsf-el.html</link>
      <guid>http://blog.javachen.com/2012/02/14/this-expression-of-jsf-el.html</guid>
      <pubDate>2012-02-14T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>使用Kettle数据迁移添加主键和索引</title>
      <description>&lt;p&gt;Kettle是一款国外开源的etl工具，纯java编写，绿色无需安装，主要用于&lt;strong&gt;数据抽取、转换、装载&lt;/strong&gt;。kettle兼容了市面上几十种数据库，故用kettle来做数据库的迁移视乎是个不错的选择。&lt;/p&gt;

&lt;p&gt;kettle的数据抽取主要在于抽取数据，而没有考虑数据库的&lt;strong&gt;函数、存储过程、视图、表结构以及索引、约束&lt;/strong&gt;等等，而这些东西恰恰都是数据迁移需要考虑的事情。当然，如果在不考虑数据库中的函数、存储过程、视图的情况下，使用kettle进行数据的迁移还算是一个可行的方案。&lt;/p&gt;

&lt;p&gt;这篇文章主要是讲述在使用kettle进行数据库的迁移的时候如何迁移主键和索引，为什么要迁移主键和索引？异构数据库之间的迁移很难无缝的实现自定义函数、存储过程、视图、表结构、索引、约束以及数据的迁移，所以多数情况下只需要异构数据库之间类型兼容、数据一致就可以了。但是在有些情况下需要对输出表进行查询以及数据比对的时候，&lt;strong&gt;需要有主键和索引方便对比和加快查询速度&lt;/strong&gt;。&lt;br /&gt;
先来看看kettle中的一些组件。&lt;/p&gt;

&lt;p&gt;下图是kettle中的一个表输出组件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2012/01/kettle-table-out.png&quot; alt=&quot;kettle中的表输出组件&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在该组件里可以指定表名、字段等信息，并且还可以建表的sql语句。打开建表的sql语句，你可以看到该语句里只指定了字段名称和类型，没有指定主外键、约束、和索引。显然，该组件只是完成了数据的输出并没有将表的主键迁移过去。&lt;br /&gt;
&lt;!--more--&gt;&lt;br /&gt;
下图是kettle中纬度更新/查询的组件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2012/01/kettle-look-up.png&quot; alt=&quot;kettle中纬度更新/查询的组件&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该组件可以指定输出表名、映射字段、纬度字段、并且指定主键（图中翻译为关键字段），该组件比表输出组件多了一个功能，即指定主键。&lt;br /&gt;
从上面两个组件中可以看出，kettle实际上预留了设置主键的接口，具体的接口说明需要查看api或者源代码，只是kettle没有智能的查处输入表的主键字段，而是需要用户在kettle ui界面指定一个主键名称。&lt;/p&gt;

&lt;p&gt;如果现在想使用kettle实现&lt;strong&gt;异构数据库的数据以及主键和索引的迁移&lt;/strong&gt;，有没有一个完整方便的解决方案呢？我能想到的解决方案如下：&lt;br /&gt;
&lt;strong&gt;1.&lt;/strong&gt;使用kettle向导中的多表复制菜单进行数据库的迁移，这只能实现数据的迁移还需要额外的方法添加主键和索引，你可以手动执行一些脚步添加约束。&lt;br /&gt;
&lt;strong&gt;2.&lt;/strong&gt;针对源数据库中的每一张表创建一个转换，转换中使用纬度更新/查询组件，在该主键中指定主键。创建完所有的转换之后，创建一个作业将这些转换串联起来即可。&lt;br /&gt;
&lt;strong&gt;3.&lt;/strong&gt;扩展kettle向导中的多表复制菜单里的功能，在该功能创建的作业中添加一些节点用于添加输出表的主键和索引。这些节点可以是执行sql语句的主键，故只需要通过jdbc代码获取添加主键和索引的sql语句。&lt;/p&gt;

&lt;p&gt;方案1需要单独执行脚步实现添加主键和索引，创建或生成这些脚步需要些时间；方案2需要针对每个表认为的指定主键，工作量大，而且无法实现添加索引；方案3最容易实现和扩展。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;下面是方案3的具体的实现。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先需要在每一个表的建表语句节点和复制数据节点之后添加一个执行sql语句的节点，该节点用于添加主键和索引。&lt;br /&gt;
多表复制向导的核心代码在src-db/org.pentaho.di.ui.spoon.delegates.SpoonJobDelegate.java的public void ripDBWizard()方法中。该方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void ripDBWizard(final int no) {
	final List databases = spoon.getActiveDatabases();
	if (databases.size() == 0)
		return;&amp;lt;/pre&amp;gt;

	final RipDatabaseWizardPage1 page1 = new RipDatabaseWizardPage1(&quot;1&quot;,
			databases);
	final RipDatabaseWizardPage2 page2 = new RipDatabaseWizardPage2(&quot;2&quot;);
	final RipDatabaseWizardPage3 page3 = new RipDatabaseWizardPage3(&quot;3&quot;,
			spoon.getRepository());
	Wizard wizard = new Wizard() {
		public boolean performFinish() {
			try {
				JobMeta jobMeta = ripDBByNo(no, databases,
					page3.getJobname(), page3.getRepositoryDirectory(),
					page3.getDirectory(), page1.getSourceDatabase(),
					page1.getTargetDatabase(), page2.getSelection());

				if (jobMeta == null)
					return false;

				if (page3.getRepositoryDirectory() != null) {
					spoon.saveToRepository(jobMeta, false);
				} else {
					spoon.saveToFile(jobMeta);
				}

				addJobGraph(jobMeta);
				return true;
			} catch (Exception e) {
				new ErrorDialog(spoon.getShell(), &quot;Error&quot;,
						&quot;An unexpected error occurred!&quot;, e);
				return false;
			}
		}

		public boolean canFinish() {
			return page3.canFinish();
		}
	};

	wizard.addPage(page1);
	wizard.addPage(page2);
	wizard.addPage(page3);

	WizardDialog wd = new WizardDialog(spoon.getShell(), wizard);
	WizardDialog.setDefaultImage(GUIResource.getInstance().getImageWizard());
	wd.setMinimumPageSize(700, 400);
	wd.updateSize();
	wd.open();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法主要是创建一个向导，该向导中包括三个向导页，第一个向导页用于&lt;strong&gt;选择数据库连接&lt;/strong&gt;：源数据库和目标数据库连接；第二个向导页用于&lt;strong&gt;选表&lt;/strong&gt;；第三个向导页用于&lt;strong&gt;指定作业保存路径&lt;/strong&gt;。在向导完成的时候，即performFinish()方法里，会根据选择的数据源和表生成一个作业，即JobMeta对象。&lt;br /&gt;
创建Jobmeta的方法为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public JobMeta ripDB(final List databases,final String jobname, final
    RepositoryDirectoryInterface repdir,final String directory, final DatabaseMeta
    sourceDbInfo,final DatabaseMeta targetDbInfo, final String[] tables){
 //此处省略若干代码
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法主要逻辑在下面代码内：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IRunnableWithProgress op = new IRunnableWithProgress() {
	public void run(IProgressMonitor monitor)
	 throws InvocationTargetException, InterruptedException {
	   //此处省略若干代码
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中有以下代码用于遍历所选择的表生成作业中的一些节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (int i = 0; i &amp;amp;lt; tables.length &amp;amp;amp;&amp;amp;amp; !monitor.isCanceled(); i++) {
    //此处省略若干代码
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;针对每一张表先会创建一个JobEntrySQL节点，然后创建一个转换JobEntryTrans，可以在创建转换之后再创建一个JobEntrySQL节点，该节点用于添加主键和索引。&lt;br /&gt;
这部分的代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;String pksql = JdbcDataMetaUtil.exportPkAndIndex(
		sourceDbInfo, sourceCon, tables[i],
		targetDbInfo, targetCon, tables[i]);
if (!Const.isEmpty(pksql)) {
	location.x += 300;
	JobEntrySQL jesql = new JobEntrySQL(
		BaseMessages.getString(PKG,&quot;Spoon.RipDB.AddPkAndIndex&quot;)
			+ tables[i] + &quot;]&quot;);
	jesql.setDatabase(targetDbInfo);
	jesql.setSQL(pksql);
	jesql.setDescription(BaseMessages.getString(PKG,
			&quot;Spoon.RipDB.AddPkAndIndex&quot;)
			+ tables[i]
			+ &quot;]&quot;);
	JobEntryCopy jecsql = new JobEntryCopy();
	jecsql.setEntry(jesql);
	jecsql.setLocation(new Point(location.x, location.y));
	jecsql.setDrawn();
	jobMeta.addJobEntry(jecsql);
	// Add the hop too...
	JobHopMeta jhi = new JobHopMeta(previous, jecsql);
	jobMeta.addJobHop(jhi);
	previous = jecsql;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取添加主键和索引的sql语句，主要是采用jdbc的方式读取两个数据库，判断源数据库的表中是否存在主键和索引，如果有则返回添加主键或索引的sql语句。这部分代码封装在JdbcDataMetaUtil类中。&lt;br /&gt;
该代码见：&lt;a href=&quot;https://gist.github.com/1564353.js&quot; target=&quot;_blank&quot;&gt;https://gist.github.com/1564353.js&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;最后的效果图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2012/01/kettle-add-primary-key-and-indexes.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;infor&quot;&gt;说明：
1.以上代码使用的是jdbc的方法获取主键或索引，不同的数据库的jdbc驱动实现可能不同而且不同数据库的语法可能不同，故上面代码可能有待完善。
2.如果一个数据库中存在多库并且这多个库中有相同的表，使用上面的代码针对一个表名会查出多个主键或索引。这一点也是可以改善的&lt;/div&gt;
</description>
      <link>http://blog.javachen.com/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle.html</link>
      <guid>http://blog.javachen.com/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle.html</guid>
      <pubDate>2012-01-05T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>kettle进行数据迁移遇到的问题</title>
      <description>&lt;p&gt;使用kettle进行oracle或db2数据导入到mysql或postgres数据库过程中遇到以下问题，以下只是一个简单描述，详细的说明以及所做的代码修改没有提及。下面所提到的最新的pdi程序是我修改kettle源码并编译之后的版本。&lt;/p&gt;

&lt;h4 id=&quot;pdioraclemysqloraclepostgres&quot;&gt;同时运行两个pdi程序，例如：一个为oracle到mysql，另一个为oracle到postgres，其中一个停止运行&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：从oracle迁移到mysql创建的作业和转换文件和oracle到postgres的作业和转换保存到一个路径，导致同名称的转换相互之间被覆盖，故在运行时候会出现混乱。&lt;/li&gt;
  &lt;li&gt;解决办法：将新建的作业和转换分别保存在两个不同的路径，最好是新建两个不同路径的仓库，关于如何新建仓库，请参考《kettle使用说明》文档。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section&quot;&gt;关键字的问题。&lt;/h4&gt;
&lt;p&gt;Oracle初始化到mysql，关键字前面会加上前缀“MY_”。如果在建表的时候出现错误，则需要检查表的字段中是否有关键字。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解决办法：出差的表单独进行处理，新建一个转换，实现关键字段该名称然后初始化出错的表。具体操作参见文档。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;oraclemysql&quot;&gt;oracle中的字段名从中可以有#号，但是到mysql会报错&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;解决办法：字段改名称，去掉#号&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;db2mysqlpostgres&quot;&gt;Db2初始化到mysql或是postgres出错&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：1）db2数据库连接用户没有权限访问出错的表；2）出错的表名存在小写字母&lt;/li&gt;
  &lt;li&gt;解决办法：使用更新后的pdi程序，更新后的程序会将db2的表名使用双引号括起来。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;oraclemysqlpg&quot;&gt;Oracle到mysql和pg时日期类型数据值有偏差&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：从oracle中读取日期类型的数据时候，读取结果与oracle数据库中的数据已经存在偏差。少数记录使用oracle10g的驱动读取数据少一个小时，用oracle11g的驱动会多一个小时，该问题尚待oracle工程师给出解决方案。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;oraclemysqlpostgres&quot;&gt;主键从ORACLE导入不到MYSQL和POSTGRES&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：pdi程序中没有对主键进行处理&lt;/li&gt;
  &lt;li&gt;解决办法：使用更新的pdi程序，执行Tools####Wizzard####Copy Tables Extension…功能添加主键；执行Tools####Wizzard####Copy Tables Data Only…功能可以只复制数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;oracleasciipostgreserror-invalid-byte-sequence-for-encoding-utf8-0x00&quot;&gt;Oracle中存在ascii字符导入到postgres时候报错：ERROR: invalid byte sequence for encoding “UTF8”: 0x00&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：PostgreSQL内部采用C语言风格的字符串（以0x00）表示结尾，因而不允许字符串中包括0x00，建议在转换时先对字符串类型的数据进行清洗，也就是增加一个节点用于删除字符串数据中的特殊字符0x00。&lt;/li&gt;
  &lt;li&gt;解决办法:使用新的pdi程序。在kettle的DataBase类中修改PreparedStatement.setString(int index,String value)方法传入的参数，将value的值trim之后在setString&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mysqlpostgres&quot;&gt;异构数据库之间的类型兼容问题。日期类型和时间类型的数据初始化到mysql或postgres中都为时间类型的数据，导致数据对比时候数据不一致。&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：Pdi程序中的类型转换采用的是向上兼容的方式，故日期和时间类型都转换为时间类型数据。&lt;/li&gt;
  &lt;li&gt;解决办法：针对与db2数据初始化到mysql和postgres，该问题在最新的pdi程序中已经处理。因为oracle中的日期类型字段既可以存日期又可以存时间，故没针对oracle数据做出处理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;db2mysqlpostgres-1&quot;&gt;Db2中没有主键的数据初始化到mysql和postgres需要添加索引&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;解决办法：使用最新的pdi程序，最新的pdi程序会添加主键和索引。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;db2decimalnmpostgres&quot;&gt;Db2中decimal（n,m）类型的数据初始化到postgres数据库被四舍五入。&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：Db2中decimal（n,m）类型的数据初始化到postgres中的类型不对。&lt;/li&gt;
  &lt;li&gt;解决办法：使用最新的pdi程序。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-1&quot;&gt;导数据中途时没有报错，直接软件退出&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：1）jvm内存溢出，需要修改jvm参数；2）pdi程序报swt错误&lt;/li&gt;
  &lt;li&gt;解决办法：修改jvm参数&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;kettledb2&quot;&gt;初次使用kettle做db2的初始化会报错&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;原因：kettle中的db2的jdbc驱动与使用的db2版本不对应。&lt;/li&gt;
  &lt;li&gt;解决办法：从db2的安装目录下拷贝jdbc驱动到kettle目录（libext/JDBC）下&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2012/01/04/some-problems-about-migrating-database-datas-with-kettle.html</link>
      <guid>http://blog.javachen.com/2012/01/04/some-problems-about-migrating-database-datas-with-kettle.html</guid>
      <pubDate>2012-01-04T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Mondrian and OLAP</title>
      <description>&lt;p&gt;Mondrian是一个用Java编写的OLAP引擎。他执行用MDX语言编写的查询，从关系数据库（RDBMS）中读取数据并且通过Java API以多维度的格式展示查询结果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00ff00;&quot;&gt;Online Analytical Processing&lt;/span&gt;&lt;/strong&gt;
联机分析处理（OLAP）指在线实时的分析大量数据。与联机事务处理系统（On-&lt;wbr /&gt;Line Transaction Processing，简称OLTP）不同，OLTP中典型的操作如读和修改单个的少量的记录，而OLAP批量处理数据并且所有操作都是只读的。“online”意味着即使是处理大量的数据----百万条数据记录，占有几个GB内存----系统必须足够快的反回查询结果以允许数据的交互式响应。正如我们将看到，数据展示面临相当大的技术挑战。&lt;/p&gt;

&lt;p&gt;OLAP引入了一种多维度查询的技术。鉴于一个关系数据库以行和列的形式存储所有数据，一个多维数据集包括轴和列。考虑下面的数据集：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2011/12/olap_examples20111217.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;行轴包括&quot;All products&quot;, &quot;Books&quot;,&quot;Fiction&quot;等等，并且列轴包括生产年份&quot;2000&quot;”和&quot;2001&quot;、&quot;Growth&quot;的计算值以及&quot;Unit sales&quot;和&quot;Dollar sales&quot;的测量值。每个单元代表在某一年的一个产品类别的销售额，例如2001年Magazines的$销售额是2426美元。&lt;/p&gt;

&lt;p&gt;这是一个比关系型数据库展现出来的更加丰富的视图。多维数据集的只不是永远都来自于一个关系数据库的列。 &#39;Total&#39;, &#39;Books&#39; and &#39;Fiction&#39; 是一个具有层次结构连续的成员，每一个成员都包括其下一层的成员。即使是在&quot;2000&quot;和&quot;2001&quot;一行，&quot;Growth&quot;是一个计算出来的值，它引入一个公式从其他列计算当前列的值。&lt;/p&gt;

&lt;p&gt;该例中使用的维度有：产品、生产线和测量值，仅仅是这个数据集可以分类和过滤的许多维度中的三个。维度，层次结构和测量值的集合被称为一个立方体。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00ff00;&quot;&gt;结论&lt;/span&gt;&lt;/strong&gt;
我希望我已经证明垛位是一个首选的数据显示方式。虽然一些多维数据库以多维度的格式存储数据库，我仍然认为这比以关系的格式存储数据要简单。&lt;br /&gt;
现在，你可以看看OLAP系统的架构。查看Mondrian architecture。http://mondrian.pentaho.com/documentation/architecture.php&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00ff00;&quot;&gt;说明&lt;/span&gt;&lt;/strong&gt;
&lt;div class=&quot;note&quot;&gt;
这是一篇翻译，原文来自http://mondrian.pentaho.com/documentation/olap.php。翻译水平有限，难免翻译不当，请见谅。&lt;/div&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2011/12/07/mondrian-and-olap.html</link>
      <guid>http://blog.javachen.com/2011/12/07/mondrian-and-olap.html</guid>
      <pubDate>2011-12-07T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>XUL 用户界面语言介绍</title>
      <description>&lt;p&gt;XUL[1]是英文“&lt;span style=&quot;color: #339966;&quot;&gt;XML User Interface Language&lt;/span&gt;”的首字母缩写。它是为了支持Mozilla系列的应用程序（如Mozilla Firefox和Mozilla Thunderbird）而开发的用户界面标示语言。顾名思义，它是一种应用XML来描述用户界面的标示语言。&lt;br /&gt;
XUL是开放标准，重用了许多现有的标准和技术[2]，包括CSS、JavaScript、DTD和RDF等。所以对于有网络编程和设计经验的人士来说，学习XUL比学习其他用户界面标示语言相对简单。&lt;br /&gt;
使用XUL的主要好处在于它提供了一套简易和跨平台的widget定义。这节省了编程人员在开发软件时所付出的努力。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #339966;&quot;&gt;&lt;strong&gt;XUL元素&lt;/strong&gt;&lt;/span&gt;
XUL定义了一套丰富的元素。它们大致上可分为以下几种：&lt;br /&gt;
基层元素：例如视窗、page、对话框、向导&lt;br /&gt;
Widget:例如标签、按钮、文字方块、条列式菜单、组合方块、选择钮、复选框、树、菜单、工具栏、分组框、标签页、色彩选择器、spacer、splitter&lt;br /&gt;
排版:例如方框、网格、堆栈、叠&lt;br /&gt;
事件和脚本:例如脚本、命令、key、broadcaster、observer&lt;br /&gt;
数据源:例如template、rule&lt;br /&gt;
其他:例如overlay（类似SSI，但在客户端运作，而且更为强大）、iframe、浏览器、编辑器&lt;br /&gt;
一个XUL文件中也可以包含其他XML命名空间的元素，例如XHTML、SVG和MathML。&lt;br /&gt;
现时的XUL还未在提供一些普遍的widget，例如spinbox、slider和canvas。XUL 2.0[3]计划中将会包括这些缺乏的控件。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #339966;&quot;&gt;&lt;strong&gt;XUL是如何处理的&lt;/strong&gt;&lt;/span&gt;[4]&lt;br /&gt;
Mozilla浏览器内部使用跟HTML的处理非常相似的方法来处理XUL：当你在浏览器的地址栏里面输入HTML页面的URL以后，浏览器就定位这个网址并下载页面内容，然后Mozilla将页面内容转换成树的数据结构，最后再将树转换成对象集合，集合中的对象最终被展现在屏幕上就成了我们所见的网页。CSS, 图片以及其他技术被用来控制页面的展现。XUL的处理过程与此非常类似。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #339966;&quot;&gt;&lt;strong&gt;XUL应用&lt;/strong&gt;&lt;/span&gt;
虽然XUL的设计原意是为了创作Mozilla程序及其扩展，但事实上人们也能利用它来编写基于HTTP的网络应用程序和基于swt/swing/gwt的客户端程序。一些开源的架构使用了XUL，例如Pentaho XUL Framework[5]。Pentaho XUL使用XUl跨多种技术（Swing, SWT, GWT）渲染用户界面，来实现业务逻辑的可重用性。shandor-xul[6]项目也是基于XUl开发的,项目地址见参考资料[6]。&lt;br /&gt;
Firefox里内置的一些XUL 地址见：&lt;a href=&quot;http://www.cnblogs.com/jxsoft/archive/2011/04/07/2008202.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/jxsoft/archive/2011/04/07/2008202.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #339966;&quot;&gt;&lt;strong&gt;运行XUL应用程序&lt;/strong&gt;&lt;/span&gt;
可以选择 3 种方式来运行 XUL 应用程序：&lt;br /&gt;
1.使用基于 Mozilla 的浏览器进行简单测试&lt;br /&gt;
2.使用XULRunner&lt;br /&gt;
3.使用Firefox 3.0作为XUL运行时，它的功能和 XULRunner很相似&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #339966;&quot;&gt;总结&lt;/span&gt;&lt;/strong&gt;
XUL用户界面语言是一种可用于开发Mozilla独立应用程序和浏览器扩展的通用语言，还可以用来实现跨多种UI技术的用户接口，提高业务逻辑代码的重用性，第二点视乎是更值得推荐使用的。关于XUl的教程见参考资料。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #339966;&quot;&gt;&lt;strong&gt;参考资料&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;li&gt;
1.XUL Wiki :&lt;a href=&quot;http://zh.wikipedia.org/wiki/XUL&quot; target=&quot;_blank&quot;&gt;http://zh.wikipedia.org/wiki/XUL&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;2.XML 用户界面语言（XUL）开发简介：&lt;a href=&quot;http://www.ibm.com/developerworks/cn/education/xml/x-xulintro/section2.html&quot; target=&quot;_blank&quot;&gt;http://www.ibm.com/developerworks/cn/education/xml/x-xulintro/section2.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;3.XUL 2.0: &lt;a href=&quot;https://wiki.mozilla.org/XUL:Home_Page&quot; target=&quot;_blank&quot;&gt;https://wiki.mozilla.org/XUL:Home_Page&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;4.[XUL结构](https://developer.mozilla.org/cn/XUL_%E6%95%99%E7%A8%8B/1-2_XUL%E7%9A%84%E7%BB%93%E6%9E%84)&lt;/li&gt;
&lt;li&gt;5.Pentaho XUL ramework: &lt;a href=&quot;http://wiki.pentaho.com/display/ServerDoc2x/The+Pentaho+XUL+Framework+Developer&#39;s+Guide&quot; target=&quot;_blank&quot;&gt;http://wiki.pentaho.com/display/ServerDoc2x/The+Pentaho+XUL+Framework+Developer&#39;s+Guide&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;6.shandor-xul:&lt;a href=&quot;http://code.google.com/p/shandor-xul/&quot; target=&quot;_blank&quot;&gt;http://code.google.com/p/shandor-xul/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;7.Mozilla XUL教程: &lt;a href=&quot;https://developer.mozilla.org/index.php?title=cn/XUL_%E6%95%99%E7%A8%8B&quot; target=&quot;_blank&quot;&gt;https://developer.mozilla.org/index.php&lt;/a&gt;
&lt;/li&gt;

</description>
      <link>http://blog.javachen.com/2011/11/25/xml-user-interface-language-introuction.html</link>
      <guid>http://blog.javachen.com/2011/11/25/xml-user-interface-language-introuction.html</guid>
      <pubDate>2011-11-25T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>在eclipse中构建Pentaho BI Server工程</title>
      <description>&lt;p&gt;首先需要说明的是，Pentaho BI Server源代码在&lt;em&gt;svn://source.pentaho.org/svnroot/bi-platform-v2/trunk/&lt;/em&gt;，并且用ivy构建。ivy没有用过也不熟悉，故不打算从这里使用ivy构建源码。&lt;/p&gt;

&lt;p&gt;当然，您可以参考&lt;a href=&quot;http://wiki.pentaho.com/display/ServerDoc2x/Building+and+Debugging+Pentaho+with+Eclipse&quot; target=&quot;_blank&quot;&gt;官方文档&lt;/a&gt;构建源码。&lt;/p&gt;

&lt;p&gt;Pentaho BI Server打包后的文件存于&lt;a href=&quot;http://sourceforge.net/projects/pentaho/files/Business%20Intelligence%20Server/&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;，其中包括（本文使用的是3.9.0版本）：biserver-ce-3.9.0-stable.zip，bi-platform-3.9.0-stable-sources.zip，biserver-ce-3.9.0-stable-javadoc.zip。&lt;/p&gt;

&lt;p&gt;将biserver-ce-3.9.0-stable.zip解压之后执行&lt;em&gt;biserver-ce/start-pentaho.bat&lt;/em&gt;（或是再linux环境下：&lt;em&gt;biserver-ce/start-pentaho.sh&lt;/em&gt;），即可成功启动biserver。现在我想将这个工程导入到eclipse然后调式跟踪代码，怎么做呢？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;以下操作是在eclipse3.7+tomcat 6.20的环境中进行的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在eclipse中创建一个web项目，名称为pentaho，然后将&lt;em&gt;biserver-ce/tomcat/webapps&lt;/em&gt;下的&lt;code&gt;pentaho-style&lt;/code&gt;和&lt;code&gt;sw-style&lt;/code&gt;拷贝到你的tomcat 6服务器的webapps目录下，将pentaho文件下的所有文件拷贝到工程下的WebContent目录下。由于biserver需要访问pentaho-solutions下的文件，故还需要修改&lt;code&gt;WEB-INF/web.xml&lt;/code&gt;文件你的以下配置，用于指定pentaho-solutions的路径：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt; context-param &amp;gt;
	&amp;lt; param-name &amp;gt;solution-path&amp;lt; /param-name&amp;gt;
	&amp;lt; param-value &amp;gt;/home/june.chan/opt/biserver-ce/pentaho-solutions&amp;lt; /param-value&amp;gt;
&amp;lt; /context-param &amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在即可部署项目，运行&lt;code&gt;biserver-ce/data/start_hypersonic.bat&lt;/code&gt;（用于启动数据库），然后启动tomcat，就可以通过&lt;em&gt;http://localhost:8080/pentaho&lt;/em&gt;访问biserver。如果启动报错，需要将hsqldb-1.8.0.7.jar包，拷贝到应用路径下（&lt;em&gt;\tomcat-pci-test\biserver-ce\tomcat\webapps\pentaho\WEB-INF\lib&lt;/em&gt;）。&lt;br /&gt;&lt;br /&gt;
现在可以看到biserver的登录页面，但是还是没有看到biserver的源代码。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;接下来，构建源代码。&lt;/strong&gt;&lt;br /&gt;
在biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib下面有很多名称为pentaho-bi-platform-########-3.9.0-stable.jar的jar文件，这些即是biserver源码编译之后的class文件。在bi-platform-3.9.0-stable-sources.zip压缩文件你即可以看到这些class文件的源代码。将这些src包解压然后拷贝到之前新建的pentaho工程的src目录下。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;需要注意的是：&lt;/font&gt;&lt;/strong&gt;&lt;br /&gt;
1.这些src jar包你只报告java文件，不包括配置文件：log4j配置文件，hibernate配置和实体映射文件，ehcache配置文件&lt;br /&gt;&lt;br /&gt;
2.上面的配置文件需要到biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib目录下的pentaho-bi-platform-########-3.9.0-stable.jar文件中寻找。&lt;br /&gt;&lt;br /&gt;
3.&lt;br /&gt;
* biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-engine-security-3.9.0-stable.jar文件中有ldap的配置文件，&lt;br /&gt;
* biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-engine-services-3.9.0-stable.jar文件中有ehcache的配置文件，&lt;br /&gt;
* biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-plugin-actions-3.9.0-stable.jar文件中有log4j的配置文件，&lt;br /&gt;
* biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-repository-3.9.0-stable.jar文件中有hibernate配置文件，&lt;br /&gt;
* biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib/pentaho-bi-platform-security-userroledao-3.9.0-stable.jar文件中有hibernated的实体映射文件。&lt;/p&gt;

&lt;p&gt;4.biserver-ce-3.9.0-stable.zip的lib（&lt;code&gt;biserver-ce/tomcat/webapps/pentaho/WEB-INF/lib&lt;/code&gt;）目录下的servlete jar包的版本为2.3，版本过低需要替换为更高版本知道源码中不在有servlete编译错误&lt;/p&gt;

</description>
      <link>http://blog.javachen.com/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse.html</link>
      <guid>http://blog.javachen.com/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse.html</guid>
      <pubDate>2011-09-28T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Pentaho现场支持遇到问题及解决办法</title>
      <description>&lt;p&gt;很久没写文章了，最近在关注Pentaho。&lt;br /&gt;
 以下是9月16日现场提出的问题解决办法：&lt;br /&gt;
      1、PDF预览中文没显示，txt预览中文乱码：&lt;br /&gt;
           1）、设置File-&amp;gt;Configuration -&amp;gt;output-pageable-pdf的encoding 为Identity-H&lt;br /&gt;
           2）、将需要输出中文的报表项目的字体设置为中文字体，例如宋体&lt;br /&gt;
           3）、如要发布到服务器，需要修改如下的配置：&lt;br /&gt;
             pentaho/server/biserver-ee/tomcat/webapps/pentaho/WEB-INF/classes/classic-engine.properties：&lt;br /&gt;
             org.pentaho.reporting.engine.classic.core.modules.output.pageable.pdf.Encoding=Identity-H&lt;br /&gt;
     2、实现文件拷贝方式发布报表&lt;br /&gt;
           可以通过文件方式发布，只要将报表的prpt文件拷贝到Solution的目录（Pentaho安装路径的server\biserver-ee\pentaho-solutions）下就可以了&lt;br /&gt;
     3、报表链接参数传递问题&lt;br /&gt;
          由于参数带中文造成的，可以对参数的值URLENCODE(&quot;value&quot;; &quot;utf-8&quot;)来解决&lt;br /&gt;
     4、查询参数缺省值问题&lt;br /&gt;
          关于日期的默认值。可以使用报表系统提供的日期变量设置，如TODAY，DATE，YEAR。。。&lt;br /&gt;
     5、实现在pie chart上显示文字&lt;br /&gt;
          以把label默认显示的百分比改为文字：label-formate = {0}， 但是label显示百分比，同时在pie图的划分区域显示文字是不能的。&lt;br /&gt;
     6、报表集成时候垂直滚动条是否可以去掉&lt;br /&gt;
          改变报表的高度：报表设计器 file-page setup&lt;br /&gt;
     7、报表中的chart不能导出到Excel2007&lt;br /&gt;
          目前为系统bug，excel2003能够正常导出&lt;br /&gt;
     8、实现隔行换色&lt;br /&gt;
          选中Details中的field再attribute面板上设置name的名称（如“row-band”），然后通过Format--&amp;gt;Row-Banding，可以设置Visible Color 、Inisible Color，再Element中输入&quot;row-band&quot;&lt;br /&gt;
     9、显示top N  ：托一个message field，在里面输入表达式，如，$（topn）,topn为传入的参数&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2011/09/26/resolved-pentaho-problems-9-16.html</link>
      <guid>http://blog.javachen.com/2011/09/26/resolved-pentaho-problems-9-16.html</guid>
      <pubDate>2011-09-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>在Fedora 15 上搭建Eucalyptus</title>
      <description>&lt;p&gt;&lt;div class=&quot;pic&quot;&gt;&lt;img src=&quot;http://open.eucalyptus.com/themes/eucalyptus/img/eucalyptus_logo_awh.png&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
在Fedora 15 上搭建Eucalyptus平台，在Fedora 15 上搭建Eucalyptus与在Centos上搭建Eucalyptus有什么区别呢？参照这篇文章&lt;a href=&quot;http://open.eucalyptus.com/wiki/EucalyptusInstallationFedora_v2.0&quot; target=&quot;_blank&quot;&gt;Installing Eucalyptus (2.0) on Fedora 12&lt;/a&gt;，然后注意一些细节，视乎就能安装成功。不管你信不信，我是在虚拟机中安装fedora15，然后安装Eucalyptus失败了，失败的原因是xen的网络没有配置好，查看资源的时候free / max都为0000.&lt;/p&gt;

&lt;p&gt;毕竟是第一次接触云计算，第一次接触XEN，第一次接触Eucalyptus，Eucalyptus改装的都装了，就是XEN的网络没有配置好，当时很是迷糊。在接触了OpenNebula 和OpenStack之后，横向对比，视乎明白了很多千丝万缕的关联与奥秘。在安装OpenNebula，最主要是安装OpenStack成功之后，想到了之前Eucalyptus安装失败的原因。限于现在精力不在云计算上，暂且不去重新安装Eucalyptus，等之后再去尝试。下次尝试，定是醍醐灌顶，行云流水，很是期待。&lt;/p&gt;

&lt;p&gt;如果你也在Fedora上安装Eucalyptus平台，咱们可以交流交流，等到时机成熟，会将在Fedora 15 上搭建Eucalyptus的过程及遇到的问题发表在博客上；如果你想研究Eucalyptus平台java部分的代码，咱们也可以彼此分享各自的心得。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2011/08/20/install-eucalyptus-on-fedora-15.html</link>
      <guid>http://blog.javachen.com/2011/08/20/install-eucalyptus-on-fedora-15.html</guid>
      <pubDate>2011-08-20T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Export DhtmlxGrid to PDF in Java</title>
      <description>&lt;p&gt;将DhtmlxGrid数据导出到pdf这是很常见的需求，dhtmlx官网提供了php和java版本的例子，你可以去官网查看这篇文章《&lt;a href=&quot;http://www.dhtmlx.com/blog/?p=855&quot;&gt;Grid-to-Excel, Grid-to-PDF Available for Java&lt;/a&gt;》，你可以从以下地址下载导出程序源码：&lt;br /&gt;
&lt;a href=&quot;http://www.dhtmlx.com/x/download/regular/export/XML2Excel.war&quot;&gt;Export to Excel&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://www.dhtmlx.com/x/download/regular/export/XML2PDF.war&quot;&gt;Export to PDF&lt;/a&gt;&lt;br /&gt;
当然，还有一个示例工程：&lt;a href=&quot;http://www.dhtmlx.com/x/download/regular/export/javaexport_sample.zip&quot;&gt; .zip archive with an example&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;XML2PDF和XML2Excel工程内代码很相似，XML2PDF内部使用了PDFjet.jar导出PDF，而XML2Excel使用JXL导出Excel。&lt;br /&gt;
需要说明的是，还需要引入dhtmlxgrid_export.js文件，该文件是导出grid的js源码，主要用于将表格数据，包括表头、样式等，序列化为xml字符串，然后模拟一个Form表单提交数据。&lt;/p&gt;

&lt;p&gt;将上面三个工程导入到一个工程然后打开sample.html页面，效果如下：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2011/08/export-dhtmlxgrid-to-pdf.png&quot; alt=&quot;&quot; title=&quot;export dhtmlxgrid to pdf&quot; width=&quot;300&quot; height=&quot;166&quot; class=&quot;aligncenter size-medium wp-image-2385&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;点击Get as PDF按钮，你会发现会打开一个新的窗口，然后页面什么都没有，而eclipse控制台报空指针异常。异常的主要原因在于下段代码：。&lt;/p&gt;
&lt;pre lang=&quot;java&quot;&gt;
DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance ();
DocumentBuilder db = dbf.newDocumentBuilder();
Document dom = null;
try {
     dom = db.parse(new InputSource(new StringReader(xml)));
}catch(SAXException se) {
     se.printStackTrace();
}catch(IOException ioe) { 
     ioe.printStackTrace();
}
root = dom.getDocumentElement();
&lt;/pre&gt;

&lt;p&gt;上面的代码，DocumentBuilder解析xml字符串后dom对象内并没有数据。&lt;br /&gt;
为了能够看到DhtmlxGrid导出pdf的效果，决定将上面的代码用dom4j改写，于是有了下面的代码：&lt;/p&gt;
&lt;pre lang=&quot;java&quot;&gt;
public class PDFXMLParser {
	Element root;
	PDFColumn[][] columns;
	PDFRow[] rows;
	double[] widths;
	private Boolean header = false;
	private Boolean footer = false;
	private String profile = &quot;gray&quot;;
	private double[] orientation = null;

	public void setXML(String xml) {
		SAXReader saxReader = new SAXReader();

		Document document = null;
		try {
			document = saxReader.read(new ByteArrayInputStream(xml.getBytes()));
		} catch (DocumentException e) {
			e.printStackTrace();
		}
		root = document.getRootElement();

		if ((root.attributeValue(&quot;header&quot;) != null)
				&amp;amp;&amp;amp; (root.attributeValue(&quot;header&quot;).equalsIgnoreCase(&quot;true&quot;) == true)) {
			header = true;
		}
		String footer_string = root.attributeValue(&quot;footer&quot;);
		if ((footer_string != null)
				&amp;amp;&amp;amp; (footer_string.equalsIgnoreCase(&quot;true&quot;) == true)) {
			footer = true;
		}
		String profile_string = root.attributeValue(&quot;profile&quot;);
		if (profile_string != null) {
			profile = profile_string;
		}

		String orientation_string = root.attributeValue(&quot;orientation&quot;);
		if (orientation_string != null) {
			if (orientation_string.equalsIgnoreCase(&quot;landscape&quot;)) {
				orientation = A4.LANDSCAPE;
			} else {
				orientation = A4.PORTRAIT;
			}
		} else {
			orientation = Letter.PORTRAIT;
		}
	}

	public PDFColumn[][] getColumnsInfo() {
		PDFColumn[] colLine = null;
		List n1 = root.element(&quot;head&quot;).elements(&quot;columns&quot;);
		if ((n1 != null) &amp;amp;&amp;amp; (n1.size() &amp;gt; 0)) {
			columns = new PDFColumn[n1.size()][];
			for (int i = 0; i &amp;lt; n1.size(); i++) {
				Element cols = (Element) n1.get(i);
				List n2 = cols.elements(&quot;column&quot;);
				if ((n2 != null) &amp;amp;&amp;amp; (n2.size() &amp;gt; 0)) {
					colLine = new PDFColumn[n2.size()];
					for (int j = 0; j &amp;lt; n2.size(); j++) {
						Element col_xml = (Element) n2.get(j);
						PDFColumn col = new PDFColumn();
						col.parse(col_xml);
						colLine[j] = col;
					}
				}
				columns[i] = colLine;
			}
		}
		createWidthsArray();
		optimizeColumns();
		return columns;
	}
        public PDFRow[] getGridContent() {
		List nodes = root.elements(&quot;row&quot;);
		if ((nodes != null) &amp;amp;&amp;amp; (nodes.size() &amp;gt; 0)) {
			rows = new PDFRow[nodes.size()];
			for (int i = 0; i &amp;lt; nodes.size(); i++) {
				rows[i] = new PDFRow();
				rows[i].parse((Element) nodes.get(i));
			}
		}
		return rows;

	}

       *****
}
&lt;/pre&gt;

&lt;p&gt;还需要修改PDFRow类的parse方法和PDFColumn的parse方法。&lt;/p&gt;
&lt;pre lang=&quot;java&quot;&gt;
public class PDFRow {

	private String[] cells;

	public void parse(Element parent) {
		List nodes = ((Element) parent).elements(&quot;cell&quot;);
		if ((nodes != null) &amp;amp;&amp;amp; (nodes.size() &amp;gt; 0)) {
			cells = new String[nodes.size()];
			for (int i = 0; i &amp;lt; nodes.size(); i++) {
				cells[i] = ((Element) nodes.get(i)).getTextTrim();
			}
		}
	}

	public String[] getCells() {
		return cells;
	}
}

public class PDFColumn {

	public void parse(Element parent) {
		colName = parent.getText();
		String width_string = parent.attributeValue(&quot;width&quot;);
		if (width_string!=null&amp;amp;&amp;amp;width_string.length() &amp;gt; 0) {
			width = Integer.parseInt(width_string);
		}
		type = parent.attributeValue(&quot;type&quot;);
		align = parent.attributeValue(&quot;align&quot;);
		String colspan_string = parent.attributeValue(&quot;colspan&quot;);
		if (colspan_string!=null&amp;amp;&amp;amp;colspan_string.length() &amp;gt; 0) {
			colspan = Integer.parseInt(colspan_string);
		}
		String rowspan_string = parent.attributeValue(&quot;rowspan&quot;);
		if (rowspan_string!=null&amp;amp;&amp;amp;rowspan_string.length() &amp;gt; 0) {
			rowspan= Integer.parseInt(rowspan_string);
		}
	}
}
&lt;/pre&gt;

&lt;p&gt;这样xml字符串就能正常解析了，然后使用pdfjet.jar包就可以导出pdf了，最后的效果如下：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2011/08/export-dhtmlx-to-pdf-pdf.png&quot; alt=&quot;&quot; title=&quot;export dhtmlx to pdf -pdf&quot; width=&quot;300&quot; height=&quot;134&quot; class=&quot;aligncenter size-medium wp-image-2386&quot; /&gt;
&lt;/div&gt;

&lt;h2&gt;结论：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;1.导出pdf和导出Excel代码差不多，这里不做说明。&lt;/li&gt;
  &lt;li&gt;2.使用上面的工具，可以将dhtmlxgrid的数据导出到pdf，并且导出的pdf还保持了grid表格的样式（包括颜色、多表头、表头合并、复选框等等），这点很不错。&lt;/li&gt;
  &lt;li&gt;3.导出的pdf为多页显示，每页有表头&lt;/li&gt;
  &lt;li&gt;4.导出后的pdf页面可以直接打印，当然如果在代码上做点处理，可以直接将pdf保存为一个文件，让用户下载。&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2011/08/11/export-dhtmlxgrid-to-pdf-in-java.html</link>
      <guid>http://blog.javachen.com/2011/08/11/export-dhtmlxgrid-to-pdf-in-java.html</guid>
      <pubDate>2011-08-11T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>自定义dhtmlxGrid表头菜单</title>
      <description>&lt;p&gt;dhtmlxGrid可以定义表头菜单以及表格右键菜单，表格右键菜单可以自定义，但是表头菜单只能使用其提供的菜单。dhtmlxGrid默认的表头菜单可以决定表格中每一列是否在表格中显示，并没有提供更多的扩展，如果我想自定义表头菜单，该怎么做呢？本文就是基于自定义表格菜单，说说我的实现方式。&lt;br /&gt;
以下是dhtmlxGrid的表头菜单效果：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img class=&quot;aligncenter size-medium wp-image-2287&quot; title=&quot;dhtmlxgrid-head-menu&quot; src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2011/07/dhtmlxgrid-head-menu.jpg&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;174&quot; /&gt;&lt;/div&gt;
&lt;p&gt;其功能过于单一，以下是表格右键菜单效果：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;&lt;img class=&quot;aligncenter size-medium wp-image-2288&quot; title=&quot;dhtmlxgrid-context-menu&quot; src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2011/07/dhtmlxgrid-context-menu.jpg&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;126&quot; /&gt;&lt;/div&gt;
&lt;p&gt;如果能够像表格菜单一样自定义表头菜单，那会是一件非常有意义的事情，因为dhtmlxGrid菜单都是一些针对行和单元格的操作，没有提过针对列的操作，比如我可能需要在某一列上实现该列的显示与隐藏、排序、改变列属性以及在该列右边添加一新的列，等等。&lt;br /&gt;
如何实现表头菜单的自定义呢？可不可将表格右键菜单移到表头上去呢？&lt;!--more--&gt;&lt;br /&gt;
首先，来看看context menu的实现方式，下面代码来自dhtmlxGrid Samples中的Context menu例子源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function onButtonClick(menuitemId, type) {
    var data = mygrid.contextID.split(&quot;_&quot;);
    //rowId_colInd;
    mygrid.setRowTextStyle(data[0], &quot;color:&quot; + menuitemId.split(&quot;_&quot;)[1]);
    return true;
}
menu = new dhtmlXMenuObject();
menu.setIconsPath(&quot;../common/images/&quot;);
menu.renderAsContextMenu();
menu.attachEvent(&quot;onClick&quot;, onButtonClick);
menu.loadXML(&quot;../common/_context.xml&quot;);
mygrid = new dhtmlXGridObject(&#39;gridbox&#39;);
mygrid.setImagePath(&quot;../../codebase/imgs/&quot;);
mygrid.setHeader(&quot;Author,Title&quot;);
mygrid.setInitWidths(&quot;250,250&quot;);
mygrid.enableAutoWidth(true);
mygrid.setColAlign(&quot;left,left&quot;);
mygrid.setColTypes(&quot;ro,link&quot;);
mygrid.setColSorting(&quot;str,str&quot;);
mygrid.enableContextMenu(menu);
mygrid.init();
mygrid.setSkin(&quot;dhx_skyblue&quot;);
mygrid.loadXML(&quot;../common/grid_links.xml&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码创建了一个menu并将其作为context menu附件到grid上面去，下面为最关键的两行行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;menu.renderAsContextMenu();
mygrid.enableContextMenu(menu);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面对于context menu提供的方法太少，这时候可以看看dhtmlxMenu api，看看有没有设置context menu生效位置的方法（指定context menu在哪片区域有效）。在dhtmlxMenu API Methods里没有找到需要的方法，这时候在官网论坛搜搜，也许可以找到点什么。&lt;br /&gt;
在论坛里可以找到一个例子，大致代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function onButtonClick(menuitemId, type) {
    var data = mygrid.contextID.split(&quot;_&quot;);
    //rowId_colInd;
    mygrid.setRowTextStyle(data[0], &quot;color:&quot; + menuitemId.split(&quot;_&quot;)[1]);
    return true;
}
menu = new dhtmlXMenuObject();
menu.setIconsPath(&quot;../common/images/&quot;);
menu.attachEvent(&quot;onClick&quot;, onButtonClick);
menu.loadXML(&quot;../common/_context.xml&quot;);

mygrid = new dhtmlXGridObject(&#39;gridbox&#39;);
mygrid.setImagePath(&quot;../../codebase/imgs/&quot;);
mygrid.setHeader(&quot;Author,Title&quot;);
mygrid.setInitWidths(&quot;250,250&quot;);
mygrid.enableAutoWidth(true);
mygrid.setColAlign(&quot;left,left&quot;);
mygrid.setColTypes(&quot;ro,link&quot;);
mygrid.setColSorting(&quot;str,str&quot;);
//mygrid.enableContextMenu(menu); //使其失效
mygrid.init();
mygrid.setSkin(&quot;dhx_skyblue&quot;);
mygrid.loadXML(&quot;../common/grid_links.xml&quot;);

mygrid.hdr.id = &quot;header_id&quot;;
var header_row = mygrid.hdr.rows[1];
for ( var i = 0; i &amp;amp;lt; header_row.cells.length; i++) {
   header_row.cells[i].id = &quot;context_zone_&quot; + i;
}
menu.addContextZone(&quot;header_id&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面最关键的代码在最后几行，给dhtmlxGrid表头设置了一个id，然后调用menu的addContextZone()方法指定centext的有效区域。视乎这就是我们所需要的，但是你执行以上代码你会发现onButtonClick方法里mygrid.contextID会报错，原因是mygrid没有contextID属性（在context menu中通过该属性可以获知鼠标焦点在哪一行，但是现在在表头上强加了该menu，所以并不存在该属性了）。&lt;br /&gt;
剩下的问题是需要解决，菜单单击事件了。我们可以在表头的contextmenu事件处罚的时候获取鼠标焦点，并将自定义的菜单在该位置显示，该方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dhtmlxEvent(mygrid.hdr, &quot;contextmenu&quot;, function(ev) {
	ev = ev || event;
	var el = ev.target || ev.srcElement;
	var zel = el;
	while (zel.tagName != &quot;TABLE&quot;)
		zel = zel.parentNode;
	var grid = zel.grid;
	if (!grid)
		return;
	grid.setActive();

	el = grid.getFirstParentOfType(el, &quot;TD&quot;)

	if ((grid) &amp;amp;amp;&amp;amp;amp; (!grid._colInMove)) {
		grid.resized = null;
		if ((!grid._mCols) || (grid._mCols[el._cellIndex] == &quot;true&quot;))
			colId = el._cellIndex + 1;//获得表头右键菜单焦点所在列索引
	}

	function mouseCoords(ev) {
		if (ev.pageX || ev.pageY) {
			return {
				x : ev.pageX,
				y : ev.pageY
			};
		}
		var d = _isIE &amp;amp;amp;&amp;amp;amp; document.compatMode != &quot;BackCompat&quot; ? 
	            document.documentElement: document.body;
		return {
			x : ev.clientX + d.scrollLeft - d.clientLeft,
			y : ev.clientY + d.scrollTop - d.clientTop
		};
	}

	var coords = mouseCoords(ev);
	menu.addContextZone(&quot;header_id&quot;);
	menu.showContextMenu(coords.x, coords.y);//强制显示
	return true;
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上面的代码里，我们获得表头右键菜单焦点所在列索引，将其值赋给colId，然后在菜单单击事件的时候添加一新的列并将colId重置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function onButtonClick(menuitemId, type, e) {
	mygrid.insertColumn(colId, &quot;12&quot;, &quot;ed&quot;, 80);
	colId = 0;
	return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，需要禁止掉表格数据区域的菜单显示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mygrid.attachEvent(&quot;onBeforeContextMenu&quot;, function(rid, cid, e) {
	return false;//禁止数据区域菜单
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后的最后，最后的代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var mygrid, colId;

function onButtonClick(menuitemId, type, e) {
	mygrid.insertColumn(colId, &quot;12&quot;, &quot;ed&quot;, 80);
	colId = 0;
	return true;
}

menu = new dhtmlXMenuObject();
menu.setIconsPath(&quot;../common/images/&quot;);
menu.renderAsContextMenu();
menu.attachEvent(&quot;onClick&quot;, onButtonClick);
menu.loadXML(&quot;../common/_context.xml&quot;);
menu.attachEvent(&quot;onBeforeContextMenu&quot;, function(zoneId, e) {
	var hdr = document.getElementById(zoneId)
	return true;
});

mygrid = new dhtmlXGridObject(&#39;gridbox&#39;);
mygrid.setImagePath(&quot;../codebase/imgs/&quot;);
mygrid.setHeader(&quot;Sales,Book Title,Author,Price,In Store,Shipping,Bestseller,
          Date of Publication&quot;);
mygrid.setInitWidths(&quot;50,150,100,80,80,80,80,200&quot;);
mygrid.setColAlign(&quot;right,left,left,right,center,left,center,center&quot;);
mygrid.setColTypes(&quot;dyn,edtxt,ed,price,ch,co,ra,ro&quot;);

mygrid.init();
mygrid.setSkin(&quot;dhx_skyblue&quot;);
//mygrid.enableHeaderMenu();
mygrid.enableColumnMove(true);
mygrid.enableContextMenu(menu);
dhtmlxEvent(mygrid.hdr, &quot;contextmenu&quot;, function(ev) {
	ev = ev || event;
	var el = ev.target || ev.srcElement;
	var zel = el;
	while (zel.tagName != &quot;TABLE&quot;)
		zel = zel.parentNode;
	var grid = zel.grid;
	if (!grid)
		return;
	grid.setActive();

	el = grid.getFirstParentOfType(el, &quot;TD&quot;)

	if ((grid) &amp;amp;#038;&amp;amp; (!grid._colInMove)) {
		grid.resized = null;
		if ((!grid._mCols) || (grid._mCols[el._cellIndex] == &quot;true&quot;))
                            //获得表头右键菜单焦点所在列索引
			colId = el._cellIndex + 1;
	}

	function mouseCoords(ev) {
		if (ev.pageX || ev.pageY) {
			return {
				x : ev.pageX,
				y : ev.pageY
			};
		}
		var d = _isIE &amp;amp;#038;&amp;amp; document.compatMode != &quot;BackCompat&quot; ? 
                         document.documentElement: document.body;
		return {
			x : ev.clientX + d.scrollLeft - d.clientLeft,
			y : ev.clientY + d.scrollTop - d.clientTop
		};
	}

	var coords = mouseCoords(ev);
	menu.addContextZone(&quot;header_id&quot;);
	menu.showContextMenu(coords.x, coords.y);//强制显示
	return true;
});

mygrid.attachEvent(&quot;onBeforeContextMenu&quot;, function(rid, cid, e) {
	return false;//禁止数据区域菜单
});

mygrid.loadXML(&quot;../common/grid_ml_16_rows_columns_manipulations.xml&quot;);

mygrid.hdr.id = &quot;header_id&quot;;
var header_row = mygrid.hdr.rows[1];
for ( var i = 0; i &amp;lt; header_row.cells.length; i++) {
	header_row.cells[i].id = &quot;context_zone_&quot; + i;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果图如下;&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2011/07/dhtmlxgrid-custom-head-menu.jpg&quot; alt=&quot;&quot; title=&quot;dhtmlxgrid-custom-head-menu&quot; width=&quot;300&quot; height=&quot;154&quot; class=&quot;aligncenter size-medium wp-image-2291&quot; /&gt;&lt;/div&gt;

</description>
      <link>http://blog.javachen.com/2011/07/31/custom-dhtmlxgrid-header-menu.html</link>
      <guid>http://blog.javachen.com/2011/07/31/custom-dhtmlxgrid-header-menu.html</guid>
      <pubDate>2011-07-31T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Drag an item to dhtmlxGrid and add a column</title>
      <description>&lt;p&gt;dhtmlxGrid支持tree和grid、grid之间、grid内部进行拖拽，如在grid内部进行拖拽，可以增加一行；在grid之间拖拽，第一个grid的记录删除，第二个grid增加一行记录。如果我想在拖拽之后不是添加一行而是一列，该怎么做呢？&lt;br /&gt;
现在有个需求，就是左边有个tree，右边有个grid，将左边tree的一个节点拖到右边grid的表头并动态增加一列。这个怎么做呢？&lt;br /&gt;
如果你想快点看到最后的实现方法，你可以直接跳到本文的最后参看源码。&lt;br /&gt;
首先看看dhtmlxTree 关于&lt;a href=&quot;http://www.dhtmlx.com/docs/products/dhtmlxGrid/samples/05_drag_n_drop/&quot;&gt;Drag-n-Drop&lt;/a&gt;的例子，其中有这样一个例子&lt;a href=&quot;http://www.dhtmlx.com/docs/products/dhtmlxTree/samples/05_drag_n_drop/08_pro_drag_out.html&quot;&gt;Custom Drag Out&lt;/a&gt;。&lt;br /&gt;
上面的例子，右边定义了一个输入框，其id为“sInput”，代码如下：&lt;/p&gt;
&lt;pre escaped=&quot;true&quot; lang=&quot;javascript&quot; line=&quot;1&quot;&gt;function maf() {
    return false;
}
tree = new dhtmlXTreeObject(&quot;treeboxbox_tree&quot;, &quot;100%&quot;, &quot;100%&quot;, 0);

tree.setSkin(&#39;dhx_skyblue&#39;);
tree.setImagePath(&quot;../../codebase/imgs/csh_yellowbooks/&quot;);
tree.enableDragAndDrop(true);
tree.setDragHandler(maf);
tree.enableSmartXMLParsing(true);
tree.loadXML(&quot;../common/tree_05_drag_n_drop.xml&quot;);

function s_control() {
    this._drag = function(sourceHtmlObject, dhtmlObject, targetHtmlObject) {
        targetHtmlObject.style.backgroundColor = &quot;&quot;;
        targetHtmlObject.value = sourceHtmlObject.parentObject.label;
    }
    this._dragIn = function(htmlObject, shtmlObject) {
        htmlObject.style.backgroundColor = &quot;#fffacd&quot;;
        return htmlObject;
    }
    this._dragOut = function(htmlObject) {
        htmlObject.style.backgroundColor = &quot;&quot;;
        return this;
    }
}
var sinput = document.getElementById(&#39;sInput&#39;);
tree.dragger.addDragLanding(sinput, new s_control);
&lt;/pre&gt;

&lt;p&gt;为了使tree支持拖拽功能，必须添加以下代码：&lt;/p&gt;
&lt;pre escaped=&quot;true&quot; lang=&quot;javascript&quot; line=&quot;1&quot;&gt;tree.enableDragAndDrop(true);&lt;/pre&gt;

&lt;p&gt;为了实现自定义拖拽的输出，添加了以下代码：&lt;/p&gt;
&lt;pre escaped=&quot;true&quot; lang=&quot;javascript&quot; line=&quot;1&quot;&gt;tree.dragger.addDragLanding(sinput, new s_control);&lt;/pre&gt;

&lt;p&gt;从上面的字母意思可以看出，是在tree的拖拽对象dragger对象上添加一个拖拽着地对象，第一个常数是指拖拽到哪一个区域，第二个常数定义拖拽的三个方法：&lt;/p&gt;
&lt;pre escaped=&quot;true&quot; lang=&quot;javascript&quot; line=&quot;1&quot;&gt;    this._drag = function(sourceHtmlObject, dhtmlObject, targetHtmlObject) {
        targetHtmlObject.style.backgroundColor = &quot;&quot;;
        targetHtmlObject.value = sourceHtmlObject.parentObject.label;
    }
    this._dragIn = function(htmlObject, shtmlObject) {
        htmlObject.style.backgroundColor = &quot;#fffacd&quot;;
        return htmlObject;
    }
    this._dragOut = function(htmlObject) {
        htmlObject.style.backgroundColor = &quot;&quot;;
        return this;
    }
&lt;/pre&gt;

&lt;p&gt;参照上面的思路，我们可以在grid的表头上面定义一个id，然后通过该id获得表头的dom对象，更好的一个方法是通过mygrid.hdr（看看源码就知道列）能过获得grid的表头对象，然后调用下面的方法，定义tree拖拽到grid的表头：&lt;/p&gt;
&lt;pre escaped=&quot;true&quot; lang=&quot;javascript&quot; line=&quot;1&quot;&gt;tree.dragger.addDragLanding(mygrid.hdr, new s_control);&lt;/pre&gt;

&lt;p&gt;但是这个时候，你将tree的一个节点拖到grid的表头，grid不会有任何反应，故需要改写s_control对象的方法，这里主要是改写一个方法：&lt;/p&gt;

&lt;pre escaped=&quot;true&quot; lang=&quot;javascript&quot; line=&quot;1&quot;&gt;
	var insertId;
	this._drag = function(sourceHtmlObject, dhtmlObject,
		targetHtmlObject, e) {
	var zel = e;
	while (zel.tagName != &quot;TABLE&quot;)
		zel = zel.parentNode;
	var grid = zel.grid;
	if (!grid)
		return;
	grid.setActive();
	if (!grid._mCol || e.button == 2)
		return;
	e = grid.getFirstParentOfType(e, &quot;TD&quot;)

	if ((grid) &amp;amp;&amp;amp; (!grid._colInMove)) {
		grid.resized = null;
		if ((!grid._mCols) || (grid._mCols[e._cellIndex] == &quot;true&quot;))
			insertId = e._cellIndex + 1;
	}

	mygrid.insertColumn(insertId, &quot;12&quot;, &quot;ed&quot;, 80);
}
&lt;/pre&gt;
&lt;p&gt;该方法主要做的事情是计算拖拽落脚时候鼠标焦点所在的列，然后在其右边添加一新的列。&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;
&lt;img class=&quot;aligncenter&quot; title=&quot;QQ20110724211631&quot; src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2011/07/QQ20110724211631.png&quot; alt=&quot;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;本例最后的代码：&lt;/p&gt;
&lt;pre escaped=&quot;true&quot; lang=&quot;javascript&quot; line=&quot;1&quot;&gt;
	var mygrid;
	function maf() {
		return false;
	}

	tree = new dhtmlXTreeObject(&quot;treeboxbox_tree&quot;, &quot;100%&quot;, &quot;100%&quot;, 0);
	tree.setSkin(&#39;dhx_skyblue&#39;);
	tree.setImagePath(&quot;../../dhtmlxTree/codebase/imgs/csh_yellowbooks/&quot;);
	tree.enableDragAndDrop(true);
	//tree.setDragHandler(maf);
	tree.enableSmartXMLParsing(true);
	tree.loadXML(&quot;../../dhtmlxTree/samples/common/tree_05_drag_n_drop.xml&quot;)
	tree.openAllItems(0);

	function s_control() {
		var insertId;
		this._drag = function(sourceHtmlObject, dhtmlObject,
				targetHtmlObject, e) {
			var zel = e;
			while (zel.tagName != &quot;TABLE&quot;)
				zel = zel.parentNode;
			var grid = zel.grid;
			if (!grid)
				return;
			grid.setActive();
			if (!grid._mCol || e.button == 2)
				return;
			e = grid.getFirstParentOfType(e, &quot;TD&quot;)

			if ((grid) &amp;amp;&amp;amp; (!grid._colInMove)) {
				grid.resized = null;
				if ((!grid._mCols) || (grid._mCols[e._cellIndex] == &quot;true&quot;))
					insertId = e._cellIndex + 1;
			}

			mygrid.insertColumn(insertId, &quot;12&quot;, &quot;ed&quot;, 80);
		}
	}
	mygrid = new dhtmlXGridObject(&#39;gridbox&#39;);
	mygrid.setImagePath(&quot;../codebase/imgs/&quot;);
	mygrid.setHeader(&quot;Sales,Book Title,Author,Price,In Store,Shipping,Bestseller,
              Date of Publication&quot;);
	mygrid.setInitWidths(&quot;50,150,100,80,80,80,80,200&quot;);
	mygrid.setColAlign(&quot;right,left,left,right,center,left,center,center&quot;);
	mygrid.setColTypes(&quot;dyn,edtxt,ed,price,ch,co,ra,ro&quot;);
	mygrid.enableDragAndDrop(&quot;temporary_disabled&quot;, true);
	mygrid.init();
	mygrid.setSkin(&quot;dhx_skyblue&quot;);
	mygrid.enableHeaderMenu();
	mygrid.enableColumnMove(true);
	mygrid.setColumnHidden(2, true);
	mygrid.attachEvent(&quot;onHeaderClick&quot;, function(ind, obj) {
	});
	mygrid.loadXML(&quot;../common/grid_ml_16_rows_columns_manipulations.xml&quot;);
	tree.dragger.addDragLanding(mygrid.hdr, new s_control);
&lt;/pre&gt;
&lt;p&gt;本文实现的是将tree拖拽到grid，其实其他的一些支持拖拽的组件也可以做，并不局限于tree组件，甚至还见过有人实现jquery的dtree拖拽到dhtmlxGrid增加一行记录。&lt;/p&gt;

&lt;h2&gt;参考文章&lt;/h2&gt;
&lt;li&gt;
Custom Drag Out：&lt;a href=&quot;http://www.dhtmlx.com/docs/products/dhtmlxTree/samples/05_drag_n_drop/08_pro_drag_out.html&quot; target=&quot;_blank&quot;&gt;http://www.dhtmlx.com/docs/products/dhtmlxTree/samples/05_drag_n_drop/08_pro_drag_out.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;dhtmlxGrid doc：&lt;a href=&quot;http://docs.dhtmlx.com/doku.php?id=dhtmlxgrid:toc&quot; target=&quot;_blank&quot;&gt;http://docs.dhtmlx.com/doku.php?id=dhtmlxgrid:toc&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;dhtmlxTree doc：&lt;a href=&quot;http://docs.dhtmlx.com/doku.php?id=dhtmlxtree:toc&quot; target=&quot;_blank&quot;&gt;http://docs.dhtmlx.com/doku.php?id=dhtmlxtree:toc&lt;/a&gt;
&lt;/li&gt;
</description>
      <link>http://blog.javachen.com/2011/07/24/drag-an-item-to-dhtmlxgrid-and-add-a-column.html</link>
      <guid>http://blog.javachen.com/2011/07/24/drag-an-item-to-dhtmlxgrid-and-add-a-column.html</guid>
      <pubDate>2011-07-24T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>DhtmlxGrid Quick Start Guide</title>
      <description>&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;说明:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;本文来源于&lt;a href=&quot;http://dhtmlx.com/docs/products/dhtmlxGrid/&quot;&gt;http://dhtmlx.com/docs/products/dhtmlxGrid/&lt;/a&gt;，本人对其进行翻译整理成下文，贴出此文，紧供分享。&lt;/p&gt;

&lt;p&gt;dhtmlxGrid是一个拥有强大的数据绑定、优秀的大数据展示性能并支持ajax的JavaScript表格控件。该组件易于使用并通过富客户端的API提供了很大的扩展性。dhtmlxGrid支持不同的数据源（XML, JSON, CSV, JavaScript 数组和HTML表格），如果需要的话，还可以从自定义的xml中加载数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;跨浏览器
使用JavaScript API进行控制
Ajax支持
简单的JavaScript 或者XML 配置
与HTML集成
内建过滤、排序、查询、分组功能
表格 footer/header自动计算
行内编辑
准备使用大数据集解决方案：分页，动态加载，智能渲染
序列化为XML或CSV
从 XML或CSV加载
列锁定
剪贴板支持
简单的客户端到服务器端配置 (使用 dhtmlxConnector, 可用于 PHP, Java, .NET, ColdFusion)
支持子表格
列拖拽和移动
行或列拖拽
dhtmlxTree PRO Edition支持拖拽
可以创建一个编辑器或是列格式化 (使用 eXcell – 继承自 cell 对象)
组合框，日历以及更多的预定义eXcells
Cell支持数学方程式
不同的键盘映射
简单的CSS风格或是预定义的皮肤
对于rows/entire grid不可见的数据块 (用户数据)
客户端排序(string, integer, date, custom)
服务器端排序
广泛的事件处理
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Step 1 – 引入文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt; link rel =&quot;STYLESHEET&quot; type=&quot;text/css&quot; href=&quot;codebase/dhtmlxgrid.css&quot; /&amp;gt;
&amp;lt; script src=&quot;codebase/dhtmlxcommon.js&quot;&amp;gt;&amp;lt; /script&amp;gt;
&amp;lt; script src=&quot;codebase/dhtmlxgrid.js&quot;&amp;gt;&amp;lt; /script&amp;gt;
&amp;lt; script src=&quot;codebase/dhtmlxgridcell.js&quot;&amp;gt;&amp;lt; /script&amp;gt;
&amp;lt; script&amp;gt;
    //we&#39;ll write script commands here
&amp;lt; /script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Step 2 – 放置gird&lt;/h2&gt;
&lt;p&gt;有两种方式在一个页面放置grid，这里减少最常用的方法：创建一个div并给id熟悉设置一个惟一值。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt; div id=&quot;mygrid_container&quot; style=&quot;width:600px;height:150px;&quot;&amp;gt;&amp;lt; /div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面初始化参数，首先定一个mygrid变量，然后定一个doInitGrid方法，方法内部进行mygrid初始化工作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; var mygrid;
 function doInitGrid(){
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;doInitGrid方法会包括以下代码：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用dhtmlXGridObject构造方法创建一个基于我们之前创建的DIV的grid对象；&lt;/li&gt;
  &lt;li&gt;设置grid图片路径。这个路径包括grid外观需要的所有图片。在大多数情况下该路径为“codebase/imgs/”. 该路径最后面的一个“/”很重要。 随便说一下，这个路径和你处理表格数据所使用的图片没有关系；&lt;/li&gt;
  &lt;li&gt;使用setHeader 方法定义表头；&lt;/li&gt;
  &lt;li&gt;使用setInitWidths (单位为像素) 或setInitWidthsP (单位为百分比)定义列宽。 使用&lt;code&gt;*&lt;/code&gt;代表让列自动使用所有表格宽度；&lt;/li&gt;
  &lt;li&gt;定义一个列的水平对其方式。 Numeric values is better to align right;&lt;/li&gt;
  &lt;li&gt;使用setSkin方法设置皮肤；&lt;/li&gt;
  &lt;li&gt;最好使用这些设置通过init方法初始化grid。更多的参数之后再讨论。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前，doInitGrid方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mygrid = new dhtmlXGridObject(&#39;mygrid_container&#39;);
mygrid.setImagePath(&quot;codebase/imgs/&quot;); //指定图片路径
mygrid.setHeader(&quot;Model,Qty,Price&quot;); //设置表头显示
grid.setInitWidths(&quot;*,150,150&quot;); //设置列的初始宽度
grid.setColAlign(&quot;left,right,right&quot;); //设置列的水平对其方式
mygrid.setSkin(&quot;light&quot;); //设置皮肤
grid.init(); //显示调用初始化方法，必须的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在需要做的是运行该方法，可以将该方法加入body的onload方法里或是使用jquery的方法。下面使用body的onload方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt; body onload=&quot;doInitGrid();&quot;&amp;gt;&amp;lt; /body&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样在该页面初始化之后会显示如下：&lt;/p&gt;

&lt;div class=&quot;pic&quot;&gt;&lt;img src=&quot;http://docs.dhtmlx.com/lib/exe/fetch.php?cache=&amp;amp;media=dhtmlxgrid:step_2_last.png&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;

&lt;p&gt;说明：除了调用set方法之外，还可以如下风格定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mygrid = new dhtmlXGridObject({
		parent:&quot;a_grid&quot;,
		image_path:&quot;codebase/imgs&quot;,
		columns:[
			{ label: &quot;Sales&quot;,           width:50, 	type:&quot;ed&quot; },
			{ label:[&quot;Book title&quot;,
				 &quot;#text_filter&quot;],   width:150, 	type:&quot;ed&quot; },
			{ label:[&quot;Author&quot;,
				 &quot;#select_filter&quot;], width:150, 	type:&quot;ed&quot; },
			{ label: &quot;Price&quot;,       width:50, 	type:&quot;ed&quot; },
			{ label:&quot;In store&quot; , 	width:80, 	type:&quot;ch&quot; },
			{ label:&quot;Shipping&quot; , 	width:50, 	type:&quot;ed&quot; },
			{ label:&quot;Bestseller&quot; , 	width:50, 	type:&quot;ed&quot; },
			{ label:&quot;Date&quot; , 	width:50, 	type:&quot;ed&quot; }
		],
		xml:&quot;data.xml&quot;
	});
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Step 3 – 填充数据&lt;/h2&gt;
&lt;p&gt;你已经知道了dhtmlxGrid可以加载xml或cvs或json数据，这里主要演示dhtmlxGrid加载json数据。&lt;/p&gt;

&lt;p&gt;在上面的例子中每行有三列，故我们的json数据如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
rows:[
	{
		id: &quot;a&quot;,
		data: [Model 1, 100, 399]
	},
	{
	id: &quot;b&quot;,
		data: [Model 2, 50, 649]
	},
	{
	    id: &quot;c&quot;,
	       data: [ Model 3, 70, 499]
	}
]
} 将上面存于data.json文件，然后在doInitGrid方法里调用以下方法：

mygrid.load (&quot;data.json&quot;，,&quot;json&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候页面展示如下：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;&lt;img src=&quot;http://docs.dhtmlx.com/lib/exe/fetch.php?cache=&amp;amp;media=dhtmlxgrid:step_3.png&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;

&lt;h2&gt;Step 4 – 客户端排序&lt;/h2&gt;
&lt;p&gt;为了能够实习表格的客户端排序，必须调用grid的setColSorting（sortTypesStr）方法。sortTypesStr是一个类型列表，以逗号分隔。&lt;/p&gt;

&lt;p&gt;该类型值有以下四种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;str – 作为字符串排序&lt;/li&gt;
  &lt;li&gt;int - 以Integer值排序 (通常可以是任何数字);&lt;/li&gt;
  &lt;li&gt;date – 以日期排序&lt;/li&gt;
  &lt;li&gt;custom sorting –自定义的更加复杂的排序方式(for example to sort days of week).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接下来我们对上面的例子进行排序。上例中每行有三列，第一列为字符串，后两列为数字，故可以调用以下方法进行排序。注意，该方法应该在init方法之前执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mygrid.setColSorting(&quot;str,int,int&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候单击最后一列表头，结果如下：&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;&lt;img src=&quot;http://docs.dhtmlx.com/lib/exe/fetch.php?media=dhtmlxgrid:step_4.png&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;

&lt;h2&gt;Step 5 – 单元格格式化和编辑&lt;/h2&gt;
&lt;p&gt;Grid中使用单元格的编辑器（或是eXcells – 继承自 Cells, Cell 或 Columns types）来定义值的格式和编辑方式。你可以根据你的需要创建eXcells。&lt;/p&gt;

&lt;p&gt;设定单元格的类型非常容易，其可以用一行代码定义。这里有一些常见的编辑器，如简单的编辑器代码为“ed”，多行编辑“txt”，只读单元格“ro”，复选框“ch”，价格的格式化“price”。&lt;br /&gt;
默认情况下所有的列是“ro”，也可以使用以下方法类设置编辑类型：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mygrid.setColTypes(&quot;ed,ed,price&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Excells格式化有以下几种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;link：超链接&lt;/li&gt;
  &lt;li&gt;img：图片&lt;/li&gt;
  &lt;li&gt;price：价格&lt;/li&gt;
  &lt;li&gt;dyn：动态行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Excell 复杂编辑器有以下几种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cp：colorpicker&lt;/li&gt;
  &lt;li&gt;calck：允许调用grid.setNumberFormat的计算器&lt;/li&gt;
  &lt;li&gt;dhxCalendar：日历，日期格式可以通过grid.setDateFormat设置&lt;/li&gt;
  &lt;li&gt;dhxCalendarA：日历，日期格式可以通过grid.setDateFormat设置，单元格可以编辑&lt;/li&gt;
  &lt;li&gt;calendar：YUI Calendar&lt;/li&gt;
  &lt;li&gt;clist：多选组件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用其他组件作为单元格编辑器:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;grid：使用dhtmlxgrid&lt;/li&gt;
  &lt;li&gt;stree ：使用dhtmlxtree&lt;/li&gt;
  &lt;li&gt;context：使用dhtmlxmenu&lt;/li&gt;
  &lt;li&gt;combo：使用dhtmlxCombo&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Excells特别用途&lt;br /&gt;
sub_row：允许单元格作为一个可展开的子单元格，就想查看明细一样。&lt;/p&gt;

&lt;p&gt;两个扩展:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sub_row_ajax – 单元格数据被认为是ajax请求的url&lt;/li&gt;
  &lt;li&gt;sub_row_grid – 允许创建一个子表作为一个子行的内容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现在你可以双击或是F2进入编辑模式，你可以用tab键在单元格之间导航。&lt;/p&gt;
&lt;div class=&quot;pic&quot;&gt;&lt;img src=&quot;http://docs.dhtmlx.com/lib/exe/fetch.php?media=dhtmlxgrid:step_5.png&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;

&lt;h2&gt;Step 6 – 行操作方法&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function addRow(){
var newId = (new Date()).valueOf()
mygrid.addRow(newId,&quot;&quot;,mygrid.getRowsNum())
mygrid.selectRow(mygrid.getRowIndex(newId),false,false,true);
}
function removeRow(){
var selId = mygrid.getSelectedId()
mygrid.deleteRow(selId);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码中addRow() 方法的一些说明：&lt;br /&gt;
* 创建一个惟一值 (number of millisecond since 1970) 来作为row的标识；&lt;br /&gt;
* 在最后一行后面添加一新行，该行有新的id，值为空；&lt;br /&gt;
* 选中最近创建的行 (by index), 不掉用 On-Select事件，不掉用选中行之前事件并且聚焦到选中行(如果垂直滚动条存在，则滚动对应位置)。&lt;/p&gt;

&lt;p&gt;代码中removeRow() 的一些说明（一行行的）:&lt;br /&gt;
* 得到选中行id；&lt;br /&gt;
* 删除指定行id的行&lt;/p&gt;

&lt;h2&gt;Step 7 – 事件&lt;/h2&gt;
&lt;p&gt;添加事件调用attachEvent 方法，如下行选中事件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function doOnRowSelected(rowID,celInd){
alert(&quot;Selected row ID is &quot;+rowID+&quot;\nUser clicked cell with index &quot;+celInd);
}
mygrid.attachEvent(&quot;onRowSelect&quot;,doOnRowSelected);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Step 8 – Code&lt;/h2&gt;
&lt;p&gt;最后的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt; title&amp;gt;dhtmlxGrid Sample Page&amp;lt; /title&amp;gt;
&amp;lt; link rel=&quot;STYLESHEET&quot; type=&quot;text/css&quot; href=&quot;codebase/dhtmlxgrid.css&quot; /&amp;gt;
&amp;lt; script src=&quot;codebase/dhtmlxcommon.js&quot;&amp;gt;&amp;lt; /script&amp;gt;
&amp;lt; script src=&quot;codebase/dhtmlxgrid.js&quot;&amp;gt;&amp;lt; /script&amp;gt;
&amp;lt; script src=&quot;codebase/dhtmlxgridcell.js&quot;&amp;gt;&amp;lt; /script&amp;gt;
&amp;lt; script&amp;gt;
var mygrid;
function doInitGrid(){
mygrid = new dhtmlXGridObject(&#39;mygrid_container&#39;);
mygrid.setImagePath(&quot;codebase/imgs/&quot;);
mygrid.setHeader(&quot;Model,Qty,Price&quot;);
mygrid.setInitWidths(&quot;*,150,150&quot;);
mygrid.setColAlign(&quot;left,right,right&quot;)
mygrid.setSkin(&quot;light&quot;);
mygrid.setColSorting(&quot;str,int,int&quot;);
mygrid.setColTypes(&quot;ed,ed,price&quot;);
mygrid.attachEvent(&quot;onRowSelect&quot;,doOnRowSelected);
mygrid.init();
mygrid.load (&quot;data.json&quot;,&quot;json&quot;);
}
function addRow(){
var newId = (new Date()).valueOf()
mygrid.addRow(newId,&quot;&quot;,mygrid.getRowsNum())
mygrid.selectRow(mygrid.getRowIndex(newId),false,false,true);
}
function removeRow(){
var selId = mygrid.getSelectedId()
mygrid.deleteRow(selId);
}
function doOnRowSelected(rowID,celInd){
alert(&quot;Selected row ID is &quot;+rowID+&quot;\nUser clicked cell with index &quot;+celInd);
}
&amp;lt; /script&amp;gt;
&amp;lt; body onload=&quot;doInitGrid()&quot;&amp;gt;
&amp;lt; div id=&quot;mygrid_container&quot; style=&quot;width:600px;height:150px;&quot;&amp;gt;&amp;lt; /div&amp;gt;
&amp;lt; /body&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

</description>
      <link>http://blog.javachen.com/2011/07/19/dhtmlxgrid-quick-start-guide.html</link>
      <guid>http://blog.javachen.com/2011/07/19/dhtmlxgrid-quick-start-guide.html</guid>
      <pubDate>2011-07-19T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>网上收集的关于OpenStack的一些资源</title>
      <description>&lt;li&gt;
&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: 13px; font-weight: normal;&quot;&gt;OpenStack Nova code：&lt;a href=&quot;https://bugs.launchpad.net/nova&quot; target=&quot;_blank&quot;&gt;https://bugs.launchpad.net/nova&lt;/a&gt;&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
OpenStack Blog：&lt;a href=&quot;http://planet.openstack.org/&quot; target=&quot;_blank&quot;&gt;http://planet.openstack.org/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;OpenStack 官方文档：&lt;a href=&quot;http://docs.openstack.org/cactus/openstack-compute/admin/content/ch_getting-started-with-openstack.html&quot; target=&quot;_blank&quot;&gt;http://docs.openstack.org/cactus/openstack-compute/admin/content/ch_getting-started-with-openstack.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;OpenStack 中国门户：&lt;a href=&quot;http://blu001068.chinaw3.com/bbs/portal.php&quot; target=&quot;_blank&quot;&gt;http://blu001068.chinaw3.com/bbs/portal.php&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;在 Ubuntu 上安装和配置 OpenStack Nova：&lt;a href=&quot;http://www.vpsee.com/2011/05/install-openstack-nova-on-ubuntu/&quot; target=&quot;_blank&quot;&gt;http://www.vpsee.com/2011/05/install-openstack-nova-on-ubuntu/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Centos安装过程：&lt;a href=&quot;http://wiki.openstack.org/NovaInstall/CentOSNotes&quot; target=&quot;_blank&quot;&gt;http://wiki.openstack.org/NovaInstall/CentOSNotes&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Running OpenStack Compute (Nova)：&lt;a href=&quot;http://wiki.openstack.org/RunningNova&quot; target=&quot;_blank&quot;&gt;http://wiki.openstack.org/RunningNova&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;开源黄页 -  OpenStack：&lt;a href=&quot;http://yp.oss.org.cn/appcenter/software/show_software.php?sw_id=1733&quot; target=&quot;_blank&quot;&gt;http://yp.oss.org.cn/appcenter/software/show_software.php?sw_id=1733&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Installation on Debian, Fedora orCentOS：&lt;a href=&quot;http://nova.openstack.org/adminguide/distros/others.html&quot; target=&quot;_blank&quot;&gt;http://nova.openstack.org/adminguide/distros/others.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Installing Nova on a Single Host：&lt;a href=&quot;http://nova.openstack.org/adminguide/single.node.install.html&quot; target=&quot;_blank&quot;&gt;http://nova.openstack.org/adminguide/single.node.install.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;openstack swift 典型架构和openstack swift 简要说明：&lt;a href=&quot;http://blog.sina.com.cn/s/blog_6b98772b0100pk7p.html&quot; target=&quot;_blank&quot;&gt;http://blog.sina.com.cn/s/blog_6b98772b0100pk7p.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Swift 技术验证简单报告 :&lt;a href=&quot;http://www.douban.com/group/topic/17621229/&quot; target=&quot;_blank&quot;&gt;http://www.douban.com/group/topic/17621229/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;新浪上openstack_object_storage的一些文章：&lt;a href=&quot;http://blog.sina.com.cn/s/blog_6b98772b0100pk7p.html&quot; target=&quot;_blank&quot;&gt;http://blog.sina.com.cn/s/articlelist_1805154091_2_1.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;豆瓣上OpenStack收集资源：&lt;a href=&quot;http://www.douban.com/group/openstack/&quot; target=&quot;_blank&quot;&gt;http://www.douban.com/group/openstack/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;OpenStack服务部署： &lt;a href=&quot;http://hi.baidu.com/juacm/blog/item/bd05d154e7581e451138c277.html&quot; target=&quot;_blank&quot;&gt;http://hi.baidu.com/juacm/blog/item/bd05d154e7581e451138c277.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;陈沙克日志：&lt;a href=&quot;http://hi.baidu.com/chenshake/home&quot; target=&quot;_blank&quot;&gt;http://hi.baidu.com/chenshake/home&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Openstack-nova-architecture：&lt;a href=&quot;http://ken.pepple.info/openstack/2011/04/22/openstack-nova-architecture/&quot; target=&quot;_blank&quot;&gt;http://ken.pepple.info/openstack/2011/04/22/openstack-nova-architecture/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;OpenStack 架构：&lt;a href=&quot;http://blog.csdn.net/anghlq/article/details/6543880&quot; target=&quot;_blank&quot;&gt;http://blog.csdn.net/anghlq/article/details/6543880&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
安装OpenStack：&lt;a href=&quot;http://blog.csdn.net/anghlq/article/details/6566370&quot; target=&quot;_blank&quot;&gt;http://blog.csdn.net/anghlq/article/details/6566370&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
安装OpenStack-dashboard：&lt;a href=&quot;http://blog.csdn.net/anghlq/article/details/6572868&quot; target=&quot;_blank&quot;&gt;http://blog.csdn.net/anghlq/article/details/6572868&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
Centos-nova-install.sh： &lt;a href=&quot;https://gist.github.com/837797&quot; target=&quot;_blank&quot;&gt;https://gist.github.com/837797&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
RabbitMQ and Nova：&lt;a href=&quot;http://blog.163.com/clevertanglei900@126/blog/static/11135225920101110393888/&quot; target=&quot;_blank&quot;&gt;http://blog.163.com/clevertanglei900@126/blog/static/11135225920101110393888/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;OpenStack(diablo-2)のNovaをインストール@CentOS6　メモ(1/n)：&lt;a href=&quot;http://blog.livedoor.jp/techpub/archives/3797358.html&quot; target=&quot;_blank&quot;&gt;OpenStack(diablo-2)のNovaをインストール@CentOS6　メモ(1/n)&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;openstack nova部署完整实例-参考手册-内容列表（0）：&lt;a href=&quot;http://bbs.chinaunix.net/thread-3563033-1-1.html&quot; target=&quot;_blank&quot;&gt;http://bbs.chinaunix.net/thread-3563033-1-1.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;openstack nova部署完整实例-参考手册-基础部分（1）：&lt;a href=&quot;http://bbs.chinaunix.net/thread-3563017-1-1.html&quot; target=&quot;_blank&quot;&gt;http://bbs.chinaunix.net/thread-3563017-1-1.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;openstack nova部署完整实例-参考手册-增强部分（2）：&lt;a href=&quot;http://bbs.chinaunix.net/thread-3563046-1-1.html&quot; target=&quot;_blank&quot;&gt;http://bbs.chinaunix.net/thread-3563046-1-1.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;openstack nova部署完整实例-参考手册-增强部分（3）：&lt;a href=&quot;http://bbs.chinaunix.net/thread-3563049-1-1.html&quot; target=&quot;_blank&quot;&gt;http://bbs.chinaunix.net/thread-3563049-1-1.html&lt;/a&gt;
&lt;/li&gt;
</description>
      <link>http://blog.javachen.com/2011/07/07/some-resources-about-openstack.html</link>
      <guid>http://blog.javachen.com/2011/07/07/some-resources-about-openstack.html</guid>
      <pubDate>2011-07-07T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Centos上安装 OpenNebula Management Console</title>
      <description>&lt;p&gt;我们可以通过onehost/onevm/onevnet等等 这些命令行工具来管理 OpenNebula 云计算平台，也可以通过OpenNebula项目组开发的web控制台来访问OpenNebula。OpenNebula项目组提供了两个web程序来管理OpenNebula，一个即本文提到的&lt;a href=&quot;http://dev.opennebula.org/projects/management-console&quot;&gt;OpenNebula Management Console&lt;/a&gt;，一个是&lt;a href=&quot;http://opennebula.org/documentation:rel2.2:sunstone&amp;quot; target=&amp;quot;_blank&quot;&gt;The Cloud Operations Center&lt;/a&gt;，前者需要额外下载，后者内嵌与OpenNebula安装包内。&lt;/p&gt;

&lt;p&gt;OpenNebula 2.2提供的文档相对较少并且零散，在网上可以找到一篇关于OpenNebula Management Console安装的文章：&lt;br /&gt;
&lt;a href=&quot;http://www.vpsee.com/2011/03/install-opennebula-management-console-on-centos/&quot;&gt;安装 OpenNebula 基于 Web 的管理控制台&lt;/a&gt;，我的这篇文章参考了这篇文章并加以完善，这篇文章对我完成OpenNebula Management Console的安装起到很大帮助，感谢原文作者。&lt;/p&gt;

&lt;p&gt;我的安装环境：centos5.6 ，OpenNebula2.2，在安装OpenNebula2.2之前，我执行了yum update，即更新系统的软件。&lt;/p&gt;

&lt;p&gt;以下来自&lt;a href=&quot;http://dev.opennebula.org/projects/management-console/wiki&quot;&gt;官方文档&lt;/a&gt;：&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;要求:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Apache or whatever webserver.&lt;/li&gt;
  &lt;li&gt;php5 (May work with php4 but not tested)&lt;/li&gt;
  &lt;li&gt;php-adodb&lt;/li&gt;
  &lt;li&gt;And you need a db driver for adodb: php-mysql or php-pgsql.&lt;/li&gt;
  &lt;li&gt;Mysql or postgresql database&lt;/li&gt;
  &lt;li&gt;php-curl&lt;/li&gt;
  &lt;li&gt;php-xmlrpc&lt;/li&gt;
  &lt;li&gt;php-pear: pecl install uploadprogress (Only if you want a nice upload progress bar)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果你想查看更多资料，您可以去官网：&lt;a href=&quot;http://dev.opennebula.org/projects/management-console/wiki&quot;&gt;OpenNebula Management Console Wiki&lt;/a&gt;；如果你想在ubutun上安装OpenNebula Management Console，参照这篇文章：&lt;a href=&quot;http://dev.opennebula.org/projects/management-console/wiki/onemc_install_ubuntu&quot;&gt;Install onemc on ubuntu&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;安装过程&lt;/h1&gt;

&lt;h2 id=&quot;section-2&quot;&gt;必要软件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# yum -y install php mysql-server httpd mysql-connector-odbc mysql-devel libdbi-dbd-mysql
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;php-adodb&quot;&gt;安装php-adodb&lt;/h2&gt;

&lt;p&gt;从http://sourceforge.net/projects/adodbhttp://7xnrdo.com1.z0.glb.clouddn.com/adodb-php5-only下载&lt;/p&gt;

&lt;p&gt;注意：将adobd包解压拷贝到/var/www/html/onemc/include/，将文件名改为adobd&lt;/p&gt;

&lt;h2 id=&quot;php&quot;&gt;安装php的扩展&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# yum -y install php-gd php-xml php-mbstring php-ldap php-pear php-xmlrpc php-curl php-mysql
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;apache&quot;&gt;安装apache扩展&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# yum -y install httpd-manual mod_ssl mod_perl mod_auth_mysql
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-3&quot;&gt;修改配置文件权限&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# chmod 644 /var/www/html/onemc/include/config.php
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我下载的是OpenNebula 2.2其中/config.php的权限很特别，如果你从浏览器访问onemc时候页面都是空白的，你可以看看日志（我使用的是httpd，日志在httpd.log），可以看到日志中提示没有权限访问&lt;code&gt;/var/www/html/onemc/include/config.php&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;onemc&quot;&gt;下载 onemc&lt;/h2&gt;

&lt;p&gt;下载和解压 onemc-1.0.0.tar.gz 后直接放在 apache 的默认目录里：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cd /var/www/html
# wget http://dev.opennebula.org/attachments/download/128/onemc-1.0.0.tar.gz
# tar zxvf onemc-1.0.0.tar.gz
# cd onemc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;配置数据库&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# mysql -uroot -p
Enter password:
mysql&amp;gt; create database onemc;
mysql&amp;gt; create user &#39;oneadmin&#39;@&#39;localhost&#39; identified by &#39;oneadmin&#39;;
mysql&amp;gt; grant all privileges on onemc.* to &#39;oneadmin&#39;@&#39;localhost&#39;;
mysql&amp;gt; \q
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;初始化数据库&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# mysql -u oneadmin -p onemc &amp;lt; /var/www/html/onemc/include/mysql.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;onemc-1&quot;&gt;配置onemc&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# vi /var/www/html/onemc/include/config.php
...
// vmm: kvm or xen
$vmm = &quot;xen&quot;;
...
// ADODB settings
$adodb_type = &quot;mysql&quot;;
$adodb_server = &quot;localhost&quot;;
$adodb_user = &quot;oneadmin&quot;;
$adodb_pass = &quot;oneadmin&quot;;
$adodb_name = &quot;onemc&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;登录&lt;/h2&gt;

&lt;p&gt;如果系统设置了 http_proxy 环境变量的话一定要先关闭，然后重启 one 和 httpd：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# unset http_proxy
# one stop; one start
# /etc/init.d/httpd restar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问地址为&lt;code&gt;http://localhost/onemc/index.php&lt;/code&gt;，用户名和密码在&lt;code&gt;one_auth&lt;/code&gt; 中。&lt;/p&gt;

&lt;h1 id=&quot;section-7&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;以上步骤最重要的是配置好centos的yum源，一次将php和mysql及相关组件安装成功，然后需要注意的是上面红色部分标出的部分。其实，除了红色那部分之外，其余和开头提到的那篇文章内容没什么差别。&lt;/p&gt;

&lt;h1 id=&quot;section-8&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1]&lt;a href=&quot;http://dev.opennebula.org/projects/management-console/wiki&quot;&gt;OpenNebula Management Console Wiki&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2]&lt;a href=&quot;http://dev.opennebula.org/projects/management-console/wiki/onemc_install_ubuntu&quot;&gt;Install onemc on ubuntu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2011/06/29/install-opennebula-management-console-in-centos5-6.html</link>
      <guid>http://blog.javachen.com/2011/06/29/install-opennebula-management-console-in-centos5-6.html</guid>
      <pubDate>2011-06-29T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>OpenNebula 2.2的特性</title>
      <description>&lt;p&gt;以下这篇文章由&lt;a title=&quot;OpenNebula 2.2 Features&quot; href=&quot;http://opennebula.org/documentation:features&quot;&gt;OpenNebula 2.2 Features&lt;/a&gt;翻译而来。&lt;/p&gt;

&lt;p&gt;OpenNebula是一款为云计算而打造的开源工具箱。它允许你与Xen，KVM或VMware ESX一起建立和管理私有云，同时还提供Deltacloud适配器与Amazon EC2相配合来管理混合云。除了像Amazon一样的商业云服务提供商，在不同OpenNebula实例上运行私有云的Amazon合作伙伴也同样可以作为远程云服务供应商。&lt;/p&gt;

&lt;p&gt;目前版本，可支持XEN、KVM和VMware，以及实时存取EC2和 ElasticHosts，它也支持印象档的传输、复制和虚拟网络管理网络。
&lt;h2&gt;主要特点和优势&lt;/h2&gt;
&lt;strong&gt;私有云计算&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为私有数据中心或集群（管理功能&lt;strong&gt;私有云计算&lt;/strong&gt;）上运行&lt;strong&gt;的&lt;/strong&gt;&lt;strong&gt;Xen&lt;/strong&gt;，&lt;strong&gt;KVM&lt;/strong&gt;和&lt;strong&gt;VMware&lt;/strong&gt;&lt;strong&gt;的&lt;/strong&gt;。
&lt;table width=&quot;100%&quot;&gt;
&lt;tbody&gt;
&lt;tr bgcolor=&quot;cornsilk&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;模块&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;&lt;strong&gt;功能&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;用户管理&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;用户管理，认证框架，多个云用户和管理员角色，会计，配额管理，安全的多租户。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;VM&lt;/strong&gt;&lt;strong&gt;图像管理&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;带目录的镜像仓库和镜像管理，访问控制，以及从正在运行的虚拟机创建镜像。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;虚拟网络管理&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;对互联的虚拟机;一定范围或固定的网络;虚拟网络共享;相关的第2层虚拟网络和网络隔离的通用属性定义提供虚拟网络管理。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;虚拟机管理&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;虚拟机管理功能，支持在同一物理结构中的多个hypervisors，分布式环境的多个hypervisor管理，虚拟机自动配置，以及脚本在虚拟机的状态变化时的触发管理。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;服务管理&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;部署由多层次的相互联系的虚拟机组成的群体服务;在启动时自动配置，以及对微软Windows和Linux镜像的支持。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;基础设施管理&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;管理物理主机;创建本地集群，占地面积小，占用空间不到700KB。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;存储管理&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;虚拟机映像管理，支持多种硬件平台（FibreChannel, iSCSI, NAS shared storage…）和存储后端传输镜像。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;信息管理&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;虚拟机和物理基础设施的监控，并与数据监测工具集成，如&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;调度&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;强大和灵活的竞价/排名调度、工作量和资源分配政策，如包装，分割，负载感知.....&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;用户界面&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;Unix类似的云基础设施管理命令行。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;运营中心&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;图形化管理的云基础设施。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;混合云计算&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;本地基础设施与远程云资源的扩展（&lt;strong&gt;混合云计算&lt;/strong&gt;）
&lt;table width=&quot;100%&quot;&gt;
&lt;tbody&gt;
&lt;tr bgcolor=&quot;cornsilk&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;111&quot;&gt;&lt;strong&gt;模块&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;449&quot;&gt;&lt;strong&gt;功能&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;111&quot;&gt;&lt;strong&gt;Cloudbursting&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;449&quot;&gt;本地的基础设施，可以辅从外部云计算能力，以满足高峰需求，更好地服务用户的访问请求，或者为了实现高可用性策略。支持亚马逊EC2，并同时访问多个云。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;111&quot;&gt;&lt;strong&gt;Federation&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;449&quot;&gt;不同的云实例以构建一个独立的虚拟化集群层次;更高水平的可扩展性。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;公共云计算&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;暴露云接口给私有的基础设施功能（&lt;strong&gt;公共云计算&lt;/strong&gt;）
&lt;table width=&quot;100%&quot;&gt;
&lt;tbody&gt;
&lt;tr bgcolor=&quot;cornsilk&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;功能&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;&lt;strong&gt;功能&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;云接口&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;通过提供给用户的REST接口;实现OGF OCCI和亚马逊EC2接口，使本地的基础架构转变为一个公开云;支持同时公开多种云API，客户端工具，以及安全访问。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;主要特点和集成优势&lt;/h2&gt;
&lt;table width=&quot;100%&quot;&gt;
&lt;tbody&gt;
&lt;tr bgcolor=&quot;cornsilk&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;功能&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;&lt;strong&gt;功能&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;基础设施抽象&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;无缝与任何操作平台的验证/授权，虚拟化，网络和存储平台融合，采用模块化结构，以适应任何数据中心。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;适应性和定制&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;启用任何云架构的部署：公共，私有，混合和联合;定制插件来访问虚拟化、存储、信息、认证/授权和远程云服务，新的插件可以很容易地在任何语言编写，配置和改变参数调整云管理实例的行为以满足环境和用例要求;钩机制，当虚拟机的状态改变使触发管理脚本的执行。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;互操作性和标准&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;开放标准为基础的架构，以避免厂商锁定，提高互操作性​​，以及开放的实施标准。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;开放&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;开源Apache许可下发布协议。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;编程接口&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;提供Ruby和Java XMLRPC的原生云API创建新的云接口和访问核心功能。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;主要特点和生产效益&lt;/h2&gt;
&lt;table width=&quot;100%&quot;&gt;
&lt;tbody&gt;
&lt;tr bgcolor=&quot;cornsilk&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;特点&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;&lt;strong&gt;功能&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;安全&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;验证框架的密码，或基于SSH的RSA密钥对LDAP，外部和内部通信通过SSL，安全的多​​租户;隔离的网络。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;健壮性&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;持久数据库后端存储主机、网络、虚拟机信息。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;容错&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;配置主机、虚拟机或OpenNebula实例故障事件。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;可扩展性&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;测试在大规模的核心和成千上万的基础设施;高度可扩展的后端，并为MySQL和SQLite支持。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;性能&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;非常高效的内核开发C++语言。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;可靠性&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;自动化的功能、可扩展性、性能、可靠性和稳定性测试过程。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;利用充满活力的云生态系统&lt;/h2&gt;
&lt;table width=&quot;100%&quot;&gt;
&lt;tbody&gt;
&lt;tr bgcolor=&quot;cornsilk&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;&lt;strong&gt;功能&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr bgcolor=&quot;aliceblue&quot;&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;OpenNebula Ecosystem&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;充分利用&lt;a title=&quot;http://www.opennebula.org/software:ecosystem&quot; href=&quot;http://www.opennebula.org/software:ecosystem&quot;&gt;OpenNebula开放云生态系统&lt;/a&gt;与新元件加强了OpenNebula云工具包提供的功能并能够与其他产品的集成：vCloud的API、OpenNebula块、Haizea调度、 Libcloud、Deltacloud、Web管理控制台，Deltacloud的混合云适配器...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;107&quot;&gt;&lt;strong&gt;Other Cloud Ecosystems&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;453&quot;&gt;围绕Amazon AWS, OGC OCCI and VMware vCloud构建的生态系统。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2011/06/26/opennebula-2-2-features.html</link>
      <guid>http://blog.javachen.com/2011/06/26/opennebula-2-2-features.html</guid>
      <pubDate>2011-06-26T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Eucalyptus使用的技术</title>
      <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://libvirt.org/&quot;&gt;libvirt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Libvirt 库是一种实现 Linux 虚拟化功能的 Linux® API，它支持各种虚拟机监控程序，包括 Xen 和 KVM，以及 QEMU 和用于其他操作系统的一些虚拟产品。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jboss.org/netty/&quot;&gt;Netty&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Netty 提供异步的、事件驱动的网络应用程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ws.apache.org/axis2/&quot;&gt;Axis2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Axis2是下一代 Apache Axis。Axis2 虽然由 Axis 1.x 处理程序模型提供支持，但它具有更强的灵活性并可扩展到新的体系结构。Axis2 基于新的体系结构进行了全新编写，而且没有采用 Axis 1.x 的常用代码。支持开发 Axis2 的动力是探寻模块化更强、灵活性更高和更有效的体系结构，这种体系结构可以很容易地插入到其他相关 Web 服务标准和协议（如 WS-Security、WS-ReliableMessaging 等）的实现中。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ws.apache.org/axis2/c/&quot;&gt;Axis2c&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apache Axis2/C is a Web services engine implemented in the C programming language. It is based on the extensible and flexible Axis2 architecture.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ws.apache.org/rampart/c/&quot;&gt;Rampart/C&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apache Axis2/C的安全模块&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://jibx.sourceforge.net/&quot;&gt;JiBX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JiBX是一款非常优秀的XML（Extensible Markup Language）数据绑定框架。它提供灵活的绑定映射文件实现数据对象与XML文件之间的转换；并不需要你修改既有的Java类。另外，另外，它的转换效率是目前很多开源项目都无法比拟的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.bouncycastle.org/java.html&quot;&gt;Bouncy Castle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bouncy Castle 是一种用于 Java 平台的开放源码的轻量级密码术包。它支持大量的密码术算法，并提供 JCE 1.2.1 的实现。因为 Bouncy Castle 被设计成轻量级的，所以从 J2SE 1.4 到 J2ME（包括 MIDP）平台，它都可以运行。它是在 MIDP 上运行的唯一完整的密码术包。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://mule.mulesource.org/display/MULE/Home&quot;&gt;Mule&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;它是一个轻量级的消息框架和整合平台，基于EIP（Enterprise Integeration Patterns,由Hohpe和Woolf编写的一本书）而实现的。Mule的核心组件是UMO(Universal Message Objects，从Mule2.0开始UMO这一概念已经被组件Componse所代替)，UMO实现整合逻辑。UMO可以是POJO,JavaBean等等。它支持20多种传输协议(file,FTP,UDP,SMTP,POP,HTTP,SOAP,JMS等)，并整合了许多流行的开源项目，比如Spring,ActiveMQ,CXF,Axis,Drools等。虽然Mule没有基于JBI来构建其架构，但是它为JBI容器提供了JBI适配器，应此可以很好地与JBI容器整合在一起。而 Mule更关注其灵活性，高效性以及易开发性。从2005年发表1.0版本以来，Mule吸引了越来越多的关注者，成为开源ESB中的一支独秀。目前许多公司都使用了Mule，比如Walmart,HP,Sony,Deutsche Bank 以及 CitiBank等公司。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.hibernate.org/&quot;&gt;Hibernate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hibernate是一个开放源代码的对象关系映射框架，它对JDBC进行了非常轻量级的对象封装，使得Java程序员可以随心所欲的使用对象编程思维来操纵数据库。 Hibernate可以应用在任何使用JDBC的场合，既可以在Java的客户端程序使用，也可以在Servlet/JSP的Web应用中使用，最具革命意义的是，Hibernate可以在应用EJB的J2EE架构中取代CMP，完成数据持久化的重任。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.hsqldb.org/&quot;&gt;HSQLDB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hsqldb是一个开放源代码的JAVA数据库，其具有标准的SQL语法和JAVA接口，它可以自由使用和分发，非常简洁和快速的。在其官网可以获得最新的程序源代码及jar包文件&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://xen.org/&quot;&gt;Xen&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Xen 是一个开放源代码&lt;a href=&quot;http://baike.baidu.com/view/1132.htm&quot;&gt;虚拟机&lt;/a&gt;监视器，由剑桥大学开发。它打算在单个计算机上运行多达100个满特征的操作系统。操作系统必须进行显式地修改（“移植”）以在Xen上运行（但是提供对用户应用的兼容性）。这使得Xen无需特殊硬件支持，就能达到高性能的虚拟化。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;[KVM}(http://www.linux-kvm.org/page/Main_Page)&lt;br /&gt;
基于内核的虚拟机(或简称为KVM)是一个由Qumrannet开发和赞助的开源项目.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://code.google.com/webtoolkit/&quot;&gt;Google Web Toolkit&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Google Web Toolkit (GWT) 允许开发人员使用Java 编程语言快速构建和维护复杂而又高性能的JavaScript 前端应用程序，从而降低了开发难度&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://sourceware.org/lvm2/&quot;&gt;LVM2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LVM是 Logical Volume Manager(逻辑卷管理)的简写，它是Linux环境下对磁盘分区进行管理的一种机制，它由Heinz Mauelshagen在Linux 2.4内核上实现，目前最新版本为：稳定版1.0.5，开发版 1.1.0-rc2，以及LVM2开发版。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://jetty.codehaus.org/jetty/&quot;&gt;Jetty&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jetty 是一个开源的servlet容器，它为基于Java的web内容，例如JSP和servlet提供运行环境。Jetty是使用Java语言编写的，它的API以一组JAR包的形式发布。开发人员可以将Jetty容器实例化成一个对象，可以迅速为一些独立运行（stand-alone）的Java应用提供网络和web连接。&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2011/06/22/the-technology-used-in-eucalyptus.html</link>
      <guid>http://blog.javachen.com/2011/06/22/the-technology-used-in-eucalyptus.html</guid>
      <pubDate>2011-06-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Eucalyptus EE的介绍及功能说明</title>
      <description>&lt;p&gt;Eucalyptus企业版2.0是一个基于Linux的软件架构，在企业现有的IT架构上实现一个可扩展的、提高效率的私有和混合云。Eucalyptus作为基础设施提供IaaS服务。这意味着用户可以通过Eucalyptus自助服务界面提供自己的资源（硬件、存储和网络）。一个Eucalyptus云是部署在企业的内部数据中心，由企业内部用户访问。因此，敏感数据可以在防火墙的保护下防止外部入侵。&lt;/p&gt;

&lt;p&gt;Eucalyptus的设计目的是从根本上易于安装和尽可能没有侵扰。该软件高度模块化，具有行业标准，和语言无关。它提供了可以与EC2兼容的云计算平台和与S3兼容的云存储平台。&lt;!--more--&gt;
&lt;h1&gt;功能亮点&lt;/h1&gt;
&lt;ul&gt;
	&lt;li&gt;无缝管理多个管理程序环境（Xen的，vSphere的，KVM，ESX，ESXi的）下一个管理控制台&lt;/li&gt;
	&lt;li&gt;启用跨平台的客户机操作系统包括微软Windows和Linux&lt;/li&gt;
	&lt;li&gt;高级存储集成器（iSCSI，SAN，NAS），您可以轻松地连接和管理Eucalyptus云内现有的存储系统&lt;/li&gt;
	&lt;li&gt;完善的用户和组管理，允许私有云资源的精确控制&lt;/li&gt;
	&lt;li&gt;测试，开发和部署能够顺利过渡到公共云或反之亦然，没有任何修改&lt;/li&gt;
	&lt;li&gt;快速，轻松地建立与基于VMware的虚拟化环境和其他公共云混合云&lt;/li&gt;
	&lt;li&gt;启用先进设备，最先进的企业，如可扩展的存储整合，监控，审计，报表&lt;/li&gt;
	&lt;li&gt;利用充满活力的生态系统围绕亚马逊AWS构建，提供解决方案，无缝地与Eucalyptus兼容&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;优点&lt;/h1&gt;
&lt;ul&gt;
	&lt;li&gt;建立一个私有云，让你接入到亚马逊AWS&lt;/li&gt;
	&lt;li&gt;允许云在原有的所有硬件和软件类型很容易的部署&lt;/li&gt;
	&lt;li&gt;客户可以利用其全球用户社区&lt;/li&gt;
	&lt;li&gt;Eucalyptus是与Linux和多个管理程序兼容&lt;/li&gt;
	&lt;li&gt;Eucalyptus还支持商业Linux发行版本：红帽企业Linux（RHEL）和SUSE Linux企业服务器（SLES）&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;对于IT管理员的好处&lt;/h1&gt;
&lt;ul&gt;
	&lt;li&gt;提供自助服务的IT基础设施供应到最终用户需要的IT资源迅速&lt;/li&gt;
	&lt;li&gt;没有额外的资金保持现有的基础设施费用，降低运营成本&lt;/li&gt;
	&lt;li&gt;保持防火墙后面的关键数据&lt;/li&gt;
	&lt;li&gt;技术是对现有的硬件和软件基础设施覆盖，而不是替代&lt;/li&gt;
	&lt;li&gt;避免锁定在第三方公共云供应商&lt;/li&gt;
	&lt;li&gt;可轻松转换之间来回私人和公共云&lt;/li&gt;
&lt;/ul&gt;
&amp;nbsp;
&lt;h1&gt;Eucalyptus EE新特性&lt;/h1&gt;
&lt;h2&gt;&lt;strong&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;对windows VM的支持&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;
1.  运行windows 虚拟机在Eucalyptus 云环境上运行，目前支持Windows 2003 Server,Windows 2008 Server和Windows 7。&lt;br /&gt;
2.  试用Euca2ools管理和控制windows虚拟机。&lt;br /&gt;
3.  试用EC2兼容的命令从正在运行的windows虚拟机创建新的虚拟机&lt;br /&gt;
4.  在Eucalyptus中通过标准的RDP客户端工具，使用AWS “get-password”访问虚拟机实例&lt;br /&gt;
5.  在多个hypervisors环境中部署windows虚拟机，包括Xen、Kvm、VMware（ESX/ESXi）&lt;br /&gt;
6.  基于windows 操作系统安装文件（ISO镜像、CD/DVD）创建新的windows虚拟机
&lt;h2&gt;&lt;strong&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;对VMware的支持&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;
1.支持VMware vCenter 4.0,ESX/ESXi 4.0!&lt;br /&gt;
2.与VMware vSphere 客户端兼容&lt;br /&gt;
3.能够合并VMware(ESX/ESXi)和开源的hypervisors（Xen、Kvm）到一个单独的云环境&lt;br /&gt;
4.通过Eucalyptus的软件扩展一些云的基本特性（例如IPs，安全组，S3）到一个VMware基础架构&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&lt;h2&gt;&lt;strong&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;引入SAN的支持&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;
Eucalyptus EE引入对SAN的支持，使你能够整合enterprise-grade SAN(Storage Area Network) 硬件设备到Eucalyptus云环境。SAN扩展SC并在Eucalyptus中运行的虚拟机和SAN设备之间提供高性能的数据通道。Eucalyptus EE的SAN支持为Eucalyptus云环境提供了一个企业级的EBS解决方案。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&lt;h1&gt;Eucalyptus的功能&lt;/h1&gt;
&lt;h2&gt;&lt;strong&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;基本组成部分及功能&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; width=&quot;614&quot; align=&quot;left&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;模块&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;功能&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;说明&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;云控制器（CLC）&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.对外提供EC2和Web接口，管理各类组件中的可用虚拟资源（服务、网络、存储）。&lt;br /&gt;
2.资源抽象，决定哪个簇将提供给实例，分发请求给CC。&lt;br /&gt;
3.管理运行的实例。&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;CLC是整个云结构的前端。CLC为客户工具提供与EC2/S3兼容的网络接口，与Eucalyptus的组件通信。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;存储控制器（SC）&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.提供与EBS类似的存储功能，能够与大量的文件存储系统交互。&lt;br /&gt;
2.使用AoE或者iSCSI协议为实例提供块存储。&lt;br /&gt;
3.允许在存储系统中（如Walrus）建立快照。&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;SC提供实例使用的块存储。&lt;br /&gt;
与EBS类似。&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;

&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;Walrus控制器（WS3）&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.允许用户存储持久化的数据。&lt;br /&gt;
2.提供REST接口操作数据，设置数据访问策略。&lt;br /&gt;
3.使用S3 API存储和获取虚拟镜像和数据。&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;WS3使用与S3 API兼容的REST和SOAP   API提供简单的存储服务&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;控制簇（CC）&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.接收CLC的请求，然后部署实例。&lt;br /&gt;
2.收集虚拟机的信息并决定在哪个节点控制上执行虚拟机。&lt;br /&gt;
3.为实例提供有效的虚拟网络。&lt;br /&gt;
4.收集NCs提交的信息，并报告给CLC。&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;CC管理NC，部署和管理在节点上的实例，在Eucalyptus联网模型的类型下管理在控制节点上运行的实例的联网。&lt;br /&gt;
CC连接着云控制器CLC和控制节点NC。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;节点控制器（NC）&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.托管虚拟机实例&lt;br /&gt;
2.收集节点上相关的数据资源的可用性和利用率，并报告给控制簇CC。&lt;br /&gt;
3.管理虚拟机的生命周期，能够获取和清除镜像的本地拷贝。&lt;br /&gt;
4.维护虚拟网络&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;UEC的节点使用虚拟化技术使KVM能作为管理程序在服务器上运行。当用户安装UEC节点时，UEC将自动安装KVM。UEC的实例就是在管理程序下运行的虚拟机。Eucalyptus支持其他管理程序，如Xen。&lt;br /&gt;
节点控制器在每一个节点上运行，控制着节点上实例的生命周期。&lt;/td&gt;

&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;VMware   Broker&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;允许 Eucalyptus直接地或通过 VMware&lt;strong&gt; &lt;/strong&gt;Vcenter在   VMware设备部署虚拟机，在CC和 VMware  hypervisors(ESX/ESXi)起一个连接作用&lt;strong&gt; &lt;/strong&gt;&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;Eucalyptus   EE额外的一个组件，用于对VMware的支持&lt;/td&gt;
&lt;/tr&gt;


&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;font-size: 20px; font-weight: bold; color: #0000ff;&quot;&gt;管理员拥有的功能&lt;/span&gt;
&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; width=&quot;614&quot; align=&quot;left&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;模块&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;功能&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;说明&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;用户管理&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.  添加用户（邮件通知，设置管理员）&lt;br /&gt;
2.  查看用户，设置账户是否激活&lt;br /&gt;
3.  删除用户&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;组管理&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.  添加用户组&lt;br /&gt;
2.  查看用户组&lt;br /&gt;
3.  删除用户组&lt;br /&gt;
4.  添加/删除组员&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;权限管理&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.给组设置权限&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;Web接口&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.  查看、下载证书&lt;br /&gt;
2.  查看上传的镜像，并能修改镜像状态&lt;br /&gt;
3.  配置管理。可以设置云主机IP、DNS、Walrus、Cluster和SAN&lt;br /&gt;
4.  审计报表。查看用户状态、资源使用率、系统日志、已注册的组件&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;这部分是web 管理界面提供的功能&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;组件管理&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.  可以注册Cloud、Walrus、Storage、Node，并可以查看、删除&lt;br /&gt;
2.  启动、停止云服务&lt;br /&gt;
3.  允许转换卷的实现方式&lt;br /&gt;
4.  可以查看、修改配置文件&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;对外以SOAP和REST提供接口&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&lt;h2&gt;&lt;strong&gt;&lt;span style=&quot;color: #0000ff;&quot;&gt;使用者拥有的功能&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; width=&quot;614&quot; align=&quot;left&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;模块&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;功能&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;说明&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;Web接口&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.  用户可以注册帐号，修改信息及密码&lt;br /&gt;
2.  查看、下载证书&lt;br /&gt;
3.  查看上传的镜像&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;这部分是web 管理界面提供的功能&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;102&quot; valign=&quot;top&quot;&gt;组件管理&lt;/td&gt;
&lt;td width=&quot;279&quot; valign=&quot;top&quot;&gt;1.  启动、停止节点&lt;br /&gt;
2.  可以绑定、上传、注册、查看镜像，也可以删除、取消绑定镜像&lt;br /&gt;
3.  查看本地可用的资源&lt;br /&gt;
4.  可以查看、启动、停止、重启虚拟机&lt;br /&gt;
5.  可以登入到一个windows虚拟机实例&lt;br /&gt;
6.  创建、附件、脱离、删除快照和卷&lt;/td&gt;
&lt;td width=&quot;234&quot; valign=&quot;top&quot;&gt;通过Euca2ools工具完成这些功能&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&amp;nbsp;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2011/06/22/the-introduction-of-eucalyptus-ee-features-and-functions.html</link>
      <guid>http://blog.javachen.com/2011/06/22/the-introduction-of-eucalyptus-ee-features-and-functions.html</guid>
      <pubDate>2011-06-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>接触云服务环境Eucalyptus</title>
      <description>&lt;p&gt;最近在接触云计算平台，熟悉了&lt;a href=&quot;http://www.eucalyptus.com/&quot;&gt;Eucalyptus&lt;/a&gt;，并用其搭建云环境。通过网上的一些例子，逐渐的摸索出用Eucalyptus搭建云计算平台的方法。我所用的Eucalyptus是免费版，缺少很多企业版的功能。&lt;/p&gt;

&lt;h1 id=&quot;eucalyptus&quot;&gt;Eucalyptus&lt;/h1&gt;

&lt;p&gt;Elastic Utility Computing Architecture for Linking Your Programs To Useful Systems （Eucalyptus） 是一种开源的软件基础结构，用来通过计算集群或工作站群实现弹性的、实用的云计算。它最初是美国加利福尼亚大学 Santa Barbara 计算机科学学院的一个研究项目，现在已经商业化，发展成为了 Eucalyptus Systems Inc。不过，Eucalyptus 仍然按开源项目那样维护和开发。Eucalyptus Systems 还在基于开源的 Eucalyptus 构建额外的产品；它还提供支持服务。 它提供了如下这些高级特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;与 EC2 和 S3 的接口兼容性（SOAP 接口和 REST 接口）。使用这些接口的几乎所有现有工具都将可以与基于 Eucalyptus 的云协作。&lt;/li&gt;
  &lt;li&gt;支持运行在 Xen hypervisor 或 KVM 之上的 VM 的运行。未来版本还有望支持其他类型的 VM，比如 VMware。&lt;/li&gt;
  &lt;li&gt;用来进行系统管理和用户结算的云管理工具。&lt;/li&gt;
  &lt;li&gt;能够将多个分别具有各自私有的内部网络地址的集群配置到一个云内。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;架构&lt;/h1&gt;

&lt;p&gt;Eucalyptus 包含五个主要组件，它们能相互协作共同提供所需的云服务。这些组件使用具有 WS-Security 的 SOAP 消息传递安全地相互通信。&lt;/p&gt;

&lt;p&gt;Cloud Controller (CLC) 在 Eucalyptus 云内，这是主要的控制器组件，负责管理整个系统。它是所有用户和管理员进入 Eucalyptus 云的主要入口。所有客户机通过基于 SOAP 或 REST 的 API 只与 CLC 通信。由 CLC 负责将请求传递给正确的组件、收集它们并将来自这些组件的响应发送回至该客户机。这是 Eucalyptus 云的对外 “窗口”。&lt;/p&gt;

&lt;p&gt;Cluster Controller (CC) Eucalyptus 内的这个控制器组件负责管理整个虚拟实例网络。请求通过基于 SOAP 或 REST 的接口被送至 CC。CC 维护有关运行在系统内的 Node Controller 的全部信息，并负责控制这些实例的生命周期。它将开启虚拟实例的请求路由到具有可用资源的 Node Controller。&lt;/p&gt;

&lt;p&gt;Node Controller (NC) 它控制主机操作系统及相应的 hypervisor（Xen 或最近的 KVM，很快就会支持 VMWare）。必须在托管了实际的虚拟实例（根据来自 CC 的请求实例化）的每个机器上运行 NC 的一个实例。 Walrus (W) 这个控制器组件管理对 Eucalyptus 内的存储服务的访问。请求通过基于 SOAP 或 REST 的接口传递至 Walrus。&lt;/p&gt;

&lt;p&gt;Storage Controller (SC) Eucalyptus 内的这个存储服务实现 Amazon 的 S3 接口。SC 与 Walrus 联合工作，用于存储和访问虚拟机映像、内核映像、RAM 磁盘映像和用户数据。其中，VM 映像可以是公共的，也可以是私有的，并最初以压缩和加密的格式存储。这些映像只有在某个节点需要启动一个新的实例并请求访问此映像时才会被解密。&lt;/p&gt;

&lt;p&gt;一个 Eucalyptus 云安装可以聚合和管理来自一个或多个集群的资源。一个集群 是连接到相同 LAN 的一组机器。在一个集群中，可以有一个或多个 NC 实例，每个实例管理虚拟实例的实例化和终止。&lt;/p&gt;

&lt;h1 id=&quot;eucalyptus-java&quot;&gt;Eucalyptus java源代码&lt;/h1&gt;

&lt;p&gt;在安装过程中，我把Eucalyptus的java源代码（eucalyptus-2.0.3-src-offline.tar.gz）下下来了，并按照&lt;a href=&quot;http://open.eucalyptus.com/participate/sourcecode&quot;&gt;官方文档&lt;/a&gt;的说明好不容易把java代码通过ant编译然后手动复制粘贴导入eclipse了，现在这些代码能够通过编译了，并能够清楚的看到Eucalyptus的java代码部分的实现方式&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;参考文章&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;[1]&lt;a href=&quot;http://blog.163.com/firstsko@126/blog/static/132168891201022935737810/&quot;&gt;Eucalyptus 开启云端&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2]&lt;a href=&quot;http://open.eucalyptus.com/wiki/EucalyptusInstallationFedora_v2.0&quot;&gt;Installing Eucalyptus (2.0) on Fedora 12 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[3]&lt;a href=&quot;http://blog.csdn.net/hispania/archive/2010/09/24/5902926.aspx&quot;&gt;在Fedora 13 上搭建Eucalyptus&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[4]&lt;a href=&quot;http://bbs.chinacloud.cn/archiver/showtopic-230.aspx&quot;&gt;ubuntu 9.04 (server)下 eucalyptus 安装（推荐）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <link>http://blog.javachen.com/2011/06/16/touch-cloud-environment-which-it-is-eucalyptus.html</link>
      <guid>http://blog.javachen.com/2011/06/16/touch-cloud-environment-which-it-is-eucalyptus.html</guid>
      <pubDate>2011-06-16T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Extjs读取xml文件生成动态表格和表单(续)</title>
      <description>&lt;p&gt;很多人向我要【&lt;a href=&quot;/2009/10/22/ext_readxml_in_bjsasc_wuzi/&quot;&gt;Extjs读取xml文件生成动态表格和表单&lt;/a&gt;】一文的源代码，故花了些时间将源代码整理出来，并重新编写此文，分享当时的技术思路。&lt;/p&gt;

&lt;p&gt;需要的文件有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.html文件，此处以SASC.search.MtrUse.html为例&lt;/li&gt;
  &lt;li&gt;2.Extjs相关文件,见SASC.search.MtrUse.html文件中的引用&lt;/li&gt;
  &lt;li&gt;3.工具类，DomUtils.js&lt;/li&gt;
  &lt;li&gt;4.核心js类:SASC.extjs.search.MtrUse.js&lt;/li&gt;
  &lt;li&gt;5.java代码&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细html和js代码见相关文件，这里先描述思路。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;首先&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过一个事件打开一个弹出窗口，该窗口的url指向SASC.search.MtrUse.html文件，并附带参数xmlFile，xmlFile的值为xml文件名称，其存于服务器的某一路径下面。如：&lt;code&gt;../SASC.search.MtrUse.html?xmlFile=PC_MTRREPLACE_IMP.xml&lt;/code&gt; 。&lt;code&gt;PC_MTRREPLACE_IMP.xml&lt;/code&gt;文件的放置路径见DomUtils.js文件中的说明。&lt;/p&gt;

&lt;p&gt;在这里，前台会读取该xml生成ext界面，后天会从xml文件读取sql语句等信息，详细信息见java代码。&lt;/p&gt;

&lt;p&gt;进入SASC.search.MtrUse.html页面，执行ext的初始化方法时，会先通过当前页面的url中获取xmlFile参数的值（调用 &lt;code&gt;getForwardXmlUrl(getQsValue(&#39;xmlFile&#39;))）&lt;/code&gt;，得到xml文件的服务器路径，然后通过javascript的解析该xml文件，渲染出ext界面,这部分代码见&lt;code&gt;SASC.extjs.search.MtrUse.js&lt;/code&gt;文件内的initStoreData(xmlObj) 方法。&lt;/p&gt;

&lt;p&gt;需要说明的是，xml文件是按照一定规律编写的，详细的参考xml文件内容，以及解析xml文件的相关方法。你可以重新定义该xml的结构，然后修改解析xml文件的方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;然后&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;初始化完ext界面之后，会获取表格数据，这部分使用了struts，这不是本文重点，故不做介绍。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果还有什么不懂或者想要源代码，欢迎email我：javachen.june#gmail.com&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2009/10/31/ext_readxml_in_bjsasc_wuzi_continue.html</link>
      <guid>http://blog.javachen.com/2009/10/31/ext_readxml_in_bjsasc_wuzi_continue.html</guid>
      <pubDate>2009-10-31T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Extjs读取xml文件生成动态表格和表单</title>
      <description>&lt;p&gt;最近开发项目，需要动态读取xml文件，生成Extjs界面，xml文件通过前台页面的按钮事件传进来，可以在网上查找【javascript 弹出子窗口】的相关文章&amp;lt;/a&amp;gt;&lt;br /&gt;
获取弹出窗口url后的参数方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;	// 获取url后的参数值
	function getQueryStringValue(name) {
		var url = window.location.search;
		if (url.indexOf(&#39;?&#39;) &amp;lt; 0) {
			return null
		}
		var index = url.indexOf(name + &quot;=&quot;);
		if (index &amp;lt; 0) {
			return null
		}
		var args = url.indexOf(&#39;&amp;amp;&#39;, index);
		var value;
		if (args &amp;gt; 0) {
			value = url.substring(index + name.length + 1, args);
		} else {
			value = url.substring(index + name.length + 1, url.length);
		}
		return value;
	}
	// 获取xml的服务器路径
	function getXmlUrl(xmlFile) {
		return &#39;../bjsasc_dictionary/&#39; + getQueryStringValue(&#39;xmlFile&#39;);
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用到的一些辅助方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;	// 去掉Dom节点中的空白字符
	function cleanWhitespaces(elem) {
		var elem = elem || document;
		var childElem = elem.childNodes;
		var childElemArray = new Array;
		for (var i = 0; i &amp;lt; childElem.length; i++) {
			if (childElem[i].nodeType == 1) {
				childElemArray.push(childElem[i]);
			}
		}
		return childElemArray;
	}
	// 取得父窗口表单中键值对
	function getParentFormValues() {
		var formObj = window.opener.document.forms[&quot;frmMain&quot;].elements;
		var formValues = &quot;&quot;;
		for (var i = 0; i &amp;lt; formObj.elements.length; i++) {
			if (formObj.elements[i].value != null
					&amp;amp;&amp;amp; formObj.elements[i].value != &quot;&quot;
					&amp;amp;&amp;amp; formObj.elements[i].value.length != 0) {
				formValues += &#39;_&#39; + formObj.elements[i].name.toUpperCase() + &#39;{&#39;
						+ formObj.elements[i].value.toUpperCase() + &#39;}&#39;
						+ formObj.elements[i].name.toUpperCase() + &#39;_ &#39;;
			}
		}
		formValues += opener.getBindValue(formObj.elements);
		return formValues;
	}
	// 取得过滤条件表单的键值对
	function getCondictionValues() {
		var condictionString = &quot;&quot;;
		var formObj = form.getForm().getEl().dom;
		for (var i = 0; i &amp;lt; formObj.elements.length; i++) {
			if (formObj.elements[i].value != null
					&amp;amp;&amp;amp; formObj.elements[i].value != &quot;&quot;
					&amp;amp;&amp;amp; formObj.elements[i].value.length != 0) {
				condictionString += &#39;_&#39; + formObj.elements[i].name + &#39;{&#39;
						+ formObj.elements[i].value + &#39;}&#39;
						+ formObj.elements[i].name + &#39;_ &#39;;
			}
		}
		// alert(&quot;condictionString&quot;+condictionString);
		return condictionString;
	}
	// 判读Ext表单是否有输入
	function isFormInputed(ExtForm) {
		var flag = false;
		var formObj = ExtForm.getEl().dom;
		for (var i = 0; i &amp;lt; formObj.elements.length; i++) {
			if (formObj.elements[i].value != null
					&amp;amp;&amp;amp; formObj.elements[i].value != &quot;&quot;
					&amp;amp;&amp;amp; formObj.elements[i].value.length != 0) {
				flag = true;
				break;
			}
		}
		return flag;
	}
	// 将计算得到的结果四舍五入
	/* * ForDight(Dight,How):数值格式化函数，Dight要 * 格式化的 数字，How要保留的小数位数。 */
	function ForDight(Dight, How) {
		var Dight = Math.round(Dight * Math.pow(10, How)) / Math.pow(10, How);
		return Dight;
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;xml文件格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;	&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;gb2312&quot;?&amp;gt;
	&amp;lt;dictionary&amp;gt;
		&amp;lt;title&amp;gt;领用出库-物资选择&amp;lt;/title&amp;gt;
		&amp;lt;sql&amp;gt;
		select V_stores_list.* 
		from V_stores_list where WHID=&#39;+$getform(WHID)+&#39; AND PROJECTNO=&#39;+$getform(PROJECTNO)+&#39;
			AND CANUSEQTY&amp;gt;0 AND ??? and isblock=0
		&amp;lt;/sql&amp;gt;
		&amp;lt;fromtable&amp;gt;V_stores_list&amp;lt;/fromtable&amp;gt;
		&amp;lt;targettable&amp;gt;BO_IC_EXPORT_S&amp;lt;/targettable&amp;gt;
		&amp;lt;line&amp;gt;20&amp;lt;/line&amp;gt;
		&amp;lt;!-- 条件区开始--&amp;gt;
		&amp;lt;condition&amp;gt;
			&amp;lt;fieldname&amp;gt;MTRNAME&amp;lt;/fieldname&amp;gt;
			&amp;lt;fieldtitle&amp;gt;物资名称&amp;lt;/fieldtitle&amp;gt;
			&amp;lt;fieldtype&amp;gt;文本&amp;lt;/fieldtype&amp;gt;
			&amp;lt;comparetype&amp;gt;&amp;lt;![CDATA[like
			MTRNAME
			单行
			&amp;lt;![CDATA[]]&amp;gt;		
			&amp;lt;/comparetype&amp;gt;
		&amp;lt;/ condition&amp;gt;
	&amp;lt;/ dictionary&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，弹出窗口页面Ext入口代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;	// 全局变量
	var result = {};
	var grid;
	var form;
	var viewport;
	var store;
	var sm = new Ext.grid.CheckboxSelectionModel();
	var autoStore;
	var tempItems1 = [];
	var tempItems2 = [];
	var tempItems3 = [];
	var flag = 1;
	// 程序入口
	Ext.onReady(function() {
		Ext.QuickTips.init();// 初始化
		Ext.form.Field.prototype.msgTarget = &#39;qtip&#39;;// 统一指定错误信息提示方式
		Ext.util.CSS	.swapStyleSheet(&#39;theme&#39;, &#39;../aws_js/extjs2/css/xtheme-gray.css&#39;);// 更换皮肤
		Ext.BLANK_IMAGE_URL = &#39;../aws_js/extjs2/images/default/s.gif&#39;;
		Ext.Ajax.request({
			url : getXmlUrl(getQueryStringValue(&#39;xmlFile&#39;)), // 访问数据字典
			method : &#39;post&#39;,
			success : function(res, opt) {
				var xmlObj = res.responseXML;
				initStoreData(xmlObj); // 访问成功后执行后续工作
			}
		})
	});
	function initStoreData(xmlObj) {
		getInitData(xmlObj);
		document.title = result.winTitle;

		var dataRecorder = Ext.data.Record.create(result.gridRecords);// 指定记录集格式
		// 获取表格数据部分
		store = new Ext.data.Store({
			idProperty : &#39;ID&#39;,
			proxy : new Ext.data.HttpProxy({
				url : &#39;../search.do?method=findAll&#39;,
				failure : function() {
					// Ext.Msg.alert(&quot;Notice&quot;, &quot;网路问题&quot;);
				},
				success : function(response) {
					// Ext.Msg.alert(&quot;Notice&quot;, response.responseText);
				}
			}),

			// baseParams : {
			// parentFormValues : getParentFormValues(),// 请求发送的参数：父表单值和xml文件名
			// xmlFile : getQueryStringValue(&#39;xmlFile&#39;)
			// // cmd:&#39;search&#39;
			// },
			reader : new Ext.data.JsonReader({
				totalProperty : &#39;totalCount&#39;,
				root : &#39;data&#39;
			}, dataRecorder)
		});
		// 要分页，第一次加载数据必须传start和limit两参数
		// store.load({
		// params : {
		// start : 0,
		// limit : result.limit
		// }
		// });
		initViewport();
	}
	// 获得界面初始化的一些数据
	function getInitData(xmlObj) {
		// result.formItems = {};
		result.columnHeaders = [];
		result.gridRecords = [];
		result.dbFilterRecords = [];
		result.winTitle = xmlObj.getElementsByTagName(&quot;title&quot;)[0].firstChild.nodeValue; // 窗口title名称
		result.limit = xmlObj.getElementsByTagName(&quot;line&quot;)[0].firstChild.nodeValue;// 分页数据
		result.fromTable = xmlObj.getElementsByTagName(&quot;fromTable&quot;)[0].firstChild.nodeValue;// 来自哪个表
		// 获取过滤条件表单的界面数据
		var conections = xmlObj.getElementsByTagName(&quot;condition&quot;);
		var row = ForDight(conections.length / 3, 0);
		for (var i = 0; i &amp;lt; conections.length; i++) {
			var item = {};
			var condition = cleanWhitespaces(conections[i]);
			item.id = condition[0].firstChild.nodeValue;
			item.fieldLabel = condition[1].firstChild.nodeValue;
			item.name = condition[4].firstChild.nodeValue;
			item.anchor = &#39;95%&#39;;
			if (condition[6].firstChild.nodeValue == &#39;单行&#39;) {
				item.xtype = &#39;textfield&#39;;
			} else if (condition[6].firstChild.nodeValue == &#39;日期&#39;) {
				item.xtype = &#39;datefield&#39;;
				item.format = &#39;Y-m-d&#39;;
			} else if (condition[6].firstChild.nodeValue == &#39;数值&#39;) {
				item.xtype = &#39;numberfield&#39;;
				item.minValue = 0;
				item.minText = &#39;请输入有效的数字&#39;;
				item.decimalPrecision = 6;
			} else if (condition[6].firstChild.nodeValue == &#39;自动填充&#39;) {
				var autoStore = new Ext.data.SimpleStore({
					proxy : new Ext.data.HttpProxy({// 读取远程数据的代理
						url : &#39;../ajax/autoComplete.do?method=autoComplete&#39;,
						failure : function() {
							Ext.Msg.alert(&quot;Notice&quot;, &quot;no records&quot;);
						}
					}),
					fields : [&#39;property&#39;],
					baseParams : {
						&#39;sqlString&#39; : condition[4].firstChild.nodeValue + &#39; | &#39;
								+ result.fromTable
					}
				});
				item.xtype = &#39;combo&#39;;
				item.store = autoStore;
				item.displayField = &#39;property&#39;;
				item.typeAhead = true;
				item.allQuery = &#39;all&#39;;// 查询信息的查询字符串
				item.queryParam = &#39;keyword&#39;;// 查询的名字
				item.mode = &#39;remote&#39;;
				item.minChars = 3;// 默认最少输入4
				item.forceSelection = true;
				item.queryDelay = 0;// 查询延迟时间
				item.triggerAction = &#39;all&#39;;
				item.emptyText = &#39;&#39;;
				item.resizable = true;
				item.selectOnFocus = true;
			}
			if (i / row &amp;lt; 1) {
				tempItems1.push(item);
			}
			if (i / row &amp;lt; 2 &amp;amp;&amp;amp; i / row &amp;gt;= 1) {
				tempItems2.push(item);
			} else if (i / row &amp;gt;= 2) {
				tempItems3.push(item);
			}
		}
		// alert(Ext.util.JSON.encode(result));

		// 获取表格表头的界面数据和rcord记录的数据格式
		var fields = xmlObj.getElementsByTagName(&quot;field&quot;);
		result.columnHeaders.push(sm);// 插入多选框
		result.columnHeaders.push(new Ext.grid.RowNumberer({
			width : 20
		}));// 插入行号
		for (var i = 0; i &amp;lt; fields.length; i++) {
			var item = {};
			var record = {};
			var array = [];
			var field = cleanWhitespaces(fields[i]);
			var renderDate = function(value) {
				return value ? value.dateFormat(&#39;Y-m-d&#39;) : &#39;&#39;;
			}
			// 生成grid表格中store数据记录
			record.name = field[0].firstChild.nodeValue;
			record.mapping = field[0].firstChild.nodeValue;
			if (field[1].firstChild.nodeValue == &#39;日期&#39;) {
				record.type = &#39;date&#39;;
				record.dateFormat = &#39;Y-m-d&#39;;
				item.renderer = Ext.util.Format.dateRenderer(&#39;Y-m-d&#39;)
			} else if (field[1].firstChild.nodeValue == &#39;数值&#39;) {
				record.type = &#39;auto&#39;;
			} else {
				record.type = &#39;string&#39;;
			}
			result.gridRecords.push(record);

			// 生成grid表格表头数据记录
			item.dataIndex = field[0].firstChild.nodeValue;
			item.header = field[2].firstChild.nodeValue;
			// item.width=field[3].firstChild.nodeValue;
			item.sortable = true;
			if (field.length == 7
					&amp;amp;&amp;amp; field[5].firstChild.nodeValue.toUpperCase() == &#39;TRUE&#39;) {
				item.hidden = true;
				// item.hideable=false;
			}
			if (field.length == 6) {
				item.hidden = false;
			}
			result.columnHeaders.push(item);

			// 生成模糊过滤store的记录
			if (field[4].firstChild.nodeValue.toUpperCase() == &#39;TRUE&#39;) {
				array.push(field[2].firstChild.nodeValue);// fieldName
				array.push(field[0].firstChild.nodeValue);// fieldValue
			}
			result.dbFilterRecords.push(array);
		}
		return result;
	};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;渲染Ext界面代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;	function initViewport() {
		if (!form) {
			form = getInsertForm();
		}

		if (!grid) {
			grid = getInsertGrid();
		}

		if (!viewport) {
			var formPanel = new Ext.Panel({
				title : &#39;查询条件&#39;,
				region : &#39;north&#39;,
				split : true,
				frame : true,
				border : true,
				layout : &#39;fit&#39;,
				height : 280,
				collapsible : true,
				items : [form]

			});
			viewport = new Ext.Viewport({
				layout : &#39;border&#39;,
				modal : true,// 是否为模式窗口
				border : false,
				items : [formPanel, grid]
			});
		}
	}

	function searchByFilter() {
		var dbfilter = Ext.get(&quot;dbFilter&quot;).getValue();
		var fieldMame = Ext.get(&quot;search-type&quot;).getValue();
		if (dbfilter == null || dbfilter == &quot;&quot;) {
			alert(&quot;请输入一个关键字&quot;);
			Ext.get(&quot;dbFilter&quot;).focus();
		} else if (fieldMame == &quot;==选择过滤字段==&quot;) {
			alert(&quot;请选择一个过滤字段&quot;);
			Ext.get(&quot;search-type&quot;).focus();
		} else {
			form.getForm().reset();
			if (flag == 1) {
				store.baseParams = {
					dbfilter : dbfilter,
					parentFormValues : getParentFormValues(),
					fieldMame : Ext.get(&quot;hiddenValue&quot;).dom.value,
					xmlFile : getQueryStringValue(&#39;xmlFile&#39;),
					cmd : &#39;filter&#39;
				};
				store.load({
					params : {
						start : 0,
						limit : result.limit
					}
				});
				form.getForm().reset();
				flag = 0;
			} else {
				store.baseParams = {
					dbfilter : dbfilter,
					parentFormValues : getParentFormValues(),
					fieldMame : Ext.get(&quot;hiddenValue&quot;).dom.value,
					xmlFile : getQueryStringValue(&#39;xmlFile&#39;),
					cmd : &#39;filter&#39;
				};
				store.reload();

			}
			Ext.get(&quot;dbFilter&quot;).dom.value = &quot;&quot;;
		}
	}
	// 获取过滤条件部分的表单控件
	function getInsertForm() {
		form = new Ext.form.FormPanel({
			name : &#39;frmMain&#39;,
			height : 260,
			labelAlign : &#39;left&#39;,
			labelWidth : 110,
			layout : &#39;fit&#39;,
			waitMsgTarget : true,
			items : [{
				xtype : &#39;fieldset&#39;,
				frame : true,
				title : &#39;高级查询&#39;,
				autoHeight : true,
				layout : &#39;column&#39;,
				items : [{
					columnWidth : .333,
					layout : &#39;form&#39;,
					items : tempItems1
				}, {
					columnWidth : .333,
					layout : &#39;form&#39;,
					items : tempItems2
				}, {
					columnWidth : .333,
					layout : &#39;form&#39;,
					items : tempItems3
				}]
			}],
			tbar : [&#39;请输入模糊值: &#39;, &#39; &#39;, {
				xtype : &#39;textfield&#39;,
				width : 200,
				id : &#39;dbFilter&#39;,
				listeners : {
					specialkey : function(field, e) {
						if (e.getKey() == Ext.EventObject.ENTER) {
							searchByFilter();
						}
					}
				}
			}, &#39;-&#39;, {
				xtype : &#39;combo&#39;,
				id : &#39;search-type&#39;,
				anchor : &#39;60%&#39;,
				hiddenName : &#39;hiddenValue&#39;,
				width : 120,
				triggerAction : &#39;all&#39;,// 单击触发按钮显示全部数据
				store : new Ext.data.SimpleStore({// 定义组合框中显示的数据源
					fields : [&#39;fieldName&#39;, &#39;fieldValue&#39;],
					data : result.dbFilterRecords
				}),// 设置数据源
				displayField : &#39;fieldName&#39;,// 定义要显示的字段
				valueField : &#39;fieldValue&#39;,// 定义值字段
				mode : &#39;local&#39;,// 本地模式
				forceSelection : true,// 要求输入值必须在列表中存在
				typeAhead : true,// 允许自动选择匹配的剩余部分文本
				// value : &#39;==选择过滤字段==&#39;,
				value : &#39;==选择过滤字段==&#39;,
				handleHeight : 10
					// 下拉列表中拖动手柄的高度
					}, &#39;-&#39;, {
						xtype : &#39;button&#39;,
						text : &#39;筛选&#39;,
						tooltip : &#39;先选择查询条件，再输入模糊值&#39;,
						iconCls : &#39;find&#39;,
						handler : searchByFilter
					}, &#39;-&amp;gt;&#39;, {
						pressed : true,
						xtype : &#39;button&#39;,
						text : &#39;确认插入&#39;,
						enableToggle : true,
						tooltip : &#39;请选中一行或多行记录，再选择确认插入&#39;,
						handler : function() {
							if (sm.hasSelection()) {
								var records = sm.getSelections();
								var jsonObj = &quot;{data:[&quot;;
								for (var i = 0; i &amp;lt; records.length; i++) {
									jsonObj += Ext.encode(records[i].data);
									if (i != records.length - 1) {
										jsonObj += &quot;,&quot;;
									}
								}
								jsonObj += &quot;]}&quot;
								Ext.Ajax.request({
									url : &#39;../search.do?method=insertChoices&#39;,
									method : &#39;post&#39;,
									params : {
										xmlFile : getQueryStringValue(&#39;xmlFile&#39;)
												.toString(),
										jsonObj : jsonObj,
										parentFormValues : getParentFormValues()
									},
									callback : function(options, success, response) {
										if (response.responseText == &quot;success&quot;) {

											Ext.Msg.alert(&quot;提示&quot;, &quot;插入成功&quot;, function() {
												window.close();
												// opener.location.reload();
		                                    opener.saveForm();
											});
										} else {
											Ext.Msg.alert(&quot;提示&quot;, &quot;插入失败&quot;);
										}
									}
								})

							} else {
								alert(&quot;请选择一行或多行数据&quot;);
							}
						},
						scope : this
					}, &#39;-&#39;, {
						pressed : true,
						xtype : &#39;button&#39;,
						text : &#39;取消&#39;,
						tooltip : &#39;取消选择，直接退出&#39;,
						handler : function() {
							Ext.Msg.confirm(&#39;Notice&#39;, &#39;确认退出？&#39;, function(id) {
								if (id == &quot;yes&quot;)
									window.close();
							});
						},
						scope : this
					}],
			buttons : [{
				text : &#39;执行查询条件&#39;,
				handler : function() {
					var connections = getCondictionValues();
					if (!form.getForm().isValid()) {
						return;
					};
					if (isFormInputed(form.getForm()) == false) {
						alert(&quot;请输入查询条件&quot;);
						form.getForm().focus();
						return;
					} else {
						Ext.get(&quot;dbFilter&quot;).dom.value = &quot;&quot;;
						if (flag == 1) {
							store.baseParams = {
								xmlFile : getQueryStringValue(&#39;xmlFile&#39;),
								condictions : connections,
								parentFormValues : getParentFormValues(),
								cmd : &#39;search&#39;
							};
							store.load({
								params : {
									start : 0,
									limit : result.limit
								}
							});
							form.getForm().reset();
							flag = 0;
						} else {
							store.baseParams = {
								xmlFile : getQueryStringValue(&#39;xmlFile&#39;),
								condictions : connections,
								parentFormValues : getParentFormValues(),
								cmd : &#39;search&#39;
							};
							store.reload();
						}

						form.getForm().reset();
					}
				}
			}, {
				text : &#39;重置&#39;,
				handler : function() {
					form.getForm().reset();
				}
			}]
		});
		return form;
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后做成的效果图，如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xnrdo.com1.z0.glb.clouddn.com/2014/bjsasc.bmp&quot; alt=&quot;bjsasc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果你想要获取更详细说明，请移步：&lt;a href=&quot;/2009/10/31/ext_readxml_in_bjsasc_wuzi_continue/&quot;&gt;Extjs读取xml文件生成动态表格和表单(续)&lt;/a&gt;&lt;/p&gt;
</description>
      <link>http://blog.javachen.com/2009/10/22/ext_readxml_in_bjsasc_wuzi.html</link>
      <guid>http://blog.javachen.com/2009/10/22/ext_readxml_in_bjsasc_wuzi.html</guid>
      <pubDate>2009-10-22T00:00:00+08:00</pubDate>
    </item>
    
    <item>
      <title>Hello Jekyll!</title>
      <description>&lt;p&gt;目前，博客使用的是Jekyll搭建的，markdown语法使用的是Redcarpet。Redcarpet支持设置 extensions ，值为一个字符串数组，每个字符串都是 Redcarpet::Markdown 类的扩展，相应的扩展就会设置为 true 。&lt;/p&gt;

&lt;p&gt;配置为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;highlighter: pygments
markdown: redcarpet  # [ maruku | rdiscount | kramdown | redcarpet ]

redcarpet:
    extensions:
        - fenced_code_blocks
        - no_intra_emphasis
        - strikethrough
        - autolink
        - tables
        - superscript
        - highlight
        - prettify
        - with_toc_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;redcarpet有几个扩展：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;fenced_code_blocks&lt;/code&gt; 解析代码块，使用3个或3个以上 ~ 或者 ` 包围起来的文本会被解析为代码块，你可以在开头指定代码的语言类型&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;no_intra_emphasis&lt;/code&gt; 不解析单词中的下划线&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;strikethrough&lt;/code&gt; 支持两个 ~ 包围的文本，解析为删除线&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;space_after_headers&lt;/code&gt; #后面必须加空格，否则不会被解析为标题&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;autolink&lt;/code&gt; 自动检查文本中 http https ftp 等协议的链接文本，将之解析为链接。没有以 http:// 开头，而是直接以 www. 开头的同样会被检查到&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hard_wrap&lt;/code&gt; 如果 Markdown 文本中有折行的话，会转换为标签&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;tables&lt;/code&gt; 表格&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;superscript&lt;/code&gt; 上标，例如 : &lt;code&gt;this is the 2^(nd) time&lt;/code&gt; ，效果为：this is the 2^(nd) time&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;with_toc_data&lt;/code&gt; 给生成的 Header 标签增加锚点&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section&quot;&gt;语法&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;标题&lt;/h2&gt;

&lt;p&gt;标题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 测试 h1
## 测试 h2
### 测试 h3
#### 测试 h4
##### 测试 h5
###### 测试 h6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果：&lt;/p&gt;

&lt;h1 id=&quot;h1&quot;&gt;测试 h1&lt;/h1&gt;
&lt;p&gt;## 测试 h2&lt;br /&gt;
### 测试 h3&lt;br /&gt;
#### 测试 h4&lt;br /&gt;
##### 测试 h5&lt;br /&gt;
###### 测试 h6&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;列表&lt;/h2&gt;

&lt;p&gt;无序列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;* 项目1
* 项目2
* 项目3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;项目1&lt;/li&gt;
  &lt;li&gt;项目2&lt;/li&gt;
  &lt;li&gt;项目3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有序列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 项目1
2. 项目2
3. 项目3
   * 项目1
   * 项目2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;项目1&lt;/li&gt;
  &lt;li&gt;项目2&lt;/li&gt;
  &lt;li&gt;项目3
    &lt;ul&gt;
      &lt;li&gt;项目1&lt;/li&gt;
      &lt;li&gt;项目2&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-3&quot;&gt;粗体与斜体&lt;/h2&gt;

&lt;p&gt;文字格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;**这是文字粗体格式**
*这是文字斜体格式*
~~在文字上添加删除线~~
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这是文字粗体格式&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;这是文字斜体格式&lt;/em&gt;&lt;br /&gt;
~~在文字上添加删除线~~&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;链接与图片&lt;/h2&gt;

&lt;p&gt;自动链接&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt; &amp;lt;http://blog.javachen.com&amp;gt;
 &amp;lt;XhstormR@foxmail.com&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;插入链接&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt; [link text](http://example.com/ &quot;optional title&quot;)

 [link text][id]
 [id]: http://example.com/  &quot;optional title here&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;插入图片&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt; ![](/path/to/img.jpg &quot;optional title&quot;){ImgCap}alt text{/ImgCap}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;图片链接&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt; [![][jane-eyre-pic]{ImgCap}{/ImgCap}][jane-eyre-douban]

 [jane-eyre-pic]: http://img3.douban.com/mpic/s1108264.jpg
 [jane-eyre-douban]: http://book.douban.com/subject/1141406/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-5&quot;&gt;代码&lt;/h2&gt;

&lt;p&gt;行代码： &lt;code&gt;code&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;用TAB键起始的段落，会被认为是代码块，或者使用三个`或~：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;/* hello world demo */
#include &amp;lt;stdio.h&amp;gt;
int main(int argc, char **argv)
{
        printf(&quot;Hello, World!\n&quot;);
        return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;表格&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;|head1|head2|head3|head4
|---|:---|---:|:---:|
|row1text1|row1text2|row1text3|row1text4
|row2text1|row2text2|row2text3|row2text4
|row3text1|row3text2|row3text3|row3text4
|row4text1|row4text2|row4text3|row4text4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;head1&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;head2&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;head3&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;head4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;row1text1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;row1text2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;row1text3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;row1text4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;row2text1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;row2text2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;row2text3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;row2text4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;row3text1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;row3text2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;row3text3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;row3text4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;row4text1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;row4text2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;row4text3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;row4text4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section-7&quot;&gt;引用&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; 第一行引用文字
&amp;gt; 第二行引用文字
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;第一行引用文字&lt;br /&gt;
第二行引用文字&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-8&quot;&gt;水平线&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;***
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;section-9&quot;&gt;参考资料&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://havee.me/internet/2013-11/jekyll-liquid-designers.html&quot;&gt;Jekyll 扩展的 Liquid 设计&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://havee.me/internet/2013-07/jekyll-liquid-extensions.html&quot;&gt;Jekyll 扩展的 Liquid 模板&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <link>http://blog.javachen.com/2009/01/01/hello-jekyll.html</link>
      <guid>http://blog.javachen.com/2009/01/01/hello-jekyll.html</guid>
      <pubDate>2009-01-01T00:00:00+08:00</pubDate>
    </item>
    
  </channel>
</rss>