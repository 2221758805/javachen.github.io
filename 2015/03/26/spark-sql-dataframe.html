<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>Spark SQL中的DataFrame - JavaChen Blog</title>
      <meta name="author" content="yuke"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2015/03/26/spark-sql-dataframe.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

      <!-- atom & rss feed -->
      <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
      <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              <li><a href="https://twitter.com/java_chen" target="_blank" title="twitter"><span class="fa fa-twitter fa-2x"></span></a></li>
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2015/03/25/converting-avro-data-to-parquet-format.html" title="将Avro数据转换为Parquet格式"><i class="fa fa-angle-double-left"></i>&nbsp;将Avro数据转换为Parquet格式</a></li>
                
                
                <li class="next"><a href="/2015/03/30/reading-list-2015-03.html" title="Reading List 2015-03">Reading List 2015-03&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> Spark SQL中的DataFrame  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2015.03.26 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <p>在2014年7月1日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上。在会议上，Databricks 表示，Shark 更多是对 Hive 的改造，替换了 Hive 的物理执行引擎，因此会有一个很快的速度。然而，不容忽视的是，Shark 继承了大量的 Hive 代码，因此给优化和维护带来了大量的麻烦。随着性能优化和先进分析整合的进一步加深，基于 MapReduce 设计的部分无疑成为了整个项目的瓶颈。 详细内容请参看 <a href="http://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html">Shark, Spark SQL, Hive on Spark, and the future of SQL on Spark</a>。</p>

<p>Spark SQL 允许 Spark 执行用 SQL, HiveQL 或者 Scala 表示的关系查询。在 Spark 1.3 之前，这个模块的核心是一个新类型的 RDD-<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD">SchemaRDD</a>。 SchemaRDDs 由<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package@Row:org.apache.spark.sql.catalyst.expressions.Row.type">行</a>对象组成，行对象拥有一个模式（scheme） 来描述行中每一列的数据类型。SchemaRDD 与关系型数据库中的表很相似，可以通过存在的 RDD、一个 <a href="http://parquet.io/">Parquet</a> 文件、结构化的文件、外部数据库、或者对存储在 Apache Hive 中的数据执行 HiveSQL 查询中创建。</p>

<p>当前 Spark SQL 还处于 alpha 阶段，一些 API 在将将来的版本中可能会有所改变。例如，<a href="http://www.infoq.com/cn/news/2015/03/apache-spark-1.3-released">Apache Spark 1.3发布，新增Data Frames API，改进Spark SQL和MLlib</a>。在 Spark 1.3 中，SchemaRDD 改为叫做 DataFrame。</p>

<p>本文是基于 Spark 1.3 写成，特此说明。</p>

<h1 id="sqlcontext">创建 SQLContext</h1>

<p>Spark SQL 中所有相关功能的入口点是 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext">SQLContext</a> 类或者它的子类， 创建一个 SQLContext 的所有需要仅仅是一个 SparkContext。</p>

<p>使用 Scala 创建方式如下：</p>

<pre><code class="language-scala">val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._
</code></pre>

<p>使用 Java 创建方式如下：</p>

<pre><code class="language-java">JavaSparkContext sc = ...; // An existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);
</code></pre>

<p>使用 Python 创建方式如下：</p>

<pre><code class="language-python">from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
</code></pre>

<p>除了一个基本的 SQLContext，你也能够创建一个 HiveContext，它支持基本 SQLContext 所支持功能的一个超集。它的额外的功能包括用更完整的 HiveQL 分析器写查询去访问 HiveUDFs 的能力、 从 Hive 表读取数据的能力。用 HiveContext 你不需要一个已经存在的 Hive 开启，SQLContext 可用的数据源对 HiveContext 也可用。HiveContext 分开打包是为了避免在 Spark 构建时包含了所有 的 Hive 依赖。如果对你的应用程序来说，这些依赖不存在问题，Spark 1.3 推荐使用 HiveContext。以后的稳定版本将专注于为 SQLContext 提供与 HiveContext 等价的功能。</p>

<p>用来解析查询语句的特定 SQL 变种语言可以通过 <code>spark.sql.dialect</code> 选项来选择。这个参数可以通过两种方式改变，一种方式是通过 <code>setConf</code> 方法设定，另一种方式是在 SQL 命令中通过 <code>SET key=value</code> 来设定。对于 SQLContext，唯一可用的方言是 “sql”，它是 Spark SQL 提供的一个简单的 SQL 解析器。在 HiveContext 中，虽然也支持”sql”，但默认的方言是 “hiveql”，这是因为 HiveQL 解析器更完整。</p>

<h1 id="dataframe">创建 DataFrame</h1>

<p>使用 SQLContext，应用可以从一个存在的 RDD、Hive 表或者数据源中创建 DataFrame。</p>

<p>下载测试数据 <a href="https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json">people.json</a>，并将其上传到 HDFS 上：</p>

<pre><code class="language-bash">$ wget https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json
$ hadoop fs -put people.json
</code></pre>

<p>下面是使用 Scala 创建方式：</p>

<pre><code class="language-scala">val sqlContext = new org.apache.spark.sql.SQLContext(sc)

val df = sqlContext.jsonFile("people.json")

// Displays the content of the DataFrame to stdout
df.show()
</code></pre>

<p>下面是使用 Java 创建方式：</p>

<pre><code class="language-java">JavaSparkContext sc = ...; // An existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

DataFrame df = sqlContext.jsonFile("people.json");

// Displays the content of the DataFrame to stdout
df.show();
</code></pre>

<p>下面是使用 Python 创建方式：</p>

<pre><code class="language-python">from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

df = sqlContext.jsonFile("people.json")

# Displays the content of the DataFrame to stdout
df.show()
</code></pre>

<p>DataFrame API 请参考 <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame">Scala</a>、<a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/DataFrame.html">Java</a> 以及 <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">Python</a>。</p>

<h2 id="dataframe-">DataFrame 操作</h2>

<p>运行 spark-shell 执行下面代码进行测试，运行的代码和输出结果如下：</p>

<pre><code class="language-scala">$ spark-shell
Spark context available as sc.
SQL context available as sqlContext.

// Create the DataFrame
scala&gt; val df = sqlContext.jsonFile("people.json")

scala&gt; df.count()
res1: Long = 3

scala&gt; df.first()
res2: org.apache.spark.sql.Row = [null,Michael]

scala&gt; df.head()
res3: org.apache.spark.sql.Row = [null,Michael]

scala&gt; df.collect()
res4: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy], [19,Justin])

scala&gt; df.collectAsList()
res5: java.util.List[org.apache.spark.sql.Row] = [[null,Michael], [30,Andy], [19,Justin]]

// Show the content of the DataFrame
scala&gt; df.show()
age  name
null Michael
30   Andy
19   Justin

scala&gt; df.take(2)
res6: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy])

scala&gt; df.columns
res7: Array[String] = Array(age, name)

scala&gt; df.dtypes
res8: Array[(String, String)] = Array((age,LongType), (name,StringType))

// Print the schema in a tree format
scala&gt; df.printSchema()
root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)

scala&gt; df.explain()
== Physical Plan ==
PhysicalRDD [age#0L,name#1], MapPartitionsRDD[96] at map at JsonRDD.scala:41

// age column
scala&gt; val ageCol = df("age")  

// The following creates a new column that increases everybody's age by 10.
scala&gt; df("age") + 10 

// Select only the "name" column
scala&gt; df.select("name").show()
name
Michael
Andy
Justin

// Select everybody, but increment the age by 1
scala&gt; df.select(df("name"), df("age")+1).show()
name    (age + 1)
Michael null
Andy    31
Justin  20

// Select people older than 21
scala&gt; df.filter(df("age") &gt; 21).show()
age name
30  Andy

// Count people by age
scala&gt; df.groupBy("age").count().show()
age  count
null 1
19   1
30   1
</code></pre>

<h2 id="sql-">运行 SQL 查询</h2>

<p>SQLContext 有一个 sql 方法，可以运行 SQL 查询。</p>

<pre><code class="language-java">sqlContext.sql("SELECT * FROM table")
</code></pre>

<p>Spark SQL 支持两种方法将存在的 RDD 转换为 DataFrame 。第一种方法使用反射来推断包含特定对象类型的 RDD 的模式。在你写 spark 程序的同时，当你已经知道了模式，这种基于反射的方法可以使代码更简洁并且程序工作得更好。</p>

<p>第二种方法是通过一个编程接口来实现，这个接口允许你构造一个模式，然后在存在的 RDD 上使用它。虽然这种方法更冗长，但是它允许你在运行期之前不知道列以及列的类型的情况下构造 DataFrame。</p>

<p>SQLContext 的 API 见 <a href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.SQLContext">SQLContext</a> 。</p>

<h3 id="section">利用反射推断模式</h3>

<p>Spark SQL的 Scala 接口支持将包含样本类的 RDD 自动转换为 DataFrame。这个样本类定义了表的模式。样本类的参数名字通过反射来读取，然后作为列的名字。样本类可以嵌套或者包含复杂的类型如序列或者数组。这个 RDD 可以隐式转化为一个 DataFrame，然后注册为一个表，表可以在后续的 sql 语句中使用。</p>

<p>以 <a href="https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.txt">people.txt</a> 作为测试数据，使用 Scala 语言来创建 DataFrame：</p>

<pre><code class="language-scala">// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

// Define the schema using a case class.
// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
// you can use custom classes that implement the Product interface.
case class People(name: String, age: Int)

// Create an RDD of Person objects and register it as a table.
val people = sc.textFile("people.txt").map(_.split(",")).map(p =&gt; People(p(0), p(1).trim.toInt)).toDF()
people.registerTempTable("people")

// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sqlContext.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19")

// The results of SQL queries are DataFrames and support all the normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
teenagers.map(t =&gt; "Name: " + t(0)).collect().foreach(println)
</code></pre>

<p>对于 Java 语言，需要创建一个 JavaBean，然后在将数据映射到它上面：</p>

<pre><code class="language-java">public static class People implements Serializable {
  private String name;
  private int age;

  public String getName() {
    return name;
  }

  public void setName(String name) {
    this.name = name;
  }

  public int getAge() {
    return age;
  }

  public void setAge(int age) {
    this.age = age;
  }
}
</code></pre>

<p>然后，使用 sqlContext 的 createDataFrame 方法，从 JavaBean 和数据上创建一个 DataFrame 并注册一个表，下面是一个比较完整的例子：</p>

<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;

import java.io.Serializable;
import java.util.Arrays;
import java.util.List;

public class JavaSparkSQLByReflection {
    public static void main(String[] args) throws Exception {
        SparkConf sparkConf = new SparkConf().setAppName("JavaSparkSQLByReflection");
        JavaSparkContext ctx = new JavaSparkContext(sparkConf);
        SQLContext sqlCtx = new SQLContext(ctx);

        System.out.println("=== Data source: RDD ===");
        // Load a text file and convert each line to a Java Bean.
        JavaRDD&lt;People&gt; people = ctx.textFile("people.txt").map(
                new Function&lt;String, People&gt;() {
                    @Override
                    public People call(String line) {
                        String[] parts = line.split(",");

                        People people = new People();
                        people.setName(parts[0]);
                        people.setAge(Integer.parseInt(parts[1].trim()));
                        return people;
                    }
                });

        // Apply a schema to an RDD of Java Beans and register it as a table.
        DataFrame schemaPeople = sqlCtx.createDataFrame(people, People.class);
        schemaPeople.registerTempTable("people");

        // SQL can be run over RDDs that have been registered as tables.
        DataFrame teenagers = sqlCtx.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19");

        // The results of SQL queries are DataFrames and support all the normal RDD operations.
        // The columns of a row in the result can be accessed by ordinal.
        List&lt;String&gt; teenagerNames = teenagers.toJavaRDD().map(new Function&lt;Row, String&gt;() {
            @Override
            public String call(Row row) {
                return "Name: " + row.getString(0);
            }
        }).collect();

        for (String name : teenagerNames) {
            System.out.println(name);
        }


        System.out.println("=== Data source: Parquet File ===");
        // DataFrames can be saved as parquet files, maintaining the schema information.
        schemaPeople.saveAsParquetFile("people.parquet");

        // Read in the parquet file created above.
        // Parquet files are self-describing so the schema is preserved.
        // The result of loading a parquet file is also a DataFrame.
        DataFrame parquetFile = sqlCtx.parquetFile("people.parquet");

        //Parquet files can also be registered as tables and then used in SQL statements.
        parquetFile.registerTempTable("parquetFile");
        DataFrame teenagers2 =
                sqlCtx.sql("SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19");
        teenagerNames = teenagers2.toJavaRDD().map(new Function&lt;Row, String&gt;() {
            @Override
            public String call(Row row) {
                return "Name: " + row.getString(0);
            }
        }).collect();
        for (String name : teenagerNames) {
            System.out.println(name);
        }

        System.out.println("=== Data source: JSON Dataset ===");
        // A JSON dataset is pointed by path.
        // The path can be either a single text file or a directory storing text files.
        String path = "people.json";
        // Create a DataFrame from the file(s) pointed by path
        DataFrame peopleFromJsonFile = sqlCtx.jsonFile(path);

        // Because the schema of a JSON dataset is automatically inferred, to write queries,
        // it is better to take a look at what is the schema.
        peopleFromJsonFile.printSchema();
        // The schema of people is ...
        // root
        //  |-- age: IntegerType
        //  |-- name: StringType

        // Register this DataFrame as a table.
        peopleFromJsonFile.registerTempTable("people");

        // SQL statements can be run by using the sql methods provided by sqlCtx.
        DataFrame teenagers3 = sqlCtx.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19");

        // The results of SQL queries are DataFrame and support all the normal RDD operations.
        // The columns of a row in the result can be accessed by ordinal.
        teenagerNames = teenagers3.toJavaRDD().map(new Function&lt;Row, String&gt;() {
            @Override
            public String call(Row row) {
                return "Name: " + row.getString(0);
            }
        }).collect();
        for (String name : teenagerNames) {
            System.out.println(name);
        }

        // Alternatively, a DataFrame can be created for a JSON dataset represented by
        // a RDD[String] storing one JSON object per string.
        List&lt;String&gt; jsonData = Arrays.asList(
                "{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}");
        JavaRDD&lt;String&gt; anotherPeopleRDD = ctx.parallelize(jsonData);
        DataFrame peopleFromJsonRDD = sqlCtx.jsonRDD(anotherPeopleRDD.rdd());


        // Take a look at the schema of this new DataFrame.
        peopleFromJsonRDD.printSchema();
        // The schema of anotherPeople is ...
        // root
        //  |-- address: StructType
        //  |    |-- city: StringType
        //  |    |-- state: StringType
        //  |-- name: StringType
        peopleFromJsonRDD.registerTempTable("people2");

        DataFrame peopleWithCity = sqlCtx.sql("SELECT name, address.city FROM people2");
        List&lt;String&gt; nameAndCity = peopleWithCity.toJavaRDD().map(new Function&lt;Row, String&gt;() {
            @Override
            public String call(Row row) {
                return "Name: " + row.getString(0) + ", City: " + row.getString(1);
            }
        }).collect();
        for (String name : nameAndCity) {
            System.out.println(name);
        }

        ctx.stop();
    }
}
</code></pre>

<p>使用 Python 语言则需要用到 sqlContext 的 inferSchema 方法：</p>

<pre><code class="language-python"># sc is an existing SparkContext.
from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a Row.
lines = sc.textFile("people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))

# Infer the schema, and register the DataFrame as a table.
schemaPeople = sqlContext.inferSchema(people)
schemaPeople.registerTempTable("people")

# SQL can be run over DataFrames that have been registered as a table.
teenagers = sqlContext.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19")

# The results of SQL queries are RDDs and support all the normal RDD operations.
teenNames = teenagers.map(lambda p: "Name: " + p.name)
for teenName in teenNames.collect():
  print teenName
</code></pre>

<h3 id="section-1">编程指定模式</h3>

<p>当样本类不能提前确定（例如，记录的结构是经过编码的字符串，或者一个文本集合将会被解析，不同的字段投影给不同的用户），一个 DataFrame 可以通过三步来创建。</p>

<ul>
  <li>从原来的 RDD 创建一个行的 RDD</li>
  <li>创建由一个 StructType 表示的模式与第一步创建的 RDD 的行结构相匹配</li>
  <li>在行 RDD 上通过 applySchema 方法应用模式</li>
</ul>

<p>直接贴出代码，Scala 语言创建方式：</p>

<pre><code class="language-scala">val sc = new SparkContext(new SparkConf().setAppName("ScalaSparkSQL"))
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// Create an RDD
val people = sc.textFile("people.txt")

// The schema is encoded in a string
val schemaString = "name age"

// Import Spark SQL data types and Row.
import org.apache.spark.sql._

// Generate the schema based on the string of schema
val schema =
  StructType(
    schemaString.split(" ").map(fieldName =&gt; StructField(fieldName, StringType, true)))

// Convert records of the RDD (people) to Rows.
val rowRDD = people.map(_.split(",")).map(p =&gt; Row(p(0), p(1).trim))

// Apply the schema to the RDD.
val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)

// Register the DataFrames as a table.
peopleDataFrame.registerTempTable("people")

// SQL statements can be run by using the sql methods provided by sqlContext.
val results = sqlContext.sql("SELECT name FROM people")

// The results of SQL queries are DataFrames and support all the normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
results.map(t =&gt; "Name: " + t(0)).collect().foreach(println)
</code></pre>

<p>Java 创建的方式或许对一个 Java 程序员来说，更容易理解：</p>

<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;

import java.util.List;

public class JavaSparkSQLBySchema {
    public static void main(String[] args) throws Exception {
        SparkConf sparkConf = new SparkConf().setAppName("JavaSparkSQLBySchema");
        JavaSparkContext ctx = new JavaSparkContext(sparkConf);
        SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

        // Load a text file and convert each line to a JavaBean.
        JavaRDD&lt;String&gt; people = sc.textFile("people.txt");

        // The schema is encoded in a string
        String schemaString = "name age";

        // Generate the schema based on the string of schema
        List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;();
        for (String fieldName : schemaString.split(" ")) {
            fields.add(DataType.createStructField(fieldName, DataType.StringType, true));
        }
        StructType schema = DataType.createStructType(fields);

        // Convert records of the RDD (people) to Rows.
        JavaRDD&lt;Row&gt; rowRDD = people.map(
                new Function&lt;String, Row&gt;() {
                    public Row call(String record) throws Exception {
                        String[] fields = record.split(",");
                        return Row.create(fields[0], fields[1].trim());
                    }
                });

        // Apply the schema to the RDD.
        DataFrame peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema);

        // Register the DataFrame as a table.
        peopleDataFrame.registerTempTable("people");

        // SQL can be run over RDDs that have been registered as tables.
        DataFrame results = sqlContext.sql("SELECT name FROM people");

        // The results of SQL queries are DataFrames and support all the normal RDD operations.
        // The columns of a row in the result can be accessed by ordinal.
        List&lt;String&gt; names = results.map(new Function&lt;Row, String&gt;() {
            public String call(Row row) {
                return "Name: " + row.getString(0);
            }
        }).collect();
    }
}

</code></pre>

<p>Python 语言的例子：</p>

<pre><code class="language-python"># Import SQLContext and data types
from pyspark.sql import *

# sc is an existing SparkContext.
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a tuple.
lines = sc.textFile("people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: (p[0], p[1].strip()))

# The schema is encoded in a string.
schemaString = "name age"

fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields)

# Apply the schema to the RDD.
schemaPeople = sqlContext.createDataFrame(people, schema)

# Register the DataFrame as a table.
schemaPeople.registerTempTable("people")

# SQL can be run over DataFrames that have been registered as a table.
results = sqlContext.sql("SELECT name FROM people")

# The results of SQL queries are RDDs and support all the normal RDD operations.
names = results.map(lambda p: "Name: " + p.name)
for name in names.collect():
  print name
</code></pre>

<h1 id="section-2">总结</h1>

<p>本文主要介绍了 DataFrame 是什么以及两种从 RDD 创建 DataFrame 的方法，完整的代码见 <a href="https://github.com/javachen/spark-examples">Github</a>。</p>

<h1 id="section-3">参考文章</h1>

<ul>
  <li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes">Spark SQL and DataFrame Guide</a></li>
  <li><a href="http://endymecy.gitbooks.io/spark-programming-guide-zh-cn/content/spark-sql/README.html">Spark 编程指南简体中文版-Spark SQL</a></li>
</ul>

                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">yuke</a><br/>
                        本文链接地址：<a href="/2015/03/26/spark-sql-dataframe.html">http://blog.javachen.com/2015/03/26/spark-sql-dataframe.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2015/03/26/spark-sql-dataframe.html">Spark SQL中的DataFrame</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#spark">spark</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#spark-sql">spark-sql</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#spark">spark</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2015/03/26/spark-sql-dataframe.html" data-url="http://blog.javachen.com/2015/03/26/spark-sql-dataframe.html" data-title="Spark SQL中的DataFrame"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2015 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">yuke</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
            <script type="text/javascript">
                var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
                document.write(unescape("%3Cspan id='cnzz_stat_icon_1256628929'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1256628929' type='text/javascript'%3E%3C/script%3E"));</script>
            
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2015/03/26/spark-sql-dataframe.html'
      });
      </script>
  </body>
</html>
