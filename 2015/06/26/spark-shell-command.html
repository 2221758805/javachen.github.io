<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>spark-shell脚本分析 - JavaChen Blog</title>
      <meta name="author" content="yuke"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2015/06/26/spark-shell-command.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

      <!-- atom & rss feed -->
      <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
      <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              <li><a href="https://twitter.com/java_chen" target="_blank" title="twitter"><span class="fa fa-twitter fa-2x"></span></a></li>
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2015/06/19/scala-object.html" title="Scala中的对象"><i class="fa fa-angle-double-left"></i>&nbsp;Scala中的对象</a></li>
                
                
                <li class="next"><a href="/2015/06/29/advanced-bash-script-programming.html" title="高级Bash脚本编程入门">高级Bash脚本编程入门&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> spark-shell脚本分析  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2015.06.26 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <p>本文主要分析spark-shell脚本的运行逻辑，涉及到spark-submit、spark-class等脚本的分析，希望通过分析脚本以了解spark中各个进程的参数、JVM参数和内存大小如何设置。</p>

<h1 id="spark-shell">spark-shell</h1>

<p>使用yum安装spark之后，你可以直接在终端运行spark-shell命令，或者在spark的home目录/usr/lib/spark下运行bin/spark-shell命令，这样就可以进入到spark命令行交互模式。</p>

<p><strong>spark-shell 脚本是如何运行的呢</strong>？该脚本代码如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#</span>
<span class="c"># Shell script for starting the Spark Shell REPL</span>

<span class="nv">cygwin</span><span class="o">=</span><span class="nb">false</span>
<span class="k">case</span> <span class="s2">&quot;`uname`&quot;</span> in
  CYGWIN*<span class="o">)</span> <span class="nv">cygwin</span><span class="o">=</span><span class="nb">true</span><span class="p">;;</span>
<span class="k">esac</span>

<span class="c"># Enter posix mode for bash</span>
<span class="nb">set</span> -o posix

<span class="c">## Global script variables</span>
<span class="nv">FWDIR</span><span class="o">=</span><span class="s2">&quot;$(cd &quot;</span><span class="sb">`</span>dirname <span class="s2">&quot;$0&quot;</span><span class="sb">`</span><span class="s2">&quot;/..; pwd)&quot;</span>

<span class="k">function</span> usage<span class="o">()</span> <span class="o">{</span>
  <span class="nb">echo</span> <span class="s2">&quot;Usage: ./bin/spark-shell [options]&quot;</span>
  <span class="s2">&quot;$FWDIR&quot;</span>/bin/spark-submit --help 2&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> grep -v Usage 1&gt;<span class="p">&amp;</span>2
  <span class="nb">exit </span>0
<span class="o">}</span>

<span class="k">if</span> <span class="o">[[</span> <span class="s2">&quot;$@&quot;</span> <span class="o">=</span> *--help <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="s2">&quot;$@&quot;</span> <span class="o">=</span> *-h <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
  usage
<span class="k">fi</span>

<span class="nb">source</span> <span class="s2">&quot;$FWDIR&quot;</span>/bin/utils.sh
<span class="nv">SUBMIT_USAGE_FUNCTION</span><span class="o">=</span>usage
gatherSparkSubmitOpts <span class="s2">&quot;$@&quot;</span>

<span class="c"># SPARK-4161: scala does not assume use of the java classpath,</span>
<span class="c"># so we need to add the &quot;-Dscala.usejavacp=true&quot; flag mnually. We</span>
<span class="c"># do this specifically for the Spark shell because the scala REPL</span>
<span class="c"># has its own class loader, and any additional classpath specified</span>
<span class="c"># through spark.driver.extraClassPath is not automatically propagated.</span>
<span class="nv">SPARK_SUBMIT_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true&quot;</span>

<span class="k">function</span> main<span class="o">()</span> <span class="o">{</span>
  <span class="k">if</span> <span class="nv">$cygwin</span><span class="p">;</span> <span class="k">then</span>
    <span class="c"># Workaround for issue involving JLine and Cygwin</span>
    <span class="c"># (see http://sourceforge.net/p/jline/bugs/40/).</span>
    <span class="c"># If you&#39;re using the Mintty terminal emulator in Cygwin, may need to set the</span>
    <span class="c"># &quot;Backspace sends ^H&quot; setting in &quot;Keys&quot; section of the Mintty options</span>
    <span class="c"># (see https://github.com/sbt/sbt/issues/562).</span>
    stty -icanon min <span class="m">1</span> -echo &gt; /dev/null 2&gt;<span class="p">&amp;</span>1
    <span class="nb">export </span><span class="nv">SPARK_SUBMIT_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_SUBMIT_OPTS -Djline.terminal=unix&quot;</span>
    <span class="s2">&quot;$FWDIR&quot;</span>/bin/spark-submit --class org.apache.spark.repl.Main <span class="s2">&quot;${SUBMISSION_OPTS[@]}&quot;</span> spark-shell <span class="s2">&quot;${APPLICATION_OPTS[@]}&quot;</span>
    stty icanon <span class="nb">echo</span> &gt; /dev/null 2&gt;<span class="p">&amp;</span>1
  <span class="k">else</span>
    <span class="nb">export </span>SPARK_SUBMIT_OPTS
    <span class="s2">&quot;$FWDIR&quot;</span>/bin/spark-submit --class org.apache.spark.repl.Main <span class="s2">&quot;${SUBMISSION_OPTS[@]}&quot;</span> spark-shell <span class="s2">&quot;${APPLICATION_OPTS[@]}&quot;</span>
  <span class="k">fi</span>
<span class="o">}</span>

<span class="c"># Copy restore-TTY-on-exit functions from Scala script so spark-shell exits properly even in</span>
<span class="c"># binary distribution of Spark where Scala is not installed</span>
<span class="nv">exit_status</span><span class="o">=</span>127
<span class="nv">saved_stty</span><span class="o">=</span><span class="s2">&quot;&quot;</span>

<span class="c"># restore stty settings (echo in particular)</span>
<span class="k">function</span> restoreSttySettings<span class="o">()</span> <span class="o">{</span>
  stty <span class="nv">$saved_stty</span>
  <span class="nv">saved_stty</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="o">}</span>

<span class="k">function</span> onExit<span class="o">()</span> <span class="o">{</span>
  <span class="k">if</span> <span class="o">[[</span> <span class="s2">&quot;$saved_stty&quot;</span> !<span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
    restoreSttySettings
  <span class="k">fi</span>
  <span class="nb">exit</span> <span class="nv">$exit_status</span>
<span class="o">}</span>

<span class="c"># to reenable echo if we are interrupted before completing.</span>
<span class="nb">trap </span>onExit INT

<span class="c"># save terminal settings</span>
<span class="nv">saved_stty</span><span class="o">=</span><span class="k">$(</span>stty -g 2&gt;/dev/null<span class="k">)</span>
<span class="c"># clear on error so we don&#39;t later try to restore them</span>
<span class="k">if</span> <span class="o">[[</span> ! <span class="nv">$?</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
  <span class="nv">saved_stty</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="k">fi</span>

main <span class="s2">&quot;$@&quot;</span>

<span class="c"># record the exit status lest it be overwritten:</span>
<span class="c"># then reenable echo and propagate the code.</span>
<span class="nv">exit_status</span><span class="o">=</span><span class="nv">$?</span>
onExit
</code></pre></div>
<p>从上往下一步步分析，首先是判断是否为cygwin，这里用到了bash中的<code class="prettyprint">case</code>语法：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">cygwin</span><span class="o">=</span><span class="nb">false</span>
<span class="k">case</span> <span class="s2">&quot;`uname`&quot;</span> in
  CYGWIN*<span class="o">)</span> <span class="nv">cygwin</span><span class="o">=</span><span class="nb">true</span><span class="p">;;</span>
<span class="k">esac</span>
</code></pre></div>
<blockquote>
<p>在linux系统中，<code class="prettyprint">uname</code>命令的运行结果为linux，其值不等于<code class="prettyprint">CYGWIN*</code>，故cygwin=false。</p>
</blockquote>

<p>开启bash的posix模式：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">set</span> -o posix
</code></pre></div>
<p>获取上级目录绝对路径，这里使用到了<code class="prettyprint">dirname</code>命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">FWDIR</span><span class="o">=</span><span class="s2">&quot;$(cd &quot;</span><span class="sb">`</span>dirname <span class="s2">&quot;$0&quot;</span><span class="sb">`</span><span class="s2">&quot;/..; pwd)&quot;</span>
</code></pre></div>
<blockquote>
<p>提示：bash 中，$0 是获取脚本名称</p>
</blockquote>

<p>判断输入参数中是否有<code class="prettyprint">--help</code>或者<code class="prettyprint">-h</code>，如果有，则打印使用说明，实际上运行的是<code class="prettyprint">/bin/spark-submit --help</code>命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="k">function</span> usage<span class="o">()</span> <span class="o">{</span>
  <span class="nb">echo</span> <span class="s2">&quot;Usage: ./bin/spark-shell [options]&quot;</span>
  <span class="s2">&quot;$FWDIR&quot;</span>/bin/spark-submit --help 2&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> grep -v Usage 1&gt;<span class="p">&amp;</span>2
  <span class="nb">exit </span>0
<span class="o">}</span>

<span class="k">if</span> <span class="o">[[</span> <span class="s2">&quot;$@&quot;</span> <span class="o">=</span> *--help <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="s2">&quot;$@&quot;</span> <span class="o">=</span> *-h <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
  usage
<span class="k">fi</span>
</code></pre></div>
<blockquote>
<p>提示：</p>

<ul>
<li>2&gt;&amp;1 的意思是将标准错误也输出到标准输出当中；1&gt;&amp;2是将标准输出输出到标准错误当中</li>
<li>bash 中，$@ 是获取脚本所有的输入参数</li>
</ul>
</blockquote>

<p>再往后面是定义了一个main方法，并将spark-shell的输入参数传给该方法运行，main方法中判断是否是cygwin模式，如果不是，则运行</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">SPARK_SUBMIT_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true&quot;</span>


<span class="nb">export </span>SPARK_SUBMIT_OPTS
<span class="s2">&quot;$FWDIR&quot;</span>/bin/spark-submit --class org.apache.spark.repl.Main <span class="s2">&quot;${SUBMISSION_OPTS[@]}&quot;</span> spark-shell <span class="s2">&quot;${APPLICATION_OPTS[@]}&quot;</span>
</code></pre></div>
<blockquote>
<p>提示：&quot;${SUBMISSION_OPTS[@]}&quot; 这是什么意思？</p>
</blockquote>

<p>从上面可以看到，其实最后调用的是spark-submit命令，并指定<code class="prettyprint">--class</code>参数为<code class="prettyprint">org.apache.spark.repl.Main</code>类，后面接的是spark-submit的提交参数，再后面是spark-shell，最后是传递应用的参数。</p>

<p>最后，是获取main方法运行结果：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">exit_status</span><span class="o">=</span><span class="nv">$?</span>
onExit
</code></pre></div>
<blockquote>
<p>提示： bash 中，<code class="prettyprint">$?</code>是获取上个命令运行结束返回的状态码</p>
</blockquote>

<p>如果以调试模式运行spark-shell，在不加参数的情况下，输出内容为：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">+ cygwin=false
+ case &quot;`uname`&quot; in
++ uname
+ set -o posix
+++ dirname /usr/lib/spark/bin/spark-shell
++ cd /usr/lib/spark/bin/..
++ pwd
+ FWDIR=/usr/lib/spark
+ [[ &#39;&#39; = *--help ]]
+ [[ &#39;&#39; = *-h ]]
+ source /usr/lib/spark/bin/utils.sh
+ SUBMIT_USAGE_FUNCTION=usage
+ gatherSparkSubmitOpts
+ &#39;[&#39; -z usage &#39;]&#39;
+ SUBMISSION_OPTS=()
+ APPLICATION_OPTS=()
+ (( 0 ))
+ export SUBMISSION_OPTS
+ export APPLICATION_OPTS
+ SPARK_SUBMIT_OPTS=&#39; -Dscala.usejavacp=true&#39;
+ exit_status=127
+ saved_stty=
+ trap onExit INT
++ stty -g
+ saved_stty=500:5:bf:8a3b:3:1c:7f:15:4:0:1:0:11:13:1a:0:12:f:17:16:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0
+ [[ ! -n 0 ]]
+ main
+ false
+ export SPARK_SUBMIT_OPTS
+ /usr/lib/spark/bin/spark-submit --class org.apache.spark.repl.Main spark-shell
</code></pre></div>
<blockquote>
<p>提示：通过运行<code class="prettyprint">set -x</code>可以开启bash调试代码的特性。</p>
</blockquote>

<p>接下来就涉及到spark-submit命令的逻辑了。</p>

<h1 id="spark-submit">spark-submit</h1>

<p>完整的spark-submit脚本内容如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># NOTE: Any changes in this file must be reflected in SparkSubmitDriverBootstrapper.scala!</span>

<span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span><span class="s2">&quot;$(cd &quot;</span><span class="sb">`</span>dirname <span class="s2">&quot;$0&quot;</span><span class="sb">`</span><span class="s2">&quot;/..; pwd)&quot;</span>
<span class="nv">ORIG_ARGS</span><span class="o">=(</span><span class="s2">&quot;$@&quot;</span><span class="o">)</span>

<span class="c"># Set COLUMNS for progress bar</span>
<span class="nb">export </span><span class="nv">COLUMNS</span><span class="o">=</span><span class="sb">`</span>tput cols<span class="sb">`</span>

<span class="k">while</span> <span class="o">((</span><span class="nv">$#)</span><span class="o">)</span><span class="p">;</span> <span class="k">do</span>
  <span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;$1&quot;</span> <span class="o">=</span> <span class="s2">&quot;--deploy-mode&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nv">SPARK_SUBMIT_DEPLOY_MODE</span><span class="o">=</span><span class="nv">$2</span>
  <span class="k">elif</span> <span class="o">[</span> <span class="s2">&quot;$1&quot;</span> <span class="o">=</span> <span class="s2">&quot;--properties-file&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nv">SPARK_SUBMIT_PROPERTIES_FILE</span><span class="o">=</span><span class="nv">$2</span>
  <span class="k">elif</span> <span class="o">[</span> <span class="s2">&quot;$1&quot;</span> <span class="o">=</span> <span class="s2">&quot;--driver-memory&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nb">export </span><span class="nv">SPARK_SUBMIT_DRIVER_MEMORY</span><span class="o">=</span><span class="nv">$2</span>
  <span class="k">elif</span> <span class="o">[</span> <span class="s2">&quot;$1&quot;</span> <span class="o">=</span> <span class="s2">&quot;--driver-library-path&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nb">export </span><span class="nv">SPARK_SUBMIT_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$2</span>
  <span class="k">elif</span> <span class="o">[</span> <span class="s2">&quot;$1&quot;</span> <span class="o">=</span> <span class="s2">&quot;--driver-class-path&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nb">export </span><span class="nv">SPARK_SUBMIT_CLASSPATH</span><span class="o">=</span><span class="nv">$2</span>
  <span class="k">elif</span> <span class="o">[</span> <span class="s2">&quot;$1&quot;</span> <span class="o">=</span> <span class="s2">&quot;--driver-java-options&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nb">export </span><span class="nv">SPARK_SUBMIT_OPTS</span><span class="o">=</span><span class="nv">$2</span>
  <span class="k">elif</span> <span class="o">[</span> <span class="s2">&quot;$1&quot;</span> <span class="o">=</span> <span class="s2">&quot;--master&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nb">export </span><span class="nv">MASTER</span><span class="o">=</span><span class="nv">$2</span>
  <span class="k">fi</span>
  <span class="nb">shift</span>
<span class="k">done</span>

<span class="k">if</span> <span class="o">[</span> -z <span class="s2">&quot;$SPARK_CONF_DIR&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="nb">export </span><span class="nv">SPARK_CONF_DIR</span><span class="o">=</span><span class="s2">&quot;$SPARK_HOME/conf&quot;</span>
<span class="k">fi</span>
<span class="nv">DEFAULT_PROPERTIES_FILE</span><span class="o">=</span><span class="s2">&quot;$SPARK_CONF_DIR/spark-defaults.conf&quot;</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;$MASTER&quot;</span> <span class="o">==</span> <span class="s2">&quot;yarn-cluster&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="nv">SPARK_SUBMIT_DEPLOY_MODE</span><span class="o">=</span>cluster
<span class="k">fi</span>
<span class="nb">export </span><span class="nv">SPARK_SUBMIT_DEPLOY_MODE</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_SUBMIT_DEPLOY_MODE</span><span class="k">:-</span><span class="s2">&quot;client&quot;</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">SPARK_SUBMIT_PROPERTIES_FILE</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_SUBMIT_PROPERTIES_FILE</span><span class="k">:-</span><span class="s2">&quot;$DEFAULT_PROPERTIES_FILE&quot;</span><span class="k">}</span>

<span class="c"># For client mode, the driver will be launched in the same JVM that launches</span>
<span class="c"># SparkSubmit, so we may need to read the properties file for any extra class</span>
<span class="c"># paths, library paths, java options and memory early on. Otherwise, it will</span>
<span class="c"># be too late by the time the driver JVM has started.</span>

<span class="k">if</span> <span class="o">[[</span> <span class="s2">&quot;$SPARK_SUBMIT_DEPLOY_MODE&quot;</span> <span class="o">==</span> <span class="s2">&quot;client&quot;</span> <span class="o">&amp;&amp;</span> -f <span class="s2">&quot;$SPARK_SUBMIT_PROPERTIES_FILE&quot;</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
  <span class="c"># Parse the properties file only if the special configs exist</span>
  <span class="nv">contains_special_configs</span><span class="o">=</span><span class="k">$(</span>
    grep -e <span class="s2">&quot;spark.driver.extra*\|spark.driver.memory&quot;</span> <span class="s2">&quot;$SPARK_SUBMIT_PROPERTIES_FILE&quot;</span> <span class="p">|</span> <span class="se">\</span>
    grep -v <span class="s2">&quot;^[[:space:]]*#&quot;</span>
  <span class="k">)</span>
  <span class="k">if</span> <span class="o">[</span> -n <span class="s2">&quot;$contains_special_configs&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nb">export </span><span class="nv">SPARK_SUBMIT_BOOTSTRAP_DRIVER</span><span class="o">=</span>1
  <span class="k">fi</span>
<span class="k">fi</span>

<span class="nb">exec</span> <span class="s2">&quot;$SPARK_HOME&quot;</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class="s2">&quot;${ORIG_ARGS[@]}&quot;</span>
</code></pre></div>
<p>首先是设置<code class="prettyprint">SPARK_HOME</code>，并保留原始输入参数：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span><span class="s2">&quot;$(cd &quot;</span><span class="sb">`</span>dirname <span class="s2">&quot;$0&quot;</span><span class="sb">`</span><span class="s2">&quot;/..; pwd)&quot;</span>
<span class="nv">ORIG_ARGS</span><span class="o">=(</span><span class="s2">&quot;$@&quot;</span><span class="o">)</span>
</code></pre></div>
<p>接下来，使用while语句配合<code class="prettyprint">shift</code>命令，依次判断输入参数。</p>

<blockquote>
<p>说明：shift是将输入参数位置向左移位</p>
</blockquote>

<p>设置<code class="prettyprint">SPARK_CONF_DIR</code>变量，并判断spark-submit部署模式。</p>

<p>如果<code class="prettyprint">$SPARK_CONF_DIR/spark-defaults.conf</code>文件存在，则检查是否设置<code class="prettyprint">spark.driver.extra</code>开头的和<code class="prettyprint">spark.driver.memory</code>变量，如果设置了，则<code class="prettyprint">SPARK_SUBMIT_BOOTSTRAP_DRIVER</code>设为1。</p>

<p>最后，执行的是spark-class命令，输入参数为<code class="prettyprint">org.apache.spark.deploy.SparkSubmit</code>类名和原始参数。</p>

<h1 id="spark-class">spark-class</h1>

<p>该脚本首先还是判断是否是cygwin，并设置SPARK_HOME和SPARK_CONF_DIR变量。</p>

<p>运行bin/load-spark-env.sh，加载spark环境变量。</p>

<p>spark-class至少需要传递一个参数，如果没有，则会打印脚本使用说明<code class="prettyprint">Usage: spark-class &lt;class&gt; [&lt;args&gt;]</code>。</p>

<p>如果设置了<code class="prettyprint">SPARK_MEM</code>变量，则提示<code class="prettyprint">SPARK_MEM</code>变量过时，应该使用<code class="prettyprint">spark.executor.memory</code>或者<code class="prettyprint">spark.driver.memory</code>变量。</p>

<p>设置默认内存<code class="prettyprint">DEFAULT_MEM</code>为512M，如果<code class="prettyprint">SPARK_MEM</code>变量存在，则使用<code class="prettyprint">SPARK_MEM</code>的值。</p>

<p>使用case语句判断spark-class传入的第一个参数的值：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">SPARK_DAEMON_JAVA_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_DAEMON_JAVA_OPTS -Dspark.akka.logLifecycleEvents=true&quot;</span>

<span class="c"># Add java opts and memory settings for master, worker, history server, executors, and repl.</span>
<span class="k">case</span> <span class="s2">&quot;$1&quot;</span> in
  <span class="c"># Master, Worker, and HistoryServer use SPARK_DAEMON_JAVA_OPTS (and specific opts) + SPARK_DAEMON_MEMORY.</span>
  <span class="s1">&#39;org.apache.spark.deploy.master.Master&#39;</span><span class="o">)</span>
    <span class="nv">OUR_JAVA_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_DAEMON_JAVA_OPTS $SPARK_MASTER_OPTS&quot;</span>
    <span class="nv">OUR_JAVA_MEM</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_DAEMON_MEMORY</span><span class="k">:-</span><span class="nv">$DEFAULT_MEM</span><span class="k">}</span>
    <span class="p">;;</span>
  <span class="s1">&#39;org.apache.spark.deploy.worker.Worker&#39;</span><span class="o">)</span>
    <span class="nv">OUR_JAVA_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_DAEMON_JAVA_OPTS $SPARK_WORKER_OPTS&quot;</span>
    <span class="nv">OUR_JAVA_MEM</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_DAEMON_MEMORY</span><span class="k">:-</span><span class="nv">$DEFAULT_MEM</span><span class="k">}</span>
    <span class="p">;;</span>
  <span class="s1">&#39;org.apache.spark.deploy.history.HistoryServer&#39;</span><span class="o">)</span>
    <span class="nv">OUR_JAVA_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_DAEMON_JAVA_OPTS $SPARK_HISTORY_OPTS&quot;</span>
    <span class="nv">OUR_JAVA_MEM</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_DAEMON_MEMORY</span><span class="k">:-</span><span class="nv">$DEFAULT_MEM</span><span class="k">}</span>
    <span class="p">;;</span>

  <span class="c"># Executors use SPARK_JAVA_OPTS + SPARK_EXECUTOR_MEMORY.</span>
  <span class="s1">&#39;org.apache.spark.executor.CoarseGrainedExecutorBackend&#39;</span><span class="o">)</span>
    <span class="nv">OUR_JAVA_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS&quot;</span>
    <span class="nv">OUR_JAVA_MEM</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_EXECUTOR_MEMORY</span><span class="k">:-</span><span class="nv">$DEFAULT_MEM</span><span class="k">}</span>
    <span class="p">;;</span>
  <span class="s1">&#39;org.apache.spark.executor.MesosExecutorBackend&#39;</span><span class="o">)</span>
    <span class="nv">OUR_JAVA_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS&quot;</span>
    <span class="nv">OUR_JAVA_MEM</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_EXECUTOR_MEMORY</span><span class="k">:-</span><span class="nv">$DEFAULT_MEM</span><span class="k">}</span>
    <span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="s2">&quot;$FWDIR/python:$PYTHONPATH&quot;</span>
    <span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="s2">&quot;$FWDIR/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH&quot;</span>
    <span class="p">;;</span>

  <span class="c"># Spark submit uses SPARK_JAVA_OPTS + SPARK_SUBMIT_OPTS +</span>
  <span class="c"># SPARK_DRIVER_MEMORY + SPARK_SUBMIT_DRIVER_MEMORY.</span>
  <span class="s1">&#39;org.apache.spark.deploy.SparkSubmit&#39;</span><span class="o">)</span>
    <span class="nv">OUR_JAVA_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_JAVA_OPTS $SPARK_SUBMIT_OPTS&quot;</span>
    <span class="nv">OUR_JAVA_MEM</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_DRIVER_MEMORY</span><span class="k">:-</span><span class="nv">$DEFAULT_MEM</span><span class="k">}</span>
    <span class="k">if</span> <span class="o">[</span> -n <span class="s2">&quot;$SPARK_SUBMIT_LIBRARY_PATH&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
      <span class="k">if</span> <span class="o">[[</span> <span class="nv">$OSTYPE</span> <span class="o">==</span> darwin* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
       <span class="nb">export </span><span class="nv">DYLD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;$SPARK_SUBMIT_LIBRARY_PATH:$DYLD_LIBRARY_PATH&quot;</span>
      <span class="k">else</span>
       <span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;$SPARK_SUBMIT_LIBRARY_PATH:$LD_LIBRARY_PATH&quot;</span>
      <span class="k">fi</span>
    <span class="k">fi</span>
    <span class="k">if</span> <span class="o">[</span> -n <span class="s2">&quot;$SPARK_SUBMIT_DRIVER_MEMORY&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
      <span class="nv">OUR_JAVA_MEM</span><span class="o">=</span><span class="s2">&quot;$SPARK_SUBMIT_DRIVER_MEMORY&quot;</span>
    <span class="k">fi</span>
    <span class="p">;;</span>

  *<span class="o">)</span>
    <span class="nv">OUR_JAVA_OPTS</span><span class="o">=</span><span class="s2">&quot;$SPARK_JAVA_OPTS&quot;</span>
    <span class="nv">OUR_JAVA_MEM</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_DRIVER_MEMORY</span><span class="k">:-</span><span class="nv">$DEFAULT_MEM</span><span class="k">}</span>
    <span class="p">;;</span>
<span class="k">esac</span>
</code></pre></div>
<p>可能存在以下几种情况：</p>

<ul>
<li><code class="prettyprint">org.apache.spark.deploy.master.Master</code></li>
<li><code class="prettyprint">org.apache.spark.deploy.worker.Worker</code></li>
<li><code class="prettyprint">org.apache.spark.deploy.history.HistoryServer</code></li>
<li><code class="prettyprint">org.apache.spark.executor.CoarseGrainedExecutorBackend</code></li>
<li><code class="prettyprint">org.apache.spark.executor.MesosExecutorBackend</code></li>
<li><code class="prettyprint">org.apache.spark.deploy.SparkSubmit</code></li>
</ul>

<p>并分别设置每种情况下的Java运行参数和使用内存大小，以表格形式表示如下：</p>

<table><thead>
<tr>
<th style="text-align: left"></th>
<th style="text-align: left">OUR_JAVA_OPTS</th>
<th style="text-align: left">OUR_JAVA_MEM</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">Master</td>
<td style="text-align: left">$SPARK_DAEMON_JAVA_OPTS $SPARK_MASTER_OPTS</td>
<td style="text-align: left">${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}</td>
</tr>
<tr>
<td style="text-align: left">Worker</td>
<td style="text-align: left">$SPARK_DAEMON_JAVA_OPTS $SPARK_WORKER_OPTS</td>
<td style="text-align: left">${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}</td>
</tr>
<tr>
<td style="text-align: left">HistoryServer</td>
<td style="text-align: left">$SPARK_DAEMON_JAVA_OPTS $SPARK_HISTORY_OPTS</td>
<td style="text-align: left">${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}</td>
</tr>
<tr>
<td style="text-align: left">CoarseGrainedExecutorBackend</td>
<td style="text-align: left">$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS</td>
<td style="text-align: left">${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}</td>
</tr>
<tr>
<td style="text-align: left">MesosExecutorBackend</td>
<td style="text-align: left">$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS</td>
<td style="text-align: left">${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}</td>
</tr>
<tr>
<td style="text-align: left">SparkSubmit</td>
<td style="text-align: left">$SPARK_JAVA_OPTS $SPARK_SUBMIT_OPTS</td>
<td style="text-align: left">${SPARK_DRIVER_MEMORY:-$DEFAULT_MEM}</td>
</tr>
</tbody></table>

<p>通过上表就可以知道每一个spark中每个进程如何设置JVM参数和内存大小。</p>

<p>接下来是查找JAVA_HOME并检查Java版本。</p>

<p>设置SPARK_TOOLS_JAR变量。</p>

<p>运行bin/compute-classpath.sh计算classpath。</p>

<p>判断<code class="prettyprint">SPARK_SUBMIT_BOOTSTRAP_DRIVER</code>变量值，如果该值为1，则运行<code class="prettyprint">org.apache.spark.deploy.SparkSubmitDriverBootstrapper</code>类，以替换原来的<code class="prettyprint">org.apache.spark.deploy.SparkSubmit</code>的类，执行的脚本为<code class="prettyprint">exec &quot;$RUNNER&quot; org.apache.spark.deploy.SparkSubmitDriverBootstrapper &quot;$@&quot;</code>；否则，运行java命令<code class="prettyprint">exec &quot;$RUNNER&quot; -cp &quot;$CLASSPATH&quot; $JAVA_OPTS &quot;$@&quot;</code>。</p>

<p>从最后运行的脚本可以看到，spark-class脚本的作用主要是查找java命令、计算环境变量、设置<code class="prettyprint">JAVA_OPTS</code>等，至于运行的是哪个java类的main方法，取决于<code class="prettyprint">SPARK_SUBMIT_BOOTSTRAP_DRIVER</code>变量的值。</p>

<p>接下来，就是要分析<code class="prettyprint">org.apache.spark.deploy.SparkSubmitDriverBootstrapper</code>和<code class="prettyprint">org.apache.spark.deploy.SparkSubmit</code>类的运行逻辑以及两者之间的区别，这部分内容见下篇文章。</p>

                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">yuke</a><br/>
                        本文链接地址：<a href="/2015/06/26/spark-shell-command.html">http://blog.javachen.com/2015/06/26/spark-shell-command.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2015/06/26/spark-shell-command.html">spark-shell脚本分析</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#spark">spark</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#spark">spark</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2015/06/26/spark-shell-command.html" data-url="http://blog.javachen.com/2015/06/26/spark-shell-command.html" data-title="spark-shell脚本分析"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2015 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">yuke</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
            <script type="text/javascript">
                var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
                document.write(unescape("%3Cspan id='cnzz_stat_icon_1256628929'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1256628929' type='text/javascript'%3E%3C/script%3E"));</script>
            
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2015/06/26/spark-shell-command.html'
      });
      </script>
  </body>
</html>
