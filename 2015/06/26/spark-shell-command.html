<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>spark-shell脚本分析 - JavaChen Blog</title>
      <meta name="author" content="Junez"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2015/06/26/spark-shell-command.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

      <!-- atom & rss feed -->
      <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
      <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              <li><a href="https://twitter.com/junezchen" target="_blank" title="twitter"><span class="fa fa-twitter fa-2x"></span></a></li>
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2015/06/19/scala-object.html" title="Scala中的对象"><i class="fa fa-angle-double-left"></i>&nbsp;Scala中的对象</a></li>
                
                
                <li class="next"><a href="/2015/06/29/advanced-bash-script-programming.html" title="高级Bash脚本编程入门">高级Bash脚本编程入门&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> spark-shell脚本分析  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2015.06.26 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <p>本文主要分析spark-shell脚本的运行逻辑，涉及到spark-submit、spark-class等脚本的分析，希望通过分析脚本以了解spark中各个进程的参数、JVM参数和内存大小如何设置。</p>

<h1 id="spark-shell">spark-shell</h1>

<p>使用yum安装spark之后，你可以直接在终端运行spark-shell命令，或者在spark的home目录/usr/lib/spark下运行bin/spark-shell命令，这样就可以进入到spark命令行交互模式。</p>

<p><strong>spark-shell 脚本是如何运行的呢</strong>？该脚本代码如下：</p>

<pre><code class="language-bash">#
# Shell script for starting the Spark Shell REPL

cygwin=false
case "`uname`" in
  CYGWIN*) cygwin=true;;
esac

# Enter posix mode for bash
set -o posix

## Global script variables
FWDIR="$(cd "`dirname "$0"`"/..; pwd)"

function usage() {
  echo "Usage: ./bin/spark-shell [options]"
  "$FWDIR"/bin/spark-submit --help 2&gt;&amp;1 | grep -v Usage 1&gt;&amp;2
  exit 0
}

if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then
  usage
fi

source "$FWDIR"/bin/utils.sh
SUBMIT_USAGE_FUNCTION=usage
gatherSparkSubmitOpts "$@"

# SPARK-4161: scala does not assume use of the java classpath,
# so we need to add the "-Dscala.usejavacp=true" flag mnually. We
# do this specifically for the Spark shell because the scala REPL
# has its own class loader, and any additional classpath specified
# through spark.driver.extraClassPath is not automatically propagated.
SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"

function main() {
  if $cygwin; then
    # Workaround for issue involving JLine and Cygwin
    # (see http://sourceforge.net/p/jline/bugs/40/).
    # If you're using the Mintty terminal emulator in Cygwin, may need to set the
    # "Backspace sends ^H" setting in "Keys" section of the Mintty options
    # (see https://github.com/sbt/sbt/issues/562).
    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1
    export SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Djline.terminal=unix"
    "$FWDIR"/bin/spark-submit --class org.apache.spark.repl.Main "${SUBMISSION_OPTS[@]}" spark-shell "${APPLICATION_OPTS[@]}"
    stty icanon echo &gt; /dev/null 2&gt;&amp;1
  else
    export SPARK_SUBMIT_OPTS
    "$FWDIR"/bin/spark-submit --class org.apache.spark.repl.Main "${SUBMISSION_OPTS[@]}" spark-shell "${APPLICATION_OPTS[@]}"
  fi
}

# Copy restore-TTY-on-exit functions from Scala script so spark-shell exits properly even in
# binary distribution of Spark where Scala is not installed
exit_status=127
saved_stty=""

# restore stty settings (echo in particular)
function restoreSttySettings() {
  stty $saved_stty
  saved_stty=""
}

function onExit() {
  if [[ "$saved_stty" != "" ]]; then
    restoreSttySettings
  fi
  exit $exit_status
}

# to reenable echo if we are interrupted before completing.
trap onExit INT

# save terminal settings
saved_stty=$(stty -g 2&gt;/dev/null)
# clear on error so we don't later try to restore them
if [[ ! $? ]]; then
  saved_stty=""
fi

main "$@"

# record the exit status lest it be overwritten:
# then reenable echo and propagate the code.
exit_status=$?
onExit
</code></pre>

<p>从上往下一步步分析，首先是判断是否为cygwin，这里用到了bash中的<code>case</code>语法：</p>

<pre><code class="language-bash">cygwin=false
case "`uname`" in
  CYGWIN*) cygwin=true;;
esac
</code></pre>

<blockquote>
  <p>在linux系统中，<code>uname</code>命令的运行结果为linux，其值不等于<code>CYGWIN*</code>，故cygwin=false。</p>
</blockquote>

<p>开启bash的posix模式：</p>

<pre><code class="language-bash">set -o posix
</code></pre>

<p>获取上级目录绝对路径，这里使用到了<code>dirname</code>命令：</p>

<pre><code class="language-bash">FWDIR="$(cd "`dirname "$0"`"/..; pwd)"
</code></pre>

<blockquote>
  <p>提示：bash 中，$0 是获取脚本名称</p>
</blockquote>

<p>判断输入参数中是否有<code>--help</code>或者<code>-h</code>，如果有，则打印使用说明，实际上运行的是<code>/bin/spark-submit --help</code>命令：</p>

<pre><code class="language-bash">function usage() {
  echo "Usage: ./bin/spark-shell [options]"
  "$FWDIR"/bin/spark-submit --help 2&gt;&amp;1 | grep -v Usage 1&gt;&amp;2
  exit 0
}

if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then
  usage
fi
</code></pre>

<blockquote>
  <p>提示：</p>

  <ul>
    <li>2&gt;&amp;1 的意思是将标准错误也输出到标准输出当中；1&gt;&amp;2是将标准输出输出到标准错误当中</li>
    <li>bash 中，$@ 是获取脚本所有的输入参数</li>
  </ul>
</blockquote>

<p>再往后面是定义了一个main方法，并将spark-shell的输入参数传给该方法运行，main方法中判断是否是cygwin模式，如果不是，则运行</p>

<pre><code class="language-bash">SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"


export SPARK_SUBMIT_OPTS
"$FWDIR"/bin/spark-submit --class org.apache.spark.repl.Main "${SUBMISSION_OPTS[@]}" spark-shell "${APPLICATION_OPTS[@]}"
</code></pre>

<blockquote>
  <p>提示：”${SUBMISSION_OPTS[@]}” 这是什么意思？</p>
</blockquote>

<p>从上面可以看到，其实最后调用的是spark-submit命令，并指定<code>--class</code>参数为<code>org.apache.spark.repl.Main</code>类，后面接的是spark-submit的提交参数，再后面是spark-shell，最后是传递应用的参数。</p>

<p>最后，是获取main方法运行结果：</p>

<pre><code class="language-bash">exit_status=$?
onExit
</code></pre>

<blockquote>
  <p>提示： bash 中，<code>$?</code>是获取上个命令运行结束返回的状态码</p>
</blockquote>

<p>如果以调试模式运行spark-shell，在不加参数的情况下，输出内容为：</p>

<pre><code>+ cygwin=false
+ case "`uname`" in
++ uname
+ set -o posix
+++ dirname /usr/lib/spark/bin/spark-shell
++ cd /usr/lib/spark/bin/..
++ pwd
+ FWDIR=/usr/lib/spark
+ [[ '' = *--help ]]
+ [[ '' = *-h ]]
+ source /usr/lib/spark/bin/utils.sh
+ SUBMIT_USAGE_FUNCTION=usage
+ gatherSparkSubmitOpts
+ '[' -z usage ']'
+ SUBMISSION_OPTS=()
+ APPLICATION_OPTS=()
+ (( 0 ))
+ export SUBMISSION_OPTS
+ export APPLICATION_OPTS
+ SPARK_SUBMIT_OPTS=' -Dscala.usejavacp=true'
+ exit_status=127
+ saved_stty=
+ trap onExit INT
++ stty -g
+ saved_stty=500:5:bf:8a3b:3:1c:7f:15:4:0:1:0:11:13:1a:0:12:f:17:16:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0
+ [[ ! -n 0 ]]
+ main
+ false
+ export SPARK_SUBMIT_OPTS
+ /usr/lib/spark/bin/spark-submit --class org.apache.spark.repl.Main spark-shell
</code></pre>

<blockquote>
  <p>提示：通过运行<code>set -x</code>可以开启bash调试代码的特性。</p>
</blockquote>

<p>接下来就涉及到spark-submit命令的逻辑了。</p>

<h1 id="spark-submit">spark-submit</h1>

<p>完整的spark-submit脚本内容如下：</p>

<pre><code class="language-bash"># NOTE: Any changes in this file must be reflected in SparkSubmitDriverBootstrapper.scala!

export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
ORIG_ARGS=("$@")

# Set COLUMNS for progress bar
export COLUMNS=`tput cols`

while (($#)); do
  if [ "$1" = "--deploy-mode" ]; then
    SPARK_SUBMIT_DEPLOY_MODE=$2
  elif [ "$1" = "--properties-file" ]; then
    SPARK_SUBMIT_PROPERTIES_FILE=$2
  elif [ "$1" = "--driver-memory" ]; then
    export SPARK_SUBMIT_DRIVER_MEMORY=$2
  elif [ "$1" = "--driver-library-path" ]; then
    export SPARK_SUBMIT_LIBRARY_PATH=$2
  elif [ "$1" = "--driver-class-path" ]; then
    export SPARK_SUBMIT_CLASSPATH=$2
  elif [ "$1" = "--driver-java-options" ]; then
    export SPARK_SUBMIT_OPTS=$2
  elif [ "$1" = "--master" ]; then
    export MASTER=$2
  fi
  shift
done

if [ -z "$SPARK_CONF_DIR" ]; then
  export SPARK_CONF_DIR="$SPARK_HOME/conf"
fi
DEFAULT_PROPERTIES_FILE="$SPARK_CONF_DIR/spark-defaults.conf"
if [ "$MASTER" == "yarn-cluster" ]; then
  SPARK_SUBMIT_DEPLOY_MODE=cluster
fi
export SPARK_SUBMIT_DEPLOY_MODE=${SPARK_SUBMIT_DEPLOY_MODE:-"client"}
export SPARK_SUBMIT_PROPERTIES_FILE=${SPARK_SUBMIT_PROPERTIES_FILE:-"$DEFAULT_PROPERTIES_FILE"}

# For client mode, the driver will be launched in the same JVM that launches
# SparkSubmit, so we may need to read the properties file for any extra class
# paths, library paths, java options and memory early on. Otherwise, it will
# be too late by the time the driver JVM has started.

if [[ "$SPARK_SUBMIT_DEPLOY_MODE" == "client" &amp;&amp; -f "$SPARK_SUBMIT_PROPERTIES_FILE" ]]; then
  # Parse the properties file only if the special configs exist
  contains_special_configs=$(
    grep -e "spark.driver.extra*\|spark.driver.memory" "$SPARK_SUBMIT_PROPERTIES_FILE" | \
    grep -v "^[[:space:]]*#"
  )
  if [ -n "$contains_special_configs" ]; then
    export SPARK_SUBMIT_BOOTSTRAP_DRIVER=1
  fi
fi

exec "$SPARK_HOME"/bin/spark-class org.apache.spark.deploy.SparkSubmit "${ORIG_ARGS[@]}"
</code></pre>

<p>首先是设置<code>SPARK_HOME</code>，并保留原始输入参数：</p>

<pre><code class="language-bash">export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
ORIG_ARGS=("$@")
</code></pre>

<p>接下来，使用while语句配合<code>shift</code>命令，依次判断输入参数。</p>

<blockquote>
  <p>说明：shift是将输入参数位置向左移位</p>
</blockquote>

<p>设置<code>SPARK_CONF_DIR</code>变量，并判断spark-submit部署模式。</p>

<p>如果<code>$SPARK_CONF_DIR/spark-defaults.conf</code>文件存在，则检查是否设置<code>spark.driver.extra</code>开头的和<code>spark.driver.memory</code>变量，如果设置了，则<code>SPARK_SUBMIT_BOOTSTRAP_DRIVER</code>设为1。</p>

<p>最后，执行的是spark-class命令，输入参数为<code>org.apache.spark.deploy.SparkSubmit</code>类名和原始参数。</p>

<h1 id="spark-class">spark-class</h1>

<p>该脚本首先还是判断是否是cygwin，并设置SPARK_HOME和SPARK_CONF_DIR变量。</p>

<p>运行bin/load-spark-env.sh，加载spark环境变量。</p>

<p>spark-class至少需要传递一个参数，如果没有，则会打印脚本使用说明<code>Usage: spark-class &lt;class&gt; [&lt;args&gt;]</code>。</p>

<p>如果设置了<code>SPARK_MEM</code>变量，则提示<code>SPARK_MEM</code>变量过时，应该使用<code>spark.executor.memory</code>或者<code>spark.driver.memory</code>变量。</p>

<p>设置默认内存<code>DEFAULT_MEM</code>为512M，如果<code>SPARK_MEM</code>变量存在，则使用<code>SPARK_MEM</code>的值。</p>

<p>使用case语句判断spark-class传入的第一个参数的值：</p>

<pre><code class="language-bash">SPARK_DAEMON_JAVA_OPTS="$SPARK_DAEMON_JAVA_OPTS -Dspark.akka.logLifecycleEvents=true"

# Add java opts and memory settings for master, worker, history server, executors, and repl.
case "$1" in
  # Master, Worker, and HistoryServer use SPARK_DAEMON_JAVA_OPTS (and specific opts) + SPARK_DAEMON_MEMORY.
  'org.apache.spark.deploy.master.Master')
    OUR_JAVA_OPTS="$SPARK_DAEMON_JAVA_OPTS $SPARK_MASTER_OPTS"
    OUR_JAVA_MEM=${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}
    ;;
  'org.apache.spark.deploy.worker.Worker')
    OUR_JAVA_OPTS="$SPARK_DAEMON_JAVA_OPTS $SPARK_WORKER_OPTS"
    OUR_JAVA_MEM=${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}
    ;;
  'org.apache.spark.deploy.history.HistoryServer')
    OUR_JAVA_OPTS="$SPARK_DAEMON_JAVA_OPTS $SPARK_HISTORY_OPTS"
    OUR_JAVA_MEM=${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}
    ;;

  # Executors use SPARK_JAVA_OPTS + SPARK_EXECUTOR_MEMORY.
  'org.apache.spark.executor.CoarseGrainedExecutorBackend')
    OUR_JAVA_OPTS="$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS"
    OUR_JAVA_MEM=${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}
    ;;
  'org.apache.spark.executor.MesosExecutorBackend')
    OUR_JAVA_OPTS="$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS"
    OUR_JAVA_MEM=${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}
    export PYTHONPATH="$FWDIR/python:$PYTHONPATH"
    export PYTHONPATH="$FWDIR/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH"
    ;;

  # Spark submit uses SPARK_JAVA_OPTS + SPARK_SUBMIT_OPTS +
  # SPARK_DRIVER_MEMORY + SPARK_SUBMIT_DRIVER_MEMORY.
  'org.apache.spark.deploy.SparkSubmit')
    OUR_JAVA_OPTS="$SPARK_JAVA_OPTS $SPARK_SUBMIT_OPTS"
    OUR_JAVA_MEM=${SPARK_DRIVER_MEMORY:-$DEFAULT_MEM}
    if [ -n "$SPARK_SUBMIT_LIBRARY_PATH" ]; then
      if [[ $OSTYPE == darwin* ]]; then
       export DYLD_LIBRARY_PATH="$SPARK_SUBMIT_LIBRARY_PATH:$DYLD_LIBRARY_PATH"
      else
       export LD_LIBRARY_PATH="$SPARK_SUBMIT_LIBRARY_PATH:$LD_LIBRARY_PATH"
      fi
    fi
    if [ -n "$SPARK_SUBMIT_DRIVER_MEMORY" ]; then
      OUR_JAVA_MEM="$SPARK_SUBMIT_DRIVER_MEMORY"
    fi
    ;;

  *)
    OUR_JAVA_OPTS="$SPARK_JAVA_OPTS"
    OUR_JAVA_MEM=${SPARK_DRIVER_MEMORY:-$DEFAULT_MEM}
    ;;
esac
</code></pre>

<p>可能存在以下几种情况：</p>

<ul>
  <li><code>org.apache.spark.deploy.master.Master</code></li>
  <li><code>org.apache.spark.deploy.worker.Worker</code></li>
  <li><code>org.apache.spark.deploy.history.HistoryServer</code></li>
  <li><code>org.apache.spark.executor.CoarseGrainedExecutorBackend</code></li>
  <li><code>org.apache.spark.executor.MesosExecutorBackend</code></li>
  <li><code>org.apache.spark.deploy.SparkSubmit</code></li>
</ul>

<p>并分别设置每种情况下的Java运行参数和使用内存大小，以表格形式表示如下：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: left">OUR_JAVA_OPTS</th>
      <th style="text-align: left">OUR_JAVA_MEM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Master</td>
      <td style="text-align: left">$SPARK_DAEMON_JAVA_OPTS $SPARK_MASTER_OPTS</td>
      <td style="text-align: left">${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}</td>
    </tr>
    <tr>
      <td style="text-align: left">Worker</td>
      <td style="text-align: left">$SPARK_DAEMON_JAVA_OPTS $SPARK_WORKER_OPTS</td>
      <td style="text-align: left">${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}</td>
    </tr>
    <tr>
      <td style="text-align: left">HistoryServer</td>
      <td style="text-align: left">$SPARK_DAEMON_JAVA_OPTS $SPARK_HISTORY_OPTS</td>
      <td style="text-align: left">${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM}</td>
    </tr>
    <tr>
      <td style="text-align: left">CoarseGrainedExecutorBackend</td>
      <td style="text-align: left">$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS</td>
      <td style="text-align: left">${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}</td>
    </tr>
    <tr>
      <td style="text-align: left">MesosExecutorBackend</td>
      <td style="text-align: left">$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS</td>
      <td style="text-align: left">${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM}</td>
    </tr>
    <tr>
      <td style="text-align: left">SparkSubmit</td>
      <td style="text-align: left">$SPARK_JAVA_OPTS $SPARK_SUBMIT_OPTS</td>
      <td style="text-align: left">${SPARK_DRIVER_MEMORY:-$DEFAULT_MEM}</td>
    </tr>
  </tbody>
</table>

<p>通过上表就可以知道每一个spark中每个进程如何设置JVM参数和内存大小。</p>

<p>接下来是查找JAVA_HOME并检查Java版本。</p>

<p>设置SPARK_TOOLS_JAR变量。</p>

<p>运行bin/compute-classpath.sh计算classpath。</p>

<p>判断<code>SPARK_SUBMIT_BOOTSTRAP_DRIVER</code>变量值，如果该值为1，则运行<code>org.apache.spark.deploy.SparkSubmitDriverBootstrapper</code>类，以替换原来的<code>org.apache.spark.deploy.SparkSubmit</code>的类，执行的脚本为<code>exec "$RUNNER" org.apache.spark.deploy.SparkSubmitDriverBootstrapper "$@"</code>；否则，运行java命令<code>exec "$RUNNER" -cp "$CLASSPATH" $JAVA_OPTS "$@"</code>。</p>

<p>从最后运行的脚本可以看到，spark-class脚本的作用主要是查找java命令、计算环境变量、设置<code>JAVA_OPTS</code>等，至于运行的是哪个java类的main方法，取决于<code>SPARK_SUBMIT_BOOTSTRAP_DRIVER</code>变量的值。</p>

<p>接下来，就是要分析<code>org.apache.spark.deploy.SparkSubmitDriverBootstrapper</code>和<code>org.apache.spark.deploy.SparkSubmit</code>类的运行逻辑以及两者之间的区别，这部分内容见下篇文章。</p>


                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">Junez</a><br/>
                        本文链接地址：<a href="/2015/06/26/spark-shell-command.html">http://blog.javachen.com/2015/06/26/spark-shell-command.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2015/06/26/spark-shell-command.html">spark-shell脚本分析</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#spark">spark</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#spark">spark</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2015/06/26/spark-shell-command.html" data-url="http://blog.javachen.com/2015/06/26/spark-shell-command.html" data-title="spark-shell脚本分析"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2015 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">Junez</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
            <script type="text/javascript">
                var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
                document.write(unescape("%3Cspan id='cnzz_stat_icon_1256628929'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1256628929' type='text/javascript'%3E%3C/script%3E"));</script>
            
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2015/06/26/spark-shell-command.html'
      });
      </script>
  </body>
</html>
