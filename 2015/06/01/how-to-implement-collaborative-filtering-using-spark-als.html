<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>如何使用Spark ALS实现协同过滤 - JavaChen Blog</title>
      <meta name="author" content="yuke"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

      <!-- atom & rss feed -->
      <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
      <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              <li><a href="https://twitter.com/java_chen" target="_blank" title="twitter"><span class="fa fa-twitter fa-2x"></span></a></li>
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2015/04/30/test-hive-with-sentry.html" title="测试Hive集成Sentry"><i class="fa fa-angle-double-left"></i>&nbsp;测试Hive集成Sentry</a></li>
                
                
                <li class="next"><a href="/2015/06/05/yarn-memory-and-cpu-configuration.html" title="YARN的内存和CPU配置">YARN的内存和CPU配置&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> 如何使用Spark ALS实现协同过滤  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2015.06.01 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <p>本文主要记录最近一段时间学习和实现<a href="/2015/04/17/spark-mllib-collaborative-filtering.html">Spark MLlib中的协同过滤</a>的一些总结，希望对大家熟悉Spark ALS算法有所帮助。</p>

<blockquote>
  <p>更新：</p>

  <ol>
    <li>【2016.06.12】Spark1.4.0中MatrixFactorizationModel提供了recommendForAll方法实现离线批量推荐，见<a href="https://issues.apache.org/jira/browse/SPARK-3066">SPARK-3066</a>。</li>
  </ol>
</blockquote>

<h1 id="section">测试环境</h1>

<p>为了测试简单，在本地以local方式运行Spark，你需要做的是下载编译好的压缩包解压即可，可以参考<a href="/2015/03/30/spark-test-in-local-mode.html">Spark本地模式运行</a>。</p>

<p>测试数据使用<a href="http://grouplens.org/datasets/movielens/">MovieLens</a>的<a href="http://files.grouplens.org/datasets/movielens/ml-10m.zip">MovieLens 10M数据集</a>，下载之后解压到data目录。数据的格式请参考README中的说明，需要注意的是ratings.dat中的数据被处理过，<code>每个用户至少访问了20个商品</code>。</p>

<p>下面的代码均在spark-shell中运行，启动时候可以根据你的机器内存设置JVM参数，例如：</p>

<pre><code class="language-bash">bin/spark-shell --executor-memory 3g --driver-memory 3g --driver-java-options '-Xms2g -Xmx2g -XX:+UseCompressedOops'
</code></pre>

<h1 id="section-1">预测评分</h1>

<p>这个例子主要演示如何训练数据、评分并计算根均方差。</p>

<h2 id="section-2">准备工作</h2>

<p>首先，启动spark-shell，然后引入mllib包，我们需要用到ALS算法类和Rating评分类：</p>

<pre><code class="language-scala">import org.apache.spark.mllib.recommendation.{ALS, Rating}
</code></pre>

<p>Spark的日志级别默认为INFO，你可以手动设置为WARN级别，同样先引入log4j依赖：</p>

<pre><code class="language-scala">import org.apache.log4j.{Logger,Level}
</code></pre>

<p>然后，运行下面代码：</p>

<pre><code class="language-scala">Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)
</code></pre>

<h2 id="section-3">加载数据</h2>

<p>spark-shell启动成功之后，sc为内置变量，你可以通过它来加载测试数据：</p>

<pre><code class="language-scala">val data = sc.textFile("data/ml-1m/ratings.dat")
</code></pre>

<p>接下来解析文件内容，获得用户对商品的评分记录：</p>

<pre><code class="language-scala">val ratings = data.map(_.split("::") match { case Array(user, item, rate, ts) =&gt;
  Rating(user.toInt, item.toInt, rate.toDouble)
}).cache()
</code></pre>

<p>查看第一条记录：</p>

<pre><code class="language-scala">scala&gt; ratings.first
res81: org.apache.spark.mllib.recommendation.Rating = Rating(1,1193,5.0)
</code></pre>

<p>我们可以统计文件中用户和商品数量：</p>

<pre><code class="language-scala">val users = ratings.map(_.user).distinct()
val products = ratings.map(_.product).distinct()
println("Got "+ratings.count()+" ratings from "+users.count+" users on "+products.count+" products.")
</code></pre>

<p>可以看到如下输出：</p>

<pre><code class="language-scala">//Got 1000209 ratings from 6040 users on 3706 products.
</code></pre>

<p>你可以对评分数据生成训练集和测试集，例如：训练集和测试集比例为8比2：</p>

<pre><code class="language-scala">val splits = ratings.randomSplit(Array(0.8, 0.2), seed = 111l)
val training = splits(0).repartition(numPartitions)
val test = splits(1).repartition(numPartitions)
</code></pre>

<p>这里，我们是将评分数据全部当做训练集，并且也为测试集。</p>

<h2 id="section-4">训练模型</h2>

<p>接下来调用<code>ALS.train()</code>方法，进行模型训练：</p>

<pre><code class="language-scala">val rank = 12
val lambda = 0.01
val numIterations = 20
val model = ALS.train(ratings, rank, numIterations, lambda)
</code></pre>

<p>训练完后，我们看看model中的用户和商品特征向量：</p>

<pre><code class="language-scala">model.userFeatures
//res82: org.apache.spark.rdd.RDD[(Int, Array[Double])] = users MapPartitionsRDD[400] at mapValues at ALS.scala:218

model.userFeatures.count
//res84: Long = 6040

model.productFeatures
//res85: org.apache.spark.rdd.RDD[(Int, Array[Double])] = products MapPartitionsRDD[401] at mapValues at ALS.scala:222

model.productFeatures.count
//res86: Long = 3706
</code></pre>

<h2 id="section-5">评测</h2>

<p>我们要对比一下预测的结果，注意：<strong>我们将训练集当作测试集</strong>来进行对比测试。从训练集中获取用户和商品的映射：</p>

<pre><code class="language-scala">val usersProducts= ratings.map { case Rating(user, product, rate) =&gt;
  (user, product)
}
</code></pre>

<p>显然，测试集的记录数等于评分总记录数，验证一下：</p>

<pre><code class="language-scala">usersProducts.count  //Long = 1000209
</code></pre>

<p>使用推荐模型对用户商品进行预测评分，得到预测评分的数据集：</p>

<pre><code class="language-scala">var predictions = model.predict(usersProducts).map { case Rating(user, product, rate) =&gt;
    ((user, product), rate)
}
</code></pre>

<p>查看其记录数：</p>

<pre><code class="language-scala">predictions.count //Long = 1000209
</code></pre>

<p>将真实评分数据集与预测评分数据集进行合并，这样得到用户对每一个商品的实际评分和预测评分：</p>

<pre><code class="language-scala">val ratesAndPreds = ratings.map { case Rating(user, product, rate) =&gt;
  ((user, product), rate)
}.join(predictions)

ratesAndPreds.count  //Long = 1000209
</code></pre>

<p>然后计算根均方差：</p>

<pre><code class="language-scala">val rmse= math.sqrt(ratesAndPreds.map { case ((user, product), (r1, r2)) =&gt;
  val err = (r1 - r2)
  err * err
}.mean())

println(s"RMSE = $rmse")
</code></pre>

<p>上面这段代码其实就是<code>对测试集进行评分预测并计算相似度</code>，这段代码可以抽象为一个方法，如下：</p>

<pre><code class="language-scala">/** Compute RMSE (Root Mean Squared Error). */
def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating]) = {
  val usersProducts = data.map { case Rating(user, product, rate) =&gt;
    (user, product)
  }

  val predictions = model.predict(usersProducts).map { case Rating(user, product, rate) =&gt;
    ((user, product), rate)
  }

  val ratesAndPreds = data.map { case Rating(user, product, rate) =&gt;
    ((user, product), rate)
  }.join(predictions)

  math.sqrt(ratesAndPreds.map { case ((user, product), (r1, r2)) =&gt;
    val err = (r1 - r2)
    err * err
  }.mean())
}
</code></pre>

<p>除了RMSE指标，我们还可以及时AUC以及Mean average precision at K (MAPK)，关于AUC的计算方法，参考<a href="https://github.com/sryza/aas/blob/master/ch03-recommender/src/main/scala/com/cloudera/datascience/recommender/RunRecommender.scala">RunRecommender.scala</a>，关于MAPK的计算方法可以参考《<a href="http://f.dataguru.cn/thread-495493-1-1.html">Packt.Machine Learning with Spark.2015.pdf</a>》一书第四章节内容，或者你可以看本文后面内容。</p>

<h2 id="section-6">保存真实评分和预测评分</h2>

<p>我们还可以保存用户对商品的真实评分和预测评分记录到本地文件：</p>

<pre><code class="language-scala">ratesAndPreds.sortByKey().repartition(1).sortBy(_._1).map({
  case ((user, product), (rate, pred)) =&gt; (user + "," + product + "," + rate + "," + pred)
}).saveAsTextFile("/tmp/result")
</code></pre>

<p>上面这段代码先按用户排序，然后重新分区确保目标目录中只生成一个文件。如果你重复运行这段代码，则需要先删除目标路径：</p>

<pre><code class="language-scala">import scala.sys.process._
"rm -r /tmp/result".!
</code></pre>

<p>我们还可以对预测的评分结果按用户进行分组并按评分倒排序：</p>

<pre><code class="language-scala">predictions.map { case ((user, product), rate) =&gt;
  (user, (product, rate))
}.groupByKey(numPartitions).map{case (user_id,list)=&gt;
  (user_id,list.toList.sortBy {case (goods_id,rate)=&gt; - rate})
}
</code></pre>

<h1 id="section-7">给一个用户推荐商品</h1>

<p>这个例子主要是记录如何给一个或大量用户进行推荐商品，例如，对用户编号为384的用户进行推荐，查出该用户在测试集中评分过的商品。</p>

<p>找出5个用户：</p>

<pre><code class="language-scala">users.take(5) 
//Array[Int] = Array(384, 1084, 4904, 3702, 5618)
</code></pre>

<p>查看用户编号为384的用户的预测结果中预测评分排前10的商品：</p>

<pre><code class="language-scala">val userId = users.take(1)(0) //384
val K = 10
val topKRecs = model.recommendProducts(userId, K)
println(topKRecs.mkString("\n"))
//    Rating(384,2545,8.354966018818265)
//    Rating(384,129,8.113083736094676)
//    Rating(384,184,8.038113395650853)
//    Rating(384,811,7.983433591425284)
//    Rating(384,1421,7.912044967873945)
//    Rating(384,1313,7.719639594879865)
//    Rating(384,2892,7.53667094600392)
//    Rating(384,2483,7.295378004543803)
//    Rating(384,397,7.141158013610967)
//    Rating(384,97,7.071089782695754)
</code></pre>

<p>查看该用户的评分记录：</p>

<pre><code class="language-scala">val goodsForUser=ratings.keyBy(_.user).lookup(384)
// Seq[org.apache.spark.mllib.recommendation.Rating] = WrappedArray(Rating(384,2055,2.0), Rating(384,1197,4.0), Rating(384,593,5.0), Rating(384,599,3.0), Rating(384,673,2.0), Rating(384,3037,4.0), Rating(384,1381,2.0), Rating(384,1610,4.0), Rating(384,3074,4.0), Rating(384,204,4.0), Rating(384,3508,3.0), Rating(384,1007,3.0), Rating(384,260,4.0), Rating(384,3487,3.0), Rating(384,3494,3.0), Rating(384,1201,5.0), Rating(384,3671,5.0), Rating(384,1207,4.0), Rating(384,2947,4.0), Rating(384,2951,4.0), Rating(384,2896,2.0), Rating(384,1304,5.0))

productsForUser.size //Int = 22
productsForUser.sortBy(-_.rating).take(10).map(rating =&gt; (rating.product, rating.rating)).foreach(println)
//    (593,5.0)
//    (1201,5.0)
//    (3671,5.0)
//    (1304,5.0)
//    (1197,4.0)
//    (3037,4.0)
//    (1610,4.0)
//    (3074,4.0)
//    (204,4.0)
//    (260,4.0)
</code></pre>

<p>可以看到该用户对22个商品评过分以及浏览的商品是哪些。</p>

<p>我们可以该用户对某一个商品的实际评分和预测评分方差为多少：</p>

<pre><code class="language-scala">val actualRating = productsForUser.take(1)(0)
//actualRating: org.apache.spark.mllib.recommendation.Rating = Rating(384,2055,2.0)    val predictedRating = model.predict(789, actualRating.product)
val predictedRating = model.predict(384, actualRating.product)
//predictedRating: Double = 1.9426030777174637
val squaredError = math.pow(predictedRating - actualRating.rating, 2.0)
//squaredError: Double = 0.0032944066875075172
</code></pre>

<p>如何找出和一个已知商品最相似的商品呢？这里，我们可以使用余弦相似度来计算：</p>

<pre><code class="language-scala">import org.jblas.DoubleMatrix

/* Compute the cosine similarity between two vectors */
def cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = {
  vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())
}
</code></pre>

<p>以2055商品为例，计算实际评分和预测评分相似度</p>

<pre><code class="language-scala">val itemId = 2055
val itemFactor = model.productFeatures.lookup(itemId).head
//itemFactor: Array[Double] = Array(0.3660752773284912, 0.43573060631752014, -0.3421429991722107, 0.44382765889167786, -1.4875195026397705, 0.6274569630622864, -0.3264533579349518, -0.9939845204353333, -0.8710321187973022, -0.7578890323638916, -0.14621856808662415, -0.7254264950752258)
val itemVector = new DoubleMatrix(itemFactor)
//itemVector: org.jblas.DoubleMatrix = [0.366075; 0.435731; -0.342143; 0.443828; -1.487520; 0.627457; -0.326453; -0.993985; -0.871032; -0.757889; -0.146219; -0.725426]

cosineSimilarity(itemVector, itemVector)
// res99: Double = 0.9999999999999999
</code></pre>

<p>找到和该商品最相似的10个商品：</p>

<pre><code class="language-scala">val sims = model.productFeatures.map{ case (id, factor) =&gt;
  val factorVector = new DoubleMatrix(factor)
  val sim = cosineSimilarity(factorVector, itemVector)
  (id, sim)
}
val sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { case (id, similarity) =&gt; similarity })
//sortedSims: Array[(Int, Double)] = Array((2055,0.9999999999999999), (2051,0.9138311231145874), (3520,0.8739823400539756), (2190,0.8718466671129721), (2050,0.8612639515847019), (1011,0.8466911667526461), (2903,0.8455764332511272), (3121,0.8227325520485377), (3674,0.8075743004357392), (2016,0.8063817280259447))
println(sortedSims.mkString("\n"))
//    (2055,0.9999999999999999)
//    (2051,0.9138311231145874)
//    (3520,0.8739823400539756)
//    (2190,0.8718466671129721)
//    (2050,0.8612639515847019)
//    (1011,0.8466911667526461)
//    (2903,0.8455764332511272)
//    (3121,0.8227325520485377)
//    (3674,0.8075743004357392)
//    (2016,0.8063817280259447)
</code></pre>

<p>显然第一个最相似的商品即为该商品本身，即2055，我们可以修改下代码，取前k+1个商品，然后排除第一个：</p>

<pre><code class="language-scala">val sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] { case (id, similarity) =&gt; similarity })
//sortedSims2: Array[(Int, Double)] = Array((2055,0.9999999999999999), (2051,0.9138311231145874), (3520,0.8739823400539756), (2190,0.8718466671129721), (2050,0.8612639515847019), (1011,0.8466911667526461), (2903,0.8455764332511272), (3121,0.8227325520485377), (3674,0.8075743004357392), (2016,0.8063817280259447), (3672,0.8016276723120674))

sortedSims2.slice(1, 11).map{ case (id, sim) =&gt; (id, sim) }.mkString("\n")
//    (2051,0.9138311231145874)
//    (3520,0.8739823400539756)
//    (2190,0.8718466671129721)
//    (2050,0.8612639515847019)
//    (1011,0.8466911667526461)
//    (2903,0.8455764332511272)
//    (3121,0.8227325520485377)
//    (3674,0.8075743004357392)
//    (2016,0.8063817280259447)
//    (3672,0.8016276723120674)
</code></pre>

<p>接下来，我们可以计算给该用户推荐的前K个商品的平均准确度MAPK，该算法定义如下（该算法是否正确还有待考证）：</p>

<pre><code class="language-scala">/* Function to compute average precision given a set of actual and predicted ratings */
// Code for this function is based on: https://github.com/benhamner/Metrics
def avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int): Double = {
  val predK = predicted.take(k)
  var score = 0.0
  var numHits = 0.0
  for ((p, i) &lt;- predK.zipWithIndex) {
    if (actual.contains(p)) {
      numHits += 1.0
      score += numHits / (i.toDouble + 1.0)
    }
  }
  if (actual.isEmpty) {
    1.0
  } else {
    score / scala.math.min(actual.size, k).toDouble
  }
}
</code></pre>

<p>给该用户推荐的商品为：</p>

<pre><code class="language-scala">val actualProducts = productsForUser.map(_.product)
//actualProducts: Seq[Int] = ArrayBuffer(2055, 1197, 593, 599, 673, 3037, 1381, 1610, 3074, 204, 3508, 1007, 260, 3487, 3494, 1201, 3671, 1207, 2947, 2951, 2896, 1304)
</code></pre>

<p>给该用户预测的商品为：</p>

<pre><code class="language-scala"> val predictedProducts = topKRecs.map(_.product)
//predictedProducts: Array[Int] = Array(2545, 129, 184, 811, 1421, 1313, 2892, 2483, 397, 97)
</code></pre>

<p>最后的准确度为：</p>

<pre><code class="language-scala">val apk10 = avgPrecisionK(actualProducts, predictedProducts, 10)
// apk10: Double = 0.0
</code></pre>

<h1 id="section-8">批量推荐</h1>

<p>你可以评分记录中获得所有用户然后依次给每个用户推荐：</p>

<pre><code class="language-scala">val users = ratings.map(_.user).distinct()

users.collect.flatMap { user =&gt;
  model.recommendProducts(user, 10)
}
</code></pre>

<p>这种方式是遍历内存中的一个集合然后循环调用RDD的操作，运行会比较慢，另外一种方式是直接操作model中的userFeatures和productFeatures，代码如下：</p>

<pre><code class="language-scala">val itemFactors = model.productFeatures.map { case (id, factor) =&gt; factor }.collect()
val itemMatrix = new DoubleMatrix(itemFactors)
println(itemMatrix.rows, itemMatrix.columns)
//(3706,12)

// broadcast the item factor matrix
val imBroadcast = sc.broadcast(itemMatrix)

//获取商品和索引的映射
var idxProducts=model.productFeatures.map { case (prodcut, factor) =&gt; prodcut }.zipWithIndex().map{case (prodcut, idx) =&gt; (idx,prodcut)}.collectAsMap()
val idxProductsBroadcast = sc.broadcast(idxProducts)

val allRecs = model.userFeatures.map{ case (user, array) =&gt;
  val userVector = new DoubleMatrix(array)
  val scores = imBroadcast.value.mmul(userVector)
  val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)
  //根据索引取对应的商品id
  val recommendedProducts = sortedWithId.map(_._2).map{idx=&gt;idxProductsBroadcast.value.get(idx).get}
  (user, recommendedProducts) 
}
</code></pre>

<p>这种方式其实还不是最优方法，更好的方法可以参考<a href="http://www.alesisnovik.com/?p=8">Personalised recommendations using Spark</a>，当然这篇文章中的代码还可以继续优化一下。我修改后的代码如下，供大家参考：</p>

<pre><code class="language-scala">val productFeatures = model.productFeatures.collect()
var productArray = ArrayBuffer[Int]()
var productFeaturesArray = ArrayBuffer[Array[Double]]()
for ((product, features) &lt;- productFeatures) {
  productArray += product
  productFeaturesArray += features
}

val productArrayBroadcast = sc.broadcast(productArray)
val productFeatureMatrixBroadcast = sc.broadcast(new DoubleMatrix(productFeaturesArray.toArray).transpose())

start = System.currentTimeMillis()
val allRecs = model.userFeatures.mapPartitions { iter =&gt;
  // Build user feature matrix for jblas
  var userFeaturesArray = ArrayBuffer[Array[Double]]()
  var userArray = new ArrayBuffer[Int]()
  while (iter.hasNext) {
    val (user, features) = iter.next()
    userArray += user
    userFeaturesArray += features
  }

  var userFeatureMatrix = new DoubleMatrix(userFeaturesArray.toArray)
  var userRecommendationMatrix = userFeatureMatrix.mmul(productFeatureMatrixBroadcast.value)
  var productArray=productArrayBroadcast.value
  var mappedUserRecommendationArray = new ArrayBuffer[String](params.topk)

  // Extract ratings from the matrix
  for (i &lt;- 0 until userArray.length) {
    var ratingSet =  mutable.TreeSet.empty(Ordering.fromLessThan[(Int,Double)](_._2 &gt; _._2))
    for (j &lt;- 0 until productArray.length) {
      var rating = (productArray(j), userRecommendationMatrix.get(i,j))
      ratingSet += rating
    }
    mappedUserRecommendationArray += userArray(i)+","+ratingSet.take(params.topk).mkString(",")
  }
  mappedUserRecommendationArray.iterator
}
</code></pre>

<blockquote>
  <p>2015.06.12 更新：</p>

  <p>悲哀的是，上面的方法还是不能解决问题，因为矩阵相乘会撑爆集群内存；可喜的是，如果你关注Spark最新动态，你会发现Spark1.4.0中MatrixFactorizationModel提供了<code>recommendForAll</code>方法实现离线批量推荐，详细说明见<a href="https://issues.apache.org/jira/browse/SPARK-3066">SPARK-3066</a>。因为，我使用的Hadoop版本是CDH-5.4.0，其中Spark版本还是1.3.0，所以暂且不能在集群上测试Spark1.4.0中添加的新方法。</p>
</blockquote>

<p><code>如果上面结果跑出来了，就可以验证推荐结果是否正确</code>。还是以384用户为例：</p>

<pre><code class="language-scala">allRecs.lookup(384).head.take(10)
//res50: Array[Int] = Array(1539, 219, 1520, 775, 3161, 2711, 2503, 771, 853, 759)
topKRecs.map(_.product)
//res49: Array[Int] = Array(1539, 219, 1520, 775, 3161, 2711, 2503, 771, 853, 759)
</code></pre>

<p>接下来，我们可以计算所有推荐结果的准确度了，首先，得到每个用户评分过的所有商品：</p>

<pre><code class="language-scala">val userProducts = ratings.map{ case Rating(user, product, rating) =&gt; (user, product) }.groupBy(_._1)
</code></pre>

<p>然后，预测的商品和实际商品关联求准确度：</p>

<pre><code class="language-scala">// finally, compute the APK for each user, and average them to find MAPK
val MAPK = allRecs.join(userProducts).map{ case (userId, (predicted, actualWithIds)) =&gt;
  val actual = actualWithIds.map(_._2).toSeq
  avgPrecisionK(actual, predicted, K)
}.reduce(_ + _) / allRecs.count
println("Mean Average Precision at K = " + MAPK)
//Mean Average Precision at K = 0.018827551771260383
</code></pre>

<p>其实，我们也可以使用Spark内置的算法计算RMSE和MAE：</p>

<pre><code class="language-scala">// MSE, RMSE and MAE
import org.apache.spark.mllib.evaluation.RegressionMetrics

val predictedAndTrue = ratesAndPreds.map { case ((user, product), (actual, predicted)) =&gt; (actual, predicted) }
val regressionMetrics = new RegressionMetrics(predictedAndTrue)
println("Mean Squared Error = " + regressionMetrics.meanSquaredError)
println("Root Mean Squared Error = " + regressionMetrics.rootMeanSquaredError)
// Mean Squared Error = 0.5490153087908566
// Root Mean Squared Error = 0.7409556726220918

// MAPK
import org.apache.spark.mllib.evaluation.RankingMetrics
val predictedAndTrueForRanking = allRecs.join(userProducts).map{ case (userId, (predicted, actualWithIds)) =&gt;
  val actual = actualWithIds.map(_._2)
  (predicted.toArray, actual.toArray)
}
val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)
println("Mean Average Precision = " + rankingMetrics.meanAveragePrecision)
// Mean Average Precision = 0.04417535679520426
</code></pre>

<p>计算推荐2000个商品时的准确度为：</p>

<pre><code class="language-scala">val MAPK2000 = allRecs.join(userProducts).map{ case (userId, (predicted, actualWithIds)) =&gt;
  val actual = actualWithIds.map(_._2).toSeq
  avgPrecisionK(actual, predicted, 2000)
}.reduce(_ + _) / allRecs.count
println("Mean Average Precision = " + MAPK2000)
//Mean Average Precision = 0.025228311843069083
</code></pre>

<h1 id="section-9">保存和加载推荐模型</h1>

<p>对与实时推荐，我们需要启动一个web server，在启动的时候生成或加载训练模型，然后提供API接口返回推荐接口，需要调用的相关方法为：</p>

<pre><code class="language-scala">save(model: MatrixFactorizationModel, path: String)
load(sc: SparkContext, path: String)
</code></pre>

<p>model中的userFeatures和productFeatures也可以保存起来：</p>

<pre><code class="language-scala">val outputDir="/tmp"
model.userFeatures.map{ case (id, vec) =&gt; id + "\t" + vec.mkString(",") }.saveAsTextFile(outputDir + "/userFeatures")
model.productFeatures.map{ case (id, vec) =&gt; id + "\t" + vec.mkString(",") }.saveAsTextFile(outputDir + "/productFeatures")
</code></pre>

<h1 id="section-10">总结</h1>

<p>本文主要记录如何使用ALS算法实现协同过滤并给用户推荐商品，以上代码在<a href="https://github.com/javachen/learning-spark/tree/master/src/main/scala/com/javachen/spark/examples/mllib">Github</a>仓库中的ScalaLocalALS.scala文件。</p>

<p>如果你想更加深入了解Spark MLlib算法的使用，可以看看<a href="http://f.dataguru.cn/thread-495493-1-1.html">Packt.Machine Learning with Spark.2015.pdf</a>这本电子书并下载书中的源码，本文大部分代码参考自该电子书。</p>

<h1 id="section-11">参考资料</h1>

<ul>
  <li><a href="/2015/04/17/spark-mllib-collaborative-filtering.html">Spark MLlib中的协同过滤</a></li>
  <li><a href="http://f.dataguru.cn/thread-495493-1-1.html">Packt.Machine Learning with Spark.2015.pdf</a></li>
  <li><a href="https://issues.apache.org/jira/browse/SPARK-3066">SPARK-3066</a></li>
</ul>

                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">yuke</a><br/>
                        本文链接地址：<a href="/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html">http://blog.javachen.com/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html">如何使用Spark ALS实现协同过滤</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#spark">spark</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#spark">spark</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#mlib">mlib</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#recommendation">recommendation</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#als">als</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html" data-url="http://blog.javachen.com/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html" data-title="如何使用Spark ALS实现协同过滤"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2015 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">yuke</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
            <script type="text/javascript">
                var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
                document.write(unescape("%3Cspan id='cnzz_stat_icon_1256628929'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1256628929' type='text/javascript'%3E%3C/script%3E"));</script>
            
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html'
      });
      </script>
  </body>
</html>
