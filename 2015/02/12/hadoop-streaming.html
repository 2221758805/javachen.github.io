<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>Hadoop Streaming 原理 - JavaChen Blog</title>
      <meta name="author" content="yuke"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2015/02/12/hadoop-streaming.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

      <!-- atom & rss feed -->
      <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
      <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              <li><a href="https://twitter.com/java_chen" target="_blank" title="twitter"><span class="fa fa-twitter fa-2x"></span></a></li>
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2015/02/10/useful-commands-in-hadoop.html" title="Useful Hadoop Commands"><i class="fa fa-angle-double-left"></i>&nbsp;Useful Hadoop Commands</a></li>
                
                
                <li class="next"><a href="/2015/02/28/install-and-config-hue.html" title="安装和配置Hue">安装和配置Hue&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> Hadoop Streaming 原理  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2015.02.12 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <h1 id="简介">简介</h1>

<p>Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的优势和能力，来处理大数据。</p>

<p>一个简单的示例，以 shell 脚本为例：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /usr/bin/wc
</code></pre></div>
<p>Streaming 方式是 <code class="prettyprint">基于 Unix 系统的标准输入输出</code> 来进行 MapReduce Job 的运行，它区别与 Pipes 的地方主要是通信协议，Pipes 使用的是 Socket 通信，是对使用 C++ 语言来实现 MapReduce Job 并通过 Socket 通信来与 Hadopp 平台通信，完成 Job 的执行。</p>

<p>任何支持标准输入输出特性的编程语言都可以使用 Streaming 方式来实现 MapReduce Job，基本原理就是输入从 Unix 系统标准输入，输出使用 Unix 系统的标准输出。</p>

<p>Hadoop 是使用 Java 语言编写的，所以最直接的方式的就是使用 Java 语言来实现 Mapper 和 Reducer，然后配置 MapReduce Job，提交到集群计算环境来完成计算。但是很多开发者可能对 Java 并不熟悉，而是对一些具有脚本特性的语言，如 C++、Shell、Python、 Ruby、PHP、Perl 有实际开发经验，Hadoop Streaming 为这一类开发者提供了使用 Hadoop 集群来进行处理数据的工具，即工具包 hadoop-streaming.jar。</p>

<p>在标准的输入输出中，Key 和 Value 是以 Tab 作为分隔符，并且在 Reducer 的标准输入中，Hadoop 框架保证了输入的数据是经过了按 Key 排序的。</p>

<h1 id="原理">原理</h1>

<p>Hadoop Streaming 使用了 Unix 的标准输入输出作为 Hadoop 和其他编程语言的开发接口，因此在其他的编程语言所写的程序中，只需要将标准输入作为程序的输入，将标准输出作为程序的输出就可以了。</p>

<p>mapper 和 reducer 会从标准输入中读取用户数据，一行一行处理后发送给标准输出。Streaming 工具会创建 MapReduce 作业，发送给各个 tasktracker，同时监控整个作业的执行过程。</p>

<p>如果一个文件（可执行或者脚本）作为 mapper，mapper 初始化时，每一个 mapper 任务会把该文件作为一个单独进程启动，mapper 任务运行时，它把输入切分成行并把每一行提供给可执行文件进程的标准输入。 同时，mapper 收集可执行文件进程标准输出的内容，并把收到的每一行内容转化成 key/value 对，作为 mapper 的输出。 默认情况下，一行中第一个 tab 之前的部分作为 key，之后的（不包括tab）作为 value。如果没有 tab，整行作为 key 值，value 值为 null。</p>

<p>对于 reducer，类似。</p>

<p>以上是 Map/Reduce 框架和 streaming mapper/reducer 之间的基本通信协议。</p>

<p>用户可以定义 <code class="prettyprint">stream.non.zero.exit.is.failure</code> 参数为 true 或者 false 以定义一个以非0状态退出的 streaming 的任务是失败还是成功。默认情况下，以非0状态退出的任务都任务是失败的。</p>

<h1 id="用法">用法</h1>

<p>命令如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="o">[</span>genericOptions<span class="o">]</span> <span class="o">[</span>streamingOptions<span class="o">]</span>
</code></pre></div>
<h2 id="streaming-参数">streaming 参数</h2>

<p>以 Hadoop 2.6.0 为例，可选的 streaming 参数如下：</p>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">是否可选</th>
<th style="text-align: left">描述</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">-input directoryname or filename</code></td>
<td style="text-align: left">Required</td>
<td style="text-align: left">mapper的输入路径</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-output directoryname</code></td>
<td style="text-align: left">Required</td>
<td style="text-align: left">reducer输出路径</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-mapper executable or JavaClassName</code></td>
<td style="text-align: left">Required</td>
<td style="text-align: left">Mapper可执行程序或 Java 类名</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-reducer executable or JavaClassName</code></td>
<td style="text-align: left">Required</td>
<td style="text-align: left">Reducer 可执行程序或 Java 类名</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-file filename</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">mapper, reducer 或 combiner 依赖的文件</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-inputformat JavaClassName</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">key/value 输入格式，默认为 TextInputFormat</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-outputformat JavaClassName</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">key/value 输出格式，默认为  TextOutputformat</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-partitioner JavaClassName</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">Class that determines which reduce a key is sent to</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-combiner streamingCommand or JavaClassName</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">map 输出结果执行 Combiner 的命令或者类名</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-cmdenv name=value</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">环境变量</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-inputreader</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">向后兼容，定义输入的 Reader 类，用于取代输出格式</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-verbose</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">输出日志</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-lazyOutput</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">延时输出</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-numReduceTasks</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">定义 reduce 数量</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-mapdebug</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">map 任务运行失败时候，执行的脚本</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-reducedebug</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">reduce 任务运行失败时候，执行的脚本</td>
</tr>
</tbody></table>

<p>定义 Java 类作为 mapper 和 reducer：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat <span class="se">\</span>
    -mapper org.apache.hadoop.mapred.lib.IdentityMapper <span class="se">\</span>
    -reducer /usr/bin/wc
</code></pre></div>
<p>如果 mapper 和 reducer 的可执行文件在集群上不存在，则可以通过  <code class="prettyprint">-file</code> 参数将其提交到集群上去：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper myPythonScript.py <span class="se">\</span>
    -reducer /usr/bin/wc <span class="se">\</span>
    -file myPythonScript.py
</code></pre></div>
<p>你也可以将 mapper 和 reducer 的可执行文件用到的文件和配置上传到集群上：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper myPythonScript.py <span class="se">\</span>
    -reducer /usr/bin/wc <span class="se">\</span>
    -file myPythonScript.py <span class="se">\</span>
    -file myDictionary.txt
</code></pre></div>
<p>你也可以定义其他参数：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-inputformat JavaClassName
-outputformat JavaClassName
-partitioner JavaClassName
-combiner streamingCommand or JavaClassName
</code></pre></div>
<p>定义一个环境变量：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-cmdenv <span class="nv">EXAMPLE_DIR</span><span class="o">=</span>/home/example/dictionaries/   
</code></pre></div>
<h2 id="通用参数">通用参数</h2>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">是否可选</th>
<th style="text-align: left">描述</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">-conf configuration_file</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">定义应用的配置文件</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-D property=value</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">定义参数</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-fs host:port or local</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">定义 namenode 地址</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-files</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">定义需要拷贝到 Map/Reduce 集群的文件，多个文件以逗号分隔</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-libjars</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">定义需要引入到 classpath 的 jar 文件，多个文件以逗号分隔</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-archives</code></td>
<td style="text-align: left">Optional</td>
<td style="text-align: left">定义需要解压到计算节点的压缩文件，多个文件以逗号分隔</td>
</tr>
</tbody></table>

<p>定义参数：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-D mapred.local.dir<span class="o">=</span>/tmp/local
-D mapred.system.dir<span class="o">=</span>/tmp/system
-D mapred.temp.dir<span class="o">=</span>/tmp/temp
</code></pre></div>
<p>定义 reduce 个数：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-D mapreduce.job.reduces<span class="o">=</span>0
</code></pre></div>
<p>你也可以使用 <code class="prettyprint">-D stream.reduce.output.field.separator=SEP</code> 和 <code class="prettyprint">-D stream.num.reduce.output.fields=NUM</code> 自定义 mapper 输出的分隔符为SEP，并且按 SEP 分隔之后的前 NUM 部分内容作为 key，如果分隔符少于 NUM，则整行作为 key。例如，下面的例子指定分隔符为 <code class="prettyprint">....</code>：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -D stream.map.output.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D stream.num.map.output.key.fields<span class="o">=</span><span class="m">4</span> <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper /bin/cat <span class="se">\</span>
    -reducer /bin/cat
</code></pre></div>
<p>hadoop 提供配置供用户自主设置分隔符：</p>

<p><code class="prettyprint">-D stream.map.output.field.separator</code> ：设置 map 输出中 key 和 value 的分隔符 
<code class="prettyprint">-D stream.num.map.output.key.fields</code> ：设置 map 程序分隔符的位置，该位置之前的部分作为 key，之后的部分作为 value 
<code class="prettyprint">-D map.output.key.field.separator</code> : 设置 map 输出分区时 key 内部的分割符
<code class="prettyprint">-D mapreduce.partition.keypartitioner.options</code> : 指定分桶时，key 按照分隔符切割后，其中用于分桶 key 所占的列数（配合 <code class="prettyprint">-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner</code> 使用）
<code class="prettyprint">-D stream.reduce.output.field.separator</code>：设置 reduce 输出中 key 和 value 的分隔符 
<code class="prettyprint">-D stream.num.reduce.output.key.fields</code>：设置 reduce 程序分隔符的位置</p>

<p>定义解压文件：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>ls test_jar/
cache.txt  cache2.txt

<span class="nv">$ </span>jar cvf cachedir.jar -C test_jar/ .
added manifest
adding: cache.txt<span class="o">(</span><span class="nv">in</span> <span class="o">=</span> 30<span class="o">)</span> <span class="o">(</span><span class="nv">out</span><span class="o">=</span> 29<span class="o">)(</span>deflated 3%<span class="o">)</span>
adding: cache2.txt<span class="o">(</span><span class="nv">in</span> <span class="o">=</span> 37<span class="o">)</span> <span class="o">(</span><span class="nv">out</span><span class="o">=</span> 35<span class="o">)(</span>deflated 5%<span class="o">)</span>

<span class="nv">$ </span>hdfs dfs -put cachedir.jar samples/cachefile

<span class="nv">$ </span>hdfs dfs -cat /user/root/samples/cachefile/input.txt
cachedir.jar/cache.txt
cachedir.jar/cache2.txt

<span class="nv">$ </span>cat test_jar/cache.txt
This is just the cache string

<span class="nv">$ </span>cat test_jar/cache2.txt
This is just the second cache string

<span class="nv">$ </span>hadoop jar hadoop-streaming.jar <span class="se">\</span>
                  -archives <span class="s1">&#39;hdfs://hadoop-nn1.example.com/user/root/samples/cachefile/cachedir.jar&#39;</span> <span class="se">\</span>
                  -D mapreduce.job.maps<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
                  -D mapreduce.job.reduces<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
                  -D mapreduce.job.name<span class="o">=</span><span class="s2">&quot;Experiment&quot;</span> <span class="se">\</span>
                  -input <span class="s2">&quot;/user/root/samples/cachefile/input.txt&quot;</span> <span class="se">\</span>
                  -output <span class="s2">&quot;/user/root/samples/cachefile/out&quot;</span> <span class="se">\</span>
                  -mapper <span class="s2">&quot;xargs cat&quot;</span> <span class="se">\</span>
                  -reducer <span class="s2">&quot;cat&quot;</span>

<span class="nv">$ </span>hdfs dfs -ls /user/root/samples/cachefile/out
Found <span class="m">2</span> items
-rw-r--r--   <span class="m">1</span> root supergroup        <span class="m">0</span> 2013-11-14 17:00 /user/root/samples/cachefile/out/_SUCCESS
-rw-r--r--   <span class="m">1</span> root supergroup       <span class="m">69</span> 2013-11-14 17:00 /user/root/samples/cachefile/out/part-00000

<span class="nv">$ </span>hdfs dfs -cat /user/root/samples/cachefile/out/part-00000
This is just the cache string
This is just the second cache string
</code></pre></div>
<h2 id="复杂的例子">复杂的例子</h2>

<h3 id="hadoop-partitioner-class">Hadoop Partitioner Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/lib/KeyFieldBasedPartitioner.html">KeyFieldBasedPartitioner</a>，可以将 map 输出的内容按照分隔后的一定列，而不是整个 key 内容进行分区，例如：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -D stream.map.output.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D stream.num.map.output.key.fields<span class="o">=</span><span class="m">4</span> <span class="se">\</span>
    -D map.output.key.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D mapreduce.partition.keypartitioner.options<span class="o">=</span>-k1,2 <span class="se">\</span>
    -D mapreduce.job.reduces<span class="o">=</span><span class="m">12</span> <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper /bin/cat <span class="se">\</span>
    -reducer /bin/cat <span class="se">\</span>
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
</code></pre></div>
<p>关键参数说明：</p>

<ul>
<li><code class="prettyprint">map.output.key.field.separator=.</code>：设置 map 输出分区时 key 内部的分割符为 <code class="prettyprint">.</code></li>
<li><code class="prettyprint">mapreduce.partition.keypartitioner.options=-k1,2</code>：设置按前两个字段分区</li>
<li><code class="prettyprint">mapreduce.job.reduces=12</code>：reduce 数为12</li>
</ul>

<p>假设 map 的输出为：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">11.12.1.2
11.14.2.3
11.11.4.1
11.12.1.1
11.14.2.2
</code></pre></div>
<p>按照前两个字段进行分区，则会分为三个分区：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">11.11.4.1
-----------
11.12.1.2
11.12.1.1
-----------
11.14.2.3
11.14.2.2
</code></pre></div>
<p>在每个分区内对整行内容排序后为：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">11.11.4.1
-----------
11.12.1.1
11.12.1.2
-----------
11.14.2.2
11.14.2.3
</code></pre></div>
<h3 id="hadoop-comparator-class">Hadoop Comparator Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html">KeyFieldBasedComparator</a>，提供了 Unix/GNU 中排序的一部分特性。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -D mapreduce.job.output.key.comparator.class<span class="o">=</span>org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator <span class="se">\</span>
    -D stream.map.output.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D stream.num.map.output.key.fields<span class="o">=</span><span class="m">4</span> <span class="se">\</span>
    -D mapreduce.map.output.key.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D mapreduce.partition.keycomparator.options<span class="o">=</span>-k2,2nr <span class="se">\</span>
    -D mapreduce.job.reduces<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper /bin/cat <span class="se">\</span>
    -reducer /bin/cat
</code></pre></div>
<p>关键参数说明：</p>

<ul>
<li><code class="prettyprint">mapreduce.partition.keycomparator.options=-k2,2nr</code>：指定第二个字段为排序字段，<code class="prettyprint">-n</code> 是指按自然顺序排序，<code class="prettyprint">-r</code> 指倒叙排序。</li>
</ul>

<p>假设 map 的输出为：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">11.12.1.2
11.14.2.3
11.11.4.1
11.12.1.1
11.14.2.2
</code></pre></div>
<p>则 reduce 输出结果为：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">11.14.2.3
11.14.2.2
11.12.1.2
11.12.1.1
11.11.4.1
</code></pre></div>
<h3 id="hadoop-aggregate-package">Hadoop Aggregate Package</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/org/apache/hadoop/mapred/lib/aggregate/package-summary.html">Aggregate</a>，Aggregate 提供了一个特定的 reduce 类和 combiner 类，以及一些对 reduce 输出的聚合函数，例如 sum、min、max 等等。</p>

<p>为了使用 Aggregate，只需要定义 <code class="prettyprint">-reducer aggregate</code>：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper myAggregatorForKeyCount.py <span class="se">\</span>
    -reducer aggregate <span class="se">\</span>
    -file myAggregatorForKeyCount.py <span class="se">\</span>
</code></pre></div>
<p>myAggregatorForKeyCount.py  文件大概内容如下：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#!/usr/bin/python</span>

<span class="kn">import</span> <span class="nn">sys</span><span class="p">;</span>

<span class="k">def</span> <span class="nf">generateLongCountToken</span><span class="p">(</span><span class="nb">id</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">&quot;LongValueSum:&quot;</span> <span class="o">+</span> <span class="nb">id</span> <span class="o">+</span> <span class="s">&quot;</span><span class="se">\t</span><span class="s">&quot;</span> <span class="o">+</span> <span class="s">&quot;1&quot;</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">argv</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">readline</span><span class="p">();</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">while</span> <span class="n">line</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span>
            <span class="n">fields</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\t</span><span class="s">&quot;</span><span class="p">);</span>
            <span class="k">print</span> <span class="n">generateLongCountToken</span><span class="p">(</span><span class="n">fields</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
            <span class="n">line</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">readline</span><span class="p">();</span>
    <span class="k">except</span> <span class="s">&quot;end of file&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
     <span class="n">main</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span>
</code></pre></div>
<h3 id="hadoop-field-selection-class">Hadoop Field Selection Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.html">FieldSelectionMapReduce</a>，运行你像 unix 中的 cut 命令一样处理文本。</p>

<p>例子：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -D mapreduce.map.output.key.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D mapreduce.partition.keypartitioner.options<span class="o">=</span>-k1,2 <span class="se">\</span>
    -D mapreduce.fieldsel.data.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D mapreduce.fieldsel.map.output.key.value.fields.spec<span class="o">=</span>6,5,1-3:0- <span class="se">\</span>
    -D mapreduce.fieldsel.reduce.output.key.value.fields.spec<span class="o">=</span>0-2:5- <span class="se">\</span>
    -D mapreduce.map.output.key.class<span class="o">=</span>org.apache.hadoop.io.Text <span class="se">\</span>
    -D mapreduce.job.reduces<span class="o">=</span><span class="m">12</span> <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper org.apache.hadoop.mapred.lib.FieldSelectionMapReduce <span class="se">\</span>
    -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce <span class="se">\</span>
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
</code></pre></div>
<p>关键参数说明：</p>

<ul>
<li><code class="prettyprint">mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0-</code>：意思是 map 的输出中 key 部分包括分隔后的第 6、5、1、2、3列，而 value 部分包括分隔后的所有的列</li>
<li><code class="prettyprint">mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5-</code>：意思是 map 的输出中 key 部分包括分隔后的第 0、1、2列，而 value 部分包括分隔后的从第5列开始的所有列</li>
</ul>

<h1 id="测试">测试</h1>

<p>上面讲了 Hadoop Streaming 的原理和一些用法，现在来运行一些例子做测试。关于如何用 Python 来编写 Hadoop Streaming 程序，可以参考 <a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a>，中文翻译在 <a href="http://www.tianjun.ml/essays/19/">这里</a>，其他非 Java 的语言，都可以参照这篇文章。</p>

<p>下面以 word count 为例做测试。</p>

<h2 id="准备测试数据">准备测试数据</h2>

<p>同 <a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a>，我们使用古腾堡项目中的三本电子书作为测试：</p>

<ul>
<li><a href="http://www.gutenberg.org/etext/20417">The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson</a></li>
<li><a href="http://www.gutenberg.org/etext/5000">The Notebooks of Leonardo Da Vinci</a></li>
<li><a href="http://www.gutenberg.org/etext/4300">Ulysses by James Joyce</a></li>
</ul>

<p>下载这些电子书的 txt格式，并将其上传到 hdfs：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mkdir /tmp/gutenberg/ <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /tmp/gutenberg/

<span class="nv">$ </span>wget http://www.gutenberg.org/files/20417/20417.txt
<span class="nv">$ </span>wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt
<span class="nv">$ </span>wget http://www.gutenberg.org/files/4300/4300.txt

<span class="nv">$ </span>hadoop fs -copyFromLocal /tmp/gutenberg gutenberg

<span class="nv">$ </span>hadoop fs -ls gutenberg
Found <span class="m">4</span> items
-rw-r--r--   <span class="m">3</span> hive hive     <span class="m">674762</span> 2015-02-11 17:34 gutenberg/20417.txt
-rw-r--r--   <span class="m">3</span> hive hive    <span class="m">1573079</span> 2015-02-11 17:34 gutenberg/4300.txt
-rw-r--r--   <span class="m">3</span> hive hive    <span class="m">1423803</span> 2015-02-11 17:34 gutenberg/pg5000.txt
</code></pre></div>
<h2 id="编写-shell-版程序">编写 Shell 版程序</h2>

<p>mapper.sh 如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#! /bin/bash</span>

<span class="k">while</span> <span class="nb">read </span>LINE<span class="p">;</span> <span class="k">do</span>
  <span class="k">for</span> word in <span class="nv">$LINE</span>
  <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&quot;$word 1&quot;</span>
  <span class="k">done</span>
<span class="k">done</span>
</code></pre></div>
<p>reducer.sh 程序如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#! /bin/bash</span>

<span class="nv">count</span><span class="o">=</span>0
<span class="nv">started</span><span class="o">=</span>0
<span class="nv">word</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="k">while</span> <span class="nb">read </span>LINE<span class="p">;</span><span class="k">do</span>
  <span class="nv">newword</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span> <span class="nv">$LINE</span> <span class="p">|</span> cut -d <span class="s1">&#39; &#39;</span>  -f 1<span class="sb">`</span>
  <span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;$word&quot;</span> !<span class="o">=</span> <span class="s2">&quot;$newword&quot;</span> <span class="o">]</span><span class="p">;</span><span class="k">then</span>
    <span class="o">[</span> <span class="nv">$started</span> -ne <span class="m">0</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="nb">echo</span> -e <span class="s2">&quot;$word\t$count&quot;</span>
    <span class="nv">word</span><span class="o">=</span><span class="nv">$newword</span>
    <span class="nv">count</span><span class="o">=</span>1
    <span class="nv">started</span><span class="o">=</span>1
  <span class="k">else</span>
    <span class="nv">count</span><span class="o">=</span><span class="k">$((</span> <span class="nv">$count</span> <span class="o">+</span> <span class="m">1</span> <span class="k">))</span>
  <span class="k">fi</span>
<span class="k">done</span>
<span class="nb">echo</span> -e <span class="s2">&quot;$word\t$count&quot;</span>
</code></pre></div>
<p>在本机以脚本方式测试：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">echo</span> <span class="s2">&quot;foo foo quux labs foo bar quux&quot;</span> <span class="p">|</span> sh mapper.sh  <span class="p">|</span>sort -k1,1<span class="p">|</span> sh reducer.sh
bar 1
foo 3
labs    1
quux    2
</code></pre></div>
<p>以 Hadoop Streaming 方式运行：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hadoop  jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar <span class="se">\</span>
    -D mapred.reduce.tasks<span class="o">=</span><span class="m">6</span> <span class="se">\</span>
    -input gutenberg/* <span class="se">\</span>
    -output gutenberg-output <span class="se">\</span>
    -mapper mapper.sh<span class="se">\</span>
    -reducer reducer.sh<span class="se">\</span>
    -file mapper.sh <span class="se">\</span>
    -file reducer.sh

15/02/11 17:50:59 INFO mapreduce.Job:  map 0% reduce 0%
15/02/11 17:51:18 INFO mapreduce.Job:  map 17% reduce 0%
15/02/11 17:51:52 INFO mapreduce.Job:  map 17% reduce 6%
15/02/11 17:51:53 INFO mapreduce.Job:  map 33% reduce 6%
15/02/11 17:51:55 INFO mapreduce.Job:  map 60% reduce 17%
15/02/11 17:51:56 INFO mapreduce.Job:  map 100% reduce 17%
15/02/11 17:51:59 INFO mapreduce.Job:  map 100% reduce 67%
15/02/11 17:53:11 INFO mapreduce.Job:  map 100% reduce 68%
15/02/11 17:54:49 INFO mapreduce.Job:  map 100% reduce 69%
15/02/11 17:57:12 INFO mapreduce.Job:  map 100% reduce 70%
15/02/11 17:58:45 INFO mapreduce.Job:  map 100% reduce 71%
15/02/11 17:58:55 INFO mapreduce.Job:  map 100% reduce 81%
15/02/11 17:59:05 INFO mapreduce.Job:  map 100% reduce 100%
15/02/11 17:59:08 INFO streaming.StreamJob: Job <span class="nb">complete</span>: job_1421752803837_5736
15/02/11 17:59:09 INFO streaming.StreamJob: Output: /user/root/gutenberg-output
</code></pre></div>
<h2 id="编写-python-版程序">编写 Python 版程序</h2>

<p>mapper.py 程序如下：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#!/usr/bin/env python</span>
<span class="sd">&quot;&quot;&quot;A more advanced Mapper, using Python iterators and generators.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">read_input</span><span class="p">(</span><span class="nb">file</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">file</span><span class="p">:</span>
        <span class="c"># split the line into words</span>
        <span class="k">yield</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">separator</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">):</span>
    <span class="c"># input comes from STDIN (standard input)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">read_input</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="c"># write the results to STDOUT (standard output);</span>
        <span class="c"># what we output here will be the input for the</span>
        <span class="c"># Reduce step, i.e. the input for reducer.py</span>
        <span class="c">#</span>
        <span class="c"># tab-delimited; the trivial word count is 1</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&#39;</span><span class="si">%s%s%d</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">separator</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>
<p>reducer.py 程序如下：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#!/usr/bin/env python</span>
<span class="sd">&quot;&quot;&quot;A more advanced Reducer, using Python iterators and generators.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">groupby</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">read_mapper_output</span><span class="p">(</span><span class="nb">file</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">file</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">separator</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">separator</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">):</span>
    <span class="c"># input comes from STDIN (standard input)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">read_mapper_output</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="n">separator</span><span class="p">)</span>
    <span class="c"># groupby groups multiple word-count pairs by word,</span>
    <span class="c"># and creates an iterator that returns consecutive keys and their group:</span>
    <span class="c">#   current_word - string containing a word (the key)</span>
    <span class="c">#   group - iterator yielding all [&quot;&amp;lt;current_word&amp;gt;&quot;, &quot;&amp;lt;count&amp;gt;&quot;] items</span>
    <span class="k">for</span> <span class="n">current_word</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">groupby</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">itemgetter</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">total_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="k">for</span> <span class="n">current_word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">group</span><span class="p">)</span>
            <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s%s%d</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">current_word</span><span class="p">,</span> <span class="n">separator</span><span class="p">,</span> <span class="n">total_count</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="c"># count was not a number, so silently discard this item</span>
            <span class="k">pass</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>
<p>关于 Java 的一些例子，这个需要单独创建一个 maven 工程，然后做一些测试。</p>

<h1 id="注意事项">注意事项</h1>

<h3 id="mapper-中不能使用-shell-的别名，但可以使用变量">mapper 中不能使用 shell 的别名，但可以使用变量</h3>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hdfs dfs -cat /user/me/samples/student_marks
alice   50
bruce   70
charlie 80
dan     75

<span class="nv">$ c2</span><span class="o">=</span><span class="s1">&#39;cut -f2&#39;</span><span class="p">;</span> hadoop jar hadoop-streaming-2.6.0.jar <span class="se">\</span>
    -D mapreduce.job.name<span class="o">=</span><span class="s1">&#39;Experiment&#39;</span> <span class="se">\</span>
    -input /user/me/samples/student_marks <span class="se">\</span>
    -output /user/me/samples/student_out <span class="se">\</span>
    -mapper <span class="s2">&quot;$c2&quot;</span> -reducer <span class="s1">&#39;cat&#39;</span>

<span class="nv">$ </span>hdfs dfs -cat /user/me/samples/student_out/part-00000
50
70
75
80
</code></pre></div>
<h3 id="mapper-中不能使用-unix-的管道">mapper 中不能使用 unix 的管道</h3>

<p><code class="prettyprint">-mapper</code> 中使用 &quot;cut -f1 | sed s/foo/bar/g&quot;，会出现 <code class="prettyprint">java.io.IOException: Broken pipe</code> 异常</p>

<h3 id="指定-streaming-临时空间">指定 streaming 临时空间</h3>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">-D stream.tmpdir<span class="o">=</span>/export/bigspace/...
</code></pre></div>
<h3 id="指定多个输入文件">指定多个输入文件</h3>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming-2.6.0.jar <span class="se">\</span>
    -input <span class="s1">&#39;/user/foo/dir1&#39;</span> -input <span class="s1">&#39;/user/foo/dir2&#39;</span> <span class="se">\</span>
    <span class="o">(</span>rest of the <span class="nb">command</span><span class="o">)</span>
</code></pre></div>
<h3 id="处理-xml">处理 XML</h3>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar hadoop-streaming-2.6.0.jar <span class="se">\</span>
    -inputreader <span class="s2">&quot;StreamXmlRecord,begin=BEGIN_STRING,end=END_STRING&quot;</span> <span class="se">\</span>
    <span class="o">(</span>rest of the <span class="nb">command</span><span class="o">)</span>
</code></pre></div>
<p>BEGIN_STRING 和 END_STRING 之前的内容会被认为是 map 任务的一条记录。</p>

<h1 id="参考文章">参考文章</h1>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html#More_Usage_Examples">Hadoop Streaming</a></li>
<li><a href="http://dongxicheng.org/mapreduce/hadoop-streaming-programming/">Hadoop Streaming 编程</a></li>
<li><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a></li>
</ul>

                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">yuke</a><br/>
                        本文链接地址：<a href="/2015/02/12/hadoop-streaming.html">http://blog.javachen.com/2015/02/12/hadoop-streaming.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2015/02/12/hadoop-streaming.html">Hadoop Streaming 原理</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#hadoop">hadoop</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#hadoop">hadoop</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#mapreduce">mapreduce</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#streaming">streaming</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2015/02/12/hadoop-streaming.html" data-url="http://blog.javachen.com/2015/02/12/hadoop-streaming.html" data-title="Hadoop Streaming 原理"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2015 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">yuke</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
            <script type="text/javascript">
                var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
                document.write(unescape("%3Cspan id='cnzz_stat_icon_1256628929'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1256628929' type='text/javascript'%3E%3C/script%3E"));</script>
            
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2015/02/12/hadoop-streaming.html'
      });
      </script>
  </body>
</html>
