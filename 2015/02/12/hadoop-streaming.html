<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>Hadoop Streaming 原理 - JavaChen Blog</title>
      <meta name="author" content="Junez"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2015/02/12/hadoop-streaming.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

      <!-- atom & rss feed -->
      <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
      <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              <li><a href="https://twitter.com/junezchen" target="_blank" title="twitter"><span class="fa fa-twitter fa-2x"></span></a></li>
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2015/02/10/useful-commands-in-hadoop.html" title="Useful Hadoop Commands"><i class="fa fa-angle-double-left"></i>&nbsp;Useful Hadoop Commands</a></li>
                
                
                <li class="next"><a href="/2015/02/28/install-and-config-hue.html" title="安装和配置Hue">安装和配置Hue&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> Hadoop Streaming 原理  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2015.02.12 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <h1 id="section">简介</h1>

<p>Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的优势和能力，来处理大数据。</p>

<p>一个简单的示例，以 shell 脚本为例：</p>

<pre><code>hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /usr/bin/wc
</code></pre>

<p>Streaming 方式是 <code>基于 Unix 系统的标准输入输出</code> 来进行 MapReduce Job 的运行，它区别与 Pipes 的地方主要是通信协议，Pipes 使用的是 Socket 通信，是对使用 C++ 语言来实现 MapReduce Job 并通过 Socket 通信来与 Hadopp 平台通信，完成 Job 的执行。</p>

<p>任何支持标准输入输出特性的编程语言都可以使用 Streaming 方式来实现 MapReduce Job，基本原理就是输入从 Unix 系统标准输入，输出使用 Unix 系统的标准输出。</p>

<p>Hadoop 是使用 Java 语言编写的，所以最直接的方式的就是使用 Java 语言来实现 Mapper 和 Reducer，然后配置 MapReduce Job，提交到集群计算环境来完成计算。但是很多开发者可能对 Java 并不熟悉，而是对一些具有脚本特性的语言，如 C++、Shell、Python、 Ruby、PHP、Perl 有实际开发经验，Hadoop Streaming 为这一类开发者提供了使用 Hadoop 集群来进行处理数据的工具，即工具包 hadoop-streaming.jar。</p>

<p>在标准的输入输出中，Key 和 Value 是以 Tab 作为分隔符，并且在 Reducer 的标准输入中，Hadoop 框架保证了输入的数据是经过了按 Key 排序的。</p>

<h1 id="section-1">原理</h1>

<p>Hadoop Streaming 使用了 Unix 的标准输入输出作为 Hadoop 和其他编程语言的开发接口，因此在其他的编程语言所写的程序中，只需要将标准输入作为程序的输入，将标准输出作为程序的输出就可以了。</p>

<p>mapper 和 reducer 会从标准输入中读取用户数据，一行一行处理后发送给标准输出。Streaming 工具会创建 MapReduce 作业，发送给各个 tasktracker，同时监控整个作业的执行过程。</p>

<p>如果一个文件（可执行或者脚本）作为 mapper，mapper 初始化时，每一个 mapper 任务会把该文件作为一个单独进程启动，mapper 任务运行时，它把输入切分成行并把每一行提供给可执行文件进程的标准输入。 同时，mapper 收集可执行文件进程标准输出的内容，并把收到的每一行内容转化成 key/value 对，作为 mapper 的输出。 默认情况下，一行中第一个 tab 之前的部分作为 key，之后的（不包括tab）作为 value。如果没有 tab，整行作为 key 值，value 值为 null。</p>

<p>对于 reducer，类似。</p>

<p>以上是 Map/Reduce 框架和 streaming mapper/reducer 之间的基本通信协议。</p>

<p>用户可以定义 <code>stream.non.zero.exit.is.failure</code> 参数为 true 或者 false 以定义一个以非0状态退出的 streaming 的任务是失败还是成功。默认情况下，以非0状态退出的任务都任务是失败的。</p>

<h1 id="section-2">用法</h1>

<p>命令如下：</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar [genericOptions] [streamingOptions]
</code></pre>

<h2 id="streaming-">streaming 参数</h2>

<p>以 Hadoop 2.6.0 为例，可选的 streaming 参数如下：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">参数</th>
      <th style="text-align: left">是否可选</th>
      <th style="text-align: left">描述</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code>-input directoryname or filename</code></td>
      <td style="text-align: left">Required</td>
      <td style="text-align: left">mapper的输入路径</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-output directoryname</code></td>
      <td style="text-align: left">Required</td>
      <td style="text-align: left">reducer输出路径</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-mapper executable or JavaClassName</code></td>
      <td style="text-align: left">Required</td>
      <td style="text-align: left">Mapper可执行程序或 Java 类名</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-reducer executable or JavaClassName</code></td>
      <td style="text-align: left">Required</td>
      <td style="text-align: left">Reducer 可执行程序或 Java 类名</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-file filename</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">mapper, reducer 或 combiner 依赖的文件</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-inputformat JavaClassName</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">key/value 输入格式，默认为 TextInputFormat</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-outputformat JavaClassName</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">key/value 输出格式，默认为  TextOutputformat</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-partitioner JavaClassName</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">Class that determines which reduce a key is sent to</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-combiner streamingCommand or JavaClassName</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">map 输出结果执行 Combiner 的命令或者类名</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-cmdenv name=value</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">环境变量</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-inputreader</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">向后兼容，定义输入的 Reader 类，用于取代输出格式</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-verbose</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">输出日志</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-lazyOutput</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">延时输出</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-numReduceTasks</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义 reduce 数量</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-mapdebug</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">map 任务运行失败时候，执行的脚本</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-reducedebug</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">reduce 任务运行失败时候，执行的脚本</td>
    </tr>
  </tbody>
</table>

<p>定义 Java 类作为 mapper 和 reducer：</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat \
    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \
    -reducer /usr/bin/wc
</code></pre>

<p>如果 mapper 和 reducer 的可执行文件在集群上不存在，则可以通过  <code>-file</code> 参数将其提交到集群上去：</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper myPythonScript.py \
    -reducer /usr/bin/wc \
    -file myPythonScript.py
</code></pre>

<p>你也可以将 mapper 和 reducer 的可执行文件用到的文件和配置上传到集群上：</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper myPythonScript.py \
    -reducer /usr/bin/wc \
    -file myPythonScript.py \
    -file myDictionary.txt
</code></pre>

<p>你也可以定义其他参数：</p>

<pre><code class="language-bash">-inputformat JavaClassName
-outputformat JavaClassName
-partitioner JavaClassName
-combiner streamingCommand or JavaClassName
</code></pre>

<p>定义一个环境变量：</p>

<pre><code class="language-bash">-cmdenv EXAMPLE_DIR=/home/example/dictionaries/   
</code></pre>

<h2 id="section-3">通用参数</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">参数</th>
      <th style="text-align: left">是否可选</th>
      <th style="text-align: left">描述</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code>-conf configuration_file</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义应用的配置文件</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-D property=value</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义参数</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-fs host:port or local</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义 namenode 地址</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-files</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义需要拷贝到 Map/Reduce 集群的文件，多个文件以逗号分隔</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-libjars</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义需要引入到 classpath 的 jar 文件，多个文件以逗号分隔</td>
    </tr>
    <tr>
      <td style="text-align: left"><code>-archives</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义需要解压到计算节点的压缩文件，多个文件以逗号分隔</td>
    </tr>
  </tbody>
</table>

<p>定义参数：</p>

<pre><code class="language-bash">-D mapred.local.dir=/tmp/local
-D mapred.system.dir=/tmp/system
-D mapred.temp.dir=/tmp/temp
</code></pre>

<p>定义 reduce 个数：</p>

<pre><code class="language-bash">-D mapreduce.job.reduces=0
</code></pre>

<p>你也可以使用 <code>-D stream.reduce.output.field.separator=SEP</code> 和 <code>-D stream.num.reduce.output.fields=NUM</code> 自定义 mapper 输出的分隔符为SEP，并且按 SEP 分隔之后的前 NUM 部分内容作为 key，如果分隔符少于 NUM，则整行作为 key。例如，下面的例子指定分隔符为 <code>....</code>：</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar \
    -D stream.map.output.field.separator=. \
    -D stream.num.map.output.key.fields=4 \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /bin/cat
</code></pre>

<p>hadoop 提供配置供用户自主设置分隔符：</p>

<p><code>-D stream.map.output.field.separator</code> ：设置 map 输出中 key 和 value 的分隔符 <br />
<code>-D stream.num.map.output.key.fields</code> ：设置 map 程序分隔符的位置，该位置之前的部分作为 key，之后的部分作为 value <br />
<code>-D map.output.key.field.separator</code> : 设置 map 输出分区时 key 内部的分割符<br />
<code>-D mapreduce.partition.keypartitioner.options</code> : 指定分桶时，key 按照分隔符切割后，其中用于分桶 key 所占的列数（配合 <code>-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner</code> 使用）<br />
<code>-D stream.reduce.output.field.separator</code>：设置 reduce 输出中 key 和 value 的分隔符 <br />
<code>-D stream.num.reduce.output.key.fields</code>：设置 reduce 程序分隔符的位置</p>

<p>定义解压文件：</p>

<pre><code class="language-bash">$ ls test_jar/
cache.txt  cache2.txt

$ jar cvf cachedir.jar -C test_jar/ .
added manifest
adding: cache.txt(in = 30) (out= 29)(deflated 3%)
adding: cache2.txt(in = 37) (out= 35)(deflated 5%)

$ hdfs dfs -put cachedir.jar samples/cachefile

$ hdfs dfs -cat /user/root/samples/cachefile/input.txt
cachedir.jar/cache.txt
cachedir.jar/cache2.txt

$ cat test_jar/cache.txt
This is just the cache string

$ cat test_jar/cache2.txt
This is just the second cache string

$ hadoop jar hadoop-streaming.jar \
                  -archives 'hdfs://hadoop-nn1.example.com/user/root/samples/cachefile/cachedir.jar' \
                  -D mapreduce.job.maps=1 \
                  -D mapreduce.job.reduces=1 \
                  -D mapreduce.job.name="Experiment" \
                  -input "/user/root/samples/cachefile/input.txt" \
                  -output "/user/root/samples/cachefile/out" \
                  -mapper "xargs cat" \
                  -reducer "cat"

$ hdfs dfs -ls /user/root/samples/cachefile/out
Found 2 items
-rw-r--r--   1 root supergroup        0 2013-11-14 17:00 /user/root/samples/cachefile/out/_SUCCESS
-rw-r--r--   1 root supergroup       69 2013-11-14 17:00 /user/root/samples/cachefile/out/part-00000

$ hdfs dfs -cat /user/root/samples/cachefile/out/part-00000
This is just the cache string
This is just the second cache string
</code></pre>

<h2 id="section-4">复杂的例子</h2>

<h3 id="hadoop-partitioner-class">Hadoop Partitioner Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/lib/KeyFieldBasedPartitioner.html">KeyFieldBasedPartitioner</a>，可以将 map 输出的内容按照分隔后的一定列，而不是整个 key 内容进行分区，例如：</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar \
    -D stream.map.output.field.separator=. \
    -D stream.num.map.output.key.fields=4 \
    -D map.output.key.field.separator=. \
    -D mapreduce.partition.keypartitioner.options=-k1,2 \
    -D mapreduce.job.reduces=12 \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /bin/cat \
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
</code></pre>

<p>关键参数说明：</p>

<ul>
  <li><code>map.output.key.field.separator=.</code>：设置 map 输出分区时 key 内部的分割符为 <code>.</code></li>
  <li><code>mapreduce.partition.keypartitioner.options=-k1,2</code>：设置按前两个字段分区</li>
  <li><code>mapreduce.job.reduces=12</code>：reduce 数为12</li>
</ul>

<p>假设 map 的输出为：</p>

<pre><code>11.12.1.2
11.14.2.3
11.11.4.1
11.12.1.1
11.14.2.2
</code></pre>

<p>按照前两个字段进行分区，则会分为三个分区：</p>

<pre><code>11.11.4.1
-----------
11.12.1.2
11.12.1.1
-----------
11.14.2.3
11.14.2.2
</code></pre>

<p>在每个分区内对整行内容排序后为：</p>

<pre><code>11.11.4.1
-----------
11.12.1.1
11.12.1.2
-----------
11.14.2.2
11.14.2.3
</code></pre>

<h3 id="hadoop-comparator-class">Hadoop Comparator Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html">KeyFieldBasedComparator</a>，提供了 Unix/GNU 中排序的一部分特性。</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar \
    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
    -D stream.map.output.field.separator=. \
    -D stream.num.map.output.key.fields=4 \
    -D mapreduce.map.output.key.field.separator=. \
    -D mapreduce.partition.keycomparator.options=-k2,2nr \
    -D mapreduce.job.reduces=1 \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /bin/cat
</code></pre>

<p>关键参数说明：</p>

<ul>
  <li><code>mapreduce.partition.keycomparator.options=-k2,2nr</code>：指定第二个字段为排序字段，<code>-n</code> 是指按自然顺序排序，<code>-r</code> 指倒叙排序。</li>
</ul>

<p>假设 map 的输出为：</p>

<pre><code>11.12.1.2
11.14.2.3
11.11.4.1
11.12.1.1
11.14.2.2
</code></pre>

<p>则 reduce 输出结果为：</p>

<pre><code>11.14.2.3
11.14.2.2
11.12.1.2
11.12.1.1
11.11.4.1
</code></pre>

<h3 id="hadoop-aggregate-package">Hadoop Aggregate Package</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/org/apache/hadoop/mapred/lib/aggregate/package-summary.html">Aggregate</a>，Aggregate 提供了一个特定的 reduce 类和 combiner 类，以及一些对 reduce 输出的聚合函数，例如 sum、min、max 等等。</p>

<p>为了使用 Aggregate，只需要定义 <code>-reducer aggregate</code>：</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper myAggregatorForKeyCount.py \
    -reducer aggregate \
    -file myAggregatorForKeyCount.py \
</code></pre>

<p>myAggregatorForKeyCount.py  文件大概内容如下：</p>

<pre><code class="language-python">#!/usr/bin/python

import sys;

def generateLongCountToken(id):
    return "LongValueSum:" + id + "\t" + "1"

def main(argv):
    line = sys.stdin.readline();
    try:
        while line:
            line = line[:-1];
            fields = line.split("\t");
            print generateLongCountToken(fields[0]);
            line = sys.stdin.readline();
    except "end of file":
        return None
if __name__ == "__main__":
     main(sys.argv)
</code></pre>

<h3 id="hadoop-field-selection-class">Hadoop Field Selection Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.html">FieldSelectionMapReduce</a>，运行你像 unix 中的 cut 命令一样处理文本。</p>

<p>例子：</p>

<pre><code class="language-bash">hadoop jar hadoop-streaming.jar \
    -D mapreduce.map.output.key.field.separator=. \
    -D mapreduce.partition.keypartitioner.options=-k1,2 \
    -D mapreduce.fieldsel.data.field.separator=. \
    -D mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0- \
    -D mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5- \
    -D mapreduce.map.output.key.class=org.apache.hadoop.io.Text \
    -D mapreduce.job.reduces=12 \
    -input myInputDirs \
    -output myOutputDir \
    -mapper org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \
    -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
</code></pre>

<p>关键参数说明：</p>

<ul>
  <li><code>mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0-</code>：意思是 map 的输出中 key 部分包括分隔后的第 6、5、1、2、3列，而 value 部分包括分隔后的所有的列</li>
  <li><code>mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5-</code>：意思是 map 的输出中 key 部分包括分隔后的第 0、1、2列，而 value 部分包括分隔后的从第5列开始的所有列</li>
</ul>

<h1 id="section-5">测试</h1>

<p>上面讲了 Hadoop Streaming 的原理和一些用法，现在来运行一些例子做测试。关于如何用 Python 来编写 Hadoop Streaming 程序，可以参考 <a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a>，中文翻译在 <a href="http://www.tianjun.ml/essays/19/">这里</a>，其他非 Java 的语言，都可以参照这篇文章。</p>

<p>下面以 word count 为例做测试。</p>

<h2 id="section-6">准备测试数据</h2>

<p>同 <a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a>，我们使用古腾堡项目中的三本电子书作为测试：</p>

<ul>
  <li><a href="http://www.gutenberg.org/etext/20417">The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson</a></li>
  <li><a href="http://www.gutenberg.org/etext/5000">The Notebooks of Leonardo Da Vinci</a></li>
  <li><a href="http://www.gutenberg.org/etext/4300">Ulysses by James Joyce</a></li>
</ul>

<p>下载这些电子书的 txt格式，并将其上传到 hdfs：</p>

<pre><code class="language-bash">$ mkdir /tmp/gutenberg/ &amp;&amp; cd /tmp/gutenberg/

$ wget http://www.gutenberg.org/files/20417/20417.txt
$ wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt
$ wget http://www.gutenberg.org/files/4300/4300.txt

$ hadoop fs -copyFromLocal /tmp/gutenberg gutenberg

$ hadoop fs -ls gutenberg
Found 4 items
-rw-r--r--   3 hive hive     674762 2015-02-11 17:34 gutenberg/20417.txt
-rw-r--r--   3 hive hive    1573079 2015-02-11 17:34 gutenberg/4300.txt
-rw-r--r--   3 hive hive    1423803 2015-02-11 17:34 gutenberg/pg5000.txt
</code></pre>

<h2 id="shell-">编写 Shell 版程序</h2>

<p>mapper.sh 如下：</p>

<pre><code class="language-bash">#! /bin/bash

while read LINE; do
  for word in $LINE
  do
    echo "$word 1"
  done
done
</code></pre>

<p>reducer.sh 程序如下：</p>

<pre><code class="language-bash">#! /bin/bash

count=0
started=0
word=""
while read LINE;do
  newword=`echo $LINE | cut -d ' '  -f 1`
  if [ "$word" != "$newword" ];then
    [ $started -ne 0 ] &amp;&amp; echo -e "$word\t$count"
    word=$newword
    count=1
    started=1
  else
    count=$(( $count + 1 ))
  fi
done
echo -e "$word\t$count"
</code></pre>

<p>在本机以脚本方式测试：</p>

<pre><code class="language-bash">$ echo "foo foo quux labs foo bar quux" | sh mapper.sh  |sort -k1,1| sh reducer.sh
bar 1
foo 3
labs    1
quux    2
</code></pre>

<p>以 Hadoop Streaming 方式运行：</p>

<pre><code class="language-bash">$ hadoop  jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.reduce.tasks=6 \
    -input gutenberg/* \
    -output gutenberg-output \
    -mapper mapper.sh\
    -reducer reducer.sh\
    -file mapper.sh \
    -file reducer.sh

15/02/11 17:50:59 INFO mapreduce.Job:  map 0% reduce 0%
15/02/11 17:51:18 INFO mapreduce.Job:  map 17% reduce 0%
15/02/11 17:51:52 INFO mapreduce.Job:  map 17% reduce 6%
15/02/11 17:51:53 INFO mapreduce.Job:  map 33% reduce 6%
15/02/11 17:51:55 INFO mapreduce.Job:  map 60% reduce 17%
15/02/11 17:51:56 INFO mapreduce.Job:  map 100% reduce 17%
15/02/11 17:51:59 INFO mapreduce.Job:  map 100% reduce 67%
15/02/11 17:53:11 INFO mapreduce.Job:  map 100% reduce 68%
15/02/11 17:54:49 INFO mapreduce.Job:  map 100% reduce 69%
15/02/11 17:57:12 INFO mapreduce.Job:  map 100% reduce 70%
15/02/11 17:58:45 INFO mapreduce.Job:  map 100% reduce 71%
15/02/11 17:58:55 INFO mapreduce.Job:  map 100% reduce 81%
15/02/11 17:59:05 INFO mapreduce.Job:  map 100% reduce 100%
15/02/11 17:59:08 INFO streaming.StreamJob: Job complete: job_1421752803837_5736
15/02/11 17:59:09 INFO streaming.StreamJob: Output: /user/root/gutenberg-output
</code></pre>

<h2 id="python-">编写 Python 版程序</h2>

<p>mapper.py 程序如下：</p>

<pre><code class="language-python">#!/usr/bin/env python
"""A more advanced Mapper, using Python iterators and generators."""

import sys

def read_input(file):
    for line in file:
        # split the line into words
        yield line.split()

def main(separator='\t'):
    # input comes from STDIN (standard input)
    data = read_input(sys.stdin)
    for words in data:
        # write the results to STDOUT (standard output);
        # what we output here will be the input for the
        # Reduce step, i.e. the input for reducer.py
        #
        # tab-delimited; the trivial word count is 1
        for word in words:
            print '%s%s%d' % (word, separator, 1)

if __name__ == "__main__":
    main()
</code></pre>

<p>reducer.py 程序如下：</p>

<pre><code class="language-python">#!/usr/bin/env python
"""A more advanced Reducer, using Python iterators and generators."""

from itertools import groupby
from operator import itemgetter
import sys

def read_mapper_output(file, separator='\t'):
    for line in file:
        yield line.rstrip().split(separator, 1)

def main(separator='\t'):
    # input comes from STDIN (standard input)
    data = read_mapper_output(sys.stdin, separator=separator)
    # groupby groups multiple word-count pairs by word,
    # and creates an iterator that returns consecutive keys and their group:
    #   current_word - string containing a word (the key)
    #   group - iterator yielding all ["&amp;lt;current_word&amp;gt;", "&amp;lt;count&amp;gt;"] items
    for current_word, group in groupby(data, itemgetter(0)):
        try:
            total_count = sum(int(count) for current_word, count in group)
            print "%s%s%d" % (current_word, separator, total_count)
        except ValueError:
            # count was not a number, so silently discard this item
            pass

if __name__ == "__main__":
    main()
</code></pre>

<p>关于 Java 的一些例子，这个需要单独创建一个 maven 工程，然后做一些测试。</p>

<h1 id="section-7">注意事项</h1>

<h3 id="mapper--shell-">mapper 中不能使用 shell 的别名，但可以使用变量</h3>

<pre><code class="language-bash">$ hdfs dfs -cat /user/me/samples/student_marks
alice   50
bruce   70
charlie 80
dan     75

$ c2='cut -f2'; hadoop jar hadoop-streaming-2.6.0.jar \
    -D mapreduce.job.name='Experiment' \
    -input /user/me/samples/student_marks \
    -output /user/me/samples/student_out \
    -mapper "$c2" -reducer 'cat'

$ hdfs dfs -cat /user/me/samples/student_out/part-00000
50
70
75
80
</code></pre>

<h3 id="mapper--unix-">mapper 中不能使用 unix 的管道</h3>

<table>
  <tbody>
    <tr>
      <td><code>-mapper</code> 中使用 “cut -f1</td>
      <td>sed s/foo/bar/g”，会出现 <code>java.io.IOException: Broken pipe</code> 异常</td>
    </tr>
  </tbody>
</table>

<h3 id="streaming--1">指定 streaming 临时空间</h3>

<pre><code class="language-bash">-D stream.tmpdir=/export/bigspace/...
</code></pre>

<h3 id="section-8">指定多个输入文件</h3>

<pre><code class="language-bash">hadoop jar hadoop-streaming-2.6.0.jar \
    -input '/user/foo/dir1' -input '/user/foo/dir2' \
    (rest of the command)
</code></pre>

<h3 id="xml">处理 XML</h3>

<pre><code class="language-bash">hadoop jar hadoop-streaming-2.6.0.jar \
    -inputreader "StreamXmlRecord,begin=BEGIN_STRING,end=END_STRING" \
    (rest of the command)
</code></pre>

<p>BEGIN_STRING 和 END_STRING 之前的内容会被认为是 map 任务的一条记录。</p>

<h1 id="section-9">参考文章</h1>

<ul>
  <li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html#More_Usage_Examples">Hadoop Streaming</a></li>
  <li><a href="http://dongxicheng.org/mapreduce/hadoop-streaming-programming/">Hadoop Streaming 编程</a></li>
  <li><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a></li>
</ul>

                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">Junez</a><br/>
                        本文链接地址：<a href="/2015/02/12/hadoop-streaming.html">http://blog.javachen.com/2015/02/12/hadoop-streaming.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2015/02/12/hadoop-streaming.html">Hadoop Streaming 原理</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#hadoop">hadoop</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#hadoop">hadoop</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#mapreduce">mapreduce</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#streaming">streaming</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2015/02/12/hadoop-streaming.html" data-url="http://blog.javachen.com/2015/02/12/hadoop-streaming.html" data-title="Hadoop Streaming 原理"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2015 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">Junez</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
            <script type="text/javascript">
                var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
                document.write(unescape("%3Cspan id='cnzz_stat_icon_1256628929'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1256628929' type='text/javascript'%3E%3C/script%3E"));</script>
            
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2015/02/12/hadoop-streaming.html'
      });
      </script>
  </body>
</html>
