<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>Hadoop Streaming 原理 - JavaChen Blog</title>
      <meta name="author" content="JavaChen"/>
      <meta name="description" content="Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的优势和能力，来处理大数据。">
      <meta name="keywords" content="java,spring,spring mvc,spring boot,mybatis,hibernate,jdbc,hadoop,hive,mapreduce,hbase,hdfs,spark,yarn,impala,presto,impala,sentry,zookeeper,pentaho,kettle,solr,scala,python,javascript,angular,jquery,postgresql">
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2015/02/12/hadoop-streaming.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2015/02/10/useful-commands-in-hadoop.html" title="Useful Hadoop Commands"><i class="fa fa-angle-double-left"></i>&nbsp;Useful Hadoop Commands</a></li>
                
                
                <li class="next"><a href="/2015/02/28/install-and-config-hue.html" title="安装和配置Hue">安装和配置Hue&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> Hadoop Streaming 原理  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2015.02.12 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <h1 id="section">简介</h1>

<p>Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的优势和能力，来处理大数据。</p>

<p>一个简单的示例，以 shell 脚本为例：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /usr/bin/wc
</code></pre>
</div>

<p>Streaming 方式是 <code class="highlighter-rouge">基于 Unix 系统的标准输入输出</code> 来进行 MapReduce Job 的运行，它区别与 Pipes 的地方主要是通信协议，Pipes 使用的是 Socket 通信，是对使用 C++ 语言来实现 MapReduce Job 并通过 Socket 通信来与 Hadopp 平台通信，完成 Job 的执行。</p>

<p>任何支持标准输入输出特性的编程语言都可以使用 Streaming 方式来实现 MapReduce Job，基本原理就是输入从 Unix 系统标准输入，输出使用 Unix 系统的标准输出。</p>

<p>Hadoop 是使用 Java 语言编写的，所以最直接的方式的就是使用 Java 语言来实现 Mapper 和 Reducer，然后配置 MapReduce Job，提交到集群计算环境来完成计算。但是很多开发者可能对 Java 并不熟悉，而是对一些具有脚本特性的语言，如 C++、Shell、Python、 Ruby、PHP、Perl 有实际开发经验，Hadoop Streaming 为这一类开发者提供了使用 Hadoop 集群来进行处理数据的工具，即工具包 hadoop-streaming.jar。</p>

<p>在标准的输入输出中，Key 和 Value 是以 Tab 作为分隔符，并且在 Reducer 的标准输入中，Hadoop 框架保证了输入的数据是经过了按 Key 排序的。</p>

<h1 id="section-1">原理</h1>

<p>Hadoop Streaming 使用了 Unix 的标准输入输出作为 Hadoop 和其他编程语言的开发接口，因此在其他的编程语言所写的程序中，只需要将标准输入作为程序的输入，将标准输出作为程序的输出就可以了。</p>

<p>mapper 和 reducer 会从标准输入中读取用户数据，一行一行处理后发送给标准输出。Streaming 工具会创建 MapReduce 作业，发送给各个 tasktracker，同时监控整个作业的执行过程。</p>

<p>如果一个文件（可执行或者脚本）作为 mapper，mapper 初始化时，每一个 mapper 任务会把该文件作为一个单独进程启动，mapper 任务运行时，它把输入切分成行并把每一行提供给可执行文件进程的标准输入。 同时，mapper 收集可执行文件进程标准输出的内容，并把收到的每一行内容转化成 key/value 对，作为 mapper 的输出。 默认情况下，一行中第一个 tab 之前的部分作为 key，之后的（不包括tab）作为 value。如果没有 tab，整行作为 key 值，value 值为 null。</p>

<p>对于 reducer，类似。</p>

<p>以上是 Map/Reduce 框架和 streaming mapper/reducer 之间的基本通信协议。</p>

<p>用户可以定义 <code class="highlighter-rouge">stream.non.zero.exit.is.failure</code> 参数为 true 或者 false 以定义一个以非0状态退出的 streaming 的任务是失败还是成功。默认情况下，以非0状态退出的任务都任务是失败的。</p>

<h1 id="section-2">用法</h1>

<p>命令如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="o">[</span>genericOptions] <span class="o">[</span>streamingOptions]
</code></pre>
</div>

<h2 id="streaming-">streaming 参数</h2>

<p>以 Hadoop 2.6.0 为例，可选的 streaming 参数如下：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">参数</th>
      <th style="text-align: left">是否可选</th>
      <th style="text-align: left">描述</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-input directoryname or filename</code></td>
      <td style="text-align: left">Required</td>
      <td style="text-align: left">mapper的输入路径</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-output directoryname</code></td>
      <td style="text-align: left">Required</td>
      <td style="text-align: left">reducer输出路径</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-mapper executable or JavaClassName</code></td>
      <td style="text-align: left">Required</td>
      <td style="text-align: left">Mapper可执行程序或 Java 类名</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-reducer executable or JavaClassName</code></td>
      <td style="text-align: left">Required</td>
      <td style="text-align: left">Reducer 可执行程序或 Java 类名</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-file filename</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">mapper, reducer 或 combiner 依赖的文件</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-inputformat JavaClassName</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">key/value 输入格式，默认为 TextInputFormat</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-outputformat JavaClassName</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">key/value 输出格式，默认为  TextOutputformat</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-partitioner JavaClassName</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">Class that determines which reduce a key is sent to</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-combiner streamingCommand or JavaClassName</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">map 输出结果执行 Combiner 的命令或者类名</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-cmdenv name=value</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">环境变量</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-inputreader</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">向后兼容，定义输入的 Reader 类，用于取代输出格式</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-verbose</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">输出日志</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-lazyOutput</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">延时输出</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-numReduceTasks</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义 reduce 数量</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-mapdebug</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">map 任务运行失败时候，执行的脚本</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-reducedebug</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">reduce 任务运行失败时候，执行的脚本</td>
    </tr>
  </tbody>
</table>

<p>定义 Java 类作为 mapper 和 reducer：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat <span class="se">\</span>
    -mapper org.apache.hadoop.mapred.lib.IdentityMapper <span class="se">\</span>
    -reducer /usr/bin/wc
</code></pre>
</div>

<p>如果 mapper 和 reducer 的可执行文件在集群上不存在，则可以通过  <code class="highlighter-rouge">-file</code> 参数将其提交到集群上去：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper myPythonScript.py <span class="se">\</span>
    -reducer /usr/bin/wc <span class="se">\</span>
    -file myPythonScript.py
</code></pre>
</div>

<p>你也可以将 mapper 和 reducer 的可执行文件用到的文件和配置上传到集群上：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper myPythonScript.py <span class="se">\</span>
    -reducer /usr/bin/wc <span class="se">\</span>
    -file myPythonScript.py <span class="se">\</span>
    -file myDictionary.txt
</code></pre>
</div>

<p>你也可以定义其他参数：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>-inputformat JavaClassName
-outputformat JavaClassName
-partitioner JavaClassName
-combiner streamingCommand or JavaClassName
</code></pre>
</div>

<p>定义一个环境变量：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>-cmdenv <span class="nv">EXAMPLE_DIR</span><span class="o">=</span>/home/example/dictionaries/   
</code></pre>
</div>

<h2 id="section-3">通用参数</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">参数</th>
      <th style="text-align: left">是否可选</th>
      <th style="text-align: left">描述</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-conf configuration_file</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义应用的配置文件</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-D property=value</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义参数</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-fs host:port or local</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义 namenode 地址</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-files</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义需要拷贝到 Map/Reduce 集群的文件，多个文件以逗号分隔</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-libjars</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义需要引入到 classpath 的 jar 文件，多个文件以逗号分隔</td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">-archives</code></td>
      <td style="text-align: left">Optional</td>
      <td style="text-align: left">定义需要解压到计算节点的压缩文件，多个文件以逗号分隔</td>
    </tr>
  </tbody>
</table>

<p>定义参数：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>-D mapred.local.dir<span class="o">=</span>/tmp/local
-D mapred.system.dir<span class="o">=</span>/tmp/system
-D mapred.temp.dir<span class="o">=</span>/tmp/temp
</code></pre>
</div>

<p>定义 reduce 个数：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>-D mapreduce.job.reduces<span class="o">=</span>0
</code></pre>
</div>

<p>你也可以使用 <code class="highlighter-rouge">-D stream.reduce.output.field.separator=SEP</code> 和 <code class="highlighter-rouge">-D stream.num.reduce.output.fields=NUM</code> 自定义 mapper 输出的分隔符为SEP，并且按 SEP 分隔之后的前 NUM 部分内容作为 key，如果分隔符少于 NUM，则整行作为 key。例如，下面的例子指定分隔符为 <code class="highlighter-rouge">....</code>：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -D stream.map.output.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D stream.num.map.output.key.fields<span class="o">=</span>4 <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper /bin/cat <span class="se">\</span>
    -reducer /bin/cat
</code></pre>
</div>

<p>hadoop 提供配置供用户自主设置分隔符：</p>

<p><code class="highlighter-rouge">-D stream.map.output.field.separator</code> ：设置 map 输出中 key 和 value 的分隔符 
<code class="highlighter-rouge">-D stream.num.map.output.key.fields</code> ：设置 map 程序分隔符的位置，该位置之前的部分作为 key，之后的部分作为 value 
<code class="highlighter-rouge">-D map.output.key.field.separator</code> : 设置 map 输出分区时 key 内部的分割符
<code class="highlighter-rouge">-D mapreduce.partition.keypartitioner.options</code> : 指定分桶时，key 按照分隔符切割后，其中用于分桶 key 所占的列数（配合 <code class="highlighter-rouge">-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner</code> 使用）
<code class="highlighter-rouge">-D stream.reduce.output.field.separator</code>：设置 reduce 输出中 key 和 value 的分隔符 
<code class="highlighter-rouge">-D stream.num.reduce.output.key.fields</code>：设置 reduce 程序分隔符的位置</p>

<p>定义解压文件：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ls test_jar/
cache.txt  cache2.txt

<span class="gp">$ </span>jar cvf cachedir.jar -C test_jar/ .
added manifest
adding: cache.txt<span class="o">(</span><span class="k">in</span> <span class="o">=</span> 30<span class="o">)</span> <span class="o">(</span><span class="nv">out</span><span class="o">=</span> 29<span class="o">)(</span>deflated 3%<span class="o">)</span>
adding: cache2.txt<span class="o">(</span><span class="k">in</span> <span class="o">=</span> 37<span class="o">)</span> <span class="o">(</span><span class="nv">out</span><span class="o">=</span> 35<span class="o">)(</span>deflated 5%<span class="o">)</span>

<span class="gp">$ </span>hdfs dfs -put cachedir.jar samples/cachefile

<span class="gp">$ </span>hdfs dfs -cat /user/root/samples/cachefile/input.txt
cachedir.jar/cache.txt
cachedir.jar/cache2.txt

<span class="gp">$ </span>cat test_jar/cache.txt
This is just the cache string

<span class="gp">$ </span>cat test_jar/cache2.txt
This is just the second cache string

<span class="gp">$ </span>hadoop jar hadoop-streaming.jar <span class="se">\</span>
                  -archives <span class="s1">'hdfs://hadoop-nn1.example.com/user/root/samples/cachefile/cachedir.jar'</span> <span class="se">\</span>
                  -D mapreduce.job.maps<span class="o">=</span>1 <span class="se">\</span>
                  -D mapreduce.job.reduces<span class="o">=</span>1 <span class="se">\</span>
                  -D mapreduce.job.name<span class="o">=</span><span class="s2">"Experiment"</span> <span class="se">\</span>
                  -input <span class="s2">"/user/root/samples/cachefile/input.txt"</span> <span class="se">\</span>
                  -output <span class="s2">"/user/root/samples/cachefile/out"</span> <span class="se">\</span>
                  -mapper <span class="s2">"xargs cat"</span> <span class="se">\</span>
                  -reducer <span class="s2">"cat"</span>

<span class="gp">$ </span>hdfs dfs -ls /user/root/samples/cachefile/out
Found 2 items
-rw-r--r--   1 root supergroup        0 2013-11-14 17:00 /user/root/samples/cachefile/out/_SUCCESS
-rw-r--r--   1 root supergroup       69 2013-11-14 17:00 /user/root/samples/cachefile/out/part-00000

<span class="gp">$ </span>hdfs dfs -cat /user/root/samples/cachefile/out/part-00000
This is just the cache string
This is just the second cache string
</code></pre>
</div>

<h2 id="section-4">复杂的例子</h2>

<h3 id="hadoop-partitioner-class">Hadoop Partitioner Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/lib/KeyFieldBasedPartitioner.html">KeyFieldBasedPartitioner</a>，可以将 map 输出的内容按照分隔后的一定列，而不是整个 key 内容进行分区，例如：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -D stream.map.output.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D stream.num.map.output.key.fields<span class="o">=</span>4 <span class="se">\</span>
    -D map.output.key.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D mapreduce.partition.keypartitioner.options<span class="o">=</span>-k1,2 <span class="se">\</span>
    -D mapreduce.job.reduces<span class="o">=</span>12 <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper /bin/cat <span class="se">\</span>
    -reducer /bin/cat <span class="se">\</span>
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
</code></pre>
</div>

<p>关键参数说明：</p>

<ul>
  <li><code class="highlighter-rouge">map.output.key.field.separator=.</code>：设置 map 输出分区时 key 内部的分割符为 <code class="highlighter-rouge">.</code></li>
  <li><code class="highlighter-rouge">mapreduce.partition.keypartitioner.options=-k1,2</code>：设置按前两个字段分区</li>
  <li><code class="highlighter-rouge">mapreduce.job.reduces=12</code>：reduce 数为12</li>
</ul>

<p>假设 map 的输出为：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>11.12.1.2
11.14.2.3
11.11.4.1
11.12.1.1
11.14.2.2
</code></pre>
</div>

<p>按照前两个字段进行分区，则会分为三个分区：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>11.11.4.1
-----------
11.12.1.2
11.12.1.1
-----------
11.14.2.3
11.14.2.2
</code></pre>
</div>

<p>在每个分区内对整行内容排序后为：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>11.11.4.1
-----------
11.12.1.1
11.12.1.2
-----------
11.14.2.2
11.14.2.3
</code></pre>
</div>

<h3 id="hadoop-comparator-class">Hadoop Comparator Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html">KeyFieldBasedComparator</a>，提供了 Unix/GNU 中排序的一部分特性。</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -D mapreduce.job.output.key.comparator.class<span class="o">=</span>org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator <span class="se">\</span>
    -D stream.map.output.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D stream.num.map.output.key.fields<span class="o">=</span>4 <span class="se">\</span>
    -D mapreduce.map.output.key.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D mapreduce.partition.keycomparator.options<span class="o">=</span>-k2,2nr <span class="se">\</span>
    -D mapreduce.job.reduces<span class="o">=</span>1 <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper /bin/cat <span class="se">\</span>
    -reducer /bin/cat
</code></pre>
</div>

<p>关键参数说明：</p>

<ul>
  <li><code class="highlighter-rouge">mapreduce.partition.keycomparator.options=-k2,2nr</code>：指定第二个字段为排序字段，<code class="highlighter-rouge">-n</code> 是指按自然顺序排序，<code class="highlighter-rouge">-r</code> 指倒叙排序。</li>
</ul>

<p>假设 map 的输出为：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>11.12.1.2
11.14.2.3
11.11.4.1
11.12.1.1
11.14.2.2
</code></pre>
</div>

<p>则 reduce 输出结果为：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>11.14.2.3
11.14.2.2
11.12.1.2
11.12.1.1
11.11.4.1
</code></pre>
</div>

<h3 id="hadoop-aggregate-package">Hadoop Aggregate Package</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/org/apache/hadoop/mapred/lib/aggregate/package-summary.html">Aggregate</a>，Aggregate 提供了一个特定的 reduce 类和 combiner 类，以及一些对 reduce 输出的聚合函数，例如 sum、min、max 等等。</p>

<p>为了使用 Aggregate，只需要定义 <code class="highlighter-rouge">-reducer aggregate</code>：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper myAggregatorForKeyCount.py <span class="se">\</span>
    -reducer aggregate <span class="se">\</span>
    -file myAggregatorForKeyCount.py <span class="se">\</span>
</code></pre>
</div>

<p>myAggregatorForKeyCount.py  文件大概内容如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/usr/bin/python</span>

<span class="kn">import</span> <span class="nn">sys</span><span class="p">;</span>

<span class="k">def</span> <span class="nf">generateLongCountToken</span><span class="p">(</span><span class="nb">id</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">"LongValueSum:"</span> <span class="o">+</span> <span class="nb">id</span> <span class="o">+</span> <span class="s">"</span><span class="se">\t</span><span class="s">"</span> <span class="o">+</span> <span class="s">"1"</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">argv</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">readline</span><span class="p">();</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">while</span> <span class="n">line</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span>
            <span class="n">fields</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">);</span>
            <span class="k">print</span> <span class="n">generateLongCountToken</span><span class="p">(</span><span class="n">fields</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
            <span class="n">line</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">readline</span><span class="p">();</span>
    <span class="k">except</span> <span class="s">"end of file"</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
     <span class="n">main</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="hadoop-field-selection-class">Hadoop Field Selection Class</h3>

<p>Hadoop 中有一个类 <a href="http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.html">FieldSelectionMapReduce</a>，运行你像 unix 中的 cut 命令一样处理文本。</p>

<p>例子：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming.jar <span class="se">\</span>
    -D mapreduce.map.output.key.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D mapreduce.partition.keypartitioner.options<span class="o">=</span>-k1,2 <span class="se">\</span>
    -D mapreduce.fieldsel.data.field.separator<span class="o">=</span>. <span class="se">\</span>
    -D mapreduce.fieldsel.map.output.key.value.fields.spec<span class="o">=</span>6,5,1-3:0- <span class="se">\</span>
    -D mapreduce.fieldsel.reduce.output.key.value.fields.spec<span class="o">=</span>0-2:5- <span class="se">\</span>
    -D mapreduce.map.output.key.class<span class="o">=</span>org.apache.hadoop.io.Text <span class="se">\</span>
    -D mapreduce.job.reduces<span class="o">=</span>12 <span class="se">\</span>
    -input myInputDirs <span class="se">\</span>
    -output myOutputDir <span class="se">\</span>
    -mapper org.apache.hadoop.mapred.lib.FieldSelectionMapReduce <span class="se">\</span>
    -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce <span class="se">\</span>
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
</code></pre>
</div>

<p>关键参数说明：</p>

<ul>
  <li><code class="highlighter-rouge">mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0-</code>：意思是 map 的输出中 key 部分包括分隔后的第 6、5、1、2、3列，而 value 部分包括分隔后的所有的列</li>
  <li><code class="highlighter-rouge">mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5-</code>：意思是 map 的输出中 key 部分包括分隔后的第 0、1、2列，而 value 部分包括分隔后的从第5列开始的所有列</li>
</ul>

<h1 id="section-5">测试</h1>

<p>上面讲了 Hadoop Streaming 的原理和一些用法，现在来运行一些例子做测试。关于如何用 Python 来编写 Hadoop Streaming 程序，可以参考 <a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a>，中文翻译在 <a href="http://www.tianjun.ml/essays/19/">这里</a>，其他非 Java 的语言，都可以参照这篇文章。</p>

<p>下面以 word count 为例做测试。</p>

<h2 id="section-6">准备测试数据</h2>

<p>同 <a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a>，我们使用古腾堡项目中的三本电子书作为测试：</p>

<ul>
  <li><a href="http://www.gutenberg.org/etext/20417">The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson</a></li>
  <li><a href="http://www.gutenberg.org/etext/5000">The Notebooks of Leonardo Da Vinci</a></li>
  <li><a href="http://www.gutenberg.org/etext/4300">Ulysses by James Joyce</a></li>
</ul>

<p>下载这些电子书的 txt格式，并将其上传到 hdfs：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>mkdir /tmp/gutenberg/ <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /tmp/gutenberg/

<span class="gp">$ </span>wget http://www.gutenberg.org/files/20417/20417.txt
<span class="gp">$ </span>wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt
<span class="gp">$ </span>wget http://www.gutenberg.org/files/4300/4300.txt

<span class="gp">$ </span>hadoop fs -copyFromLocal /tmp/gutenberg gutenberg

<span class="gp">$ </span>hadoop fs -ls gutenberg
Found 4 items
-rw-r--r--   3 hive hive     674762 2015-02-11 17:34 gutenberg/20417.txt
-rw-r--r--   3 hive hive    1573079 2015-02-11 17:34 gutenberg/4300.txt
-rw-r--r--   3 hive hive    1423803 2015-02-11 17:34 gutenberg/pg5000.txt
</code></pre>
</div>

<h2 id="shell-">编写 Shell 版程序</h2>

<p>mapper.sh 如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#! /bin/bash</span>

<span class="k">while </span><span class="nb">read </span>LINE; <span class="k">do
  for </span>word <span class="k">in</span> <span class="nv">$LINE</span>
  <span class="k">do
    </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$word</span><span class="s2"> 1"</span>
  <span class="k">done
done</span>
</code></pre>
</div>

<p>reducer.sh 程序如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#! /bin/bash</span>

<span class="nv">count</span><span class="o">=</span>0
<span class="nv">started</span><span class="o">=</span>0
<span class="nv">word</span><span class="o">=</span><span class="s2">""</span>
<span class="k">while </span><span class="nb">read </span>LINE;do
  <span class="nv">newword</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span> <span class="nv">$LINE</span> | cut -d <span class="s1">' '</span>  -f 1<span class="sb">`</span>
  <span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$word</span><span class="s2">"</span> !<span class="o">=</span> <span class="s2">"</span><span class="nv">$newword</span><span class="s2">"</span> <span class="o">]</span>;<span class="k">then</span>
    <span class="o">[</span> <span class="nv">$started</span> -ne 0 <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="nb">echo</span> -e <span class="s2">"</span><span class="nv">$word</span><span class="se">\t</span><span class="nv">$count</span><span class="s2">"</span>
    <span class="nv">word</span><span class="o">=</span><span class="nv">$newword</span>
    <span class="nv">count</span><span class="o">=</span>1
    <span class="nv">started</span><span class="o">=</span>1
  <span class="k">else
    </span><span class="nv">count</span><span class="o">=</span><span class="k">$((</span> <span class="nv">$count</span> <span class="o">+</span> <span class="m">1</span> <span class="k">))</span>
  <span class="k">fi
done
</span><span class="nb">echo</span> -e <span class="s2">"</span><span class="nv">$word</span><span class="se">\t</span><span class="nv">$count</span><span class="s2">"</span>
</code></pre>
</div>

<p>在本机以脚本方式测试：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">echo</span> <span class="s2">"foo foo quux labs foo bar quux"</span> | sh mapper.sh  |sort -k1,1| sh reducer.sh
bar 1
foo 3
labs    1
quux    2
</code></pre>
</div>

<p>以 Hadoop Streaming 方式运行：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hadoop  jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar <span class="se">\</span>
    -D mapred.reduce.tasks<span class="o">=</span>6 <span class="se">\</span>
    -input gutenberg/<span class="k">*</span> <span class="se">\</span>
    -output gutenberg-output <span class="se">\</span>
    -mapper mapper.sh<span class="se">\</span>
    -reducer reducer.sh<span class="se">\</span>
    -file mapper.sh <span class="se">\</span>
    -file reducer.sh

15/02/11 17:50:59 INFO mapreduce.Job:  map 0% reduce 0%
15/02/11 17:51:18 INFO mapreduce.Job:  map 17% reduce 0%
15/02/11 17:51:52 INFO mapreduce.Job:  map 17% reduce 6%
15/02/11 17:51:53 INFO mapreduce.Job:  map 33% reduce 6%
15/02/11 17:51:55 INFO mapreduce.Job:  map 60% reduce 17%
15/02/11 17:51:56 INFO mapreduce.Job:  map 100% reduce 17%
15/02/11 17:51:59 INFO mapreduce.Job:  map 100% reduce 67%
15/02/11 17:53:11 INFO mapreduce.Job:  map 100% reduce 68%
15/02/11 17:54:49 INFO mapreduce.Job:  map 100% reduce 69%
15/02/11 17:57:12 INFO mapreduce.Job:  map 100% reduce 70%
15/02/11 17:58:45 INFO mapreduce.Job:  map 100% reduce 71%
15/02/11 17:58:55 INFO mapreduce.Job:  map 100% reduce 81%
15/02/11 17:59:05 INFO mapreduce.Job:  map 100% reduce 100%
15/02/11 17:59:08 INFO streaming.StreamJob: Job <span class="nb">complete</span>: job_1421752803837_5736
15/02/11 17:59:09 INFO streaming.StreamJob: Output: /user/root/gutenberg-output
</code></pre>
</div>

<h2 id="python-">编写 Python 版程序</h2>

<p>mapper.py 程序如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/usr/bin/env python</span>
<span class="s">"""A more advanced Mapper, using Python iterators and generators."""</span>

<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">read_input</span><span class="p">(</span><span class="nb">file</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">file</span><span class="p">:</span>
        <span class="c"># split the line into words</span>
        <span class="k">yield</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">separator</span><span class="o">=</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">):</span>
    <span class="c"># input comes from STDIN (standard input)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">read_input</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="c"># write the results to STDOUT (standard output);</span>
        <span class="c"># what we output here will be the input for the</span>
        <span class="c"># Reduce step, i.e. the input for reducer.py</span>
        <span class="c">#</span>
        <span class="c"># tab-delimited; the trivial word count is 1</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">'</span><span class="si">%</span><span class="s">s</span><span class="si">%</span><span class="s">s</span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">separator</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre>
</div>

<p>reducer.py 程序如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/usr/bin/env python</span>
<span class="s">"""A more advanced Reducer, using Python iterators and generators."""</span>

<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">groupby</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">read_mapper_output</span><span class="p">(</span><span class="nb">file</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">file</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">separator</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">separator</span><span class="o">=</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">):</span>
    <span class="c"># input comes from STDIN (standard input)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">read_mapper_output</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="n">separator</span><span class="p">)</span>
    <span class="c"># groupby groups multiple word-count pairs by word,</span>
    <span class="c"># and creates an iterator that returns consecutive keys and their group:</span>
    <span class="c">#   current_word - string containing a word (the key)</span>
    <span class="c">#   group - iterator yielding all ["&amp;lt;current_word&amp;gt;", "&amp;lt;count&amp;gt;"] items</span>
    <span class="k">for</span> <span class="n">current_word</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">groupby</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">itemgetter</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">total_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="k">for</span> <span class="n">current_word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">group</span><span class="p">)</span>
            <span class="k">print</span> <span class="s">"</span><span class="si">%</span><span class="s">s</span><span class="si">%</span><span class="s">s</span><span class="si">%</span><span class="s">d"</span> <span class="o">%</span> <span class="p">(</span><span class="n">current_word</span><span class="p">,</span> <span class="n">separator</span><span class="p">,</span> <span class="n">total_count</span><span class="p">)</span>
        <span class="k">except</span> <span class="nb">ValueError</span><span class="p">:</span>
            <span class="c"># count was not a number, so silently discard this item</span>
            <span class="k">pass</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre>
</div>

<p>关于 Java 的一些例子，这个需要单独创建一个 maven 工程，然后做一些测试。</p>

<h1 id="section-7">注意事项</h1>

<h3 id="mapper--shell-">mapper 中不能使用 shell 的别名，但可以使用变量</h3>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs dfs -cat /user/me/samples/student_marks
alice   50
bruce   70
charlie 80
dan     75

<span class="gp">$ </span><span class="nv">c2</span><span class="o">=</span><span class="s1">'cut -f2'</span>; hadoop jar hadoop-streaming-2.6.0.jar <span class="se">\</span>
    -D mapreduce.job.name<span class="o">=</span><span class="s1">'Experiment'</span> <span class="se">\</span>
    -input /user/me/samples/student_marks <span class="se">\</span>
    -output /user/me/samples/student_out <span class="se">\</span>
    -mapper <span class="s2">"</span><span class="nv">$c2</span><span class="s2">"</span> -reducer <span class="s1">'cat'</span>

<span class="gp">$ </span>hdfs dfs -cat /user/me/samples/student_out/part-00000
50
70
75
80
</code></pre>
</div>

<h3 id="mapper--unix-">mapper 中不能使用 unix 的管道</h3>

<table>
  <tbody>
    <tr>
      <td><code class="highlighter-rouge">-mapper</code> 中使用 “cut -f1</td>
      <td>sed s/foo/bar/g”，会出现 <code class="highlighter-rouge">java.io.IOException: Broken pipe</code> 异常</td>
    </tr>
  </tbody>
</table>

<h3 id="streaming--1">指定 streaming 临时空间</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>-D stream.tmpdir<span class="o">=</span>/export/bigspace/...
</code></pre>
</div>

<h3 id="section-8">指定多个输入文件</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming-2.6.0.jar <span class="se">\</span>
    -input <span class="s1">'/user/foo/dir1'</span> -input <span class="s1">'/user/foo/dir2'</span> <span class="se">\</span>
    <span class="o">(</span>rest of the <span class="nb">command</span><span class="o">)</span>
</code></pre>
</div>

<h3 id="xml">处理 XML</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>hadoop jar hadoop-streaming-2.6.0.jar <span class="se">\</span>
    -inputreader <span class="s2">"StreamXmlRecord,begin=BEGIN_STRING,end=END_STRING"</span> <span class="se">\</span>
    <span class="o">(</span>rest of the <span class="nb">command</span><span class="o">)</span>
</code></pre>
</div>

<p>BEGIN_STRING 和 END_STRING 之前的内容会被认为是 map 任务的一条记录。</p>

<h1 id="section-9">参考文章</h1>

<ul>
  <li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html#More_Usage_Examples">Hadoop Streaming</a></li>
  <li><a href="http://dongxicheng.org/mapreduce/hadoop-streaming-programming/">Hadoop Streaming 编程</a></li>
  <li><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing an Hadoop MapReduce Program in Python</a></li>
</ul>

                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">JavaChen</a><br/>
                        本文链接地址：<a href="/2015/02/12/hadoop-streaming.html">http://blog.javachen.com/2015/02/12/hadoop-streaming.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2015/02/12/hadoop-streaming.html">Hadoop Streaming 原理</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#hadoop">hadoop</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#hadoop">hadoop</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#mapreduce">mapreduce</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#streaming">streaming</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2015/02/12/hadoop-streaming.html" data-url="http://blog.javachen.com/2015/02/12/hadoop-streaming.html" data-title="Hadoop Streaming 原理"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2016 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">JavaChen</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            

            
            <script>
              var _hmt = _hmt || [];
              (function() {
                var hm = document.createElement("script");
                hm.src = "//hm.baidu.com/hm.js?50bc6f5d9b045b5895ff44f8bbdbc611";
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(hm, s);
              })();
            </script>
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2015/02/12/hadoop-streaming.html'
      });
      </script>
  </body>
</html>
