<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JavaChen Blog</title>
  <link href="//atom.xml" rel="self"/>
  <link href=""/>
  <updated>2015-10-25T18:53:05+08:00</updated>
  <id></id>
  <author>
    <name>yuke</name>
  </author>
  
  <entry>
    <title>Bash内部变量</title>
    <link href="/2015/07/09/bash-internal-variables.html"/>
    <updated>2015-07-09T00:00:00+08:00</updated>
    <id>/2015/07/09/bash-internal-variables.html</id>
    <content type="html">Bash中存在一些内部变量。

$BASH

Bash的二进制程序文件的路径。

$ echo $BASH
/bin/bash


$BASH_ENV

这个环境变量会指向一个Bash的启动文件，当一个脚本被调用的时候，这个启动文件将会被读取。

$BASH_SUBSHELL

这个变量用来提示子shell的层次。这是一个Bash的新特性，直到版本3的Bash才被引入近来。

#!/bin/bash
# subshell.sh

echo &quot;Subshell level OUTSIDE subshell = $BASH_SUBSHELL&quot;
outer_variable=Outer

(
...</content>
  </entry>
  
  <entry>
    <title>Bash条件判断</title>
    <link href="/2015/07/08/bash-if-else.html"/>
    <updated>2015-07-08T00:00:00+08:00</updated>
    <id>/2015/07/08/bash-if-else.html</id>
    <content type="html">每个完整并且合理的程序语言都具有条件判断的功能，并且可以根据条件测试的结果做下一步的处理。Bash有test命令、各种中括号和圆括号操作，和if/then结构。

条件测试

if/then结构用来判断命令列表的退出状态码是否为0。

有一个专有命令[ (左中括号，特殊字符)。这个命令与test命令等价，并且出于效率上的考虑，这是一个内建命令。这个命令把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码(0 表示真，1表示假)。

if  [ 0 ]      
then
    echo &quot;0 is true.&quot;
else
    echo &quot;0 is fa...</content>
  </entry>
  
  <entry>
    <title>Bash中的变量</title>
    <link href="/2015/07/07/bash-variable.html"/>
    <updated>2015-07-07T00:00:00+08:00</updated>
    <id>/2015/07/07/bash-variable.html</id>
    <content type="html">变量是脚本编程中进行数据表现的一种方法。说白了，变量不过是计算机为了保留数据项，而在内存中分配的一个位置或一组位置的标识或名字。变量既可以出现在算术操作中，也可以出现在字符串分析过程中。

变量赋值

变量使用=来实现赋值操作，前后都不能有空白。例如：

a=314
echo &quot;The value of \&quot;a\&quot; is $a.&quot;


也可以使用let来赋值：

let a=16+5
echo &quot;The value of \&quot;a\&quot; is now $a.&quot;


使用read命令进行赋值：

echo -n &quot;Enter \&quot;a\&quot; &quot;
read a
echo &quot;The value of ...</content>
  </entry>
  
  <entry>
    <title>Bash中的特殊字符</title>
    <link href="/2015/07/06/bash-special-characters.html"/>
    <updated>2015-07-06T00:00:00+08:00</updated>
    <id>/2015/07/06/bash-special-characters.html</id>
    <content type="html">Bash中，用在脚本和其他地方的字符叫做特殊字符。下面依次举例介绍每个字符的用途。

#

行首以#(#!是个例外)开头是注释。

# This line is a comment.


注释也可以放在于本行命令的后边。

echo &quot;A comment will follow.&quot;   # 注释在这里。



  命令是不能放在同一行上注释的后边的。因为没有办法把注释结束掉，好让同一行上后边的”代码生效”，只能够另起一行来使用下一个命令。


在echo中转义的#是不能作为注释的，同样也可以出现在特定的参数替换结构中，或者是出现在数字常量表达式中。

echo &quot;The # here d...</content>
  </entry>
  
  <entry>
    <title>Jekyll kramdown配置</title>
    <link href="/2015/06/30/jekyll-kramdown-config.html"/>
    <updated>2015-06-30T00:00:00+08:00</updated>
    <id>/2015/06/30/jekyll-kramdown-config.html</id>
    <content type="html">之前博客是使用的redcarpet的markdown语法，其在_config.yml中的配置方式为：

markdown: redcarpet
redcarpet:
    extensions: [ &quot;fenced_code_blocks&quot;, &quot;hard_wrap&quot;,&quot;autolink&quot;, &quot;tables&quot;, &quot;strikethrough&quot;, &quot;superscript&quot;, &quot;with_toc_data&quot;, &quot;highlight&quot;, &quot;prettify&quot;,&quot;no_intra_emphasis&quot;]


这种配置支持使用 ``` 高亮代码块、自动链接、表格等特性。

现在，想尝试使用kark...</content>
  </entry>
  
  <entry>
    <title>高级Bash脚本编程入门</title>
    <link href="/2015/06/29/advanced-bash-script-programming.html"/>
    <updated>2015-06-29T00:00:00+08:00</updated>
    <id>/2015/06/29/advanced-bash-script-programming.html</id>
    <content type="html">最近在看《Advanced Bash Scripting Guide》这本书，第二章举了一个清除日志的例子，来讲述如何使用Bash进行编程并聊到了一些编程规范。本文主要是基于这部分内容记录我的读书笔记并整理一些相关知识点。

说到清除日志，你可以使用下面命令来完成清除/var/log下的log文件这件事情：

cd /var/log
cat /dev/null &amp;gt; messages 
cat /dev/null &amp;gt; wtmp
echo &quot;Logs cleaned up.&quot;


更简单的清除日志方法是：

echo &quot;&quot; &amp;gt;messages 
#或者
&amp;gt;messag...</content>
  </entry>
  
  <entry>
    <title>spark-shell脚本分析</title>
    <link href="/2015/06/26/spark-shell-command.html"/>
    <updated>2015-06-26T00:00:00+08:00</updated>
    <id>/2015/06/26/spark-shell-command.html</id>
    <content type="html">本文主要分析spark-shell脚本的运行逻辑，涉及到spark-submit、spark-class等脚本的分析，希望通过分析脚本以了解spark中各个进程的参数、JVM参数和内存大小如何设置。

spark-shell

使用yum安装spark之后，你可以直接在终端运行spark-shell命令，或者在spark的home目录/usr/lib/spark下运行bin/spark-shell命令，这样就可以进入到spark命令行交互模式。

spark-shell 脚本是如何运行的呢？该脚本代码如下：

#
# Shell script for starting the Spark...</content>
  </entry>
  
  <entry>
    <title>Scala中的对象</title>
    <link href="/2015/06/19/scala-object.html"/>
    <updated>2015-06-19T00:00:00+08:00</updated>
    <id>/2015/06/19/scala-object.html</id>
    <content type="html">Scala中没有静态方法或静态字段，但可以使用object这个语法结构来实现相同的功能。对象与类在语法层面上很相似，除了不能提供构造器参数外，对象可以拥有类的所有特性。

Scala的object定义了单个实例，其可以用来存放工具函数或常量等：

object Timer {
  var count = 0

  def currentCount(): Long = {
    count += 1
    count
  }
}


使用object中的常量或方法，通过object名称直接调用，对象构造器在对象第一次被使用时调用（如果某对象一直未被使用，那么其构造器也不会被调用）。

...</content>
  </entry>
  
  <entry>
    <title>Scala中的类</title>
    <link href="/2015/06/19/scala-class.html"/>
    <updated>2015-06-19T00:00:00+08:00</updated>
    <id>/2015/06/19/scala-class.html</id>
    <content type="html">阅读《Programming in Scala》，整理Scala类、继承、重载相关的一些知识点。

类

Scala使用class来定义类。

class Counter {
  private var value = 0 // 必须初始化字段
  def increment() { value += 1 } // 方法默认公有
  def current() = value  //空括号方法
}



  Scala中的类不能声明为public，一个Scala源文件中可以有多个类。


类的初始化和调用：

val myCounter = new Counter // 或new Cou...</content>
  </entry>
  
  <entry>
    <title>使用Scala高价函数简化代码</title>
    <link href="/2015/06/18/simplify-code-using-scala-higher-order-function.html"/>
    <updated>2015-06-18T00:00:00+08:00</updated>
    <id>/2015/06/18/simplify-code-using-scala-higher-order-function.html</id>
    <content type="html">在Scala里，带有其他函数做参数的函数叫做高阶函数，使用高阶函数可以简化代码。

减少重复代码

有这样一段代码，查找当前目录样以某一个字符串结尾的文件：

object FileMatcher {
  private def filesHere = (new java.io.File(&quot;.&quot;)).listFiles
  def filesEnding(query: String) =
    for (file &amp;lt;- filesHere; if file.getName.endsWith(query))
      yield file
}


如果，我们想查找包含某一个字符串...</content>
  </entry>
  
  <entry>
    <title>解决固定导航时锚点偏移问题</title>
    <link href="/2015/06/18/fix-anchor-offset-when-using-bootstrap-navbar-fixed-top.html"/>
    <updated>2015-06-18T00:00:00+08:00</updated>
    <id>/2015/06/18/fix-anchor-offset-when-using-bootstrap-navbar-fixed-top.html</id>
    <content type="html">最近Bootstrap修改了博客主题，使其支持响应式布局，并且将导航菜单固定住，不随滚到条滚动，这样做带来的影响是Categories和Tags页面点击某一个分类或者标签链接时，锚点定位必然定位于页面顶部，这样一来就会被固定住的导航遮挡，例如，我在Categories页面，点击hbase分类，锚点定位最后如下图：



网上查找了一些资料，找到一篇文章点击锚点让定位偏移顶部，这篇文章提到几种解决办法：

第一种，使用css将锚点偏移：

&amp;lt;a class=&quot;target-fix&quot; &amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;artivle&amp;gt;主体内容...&amp;lt;/article&amp;g...</content>
  </entry>
  
  <entry>
    <title>推荐系统笔记</title>
    <link href="/2015/06/15/note-about-recommendation-system.html"/>
    <updated>2015-06-15T00:00:00+08:00</updated>
    <id>/2015/06/15/note-about-recommendation-system.html</id>
    <content type="html">1、产生原因


  信息过载
  无明确需求


2、什么是推荐？

在信息过载又没有明确需求的情况下，找到用户感兴趣的东西。

《Mahout实战》上的定义是：推荐就是通过对喜好的这些模式进行预测，借以发现你尚未知晓，却合乎心意的新事物。

3、推荐和搜索区别：


  相同点：快速发现有用信息的工具
  不同点：搜索引擎是用户找信息；推荐系统是信息找用户


为了解决信息过载的问题，代表性的解决方案是分类目录和搜索引擎。和搜索引擎一样，推荐系统也是一种帮助用户快速发现有用信息的工具。和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从...</content>
  </entry>
  
  <entry>
    <title>使用Mahout实现协同过滤</title>
    <link href="/2015/06/10/collaborative-filtering-using-mahout.html"/>
    <updated>2015-06-10T00:00:00+08:00</updated>
    <id>/2015/06/10/collaborative-filtering-using-mahout.html</id>
    <content type="html">Mahout算法框架自带的推荐器有下面这些：


  GenericUserBasedRecommender：基于用户的推荐器，用户数量少时速度快；
  GenericItemBasedRecommender：基于商品推荐器，商品数量少时速度快，尤其当外部提供了商品相似度数据后效率更好；
  SlopeOneRecommender：基于slope-one算法的推荐器，在线推荐或更新较快，需要事先大量预处理运算，物品数量少时较好；
  SVDRecommender：奇异值分解，推荐效果较好，但之前需要大量预处理运算；
  KnnRecommender：基于k近邻算法(KNN)，适合于物品数...</content>
  </entry>
  
  <entry>
    <title>Spark On YARN内存分配</title>
    <link href="/2015/06/09/memory-in-spark-on-yarn.html"/>
    <updated>2015-06-09T00:00:00+08:00</updated>
    <id>/2015/06/09/memory-in-spark-on-yarn.html</id>
    <content type="html">本文主要了解Spark On YARN部署模式下的内存分配情况，因为没有深入研究Spark的源代码，所以只能根据日志去看相关的源代码，从而了解“为什么会这样，为什么会那样”。

说明

按照Spark应用程序中的driver分布方式不同，Spark on YARN有两种模式： yarn-client模式、yarn-cluster模式。

当在YARN上运行Spark作业，每个Spark executor作为一个YARN容器运行。Spark可以使得多个Tasks在同一个容器里面运行。

下图是yarn-cluster模式的作业执行图，图片来源于网络：



关于Spark On YARN相...</content>
  </entry>
  
  <entry>
    <title>Spark配置参数</title>
    <link href="/2015/06/07/spark-configuration.html"/>
    <updated>2015-06-07T00:00:00+08:00</updated>
    <id>/2015/06/07/spark-configuration.html</id>
    <content type="html">以下是整理的Spark中的一些配置参数，官方文档请参考Spark Configuration。

Spark提供三个位置用来配置系统：


  Spark属性：控制大部分的应用程序参数，可以用SparkConf对象或者Java系统属性设置
  环境变量：可以通过每个节点的 conf/spark-env.sh脚本设置。例如IP地址、端口等信息
  日志配置：可以通过log4j.properties配置


Spark属性

Spark属性控制大部分的应用程序设置，并且为每个应用程序分别配置它。这些属性可以直接在SparkConf上配置，然后传递给SparkContext。SparkConf...</content>
  </entry>
  
  <entry>
    <title>YARN的内存和CPU配置</title>
    <link href="/2015/06/05/yarn-memory-and-cpu-configuration.html"/>
    <updated>2015-06-05T00:00:00+08:00</updated>
    <id>/2015/06/05/yarn-memory-and-cpu-configuration.html</id>
    <content type="html">Hadoop YARN同时支持内存和CPU两种资源的调度，本文介绍如何配置YARN对内存和CPU的使用。

YARN作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据application申请的资源进行分配Container。Container是YARN里面资源分配的基本单位，具有一定的内存以及CPU资源。

在YARN集群中，平衡内存、CPU、磁盘的资源的很重要的，根据经验，每两个container使用一块磁盘以及一个CPU核的时候可以使集群的资源得到一个比较好的利用。

内存配置

关于内存相关的配置可以参考hortonwork公司的文档Determine HDP ...</content>
  </entry>
  
  <entry>
    <title>如何使用Spark ALS实现协同过滤</title>
    <link href="/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html"/>
    <updated>2015-06-01T00:00:00+08:00</updated>
    <id>/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als.html</id>
    <content type="html">本文主要记录最近一段时间学习和实现Spark MLlib中的协同过滤的一些总结，希望对大家熟悉Spark ALS算法有所帮助。


  更新：

  
    【2016.06.12】Spark1.4.0中MatrixFactorizationModel提供了recommendForAll方法实现离线批量推荐，见SPARK-3066。
  


测试环境

为了测试简单，在本地以local方式运行Spark，你需要做的是下载编译好的压缩包解压即可，可以参考Spark本地模式运行。

测试数据使用MovieLens的MovieLens 10M数据集，下载之后解压到data目录。数据的格式请...</content>
  </entry>
  
  <entry>
    <title>测试Hive集成Sentry</title>
    <link href="/2015/04/30/test-hive-with-sentry.html"/>
    <updated>2015-04-30T00:00:00+08:00</updated>
    <id>/2015/04/30/test-hive-with-sentry.html</id>
    <content type="html">本文在安装和配置Sentry基础之上测试Hive集成Sentry。注意：这里Hive中并没有配置Kerberos认证。

关于配置了Kerberos的Hive集群如何集成Sentry，请参考配置安全的Hive集群集成Sentry。

1. 配置Sentry

见安装和配置Sentry。

2. 配置Hive

Hive Metastore集成Sentry

需要在 /etc/hive/conf/hive-site.xml中添加：

    &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hive.metastore.pre.event.listeners&amp;lt;/n...</content>
  </entry>
  
  <entry>
    <title>安装和配置Sentry</title>
    <link href="/2015/04/30/install-and-config-sentry.html"/>
    <updated>2015-04-30T00:00:00+08:00</updated>
    <id>/2015/04/30/install-and-config-sentry.html</id>
    <content type="html">本文主要记录安装和配置Sentry的过程，关于Sentry的介绍，请参考Apache Sentry架构介绍。

1. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  运行用户：root


这里，我参考使用yum安装CDH Hadoop集群一文搭建了一个测试集群，并选择cdh1节点来安装sentry服务。

2. 安装

在cdh1节点上运行下面命令查看Sentry的相关组件有哪些:

$ yum list sentry*

sentry.noarch                        1.4.0+cdh5.4.0+15...</content>
  </entry>
  
  <entry>
    <title>Apache Sentry架构介绍</title>
    <link href="/2015/04/29/apache-sentry-architecture.html"/>
    <updated>2015-04-29T00:00:00+08:00</updated>
    <id>/2015/04/29/apache-sentry-architecture.html</id>
    <content type="html">介绍

Apache Sentry是Cloudera公司发布的一个Hadoop开源组件，截止目前还是Apache的孵化项目，它提供了细粒度级、基于角色的授权以及多租户的管理模式。Sentry当前可以和Hive/Hcatalog、Apache Solr 和Cloudera Impala集成，未来会扩展到其他的Hadoop组件，例如HDFS和HBase。

特性

Apache Sentry为Hadoop使用者提供了以下便利：


  能够在Hadoop中存储更敏感的数据
  使更多的终端用户拥有Hadoop数据访问权
  创建更多的Hadoop使用案例
  构建多用户应用程序
  符合规范...</content>
  </entry>
  
  <entry>
    <title>编译CDH Spark源代码</title>
    <link href="/2015/04/28/compile-cdh-spark-source-code.html"/>
    <updated>2015-04-28T00:00:00+08:00</updated>
    <id>/2015/04/28/compile-cdh-spark-source-code.html</id>
    <content type="html">本文以Cloudera维护的Spark分支项目为例，记录跟新Spark分支以及编译Spark源代码的过程。

下载代码

在Github上fork Cloudera维护的Spark项目到自己的github账号里，对应的地址为https://github.com/javachen/spark。

下载代码：

$ git clone https://github.com/javachen/spark


然后，切换到最新的分支，当前为 cdh5-1.3.0_5.4.0。

$ cd spark
$ git checkout cdh5-1.3.0_5.4.0


查看当前分支：

⇒  gi...</content>
  </entry>
  
  <entry>
    <title>Scala中下划线的用途</title>
    <link href="/2015/04/23/all-the-uses-of-an-underscore-in-scala.html"/>
    <updated>2015-04-23T00:00:00+08:00</updated>
    <id>/2015/04/23/all-the-uses-of-an-underscore-in-scala.html</id>
    <content type="html">存在性类型：

def foo(l: List[Option[_]]) = 

def f(m: M[_]) 


高阶类型参数：

case class A[K[_],T](a: K[T])

def f[M[_]] 


临时变量：

val _ = 5


临时参数：

List(1, 2, 3) foreach { _ =&amp;gt; println(&quot;Hi&quot;) }    //List(1, 2, 3) foreach { t =&amp;gt; println(&quot;Hi&quot;) }


通配模式：

Some(5) match { case Some(_) =&amp;gt; println(&quot;Yes&quot;...</content>
  </entry>
  
  <entry>
    <title>Scala集合</title>
    <link href="/2015/04/22/scala-collections.html"/>
    <updated>2015-04-22T00:00:00+08:00</updated>
    <id>/2015/04/22/scala-collections.html</id>
    <content type="html">Scala有一个非常通用，丰富，强大，可组合的集合库；集合是高阶的(high level)并暴露了一大套操作方法。很多集合的处理和转换可以被表达的简洁又可读，但不审慎地用它们的功能也会导致相反的结果。每个Scala程序员应该阅读 集合设计文档；通过它可以很好地洞察集合库，并了解设计动机。

scala集合API：http://www.scala-lang.org/docu/files/collections-api/collections.html。

怎样使用集合，请参考 Effective Scala。

架构

Scala的所有的集合类都可以在包 scala.collection ...</content>
  </entry>
  
  <entry>
    <title>Scala基本语法和概念</title>
    <link href="/2015/04/20/basic-of-scala.html"/>
    <updated>2015-04-20T00:00:00+08:00</updated>
    <id>/2015/04/20/basic-of-scala.html</id>
    <content type="html">本文主要包括Scala的安装过程并理解Scala的基本语法和概念，包括表达式、变量、基本类型、函数、流程控制等相关内容。

1. 安装

从All Versions Scala下载所需版本Scala安装包，解压到指定目录之后，配置环境变量并使其生效。

如果你使用Mac，则可以使用brew安装：

⇒  brew install scala


在终端键入scala查看Scala的版本，并进入Scala的解释器：

⇒  scala
Welcome to Scala version 2.11.6 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0...</content>
  </entry>
  
  <entry>
    <title>Spark MLlib中的协同过滤</title>
    <link href="/2015/04/17/spark-mllib-collaborative-filtering.html"/>
    <updated>2015-04-17T00:00:00+08:00</updated>
    <id>/2015/04/17/spark-mllib-collaborative-filtering.html</id>
    <content type="html">本文主要通过Spark官方的例子理解ALS协同过滤算法的原理和编码过程，然后通过对电影进行推荐来熟悉一个完整的推荐过程。

协同过滤

协同过滤常被应用于推荐系统，旨在补充用户-商品关联矩阵中所缺失的部分。MLlib当前支持基于模型的协同过滤，其中用户和商品通过一小组隐语义因子进行表达，并且这些因子也用于预测缺失的元素。Spark MLlib实现了交替最小二乘法(ALS) 来学习这些隐性语义因子。

在 MLlib 中的实现类为org.apache.spark.mllib.recommendation.ALS.scala，其有如下的参数:


  numUserBlocks：是用于并行化...</content>
  </entry>
  
  <entry>
    <title>Spark SQL中的数据源</title>
    <link href="/2015/04/03/spark-sql-datasource.html"/>
    <updated>2015-04-03T00:00:00+08:00</updated>
    <id>/2015/04/03/spark-sql-datasource.html</id>
    <content type="html">Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、cassandra 等等。

本文测试环境为 Spark 1.3。

加载和保存文件

最简单的方式是调用 load 方法加载文件，默认的格式为 parquet，你可以修改 spark.sql.sources.default 指定默认的格式：

scala&amp;gt; val df = sqlContext.load(&quot;people.parquet&quot;)
scala&amp;gt; df.select(&quot;name&quot;, &quot;age&quot;)....</content>
  </entry>
  
  <entry>
    <title>Spark本地模式运行</title>
    <link href="/2015/03/30/spark-test-in-local-mode.html"/>
    <updated>2015-03-30T00:00:00+08:00</updated>
    <id>/2015/03/30/spark-test-in-local-mode.html</id>
    <content type="html">Spark的安装分为几种模式，其中一种是本地运行模式，只需要在单节点上解压即可运行，这种模式不需要依赖Hadoop 环境。在本地运行模式中，master和worker都运行在一个jvm进程中，通过该模式，可以快速的测试Spark的功能。

下载 Spark

下载地址为http://spark.apache.org/downloads.html，根据页面提示选择一个合适的版本下载，这里我下载的是 spark-1.3.0-bin-cdh4.tgz。下载之后解压：

 cd ~
 wget http://mirror.bit.edu.cn/apache/spark/spark-1.3.0/s...</content>
  </entry>
  
  <entry>
    <title>Reading List 2015-03</title>
    <link href="/2015/03/30/reading-list-2015-03.html"/>
    <updated>2015-03-30T00:00:00+08:00</updated>
    <id>/2015/03/30/reading-list-2015-03.html</id>
    <content type="html">这个月主要在关注流式处理和推荐系统方面的技术。如何从零构建一个推荐系统？网上能找到的有指导意义的资料太少，只能一点点摸索？

Spark


  LeanCloud 离线数据分析功能介绍
  Spark在腾讯数据仓库TDW的应用 http://www.biaodianfu.com/spark-tdw.html
  Spark on Yarn：小火花照亮大数据 http://rdc.taobao.org/?p=512
  Spark on Yarn：性能调优 http://rdc.taobao.org/?p=533


Spark 教程


  Spark Shell Examples h...</content>
  </entry>
  
  <entry>
    <title>Spark SQL中的DataFrame</title>
    <link href="/2015/03/26/spark-sql-dataframe.html"/>
    <updated>2015-03-26T00:00:00+08:00</updated>
    <id>/2015/03/26/spark-sql-dataframe.html</id>
    <content type="html">在2014年7月1日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上。在会议上，Databricks 表示，Shark 更多是对 Hive 的改造，替换了 Hive 的物理执行引擎，因此会有一个很快的速度。然而，不容忽视的是，Shark 继承了大量的 Hive 代码，因此给优化和维护带来了大量的麻烦。随着性能优化和先进分析整合的进一步加深，基于 MapReduce 设计的部分无疑成为了整个项目的瓶颈。 详细内容请参看 Shark, Spark SQL, Hive on Spark, and the future o...</content>
  </entry>
  
  <entry>
    <title>将Avro数据转换为Parquet格式</title>
    <link href="/2015/03/25/converting-avro-data-to-parquet-format.html"/>
    <updated>2015-03-25T00:00:00+08:00</updated>
    <id>/2015/03/25/converting-avro-data-to-parquet-format.html</id>
    <content type="html">本文主要测试将Avro数据转换为Parquet格式的过程并查看 Parquet 文件的 schema 和元数据。

准备

将文本数据转换为 Parquet 格式并读取内容，可以参考 Cloudera 的 MapReduce 例子：https://github.com/cloudera/parquet-examples。

准备文本数据 a.txt 为 CSV 格式：

1,2
3,4
4,5


准备 Avro 测试数据，可以参考 将Avro数据加载到Spark 一文。

本文测试环境为：CDH 5.2，并且 Avro、Parquet 组件已经通过 YUM 源安装。

将 CSV 转换...</content>
  </entry>
  
  <entry>
    <title>如何将Avro数据加载到Spark</title>
    <link href="/2015/03/24/how-to-load-some-avro-data-into-spark.html"/>
    <updated>2015-03-24T00:00:00+08:00</updated>
    <id>/2015/03/24/how-to-load-some-avro-data-into-spark.html</id>
    <content type="html">这是一篇翻译，原文来自：How to load some Avro data into Spark。

首先，为什么使用 Avro ？

最基本的格式是 CSV ，其廉价并且不需要顶一个一个 schema 和数据关联。

随后流行起来的一个通用的格式是 XML，其有一个 schema 和 数据关联，XML 广泛的使用于 Web Services 和 SOA 架构中。不幸的是，其非常冗长，并且解析 XML 需要消耗内存。

另外一种格式是 JSON，其非常流行易于使用因为它非常方便易于理解。

这些格式在 Big Data 环境中都是不可拆分的，这使得他们难于使用。在他们之上使用一个压缩机...</content>
  </entry>
  
  <entry>
    <title>Avro介绍</title>
    <link href="/2015/03/20/about-avro.html"/>
    <updated>2015-03-20T00:00:00+08:00</updated>
    <id>/2015/03/20/about-avro.html</id>
    <content type="html">1. 介绍

Avro 是 Hadoop 中的一个子项目，也是 Apache 中一个独立的项目，Avro 是一个基于二进制数据传输高性能的中间件。在 Hadoop 的其他项目中，例如 HBase 和 Hive 的 Client 端与服务端的数据传输也采用了这个工具。Avro 是一个数据序列化的系统，它可以提供：


  1、丰富的数据结构类型
  2、快速可压缩的二进制数据形式
  3、存储持久数据的文件容器
  4、远程过程调用 RPC
  5、简单的动态语言结合功能，Avro 和动态语言结合后，读写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态...</content>
  </entry>
  
  <entry>
    <title>安装和测试Kafka</title>
    <link href="/2015/03/17/install-and-test-kafka.html"/>
    <updated>2015-03-17T00:00:00+08:00</updated>
    <id>/2015/03/17/install-and-test-kafka.html</id>
    <content type="html">本文主要介绍如何在单节点上安装 Kafka 并测试 broker、producer 和 consumer 功能。

下载

进入下载页面：http://kafka.apache.org/downloads.html ，选择 Binary downloads下载 （Source download需要编译才能使用），这里我下载 kafka_2.11-0.8.2.1，其对应的 Scala 版本为 2.11：

$ wget http://apache.fayea.com/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz


解压并进入目录：

$ tar -xzvf k...</content>
  </entry>
  
  <entry>
    <title>Spring Boot特性</title>
    <link href="/2015/03/13/some-spring-boot-features.html"/>
    <updated>2015-03-13T00:00:00+08:00</updated>
    <id>/2015/03/13/some-spring-boot-features.html</id>
    <content type="html">SpringApplication

SpringApplication 类是启动 Spring Boot 应用的入口类，你可以创建一个包含 main() 方法的类，来运行 SpringApplication.run 这个静态方法：

public static void main(String[] args) {
    SpringApplication.run(MySpringConfiguration.class, args);
}


运行该类会有如下输出：

  .   ____          _            __ _ _
 /\\ / ___&#39;_ __ _ _...</content>
  </entry>
  
  <entry>
    <title>如何运行Spring Boot应用</title>
    <link href="/2015/03/13/how-to-run-spring-boot-application.html"/>
    <updated>2015-03-13T00:00:00+08:00</updated>
    <id>/2015/03/13/how-to-run-spring-boot-application.html</id>
    <content type="html">介绍

Spring Boot 是 Spring 产品中一个新的子项目，致力于简便快捷地搭建基于 Spring 的独立可运行的应用。大多数的 Spring Boot  应用只需要非常少的 Spring 配置。

你能够使用 Spring Boot  创建 Java 应用并通过 java -jar 来运行或者创建传统的通过 war 来部署的应用。Spring Boot 也提供了一个命令行工具来运行 spring 脚本。

Spring Boot 的目标是：


  快速开发基于 Spring 的应用
  开箱即用的微服务
  提供一些大型项目常用的非功能性特性，例如：嵌入式服务、安全、监控...</content>
  </entry>
  
  <entry>
    <title>Spring AOP Example Tutorial</title>
    <link href="/2015/03/11/spring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations-xml-configuration.html"/>
    <updated>2015-03-11T00:00:00+08:00</updated>
    <id>/2015/03/11/spring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations-xml-configuration.html</id>
    <content type="html">
  这是一篇翻译，原文：Spring AOP Example Tutorial – Aspect, Advice, Pointcut, JoinPoint, Annotations, XML Configuration


Spring 框架发展出了两个核心概念：依赖注入 和面向切面编程（AOP）。我们已经了解了 Spring 的依赖注入 是如何实现的，今天我们来看看面向切面编程的核心概念以及 Spring 框架是如何实现它的。

AOP 概要

大多数的企业应用都会有一些共同的对横切的关注，横切是否适用于不同的对象或者模型。一些共同关注的横切有日志、事务管理以及数据校验等等。在面向对...</content>
  </entry>
  
  <entry>
    <title>快速了解RESTEasy</title>
    <link href="/2015/03/10/quick-start-of-resteasy.html"/>
    <updated>2015-03-10T00:00:00+08:00</updated>
    <id>/2015/03/10/quick-start-of-resteasy.html</id>
    <content type="html">什么是 RESTEasy

RESTEasy 是 JBoss 的一个开源项目，提供各种框架帮助你构建 RESTful Web Services 和 RESTful Java 应用程序。它是 JAX-RS 规范的一个完整实现并通过 JCP 认证。作为一个 JBOSS 的项目，它当然能和 JBOSS 应用服务器很好地集成在一起。 但是，它也能在任何运行 JDK5 或以上版本的 Servlet 容器中运行。RESTEasy 还提供一个 RESTEasy JAX-RS 客户端调用框架，能够很方便与 EJB、Seam、Guice、Spring 和 Spring MVC 集成使用，支持在客户端与服务...</content>
  </entry>
  
  <entry>
    <title>Java笔记：异常</title>
    <link href="/2015/03/04/note-about-java-exception.html"/>
    <updated>2015-03-04T00:00:00+08:00</updated>
    <id>/2015/03/04/note-about-java-exception.html</id>
    <content type="html">定义

在《java编程思想》中这样定义异常：阻止当前方法或作用域继续执行的问题。异常是Java程序设计中不可分割的一部分，如果不了解如何使用它们，那么我们只能完成很有限的工作。

分类

异常分为3种：


  Error - 描述了Java运行系统中的内部错误以及资源耗尽的情况。应用程序不应该抛出这种类型的对象。如果这种内部错误出现，除了通知用户错误发生以及尽力安全的退出程序外，在其他方面是无能为力的。
  编译时异常 Exception - 它指出了合理的应用程序想要捕获的条件。Exception又分为两类：IOException和RuntimeException。由编程导致的错误...</content>
  </entry>
  
  <entry>
    <title>安装和配置Hue</title>
    <link href="/2015/02/28/install-and-config-hue.html"/>
    <updated>2015-02-28T00:00:00+08:00</updated>
    <id>/2015/02/28/install-and-config-hue.html</id>
    <content type="html">本文主要记录使用 yum 源安装 Hue 以及配置 Hue 集成 Hdfs、Hive、Impala、Yarn、Kerberos、LDAP、Sentry、Solr 等的过程。

安装环境：


  操作系统：CentOs6.5
  Hadoop：cdh5.2.0
  Hue：3.6.0+cdh5.2.0


安装 Hue

在 Hadoop 集群的一个节点上安装 Hue server，这里我是在我的测试集群中的 cdh1 节点上安装：

$ yum install hue hue-server


配置 Hue

配置hue server

[desktop]
    http_host=...</content>
  </entry>
  
  <entry>
    <title>Hadoop Streaming 原理</title>
    <link href="/2015/02/12/hadoop-streaming.html"/>
    <updated>2015-02-12T00:00:00+08:00</updated>
    <id>/2015/02/12/hadoop-streaming.html</id>
    <content type="html">简介

Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的优势和能力，来处理大数据。

一个简单的示例，以 shell 脚本为例：

hadoop jar hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /usr/bin/wc


Strea...</content>
  </entry>
  
  <entry>
    <title>Useful Hadoop Commands</title>
    <link href="/2015/02/10/useful-commands-in-hadoop.html"/>
    <updated>2015-02-10T00:00:00+08:00</updated>
    <id>/2015/02/10/useful-commands-in-hadoop.html</id>
    <content type="html">hadoop

解压 gz 文件到文本文件

$ hadoop fs -text /hdfs_path/compressed_file.gz | hadoop fs -put - /tmp/uncompressed-file.txt


解压本地文件 gz 文件并上传到 hdfs

$ gunzip -c filename.txt.gz | hadoop fs -put - /tmp/filename.txt


使用 awk 处理 csv 文件，参考 Using awk and friends with Hadoop:

$ hadoop fs -cat people.txt | aw...</content>
  </entry>
  
  <entry>
    <title>Reading List 2015-02</title>
    <link href="/2015/02/10/reading-list-2015-02.html"/>
    <updated>2015-02-10T00:00:00+08:00</updated>
    <id>/2015/02/10/reading-list-2015-02.html</id>
    <content type="html">一直有个想法没有付诸实践，想做个分享知识的网站，类似 Leanote、开发者头条、GitHunt 等等的可检索的有思想的一个产品。作为尝试，在想法成型之前，先参考 http://www.dbthink.com/archives/910、http://yanjunyi.com/blog/category/discovery/ 的方式整理为知笔记中以前看到、搜集的一些文章、链接、工具等等的。

前端


  http://githunt.io/
    
      http://ohmycss.com/：简化版的bootstrap
      https://octicons.github...</content>
  </entry>
  
  <entry>
    <title>如何在CDH5上运行Spark应用</title>
    <link href="/2015/02/04/how-to-run-a-simple-apache-spark-app-in-cdh-5.html"/>
    <updated>2015-02-04T00:00:00+08:00</updated>
    <id>/2015/02/04/how-to-run-a-simple-apache-spark-app-in-cdh-5.html</id>
    <content type="html">
  这篇文章参考 How-to: Run a Simple Apache Spark App in CDH 5 编写而成，没有完全参照原文翻译，而是重新进行了整理，例如：spark 版本改为 1.3.0，添加了 Python 版的程序。


创建 maven 工程

使用下面命令创建一个普通的 maven 工程：

$ mvn archetype:generate -DgroupId=com.cloudera.sparkwordcount -DartifactId=sparkwordcount -DarchetypeArtifactId=maven-archetype-quicksta...</content>
  </entry>
  
  <entry>
    <title>Spark编程指南笔记</title>
    <link href="/2015/02/03/spark-programming-guide.html"/>
    <updated>2015-02-03T00:00:00+08:00</updated>
    <id>/2015/02/03/spark-programming-guide.html</id>
    <content type="html">本文是参考Spark官方编程指南（Spark 版本为1.2）整理出来的学习笔记，主要是用于加深对 Spark 的理解，并记录一些知识点。

1. Spark介绍

Spark是UC Berkeley AMP lab所开源的类Hadoop MapReduce 框架，都是基于map reduce算法所实现的分布式计算框架，拥有Hadoop MapReduce所具有的优点；不同于MapReduce的是Job中间输出和结果可以保存在内存中，而不需要读写HDFS，因此Spark能更好地适用于machine learning等需要迭代的map reduce算法。

产生原因

1、 MapReduc...</content>
  </entry>
  
  <entry>
    <title>Require.JS快速入门</title>
    <link href="/2015/02/02/quick-start-of-requirejs.html"/>
    <updated>2015-02-02T00:00:00+08:00</updated>
    <id>/2015/02/02/quick-start-of-requirejs.html</id>
    <content type="html">Require.JS 介绍

Require.JS 是一个基于 AMD 规范的 JavaScript 模块加载框架。实现 JavaScript 文件的异步加载，管理模块之间的依赖性，提升网页的加载速度。

AMD 是 Asynchronous Module Definition 的缩写，意思就是 异步模块定义。它采用异步方式加载模块，模块的加载不影响它后面语句的运行。所有依赖这个模块的语句，都定义在一个回调函数中，等到加载完成之后，这个回调函数才会运行。

官网地址：

Require.JS 的诞生主要为了解决两个问题：　　


  1）实现 JavaScript 文件的异步加载，避免网...</content>
  </entry>
  
  <entry>
    <title>用Yeoman构建AngularJS项目</title>
    <link href="/2015/02/02/build-angularjs-app-with-yeoman.html"/>
    <updated>2015-02-02T00:00:00+08:00</updated>
    <id>/2015/02/02/build-angularjs-app-with-yeoman.html</id>
    <content type="html">这篇文章不是一篇翻译也不是一篇原创文章，类似于一篇学习笔记，主要是记录一些关键的过程，方便查阅加深理解和记忆。

Yeoman 介绍

Yeoman 是 Google 的团队和外部贡献者团队合作开发的，他的目标是通过 Grunt（一个用于开发任务自动化的命令行工具）和 Bower（一个HTML、CSS、Javascript 和图片等前端资源的包管理器）的包装为开发者创建一个易用的工作流。

Yeoman 的目的不仅是要为新项目建立工作流，同时还是为了解决前端开发所面临的诸多严重问题，例如零散的依赖关系。

Yeoman 主要有三部分组成：yo（脚手架工具）、grunt（构建工具）、bow...</content>
  </entry>
  
  <entry>
    <title>Django中SQL查询</title>
    <link href="/2015/01/30/raw-sql-query-in-django.html"/>
    <updated>2015-01-30T00:00:00+08:00</updated>
    <id>/2015/01/30/raw-sql-query-in-django.html</id>
    <content type="html">当 Django 中模型提供的查询 API 不能满足要求时，你可能需要使用原始的 sql 查询，这时候就需要用到  Manager.raw() 方法。

Manager 类提供下面的一个方法，可以用于执行 sql：

Manager.raw(raw_query, params=None, translations=None)


使用方法为：

&amp;gt;&amp;gt;&amp;gt; for p in Person.objects.raw(&#39;SELECT * FROM myapp_person&#39;):
...     print(p)
John Smith
Jane Jones


raw() 可以通过...</content>
  </entry>
  
  <entry>
    <title>安装和部署Presto</title>
    <link href="/2015/01/26/install-and-deploy-presto.html"/>
    <updated>2015-01-26T00:00:00+08:00</updated>
    <id>/2015/01/26/install-and-deploy-presto.html</id>
    <content type="html">1. 安装环境


  操作系统：CentOs6.5
  Hadoop 集群：CDH5.3
  JDK 版本：jdk1.8.0_31


为了测试简单，我是将 Presto 的 coordinator 和 worker 都部署在 cdh1 节点上，并且该节点上部署了 hive-metastore 服务。下面的安装和部署过程参考自 http://prestodb.io/docs/current/installation.html。

2. 安装 Presto

下载 Presto 的压缩包，目前最新版本为 presto-server-0.90，然后解压为 presto-server-0.9...</content>
  </entry>
  
  <entry>
    <title>Presto介绍</title>
    <link href="/2015/01/23/presto-overview.html"/>
    <updated>2015-01-23T00:00:00+08:00</updated>
    <id>/2015/01/23/presto-overview.html</id>
    <content type="html">1. 简介

Presto 是一个运行在集群之上的分布式系统。一个完全的安装报考一个 coordinator  进程和多个 workers 进程。查询通过一个客户端例如 Presto CLI 提交到 coordinator 进程。这个 coordinator 进程解析、分析并且生成查询的执行计划，然后将执行过程分发到 workers 进程。

下面是一个架构图（图来自 http://www.dw4e.com/?p=141，此图将官网的架构图稍微修改了一下，增加了 Discovery 的服务，这样可能看起来会更清楚一些）：



Presto 查询引擎是一个 Master-Slave 的架...</content>
  </entry>
  
  <entry>
    <title>CDH 5.2中Impala认证集成LDAP和Kerberos</title>
    <link href="/2015/01/23/new-in-cdh-5-2-impala-authentication-with-ldap-and-kerberos.html"/>
    <updated>2015-01-23T00:00:00+08:00</updated>
    <id>/2015/01/23/new-in-cdh-5-2-impala-authentication-with-ldap-and-kerberos.html</id>
    <content type="html">这是一篇翻译的文章，原文为 New in CDH 5.2: Impala Authentication with LDAP and Kerberos。由于翻译水平有限，难免会一些翻译不准确的地方，欢迎指正！



Impala 认证现在可以通过 LDAP 和 Kerberos 联合使用来解决。下文来解释为什么和怎样解决。

Impala，是基于 Apache Hadoop 的一个开源的分析数据库，使用 Kerberos 和 LDAP 来支持认证-作为一种角色来证明你是否是你所说的你是谁。Kerberos 在1.0版本中就已经被支持了，而 LDAP 是最近才被支持，在 CDH 5.2 中，...</content>
  </entry>
  
  <entry>
    <title>Maven的一些技巧</title>
    <link href="/2015/01/20/maven-skills.html"/>
    <updated>2015-01-20T00:00:00+08:00</updated>
    <id>/2015/01/20/maven-skills.html</id>
    <content type="html">本文主要收集一些 Maven 的使用技巧，包括 Maven 常见命令、创建多模块项目、上传本地 jar 到插件以及常用的插件等等，本篇文章会保持不停的更新。

命令行创建 maven 项目：

$ mvn archetype:generate -DgroupId=com.javachen.spark -DartifactId=spark-examples -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false


Maven安装本地jar到本地仓库，举例：

$ mvn install:instal...</content>
  </entry>
  
  <entry>
    <title>Django中的ORM</title>
    <link href="/2015/01/15/django-orm.html"/>
    <updated>2015-01-15T00:00:00+08:00</updated>
    <id>/2015/01/15/django-orm.html</id>
    <content type="html">通过《如何创建一个Django网站》大概清楚了如何创建一个简单的 Django 网站，并了解了Django 中模板和模型使用方法。本篇文章主要在此基础上，了解 Django 中 ORM 相关的用法。

一个 blog 的应用中 mysite/blog/models.py 有以下实体：

from django.db import models

class Blog(models.Model):
    name = models.CharField(max_length=100)
    tagline = models.TextField()

    def __str__(sel...</content>
  </entry>
  
  <entry>
    <title>Django中的模型</title>
    <link href="/2015/01/14/django-model.html"/>
    <updated>2015-01-14T00:00:00+08:00</updated>
    <id>/2015/01/14/django-model.html</id>
    <content type="html">Django 中的模型主要用于定义数据的来源信息，其包括一些必要的字段和一些对存储的数据的操作。通常，一个模型对应着数据库中的一个表。

简单的概念：


  Django 中每一个 Model 都继承自 django.db.models.Model。
  在 Model 当中每一个属性 attribute 都代表一个数据库字段。
  通过 Django Model API 可以执行数据库的增删改查, 而不需要写一些数据库的查询语句。


1. 模型

1.1 一个示例

下面在 myapp 应用种定义了一个 Person 模型，包括两个字段：first_name 和 last_name...</content>
  </entry>
  
  <entry>
    <title>AngularJS PhoneCat代码分析</title>
    <link href="/2015/01/09/angular-phonecat-examples.html"/>
    <updated>2015-01-09T00:00:00+08:00</updated>
    <id>/2015/01/09/angular-phonecat-examples.html</id>
    <content type="html">AngularJS 官方网站提供了一个用于学习的示例项目：PhoneCat。这是一个Web应用，用户可以浏览一些Android手机，了解它们的详细信息，并进行搜索和排序操作。

本文主要分析 AngularJS 官方网站提供的一个用于学习的示例项目 PhoneCat 的构建、测试过程以及代码的运行原理。希望能够对 PhoneCat 项目有一个更加深入全面的认识。这其中包括以下内容：


  该项目如何运行起来的
  该项目如何进行前端单元测试
  AngularJS 相关代码分析


以下内容如有理解不正确，欢迎指正！

1. 环境搭建

对于 PhoneCat 项目的开发环境和测试环境...</content>
  </entry>
  
  <entry>
    <title>AngularJS基本知识点</title>
    <link href="/2015/01/08/basic-concepts-of-angularjs.html"/>
    <updated>2015-01-08T00:00:00+08:00</updated>
    <id>/2015/01/08/basic-concepts-of-angularjs.html</id>
    <content type="html">AngularJS 是一个 MV* 框架，最适于开发客户端的单页面应用。它不是个功能库，而是用来开发动态网页的框架。它专注于扩展 HTML 的功能，提供动态数据绑定（data binding），而且它能跟其它框架（如 JQuery 等）合作融洽。

1. 一个简单示例

通过下面的示例代码，可以运行一个简单的 AngularJS 应用：

&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;body&amp;gt;

&amp;lt;div ng-app=&quot;&quot;&amp;gt;
  &amp;lt;p&amp;gt;在输入框中尝试输入：&amp;lt;/p&amp;gt;
  &amp;lt;p&amp;gt;姓名：&amp;lt;input...</content>
  </entry>
  
  <entry>
    <title>Gradle构建多模块项目</title>
    <link href="/2015/01/07/build-multi-module-project-with-gradle.html"/>
    <updated>2015-01-07T00:00:00+08:00</updated>
    <id>/2015/01/07/build-multi-module-project-with-gradle.html</id>
    <content type="html">废话不多说，直接进入主题。

1. 创建项目

首先创建项目，名称为 test：

mkdir test &amp;amp;&amp;amp; cd test
gradle init


这时候的项目结构如下：

➜  test  tree
.
├── build.gradle
├── gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
└── settings.gradle

2 directories, 6 fil...</content>
  </entry>
  
  <entry>
    <title>使用Spring Boot和Gradle创建AngularJS项目</title>
    <link href="/2015/01/06/build-app-with-spring-boot-and-gradle.html"/>
    <updated>2015-01-06T00:00:00+08:00</updated>
    <id>/2015/01/06/build-app-with-spring-boot-and-gradle.html</id>
    <content type="html">Spring Boot 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。

本文主要是记录使用 Spring Boot 和 Gradle 创建项目的过程，其中会包括 Spring Boot 的安装及使用方法，希望通过这篇文章能够快速搭建一个项目。

1. 开发环境


  操作系统: mac
  JDK：1.7.0_60
  Gradle：2.2.1
  IDE：Idea


2. 创建项目

你可以通过 Spring Initializr 来创建一个空的...</content>
  </entry>
  
  <entry>
    <title>Python中的多线程</title>
    <link href="/2014/12/23/thread-in-python.html"/>
    <updated>2014-12-23T00:00:00+08:00</updated>
    <id>/2014/12/23/thread-in-python.html</id>
    <content type="html">线程模块

Python 通过两个标准库 thread 和 threading 提供对线程的支持。Python 的 thread 模块是比较底层的模块，Python 的 threading 模块是对 thread 做了一些包装的，可以更加方便的被使用。

thread 模块提供的其他方法：


  start_new_thread(function,args,kwargs=None)：生一个新线程，在新线程中用指定参数和可选的 kwargs 调用 function 函数
  allocate_lock()：分配一个 LockType 类型的锁对象（注意，此时还没有获得锁）
  inter...</content>
  </entry>
  
  <entry>
    <title>使用Django创建Blog</title>
    <link href="/2014/12/19/create-blog-using-django.html"/>
    <updated>2014-12-19T00:00:00+08:00</updated>
    <id>/2014/12/19/create-blog-using-django.html</id>
    <content type="html">本文参考 Part 1: Creating a blog system using django + markdown 使用 django、bootstrap3 创建一个支持 markdown 语法的简单 blog，这是一个很好的学习 django 的例子，基本上涉及了 django 的方方面面，本文中相关的源码见 github。

相对于原文做了一些修改：


  基于 django1.7，修复了不兼容的配置
  前端使用 bootstrap3，并修改了页面的一些内容
  【TODO】代码中关键的地方添加注释，帮助理解


1. 安装依赖

pip install markdown p...</content>
  </entry>
  
  <entry>
    <title>JPA的使用</title>
    <link href="/2014/12/02/some-usages-of-jpa.html"/>
    <updated>2014-12-02T00:00:00+08:00</updated>
    <id>/2014/12/02/some-usages-of-jpa.html</id>
    <content type="html">JPA，Java 持久化规范，是从EJB2.x以前的实体 Bean 分离出来的，EJB3 以后不再有实体 bean，而是将实体 bean 放到 JPA 中实现。

JPA 是 sun 提出的一个对象持久化规范，各 JavaEE 应用服务器自主选择具体实现，JPA 的设计者是 Hibernate 框架的作者，因此Hibernate作为Jboss服务器中JPA的默认实现，Oracle的Weblogic使用EclipseLink(以前叫TopLink)作为默认的JPA实现，IBM的Websphere和Sun的Glassfish默认使用OpenJPA(Apache的一个开源项目)作为其默认的JP...</content>
  </entry>
  
  <entry>
    <title>Hadoop集群部署权限总结</title>
    <link href="/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop.html"/>
    <updated>2014-11-25T00:00:00+08:00</updated>
    <id>/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop.html</id>
    <content type="html">这是一篇总结的文章，主要介绍 Hadoop 集群快速部署权限的步骤以及一些注意事项。如果你想了解详细的过程，请参考本博客中其他的文章。

1. 开始之前

hadoop 集群一共有三个节点，每个节点的 ip、hostname、角色如下：

192.168.56.121 cdh1 NameNode、kerberos-server、ldap-server、sentry-store
192.168.56.122 cdh2 DataNode、yarn、hive、impala
192.168.56.123 cdh3 DataNode、yarn、hive、impala


一些注意事项：


  操...</content>
  </entry>
  
  <entry>
    <title>Spring集成JPA2.0</title>
    <link href="/2014/11/24/spring-with-jpa2.html"/>
    <updated>2014-11-24T00:00:00+08:00</updated>
    <id>/2014/11/24/spring-with-jpa2.html</id>
    <content type="html">JPA 全称 Java Persistence API，是Java EE 5标准之一，是一个 ORM 规范，由厂商来实现该规范，目前有 Hibernate、OpenJPA、TopLink、EclipseJPA 等实现。Spring目前提供集成Hibernate、OpenJPA、TopLink、EclipseJPA四个JPA标准实现。

1. 集成方式

Spring提供三种方法集成JPA：


  
    
      LocalEntityManagerFactoryBean：适用于那些仅使用JPA进行数据访问的项目。
    
  
  
    
      从JNDI中获取：用...</content>
  </entry>
  
  <entry>
    <title>Zookeeper配置Kerberos认证</title>
    <link href="/2014/11/18/config-kerberos-in-cdh-zookeeper.html"/>
    <updated>2014-11-18T00:00:00+08:00</updated>
    <id>/2014/11/18/config-kerberos-in-cdh-zookeeper.html</id>
    <content type="html">关于 Hadoop 集群上配置 kerberos 以及 ldap 的过程请参考本博客以下文章：


  HDFS配置Kerberos认证
  YARN配置Kerberos认证
  Hive配置Kerberos认证
  Impala配置Kerberos认证
  Hadoop配置LDAP集成Kerberos


参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下：

192.168.56.121        cdh1     NameNode、Hive、ResourceManager、HBase、impala...</content>
  </entry>
  
  <entry>
    <title>配置安全的Impala集群集成Sentry</title>
    <link href="/2014/11/14/config-secured-impala-with-sentry.html"/>
    <updated>2014-11-14T00:00:00+08:00</updated>
    <id>/2014/11/14/config-secured-impala-with-sentry.html</id>
    <content type="html">本文主要记录配置安全的Impala集群集成Sentry的过程。Impala集群上配置了Kerberos认证，并且需要提前配置好Hive与Kerberos和Sentry的集成：


  使用yum安装CDH Hadoop集群
  Hive配置kerberos认证
  Impala配置kerberos认证
  配置安全的Hive集群集成Sentry


1. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  JDK版本：1.7.0_71
  运行用户：root


集群各节点角色规划为：

192.168.56.121        cd...</content>
  </entry>
  
  <entry>
    <title>配置安全的Hive集群集成Sentry</title>
    <link href="/2014/11/14/config-secured-hive-with-sentry.html"/>
    <updated>2014-11-14T00:00:00+08:00</updated>
    <id>/2014/11/14/config-secured-hive-with-sentry.html</id>
    <content type="html">本文主要记录配置安全的Hive集群集成Sentry的过程。Hive上配置了Kerberos认证，配置的过程请参考：


  使用yum安装CDH Hadoop集群
  Hive配置kerberos认证


1. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  JDK版本：1.7.0_71
  运行用户：root


集群各节点角色规划为：

192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impal...</content>
  </entry>
  
  <entry>
    <title>Hadoop配置LDAP集成Kerberos</title>
    <link href="/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop.html"/>
    <updated>2014-11-12T00:00:00+08:00</updated>
    <id>/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop.html</id>
    <content type="html">本文主要记录 cdh hadoop 集群集成 ldap 的过程，这里 ldap 安装的是 OpenLDAP 。LDAP 用来做账号管理，Kerberos作为认证。授权一般来说是由应用来决定的，通过在 LDAP 数据库中配置一些属性可以让应用程序来进行授权判断。

关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。

1. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  JDK版本：1.7.0_71
  OpenLDAP 版本：2.4.39
  Kerberos 版本：...</content>
  </entry>
  
  <entry>
    <title>Impala配置Kerberos认证</title>
    <link href="/2014/11/06/config-kerberos-in-cdh-impala.html"/>
    <updated>2014-11-06T00:00:00+08:00</updated>
    <id>/2014/11/06/config-kerberos-in-cdh-impala.html</id>
    <content type="html">关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。

关于 Kerberos 的安装和 YARN 配置 kerberos 认证，请参考 YARN配置kerberos认证。

关于 Kerberos 的安装和 Hive 配置 kerberos 认证，请参考 Hive配置kerberos认证。

1. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  JDK版本：1.7.0_71
  运行用户：root


集群各节点角色规划为：

192.168.56.121      ...</content>
  </entry>
  
  <entry>
    <title>Hive配置Kerberos认证</title>
    <link href="/2014/11/06/config-kerberos-in-cdh-hive.html"/>
    <updated>2014-11-06T00:00:00+08:00</updated>
    <id>/2014/11/06/config-kerberos-in-cdh-hive.html</id>
    <content type="html">关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。

关于 Kerberos 的安装和 YARN 配置 kerberos 认证，请参考 YARN配置kerberos认证。

1. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  JDK版本：1.7.0_71
  运行用户：root


集群各节点角色规划为：

192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Imp...</content>
  </entry>
  
  <entry>
    <title>YARN配置Kerberos认证</title>
    <link href="/2014/11/05/config-kerberos-in-cdh-yarn.html"/>
    <updated>2014-11-05T00:00:00+08:00</updated>
    <id>/2014/11/05/config-kerberos-in-cdh-yarn.html</id>
    <content type="html">关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。

1. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  JDK版本：1.7.0_71
  运行用户：root


集群各节点角色规划为：

192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
192.168.56.122        ...</content>
  </entry>
  
  <entry>
    <title>HDFS配置Kerberos认证</title>
    <link href="/2014/11/04/config-kerberos-in-cdh-hdfs.html"/>
    <updated>2014-11-04T00:00:00+08:00</updated>
    <id>/2014/11/04/config-kerberos-in-cdh-hdfs.html</id>
    <content type="html">本文主要记录 CDH Hadoop 集群上配置 HDFS 集成 Kerberos 的过程，包括 Kerberos 的安装和 Hadoop 相关配置修改说明。

1. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  JDK版本：1.7.0_71
  运行用户：root


集群各节点角色规划为：

192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 
19...</content>
  </entry>
  
  <entry>
    <title>Mac上使用homebrew安装PostgreSql</title>
    <link href="/2014/10/30/install-postgresql-on-mac-using-homebrew.html"/>
    <updated>2014-10-30T00:00:00+08:00</updated>
    <id>/2014/10/30/install-postgresql-on-mac-using-homebrew.html</id>
    <content type="html">安装

brew 安装 postgresql ：

$ brew install postgresql


查看安装的版本：

$ pg_ctl -V
pg_ctl (PostgreSQL) 9.3.5


安装成功之后，安装路径为：/usr/local/var/postgres

接下来，初始化数据库：

$ initdb /usr/local/var/postgres
The files belonging to this database system will be owned by user &quot;june&quot;.
This user must also own the server ...</content>
  </entry>
  
  <entry>
    <title>Django中的模板</title>
    <link href="/2014/10/30/django-template.html"/>
    <updated>2014-10-30T00:00:00+08:00</updated>
    <id>/2014/10/30/django-template.html</id>
    <content type="html">通过《如何创建一个Django网站》大概清楚了如何创建一个简单的 Django 网站，这篇文章主要是在此基础上介绍 Django 中模板相关的用法。

视图中使用模板

在《如何创建一个Django网站》中使用模板的方式如下：

from django.http import HttpResponse
from django.template import RequestContext, loader

from polls.models import Question

def index(request):
    latest_question_list = Question.ob...</content>
  </entry>
  
  <entry>
    <title>Impala查询功能测试</title>
    <link href="/2014/10/24/impala-query-table-tutorial.html"/>
    <updated>2014-10-24T00:00:00+08:00</updated>
    <id>/2014/10/24/impala-query-table-tutorial.html</id>
    <content type="html">关于 Impala 使用方法的一些测试，包括加载数据、查看数据库、聚合关联查询、子查询等等。

1. 准备测试数据

以下测试以 impala 用户来运行：

$ su - impala
-bash-4.1$ whoami
impala
$ hdfs dfs -ls /user
Found 5 items
drwxr-xr-x   - hdfs   hadoop          0 2014-09-22 18:36 /user/hdfs
drwxrwxrwt   - mapred hadoop          0 2014-07-23 21:37 /user/history
drwx...</content>
  </entry>
  
  <entry>
    <title>当前数据仓库建设过程</title>
    <link href="/2014/10/23/hive-warehouse-in-2014.html"/>
    <updated>2014-10-23T00:00:00+08:00</updated>
    <id>/2014/10/23/hive-warehouse-in-2014.html</id>
    <content type="html">一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。

1. 数据采集和存储

采集数据之前，先要定义数据如何存放在 hadoop 以及一些相关约束。约束如下：


  所有的日志数据都存放在 hdfs 上的 /logroot 路径下面
  hive 中数据库命名方式为 dw_XXXX，例如：dw_srclog 存放外部来源的原始数据，dw_stat 存放统计结果的数据
  原始数据都加工成为结构化的文本文件，字段分隔符统一使用制表符，并在 lzo 压缩之后上传到 hdfs 中。
  hive 中使用外部表保存...</content>
  </entry>
  
  <entry>
    <title>CDH 5.2.0 的改变</title>
    <link href="/2014/10/20/cdh5.2-release.html"/>
    <updated>2014-10-20T00:00:00+08:00</updated>
    <id>/2014/10/20/cdh5.2-release.html</id>
    <content type="html">最近 CDH 5.2.0 发布了，想看看其做了哪些改进、带来哪些不兼容以及是否有必要升级现有的 hadoop 集群。

1. CDH 5.2.0 新特性

1.1. Apache Avro

Avro 版本使用1.7.6，重要的一些改变：


  AVRO-1398。增加同步间隔，从16k 调整到64k，该参数可以在 mapreduce 的配置参数中通过 avro.mapred.sync.interval 参数来设置
  AVRO-1355。schema 中不能包括相同的 field 名称。


1.2. Apache Hadoop

HDFS

提供新的功能：


  HDFS Dat...</content>
  </entry>
  
  <entry>
    <title>Spring源码整体架构</title>
    <link href="/2014/09/29/spring-source-codes.html"/>
    <updated>2014-09-29T00:00:00+08:00</updated>
    <id>/2014/09/29/spring-source-codes.html</id>
    <content type="html">前言

Spring 是一个开源框架，是为了解决企业应用程序开发复杂性而创建的。框架的主要优势之一就是其分层架构，分层架构允许您选择使用哪一个组件，同时为 J2EE 应用程序开发提供集成的框架。

从这篇文章开始，我讲开始阅读并介绍 Spring 源码的设计思想，希望能改对 Spring 框架有一个初步的全面的认识，并且学习其架构设计方面的一些理念和方法。

Spring 源码地址：https://github.com/spring-projects/spring-framework

概述

Spring的整体架构

Spring 总共有十几个组件，其中核心组件只有三个：Core、Co...</content>
  </entry>
  
  <entry>
    <title>编译Dubbo源码并测试</title>
    <link href="/2014/09/24/compile-and-test-dubbo.html"/>
    <updated>2014-09-24T00:00:00+08:00</updated>
    <id>/2014/09/24/compile-and-test-dubbo.html</id>
    <content type="html">Dubbo是阿里巴巴内部的SOA服务化治理方案的核心框架，每天为2000+ 个服务提供3,000,000,000+ 次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。Dubbo自2011年开源后，已被许多非阿里系公司使用。


  项目主页：http://alibaba.github.io/dubbo-doc-static/Home-zh.htm
  项目源码：https://github.com/alibaba/dubbo


1. 安装

首先从 github 下载源代码并阅读 readme.md ，参考该文档，首先下载 opensesame，并编译：

$ git clone ...</content>
  </entry>
  
  <entry>
    <title>Mahout推荐引擎介绍</title>
    <link href="/2014/09/22/mahout-recommend-engine.html"/>
    <updated>2014-09-22T00:00:00+08:00</updated>
    <id>/2014/09/22/mahout-recommend-engine.html</id>
    <content type="html">Mahout 是一个来自 Apache 的、开源的机器学习软件库，他主要关注于推荐引擎（协同过滤）、聚类和分类。

推荐一般是基于物品或者用户进行推荐相关。

聚类是讲大量的事物组合为拥有类似属性的簇，借以在一些规模较大或难于理解的数据集上发现层次结构和顺序，以揭示一些有用的模式或让数据集更易于理解。

分类有助于判断一个新的输入或新的事物是否于以前观察到的模式相匹配，它通常还被用于筛选异常的行为或模式，来检测可疑的网络活动或欺骗行为。

推荐系统

推荐引擎算法应用最广的两大类：基于用户和基于物品的推荐。这两者都是协同过滤的范畴：仅仅通过了解用户于物品之间的关系进行推荐。这些技术无需了...</content>
  </entry>
  
  <entry>
    <title>使用Gradle构建项目</title>
    <link href="/2014/09/15/build-project-with-gradle.html"/>
    <updated>2014-09-15T00:00:00+08:00</updated>
    <id>/2014/09/15/build-project-with-gradle.html</id>
    <content type="html">Gradle 是一款基于 Groovy 语言、免费开源的构建工具，它既保持了 Maven 的优点，又通过使用 Groovy 定义的 DSL 克服了 Maven 中使用 XML 繁冗以及不灵活的缺点。

Gradle 官方网站：http://www.gradle.org/downloads

安装

一种方式是从 官方 下载解压然后配置环境变量。

Mac 上安装：

$ brew install gradle


测试是否安装成功：

$ gradle -v
------------------------------------------------------------
Grad...</content>
  </entry>
  
  <entry>
    <title>使用Groovy操作文件</title>
    <link href="/2014/09/12/file-operation-in-groovy.html"/>
    <updated>2014-09-12T00:00:00+08:00</updated>
    <id>/2014/09/12/file-operation-in-groovy.html</id>
    <content type="html">Java 读写文件比较麻烦，那 Groovy 操作文件又如何呢？

1. 读文件

读文件内容

在groovy中输出文件的内容：

println new File(&quot;tmp.csv&quot;).text  


上面代码非常简单，没有流的出现，没有资源关闭的出现，也没有异常控制的出现，所有的这些groovy已经搞定了。

读取每一行内容：

File file = new File(&#39;tmp.csv&#39;)
assert file.name == &#39;tmp.csv&#39;
assert ! file.isAbsolute()
assert file.path == &#39;tmp.csv&#39;
assert f...</content>
  </entry>
  
  <entry>
    <title>Llama的使用</title>
    <link href="/2014/09/09/llama.html"/>
    <updated>2014-09-09T00:00:00+08:00</updated>
    <id>/2014/09/09/llama.html</id>
    <content type="html">1. 介绍

Llama (Low Latency Application MAster) 是一个 Yarn 的  Application Master，用于协调 Impala 和 Yarn 之间的集群资源的管理和监控。Llama 使 Impala 能够获取、使用和释放资源配额，而不需要 Impala 使用 Yarn 管理的 container 进程。Llama 提供了 Thrift API 来和 Yarn 交互。

个人理解，Llama 的作用就是使 Impala 能够工作在 YARN 之上，使得 Impala 和 YARN 共享集群资源，提供低延迟的查询。


  Llama 官网地...</content>
  </entry>
  
  <entry>
    <title>从零开始创建Grails应用</title>
    <link href="/2014/09/09/create-a-grails-app-step-by-step.html"/>
    <updated>2014-09-09T00:00:00+08:00</updated>
    <id>/2014/09/09/create-a-grails-app-step-by-step.html</id>
    <content type="html">本篇文章主要介绍如何从零开始一步一步创建一个 Grails 应用程序。整个过程中，你将学到如何改变 Grails 运行的端口，了解 Grails 应用的基础组成部分(领域类、控制器和视图)、指定字段的缺省值，以及其他许多内容。

1. 安装

下载压缩包然后解压或者通过rpm、deb发行包安装。

这里，我在 mac 上安装 grails ：

$ brew install grails


2. 创建应用

以 blog 为例，创建一个应用程序，在命令行输入 grails create-app blog ：

$ grails create-app blog
2014-9-9 10:4...</content>
  </entry>
  
  <entry>
    <title>Groovy语法介绍</title>
    <link href="/2014/09/05/about-groovy.html"/>
    <updated>2014-09-05T00:00:00+08:00</updated>
    <id>/2014/09/05/about-groovy.html</id>
    <content type="html">1. 介绍

Groovy 是基于 JRE 的脚本语言，和Perl，Python等等脚本语言一样，它能以快速简洁的方式来完成一些工作：如访问数据库，编写单元测试用例，快速实现产品原型等等。

Groovy 是由James Strachan 和 Bob McWhirter 这两位天才发明的（JSR 241 2004 年 3 月）。Groovy 完全以Java API为基础，使用了Java开发人员最熟悉的功能和库。Groovy 的语法近似Java，并吸收了 Ruby 的一些特点，因此 Groovy 在某些场合可以扮演一种 “咖啡伴侣”的角色。

官网地址：http://groovy.code...</content>
  </entry>
  
  <entry>
    <title>安装Azkaban</title>
    <link href="/2014/08/25/install-azkaban.html"/>
    <updated>2014-08-25T00:00:00+08:00</updated>
    <id>/2014/08/25/install-azkaban.html</id>
    <content type="html">Azkaban 是由 Linkedin 开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban 定义了一种 KV 文件格式来建立任务之间的依赖关系，并提供一个易于使用的 web 用户界面维护和跟踪你的工作流。

Azkaban 官网地址：http://azkaban.github.io/
Azkaban 的下载地址：http://azkaban.github.io/downloads.html

Azkaban 包括三个关键组件：


  关系数据库：使用 Mysql数据库，主要用于保存流程、权限、任务状态、任务计划等信息。
  Azkaba...</content>
  </entry>
  
  <entry>
    <title>升级cdh4到cdh5</title>
    <link href="/2014/08/19/upgrading-from-cdh4-to-cdh5.html"/>
    <updated>2014-08-19T00:00:00+08:00</updated>
    <id>/2014/08/19/upgrading-from-cdh4-to-cdh5.html</id>
    <content type="html">本文主要记录从CDH4升级到CDH5的过程和遇到的问题，当然本文同样适用于CDH5低版本向最新版本的升级。

1. 不兼容的变化

升级前，需要注意 cdh5 有哪些不兼容的变化，具体请参考：Apache Hadoop Incompatible Changes。

2. 升级过程

2.1. 备份数据和停止所有服务

2.1.1 让 namenode 进入安全模式

在NameNode或者配置了 HA 中的 active NameNode上运行下面命令：

$ sudo -u hdfs hdfs dfsadmin -safemode enter


保存 fsimage：

$ sudo...</content>
  </entry>
  
  <entry>
    <title>Sqoop导入关系数据库到Hive</title>
    <link href="/2014/08/04/import-data-to-hive-with-sqoop.html"/>
    <updated>2014-08-04T00:00:00+08:00</updated>
    <id>/2014/08/04/import-data-to-hive-with-sqoop.html</id>
    <content type="html">Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。本文以 mysql 数据库为例，实现关系数据库导入到 hdfs 和 hive。

1. 安装 Sqoop

使用 rpm 安装即可。

yum install sqoop sqoop-metastore -y



  安装完之后需要下载 mysql jar 包到 sqoop 的 lib 目录。


这里使用 hive 的 metastore 的 mysql 数据库作为关系数据库，以 TBLS 表为例，该表结构和数据如下：

mysql&amp;gt; select * from TBLS limit 3;
+--...</content>
  </entry>
  
  <entry>
    <title>2014年7月总结</title>
    <link href="/2014/07/31/summary-of-july-in-2014.html"/>
    <updated>2014-07-31T00:00:00+08:00</updated>
    <id>/2014/07/31/summary-of-july-in-2014.html</id>
    <content type="html">在休息了将近三个月之后，7月9日终于开始上班了，新的工作还是和 hadoop 相关。7月主要的工作内容如下：


  搭建新的 hadoop 集群，hadoop 版本为 CDH4.7.0，并配置 NameNode 的 QJM HA 方案。配置 HA 方法见 CDH 中配置 HDFS HA
  购买了三本书：
    
      mahout实战
      机器学习实战
      这才是搜索引擎
    
  
  调研了 flume-ng 日志采集方案
    
      大众点评的大数据实践
      analyzing-twitter-data-with-hadoop
  ...</content>
  </entry>
  
  <entry>
    <title>Impala新特性</title>
    <link href="/2014/07/29/new-features-in-impala.html"/>
    <updated>2014-07-29T00:00:00+08:00</updated>
    <id>/2014/07/29/new-features-in-impala.html</id>
    <content type="html">本文主要整理一下 Impala 每个版本的新特性，方便了解 Impala 做了哪些改进、修复了哪些 bug。

Impala 目前最新版本为 1.4.0，其下载地址为：http://archive.cloudera.com/impala/redhat/6/x86_64/impala/

不得不说的事情：


  1.3.1 用于 CDH4
  1.4.0 用于 CDH5


1.4.0


  CDH5 中增加 DECIMAL 数据类型，可以设置精度，其语法为：DECIMAL[(precision[,scale])]
  CDH5 中，impala 可以使用 HDFS 缓存特性加快频繁访...</content>
  </entry>
  
  <entry>
    <title>Phoenix Quick Start</title>
    <link href="/2014/07/28/phoenix-quick-start.html"/>
    <updated>2014-07-28T00:00:00+08:00</updated>
    <id>/2014/07/28/phoenix-quick-start.html</id>
    <content type="html">1. 介绍

Phoenix 是 Salesforce.com 开源的一个 Java 中间件，可以让开发者在Apache HBase 上执行 SQL 查询。Phoenix完全使用Java编写，代码位于 GitHub 上，并且提供了一个客户端可嵌入的 JDBC 驱动。


  根据项目所述，Phoenix 被 Salesforce.com 内部使用，对于简单的低延迟查询，其量级为毫秒；对于百万级别的行数来说，其量级为秒。Phoenix 并不是像 HBase 那样用于 map-reduce job 的，而是通过标准化的语言来访问 HBase 数据的。

  Phoenix 为 HBase 提...</content>
  </entry>
  
  <entry>
    <title>采集日志到Hive</title>
    <link href="/2014/07/25/collect-log-to-hive.html"/>
    <updated>2014-07-25T00:00:00+08:00</updated>
    <id>/2014/07/25/collect-log-to-hive.html</id>
    <content type="html">我们现在的需求是需要将线上的日志以小时为单位采集并存储到 hive 数据库中，方便以后使用  mapreduce 或者 impala 做数据分析。为了实现这个目标调研了 flume 如何采集数据到 hive，其他的日志采集框架尚未做调研。

日志压缩

flume中有个 HdfsSink 组件，其可以压缩日志进行保存，故首先想到我们的日志应该以压缩的方式进行保存，遂选择了 lzo 的压缩格式，HdfsSink 的配置如下:

agent-1.sinks.sink_hdfs.channel = ch-1
agent-1.sinks.sink_hdfs.type = hdfs
agent-1...</content>
  </entry>
  
  <entry>
    <title>Flume-ng的原理和使用</title>
    <link href="/2014/07/22/flume-ng.html"/>
    <updated>2014-07-22T00:00:00+08:00</updated>
    <id>/2014/07/22/flume-ng.html</id>
    <content type="html">1. 介绍

Flume NG是Cloudera提供的一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中心化数据存储系统中。由原来的Flume OG到现在的Flume NG，进行了架构重构，并且现在NG版本完全不兼容原来的OG版本。经过架构重构后，Flume NG更像是一个轻量的小工具，非常简单，容易适应各种方式日志收集，并支持failover和负载均衡。

Flume 使用 java 编写，其需要运行在 Java1.6 或更高版本之上。


  官方网站：http://flume.apache.org/
  用户文档：http://...</content>
  </entry>
  
  <entry>
    <title>CDH中配置HDFS HA</title>
    <link href="/2014/07/18/install-hdfs-ha-in-cdh.html"/>
    <updated>2014-07-18T00:00:00+08:00</updated>
    <id>/2014/07/18/install-hdfs-ha-in-cdh.html</id>
    <content type="html">最近又安装 hadoop 集群， 故尝试了一下配置 HDFS 的 HA，CDH4支持Quorum-based Storage和shared storage using NFS两种HA方案，而CDH5只支持第一种方案，即 QJM 的 HA 方案。

关于 hadoop 集群的安装部署过程你可以参考 使用yum安装CDH Hadoop集群 或者 手动安装 hadoop 集群的过程。

集群规划

我一共安装了三个节点的集群，对于 HA 方案来说，三个节点准备安装如下服务：


  cdh1：hadoop-hdfs-namenode(primary) 、hadoop-hdfs-journaln...</content>
  </entry>
  
  <entry>
    <title>手动安装Hadoop集群的过程</title>
    <link href="/2014/07/17/manual-install-cdh-hadoop.html"/>
    <updated>2014-07-17T00:00:00+08:00</updated>
    <id>/2014/07/17/manual-install-cdh-hadoop.html</id>
    <content type="html">最近又安装 Hadoop 集群，由于一些原因，没有使用 Hadoop 管理工具或者自动化安装脚本来安装集群，而是手动一步步的来安装，本篇文章主要是记录我手动安装 Hadoop 集群的过程，给大家做个参考。

这里所说的手动安装，是指一步步的通过脚本来安装集群，并不是使用一键安装脚本或者一些管理界面来安装。

开始之前，还是说明一下环境：


  操作系统：CentOs6.4
  CDH版本：4.7.0
  节点数：4个


在开始之前，你可以看看我以前写的一篇文章 使用yum安装CDH Hadoop集群，因为有些细节已经为什么这样做我不会在这篇文章中讲述。

一些准备工作

在开始前，先...</content>
  </entry>
  
  <entry>
    <title>Spark集群安装和使用</title>
    <link href="/2014/07/01/spark-install-and-usage.html"/>
    <updated>2014-07-01T00:00:00+08:00</updated>
    <id>/2014/07/01/spark-install-and-usage.html</id>
    <content type="html">本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。

安装环境如下：


  操作系统：CentOs 6.5
  Hadoop 版本：cdh-5.4.0
  Spark 版本：cdh5-1.3.0_5.4.0


关于 yum 源的配置以及 Hadoop 集群的安装，请参考 使用yum安装CDH Hadoop集群。

1. 安装

首先查看 Spark 相关的包有哪些：

$ yum list |grep spark
spark-core.noarch                  1.3.0+cdh5.4.0+24-1.c...</content>
  </entry>
  
  <entry>
    <title>HBase中的一些注意事项</title>
    <link href="/2014/06/26/some-tips-about-hbase.html"/>
    <updated>2014-06-26T00:00:00+08:00</updated>
    <id>/2014/06/26/some-tips-about-hbase.html</id>
    <content type="html">1. 安装集群前


  配置SSH无密码登陆
  DNS。HBase使用本地 hostname 才获得IP地址，正反向的DNS都是可以的。你还可以设置 hbase.regionserver.dns.interface 来指定主接口，设置 hbase.regionserver.dns.nameserver 来指定nameserver，而不使用系统带的
  安装NTP服务，并配置和检查crontab是否生效
  操作系统调优，包括最大文件句柄，nproc hard 和 soft limits等等
  conf/hdfs-site.xml里面的 dfs.datanode.max.xcieve...</content>
  </entry>
  
  <entry>
    <title>MapReduce任务参数调优</title>
    <link href="/2014/06/24/tuning-in-mapreduce.html"/>
    <updated>2014-06-24T00:00:00+08:00</updated>
    <id>/2014/06/24/tuning-in-mapreduce.html</id>
    <content type="html">本文主要记录Hadoop 2.x版本中MapReduce参数调优，不涉及Yarn的调优。

Hadoop的默认配置文件（以cdh5.0.1为例）：


  core-default.xml
  hdfs-default.xml
  mapred-default.xml



  说明：

  在hadoop2中有些参数名称过时了，例如原来的mapred.reduce.tasks改名为mapreduce.job.reduces了，当然，这两个参数你都可以使用，只是第一个参数过时了。


1. 操作系统调优


  增大打开文件数据和网络连接上限，调整内核参数net.core.somaxco...</content>
  </entry>
  
  <entry>
    <title>MapReduce任务运行过程</title>
    <link href="/2014/06/24/the-running-process-of-mapreduce-job.html"/>
    <updated>2014-06-24T00:00:00+08:00</updated>
    <id>/2014/06/24/the-running-process-of-mapreduce-job.html</id>
    <content type="html">下图是MapReduce任务运行过程的一个图：



Map-Reduce的处理过程主要涉及以下四个部分：


  客户端Client：用于提交Map-reduce任务job
  JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker
  TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker
  HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件


上图中主要包括以下过程：


  提交作业
  作业初始化
  任务...</content>
  </entry>
  
  <entry>
    <title>HBase和Cassandra比较</title>
    <link href="/2014/06/24/hbase-vs-cassandra.html"/>
    <updated>2014-06-24T00:00:00+08:00</updated>
    <id>/2014/06/24/hbase-vs-cassandra.html</id>
    <content type="html">HBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Google File System一样，HBase构建于和Google File System类似的Hadoop HDFS之上。

Cassandra可以看作是Amazon Dynamo的开源实现。和Dynamo不同之处在于，Cassandra结合了Google   Bigtable的ColumnFamily的数据模型。可以简单地认为，Cassandra是一个P2P的，高可靠性并具有丰富的数据模型的分布式文件系统。

HBase vs Cassandra

...</content>
  </entry>
  
  <entry>
    <title>Hive中的排序语法</title>
    <link href="/2014/06/22/sort-in-hive-query.html"/>
    <updated>2014-06-22T00:00:00+08:00</updated>
    <id>/2014/06/22/sort-in-hive-query.html</id>
    <content type="html">ORDER BY

hive中的ORDER BY语句和关系数据库中的sql语法相似。他会对查询结果做全局排序，这意味着所有的数据会传送到一个Reduce任务上，这样会导致在大数量的情况下，花费大量时间。

与数据库中 ORDER BY 的区别在于在hive.mapred.mode = strict模式下，必须指定 limit 否则执行会报错。

hive&amp;gt; set hive.mapred.mode=strict;
hive&amp;gt; select * from test order by id;
FAILED: SemanticException 1:28 In strict mod...</content>
  </entry>
  
  <entry>
    <title>Lucene介绍</title>
    <link href="/2014/06/21/the-introduction-of-lucene.html"/>
    <updated>2014-06-21T00:00:00+08:00</updated>
    <id>/2014/06/21/the-introduction-of-lucene.html</id>
    <content type="html">1. Lucene是什么

Lucene 是一个开源的、成熟的全文索引与信息检索(IR)库，采用Java实现。信息检索式指文档搜索、文档内信息搜索或者文档相关的元数据搜索等操作。Lucene是apache软件基金会项目组的一个子项目，是一个开放源代码的全文检索引擎工具包，即它不是一个完整的搜索应用程序，而是为你的应用程序提供索引和搜索功能。

Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。全文检索（Full-Text Retrieval）是指以文本作为检索对象，找出含有指定词汇的文本。全面、准...</content>
  </entry>
  
  <entry>
    <title>Storm集群安装部署步骤</title>
    <link href="/2014/06/19/how-to-install-and-deploy-a-storm-cluster.html"/>
    <updated>2014-06-19T00:00:00+08:00</updated>
    <id>/2014/06/19/how-to-install-and-deploy-a-storm-cluster.html</id>
    <content type="html">开始学习Storm，本文主要记录Storm集群安装部署步骤，不包括对Storm的介绍。

安装storm集群，需要依赖以下组件：


  Zookeeper
  Python
  Zeromq
  Storm
  JDK
  JZMQ


故安装过程根据上面的组件分为以下几步：


  安装JDK
  安装Zookeeper集群
  安装Python及依赖
  安装Storm


另外，操作系统环境为：Centos6.4，安装用户为：root。

1. 安装JDK

安装jdk有很多方法，可以参考文博客使用yum安装CDH Hadoop集群中的jdk安装步骤，需要说明的是下面的zooke...</content>
  </entry>
  
  <entry>
    <title>Effective Java 笔记</title>
    <link href="/2014/06/17/note-about-effective-java.html"/>
    <updated>2014-06-17T00:00:00+08:00</updated>
    <id>/2014/06/17/note-about-effective-java.html</id>
    <content type="html">创建和销毁对象

NO.1 考虑用静态工厂方法代替构造函数

静态工厂方法好处：


  1、构造函数有命名的限制，而静态方法有自己的名字，更加易于理解。
  2、静态工厂方法在每次调用的时候不要求创建一个新的对象。这种做法对于一个要频繁创建相同对象的程序来说，可以极大的提高性能。它使得一个类可以保证是一个singleton；他使非可变类可以保证“不会有两个相等的实例存在”。
  3、静态工厂方法在选择返回类型时有更大的灵活性。使用静态工厂方法，可以通过调用方法时使用不同的参数创建不同类的实例，还可以创建非公有类的对象，这就封装了类的实现细节。
  4、在创建参数化类型实例的时候，他们使...</content>
  </entry>
  
  <entry>
    <title>HBase源码分析：HTable put过程</title>
    <link href="/2014/06/13/hbase-code-about-htable-put.html"/>
    <updated>2014-06-13T00:00:00+08:00</updated>
    <id>/2014/06/13/hbase-code-about-htable-put.html</id>
    <content type="html">HBase版本：0.94.15-cdh4.7.0

在 HBase中，大部分的操作都是在RegionServer完成的，Client端想要插入、删除、查询数据都需要先找到相应的 RegionServer。什么叫相应的RegionServer？就是管理你要操作的那个Region的RegionServer。Client本身并 不知道哪个RegionServer管理哪个Region，那么它是如何找到相应的RegionServer的？本文就是在研究源码的基础上了解这个过程。

首先来看看写过程的序列图：



客户端代码

1、put方法

HTable的put有两个方法：

public vo...</content>
  </entry>
  
  <entry>
    <title>Hive Over HBase的介绍</title>
    <link href="/2014/06/12/intro-of-hive-over-hbase.html"/>
    <updated>2014-06-12T00:00:00+08:00</updated>
    <id>/2014/06/12/intro-of-hive-over-hbase.html</id>
    <content type="html">Hive Over HBase是基于Hive的HQL查询引擎支持对hbase表提供及时查询的功能，它并不是将hql语句翻译成mapreduce来运行，其响应时间在秒级别。

特性

支持的字段类型：

boolean, tinyint, smallint, int, bigint, float, double, string, struct
(当hbase中的rowkey字段为struct类型，请将子字段定义为string类型，同时指定表的collection items terminated分隔字符以及各字段的长度参数:hbase.rowkey.column.length)

支持的s...</content>
  </entry>
  
  <entry>
    <title>HBase客户端实现并行扫描</title>
    <link href="/2014/06/12/hbase-parallel-client-scanner.html"/>
    <updated>2014-06-12T00:00:00+08:00</updated>
    <id>/2014/06/12/hbase-parallel-client-scanner.html</id>
    <content type="html">HBase中有一个类可以实现客户端扫描数据，叫做ClientScanner，该类不是并行的，有没有办法实现一个并行的扫描类，加快扫描速度呢？

如果是一个Scan，我们可以根据startkey和stopkey将其拆分为多个子Scan，然后让这些Scan并行的去查询数据，然后分别返回执行结果。

实现方式

说明：我使用的HBase版本为：cdh4-0.94.15_4.7.0。

在org.apache.hadoop.hbase.client创建ParallelClientScanner类，代码如下：

package org.apache.hadoop.hbase.client;

im...</content>
  </entry>
  
  <entry>
    <title>HBase实现简单聚合计算</title>
    <link href="/2014/06/12/hbase-aggregate-client.html"/>
    <updated>2014-06-12T00:00:00+08:00</updated>
    <id>/2014/06/12/hbase-aggregate-client.html</id>
    <content type="html">本文主要记录如何通过打补丁的方式将“hbase中实现简单聚合计算”的特性引入hbase源代码中，并介绍通过命令行和java代码的使用方法。

支持的简单聚合计算，包括：


  rowcount
  min
  max
  sum
  std
  avg
  median


1、 下载并编译hbase源代码

我这里使用的HBase源代码版本是：cdh4-0.94.6_4.3.0，如果你使用其他版本，有可能patch打不上。

2、 引入patch

基于提交日志add-aggregate-support-in-hbase-shell生成patch文件，然后打patch，或者也可以使用...</content>
  </entry>
  
  <entry>
    <title>Hive中数据的加载和导出</title>
    <link href="/2014/06/09/hive-data-manipulation-language.html"/>
    <updated>2014-06-09T00:00:00+08:00</updated>
    <id>/2014/06/09/hive-data-manipulation-language.html</id>
    <content type="html">关于 Hive DML 语法，你可以参考 apache 官方文档的说明:Hive Data Manipulation Language。

apache的hive版本现在应该是 0.13.0，而我使用的 hadoop 版本是 CDH5.0.1，其对应的 hive 版本是 0.12.0。故只能参考apache官方文档来看 cdh5.0.1 实现了哪些特性。

因为 hive 版本会持续升级，故本篇文章不一定会和最新版本保持一致。

1. 准备测试数据

首先创建普通表：

create table test(id int, name string) ROW FORMAT DELIMITED...</content>
  </entry>
  
  <entry>
    <title>Hive中的FetchTask任务</title>
    <link href="/2014/06/09/fetchtask-in-hive.html"/>
    <updated>2014-06-09T00:00:00+08:00</updated>
    <id>/2014/06/09/fetchtask-in-hive.html</id>
    <content type="html">Hive中有各种各样的Task任务，其中FetchTask算是最简单的一种了。FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。当你执行简单的select * with limit语句的时候，其不会运行mapreduce任务。

例如，运行下面语句不会出现mapreduce任务（说明：t表有一个字段，id为int类型，该表没有数据）：

hive&amp;gt; select * from t limit 1;            
OK
Time taken: 2.466 seconds


去掉limit语句，再执行一次，结果如下：
...</content>
  </entry>
  
  <entry>
    <title>使用Scrapy爬取知乎网站</title>
    <link href="/2014/06/08/using-scrapy-to-cralw-zhihu.html"/>
    <updated>2014-06-08T00:00:00+08:00</updated>
    <id>/2014/06/08/using-scrapy-to-cralw-zhihu.html</id>
    <content type="html">本文主要记录使用使用 Scrapy 登录并爬取知乎网站的思路。Scrapy的相关介绍请参考 使用Scrapy抓取数据。

相关代码，见 https://github.com/javachen/scrapy-zhihu-github ，在阅读这部分代码之前，请先了解 Scrapy 的一些基本用法。

使用cookie模拟登陆

关于 cookie 的介绍和如何使用 python 实现模拟登陆，请参考python爬虫实践之模拟登录。

从这篇文章你可以学习到如何获取一个网站的 cookie 信息。下面所讲述的方法就是使用 cookie 来模拟登陆知乎网站并爬取用户信息。

一个模拟登陆知乎网...</content>
  </entry>
  
  <entry>
    <title>MongoDB介绍</title>
    <link href="/2014/06/06/about-mongodb.html"/>
    <updated>2014-06-06T00:00:00+08:00</updated>
    <id>/2014/06/06/about-mongodb.html</id>
    <content type="html">MongoDB 是一个开源的，高性能，无模式（或者说是模式自由），使用 C++ 语言编写的面向文档的数据库。正因为 MongoDB 是面向文档的，所以它可以管理类似 JSON 的文档集合。又因为数据可以被嵌套到复杂的体系中并保持可以查询可索引，这样一来，应用程序便可以以一种更加自然的方式来为数据建模。

官方网站：http://www.mongodb.org/

MongoDB介绍

所谓“面向集合”（Collenction-Orented），意思是数据被分组存储在数据集中，被称为一个集合（Collenction)。每个集合在数据库中都有一个唯一的标识名，并且可以包含无限数目的文档。集合...</content>
  </entry>
  
  <entry>
    <title>不用Cloudera Manager安装Cloudera Search</title>
    <link href="/2014/06/03/install_cloudera_search_without_cm.html"/>
    <updated>2014-06-03T00:00:00+08:00</updated>
    <id>/2014/06/03/install_cloudera_search_without_cm.html</id>
    <content type="html">Cloudera Search 用来在 hadoop 基础上建立索引和全文检索，本文主要记录如何安装 CLoudera Search 的过程，其中也包括如何安装和启动 Zookeeper、Solr、MapReduce等工具和服务。

Cloudera Search介绍

Cloudera Search 核心部件包括 Hadoop 和 Solr，后者建立在 Lucene 之上；而 Hadoop 也正是在06年正式成为 Lucene 的一个子项目而发展起来的。

通过 Tika, Cloudera Search 支持大量的被广泛使用的文件格式；除此之外，Cloudera Search 还支持...</content>
  </entry>
  
  <entry>
    <title>关于CAP理论的一些笔记</title>
    <link href="/2014/05/30/note-about-brewers-cap-theorem.html"/>
    <updated>2014-05-30T00:00:00+08:00</updated>
    <id>/2014/05/30/note-about-brewers-cap-theorem.html</id>
    <content type="html">CAP的概念

2000年，Eric Brewer 教授在 ACM 分布式计算年会上指出了著名的 CAP 理论：


  分布式系统不可能同时满足一致性(C: Consistency)，可用性(A: Availability)和分区容错性(P: Tolerance of network Partition)这三个需求。大约两年后，Seth Gilbert 和 Nancy lynch 两人证明了CAP理论的正确性。


三者的含义如下：


  Consistency：一致性，一个服务是一致的完整操作或完全不操作（A service that is consistent operates ...</content>
  </entry>
  
  <entry>
    <title>使用Scrapy抓取数据</title>
    <link href="/2014/05/24/using-scrapy-to-cralw-data.html"/>
    <updated>2014-05-24T00:00:00+08:00</updated>
    <id>/2014/05/24/using-scrapy-to-cralw-data.html</id>
    <content type="html">Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。


  官方主页： http://www.scrapy.org/
  中文文档：Scrapy 0.22 文档
  GitHub项目主页：https://github.com/scrapy/scrapy


Scrapy 使用了 Twisted 异步网络库来处理网络通讯。整体架构大致如下（注：图片来自互联网）：



Scrapy主要包括了以下组件：


  引擎，用来处理整个系统的数据流处理，触发事务。
  ...</content>
  </entry>
  
  <entry>
    <title>Nutch介绍及使用</title>
    <link href="/2014/05/20/nutch-intro.html"/>
    <updated>2014-05-20T00:00:00+08:00</updated>
    <id>/2014/05/20/nutch-intro.html</id>
    <content type="html">1. Nutch介绍

Nutch是一个开源的网络爬虫项目，更具体些是一个爬虫软件，可以直接用于抓取网页内容。

现在Nutch分为两个版本，1.x和2.x。1.x最新版本为1.7，2.x最新版本为2.2.1。两个版本的主要区别在于底层的存储不同。

1.x版本是基于Hadoop架构的，底层存储使用的是HDFS，而2.x通过使用Apache Gora，使得Nutch可以访问HBase、Accumulo、Cassandra、MySQL、DataFileAvroStore、AvroStore等NoSQL。

2. 编译Nutch

Nutch1.x从1.7版本开始不再提供完整的部署文件，只提...</content>
  </entry>
  
  <entry>
    <title>Python开发框架Flask</title>
    <link href="/2014/05/11/flask-intro.html"/>
    <updated>2014-05-11T00:00:00+08:00</updated>
    <id>/2014/05/11/flask-intro.html</id>
    <content type="html">1. Flask介绍

Flask 是一个基于Python的微型的web开发框架。虽然Flask是微框架，不过我们并不需要像别的微框架建议的那样把所有代码都写到单文件中。毕竟微框架真正的含义是简单和短小。



关于Flask值得知道的一些事：


  Flask由Armin Ronacher于2010年创建。
  Flask的灵感来自Sinatra。（Sinatra是一个极力避免小题大作的创建web应用的Ruby框架。）
  Flask 依赖两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 工具集。
  Flask遵循“约定优于配置”以及合理的默认值原则。


默认情...</content>
  </entry>
  
  <entry>
    <title>Bower介绍</title>
    <link href="/2014/05/10/bower-intro.html"/>
    <updated>2014-05-10T00:00:00+08:00</updated>
    <id>/2014/05/10/bower-intro.html</id>
    <content type="html">1. bower介绍

Bower 是用于 web 前端开发的包管理器。对于前端包管理方面的问题，它提供了一套通用、客观的解决方案。它通过一个 API 暴露包之间的依赖模型，这样更利于使用更合适的构建工具。bower 没有系统级的依赖，在不同 app 之间也不互相依赖，依赖树是扁平的。



Bower 运行在 Git 之上，它将所有包都视作一个黑盒子。任何类型的资源文件都可以打包为一个模块，并且可以使用任何规范（例如：AMD、CommonJS 等）。

包管理工具一般有以下的功能：


  注册机制：每个包需要确定一个唯一的 ID 使得搜索和下载的时候能够正确匹配，所以包管理工具需要维...</content>
  </entry>
  
  <entry>
    <title>All Things Markdown</title>
    <link href="/2014/04/24/all-things-markdown.html"/>
    <updated>2014-04-24T00:00:00+08:00</updated>
    <id>/2014/04/24/all-things-markdown.html</id>
    <content type="html">目录


  概述
  特点
  语法
  编辑器
  浏览器插件
  实现版本
  参考资料


概述

Markdown 是一种轻量级标记语言，创始人为约翰·格鲁伯（John Gruber）和亚伦·斯沃茨（Aaron Swartz）。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML(或者HTML)文档”。这种语言吸收了很多在电子邮件中已有的纯文本标记的特性。

特点

兼容 HTML

要在Markdown中输写HTML区块元素，比如&amp;lt;div&amp;gt;、&amp;lt;table&amp;gt;、&amp;lt;pre&amp;gt;、&amp;lt;p&amp;gt; 等标签，必须在前后加上空行与其它...</content>
  </entry>
  
  <entry>
    <title>重装Mac系统之后</title>
    <link href="/2014/04/23/after-reinstall-mac.html"/>
    <updated>2014-04-23T00:00:00+08:00</updated>
    <id>/2014/04/23/after-reinstall-mac.html</id>
    <content type="html">本文主要记录重装Mac系统之后的一些软件安装和环境变量配置。

系统偏好配置

设置主机名：

$ sudo scutil --set HostName june－mac


设置鼠标滚轮滑动的方向：系统偏好设置－－&amp;gt;鼠标－－&amp;gt;”滚动方向：自然”前面的勾去掉

显示/隐藏Mac隐藏文件：

defaults write com.apple.finder AppleShowAllFiles -bool true  #显示Mac隐藏文件的命令
defaults write com.apple.finder AppleShowAllFiles -bool false #隐藏Mac隐...</content>
  </entry>
  
  <entry>
    <title>Ubuntu系统编译Bigtop</title>
    <link href="/2014/04/17/building-bigtop-on-ubuntu.html"/>
    <updated>2014-04-17T00:00:00+08:00</updated>
    <id>/2014/04/17/building-bigtop-on-ubuntu.html</id>
    <content type="html">1. 安装系统依赖

系统更新并安装新的包

sudo apt-get update

sudo apt-get install -y cmake git-core git-svn subversion checkinstall build-essential dh-make debhelper ant ant-optional autoconf automake liblzo2-dev libzip-dev sharutils libfuse-dev reprepro libtool libssl-dev asciidoc xmlto ssh curl

sudo apt-get in...</content>
  </entry>
  
  <entry>
    <title>Java笔记：Java内存模型</title>
    <link href="/2014/04/09/note-about-jvm-memery-model.html"/>
    <updated>2014-04-09T00:00:00+08:00</updated>
    <id>/2014/04/09/note-about-jvm-memery-model.html</id>
    <content type="html">1. 基本概念

《深入理解Java内存模型》详细讲解了java的内存模型，这里对其中的一些基本概念做个简单的笔记。以下内容摘自 《深入理解Java内存模型》读书总结

并发

定义：即，并发(同时)发生。在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理机上运行，但任一个时刻点上只有一个程序在处理机上运行。

并发需要处理两个关键问题：线程之间如何通信及线程之间如何同步。


  通信：是指线程之间如何交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。
  同步：是指程序用于控制不同线程之间操作发生相对顺序的机制。...</content>
  </entry>
  
  <entry>
    <title>RHEL系统下安装atlassian-jira-5</title>
    <link href="/2014/04/09/install-jira5-on-rhel-system.html"/>
    <updated>2014-04-09T00:00:00+08:00</updated>
    <id>/2014/04/09/install-jira5-on-rhel-system.html</id>
    <content type="html">部署环境


  操作系统：RHEL 6.4 x86_64
  Jira版本：atlassian-jira-5.2.11-x64.bin
  安装路径:/opt/atlassian/jira/
  数据保存路径：/opt/atlassian/application-data/jira
  安装用户：jira
  数据库：postgresql
  JDK：1.6.0_43


jira下载页面：https://www.atlassian.com/software/jira/download

安装步骤

运行安装文件

$ . atlassian-jira-5.2.11-x64.bin

...</content>
  </entry>
  
  <entry>
    <title>PostgreSQL测试工具PGbench</title>
    <link href="/2014/04/08/a-benchmark-tool-on-postgresql.html"/>
    <updated>2014-04-08T00:00:00+08:00</updated>
    <id>/2014/04/08/a-benchmark-tool-on-postgresql.html</id>
    <content type="html">pgbench 是一个简单的给 PostgreSQL 做性能测试的程序。它反复运行同样的 SQL 命令序列，可能是在多个并发数据库会话上头，然后检查平均的事务速度（每秒的事务数 tps）。缺省的时候，pgbench 测试一个（松散的）接近 TPC-B 的情况，每个事务包括五个 SELECT，UPDATE，和 INSERT命令。不过，我们可以很轻松地使用自己的事务脚本文件来实现其它情况。

典型的输出看上去会是这样：

transaction type: TPC-B (sort of)
scaling factor: 10
number of clients: 10
number of t...</content>
  </entry>
  
  <entry>
    <title>PostgreSQL监控指标</title>
    <link href="/2014/04/07/some-metrics-in-postgresql.html"/>
    <updated>2014-04-07T00:00:00+08:00</updated>
    <id>/2014/04/07/some-metrics-in-postgresql.html</id>
    <content type="html">数据库版本：9.3.1（不同版本数据库相关表列名可能略有不同）

数据库状态信息

数据库状态信息主要体现数据库的当前状态

1.目前客户端的连接数

postgres=# SELECT count(*) FROM pg_stat_activity WHERE NOT pid=pg_backend_pid();


2.连接状态

postgres=# SELECT pid,waiting,current_timestamp - least(query_start,xact_start) AS runtime,substr(query,1,25) AS current_query 
FR...</content>
  </entry>
  
  <entry>
    <title>RHEL系统安装PostgreSQL</title>
    <link href="/2014/04/07/install-postgresql-on-rhel-system.html"/>
    <updated>2014-04-07T00:00:00+08:00</updated>
    <id>/2014/04/07/install-postgresql-on-rhel-system.html</id>
    <content type="html">环境说明


  OS：RHEL6.4（x86_64）
  postgresql版本：PostgreSQL9.2.8


安装步骤

1. 下载所需的PostgreSQL rpm包

基础安装：


  postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm
  postgresql92-9.2.8-1PGDG.rhel6.x86_64.rpm
  postgresql92-server-9.2.8-1PGDG.rhel6.x86_64.rpm


扩展安装：


  postgresql92-contrib-9.2.8-1PGDG.rhel6.x8...</content>
  </entry>
  
  <entry>
    <title>RHEL系统安装MySQL主备环境</title>
    <link href="/2014/04/06/mysql-config-for-master-slave-replication.html"/>
    <updated>2014-04-06T00:00:00+08:00</updated>
    <id>/2014/04/06/mysql-config-for-master-slave-replication.html</id>
    <content type="html">环境准备


  操作系统： rhel6.4
  数据库： percona 5.6.14
  使用3306端口保证端口未被占用，selinux关闭状态


原理说明

mysql的复制（Replication)是一个异步的复制，从一个mysql instance(称之为master)复制到另一个mysql instance(称之为slave).实现整个复制操作主要由三个进程完成的，其中俩个进程在slave(sql进程和io进程），另外一个进程在master（IO进程）上。

要实施复制，首先要打开master端的binary log(bin-log)功能，否则无法实现。因为整个复制过程实...</content>
  </entry>
  
  <entry>
    <title>RHEL系统安装MySql</title>
    <link href="/2014/04/06/install-mysql-on-rhel-system.html"/>
    <updated>2014-04-06T00:00:00+08:00</updated>
    <id>/2014/04/06/install-mysql-on-rhel-system.html</id>
    <content type="html">环境说明


  操作系统:linux6.4
  MySql版本：percona 5.6.14
  rpm包下载地址：http://www.percona.com/downloads/Percona-Server-5.6/LATEST/RPM/rhel6/x86_64


安装步骤

1. 安装所需要的rpm包

rpm -ivh Percona-Server-shared-56-5.6.14-rel62.0.483.rhel6.x86_64.rpm
rpm -ivh Percona-Server-client-56-5.6.14-rel62.0.483.rhel6.x86_64.rpm...</content>
  </entry>
  
  <entry>
    <title>BroadleafCommerce介绍</title>
    <link href="/2014/04/04/introduction-to-broadleaf-commerce.html"/>
    <updated>2014-04-04T00:00:00+08:00</updated>
    <id>/2014/04/04/introduction-to-broadleaf-commerce.html</id>
    <content type="html">1. 介绍

BroadleafCommerce是一个Java开源电子商务网站框架。其目标是开发企业级商务网站，它提供健壮的数据和服务模型、富客户端管理平台、已经一些核心电子商务有关的工具。

2. 特性

2.1	Catalog （目录分类）

提供灵活的产品和类型管理，一个重要的特性是可以继承产品分类来满足特殊的商业需求。管理界面可以管理各种类别和产品。

2.2	Promotion System（促销系统）

可通过配置的方式管理促销。以下类促销示无需客制化而通过管理界面即可管理：


  百分比折扣、金额折扣、固定价格(Percent Off / Dollar Off / Fix...</content>
  </entry>
  
  <entry>
    <title>使用SaltStack安装JDK1.6</title>
    <link href="/2014/04/01/install-jdk-with-saltstack.html"/>
    <updated>2014-04-01T00:00:00+08:00</updated>
    <id>/2014/04/01/install-jdk-with-saltstack.html</id>
    <content type="html">创建states文件

在/srv/salt目录下创建jdk目录，并在jdk目录创建init.sls文件，init.sls文件内容如下：

jdk-file:
 file.managed:
   - source: salt://jdk/files/jdk1.6.0_39.tar.gz
   - name: /usr/java/jdk1.6.0_39.tar.gz
   - include_empty: True
 
jdk-install:
 cmd.run:
   - name: &#39;/bin/tar -zxf jdk1.6.0_39.tar.gz &amp;amp;&amp;amp; /bin/ln...</content>
  </entry>
  
  <entry>
    <title>使用Lua和OpenResty搭建验证码服务器</title>
    <link href="/2014/04/01/deploy-a-captcha-server-using-lua-and-openresty.html"/>
    <updated>2014-04-01T00:00:00+08:00</updated>
    <id>/2014/04/01/deploy-a-captcha-server-using-lua-and-openresty.html</id>
    <content type="html">Lua下有个Lua-GD图形库，通过简单的Lua语句就能控制、生成图片。

环境说明：


  操作系统：RHEL6.4
  RHEL系统默认已安装RPM包的Lua-5.1.4，但其只具有Lua基本功能，不提供 lua.h 等，但 Lua-GD 编译需要用到 lua.h，故 Lua 需要编译安装。
  Lua-GD 版本号格式为X.Y.XrW，其中X.Y.Z代表gd版本，W代表效力版本，所以 lua-gd 版本：lua-gd-2.0.33r2 相对应 gd 版本为：gd-2.0.33，须注意保持一致。
  因生成gif的lua脚本中用到md5加密，故需编译安装md5。
  因为生成图片需...</content>
  </entry>
  
  <entry>
    <title>CDH4.5.0 新特性</title>
    <link href="/2014/03/31/what-is-new-in-CDH4.5.0.html"/>
    <updated>2014-03-31T00:00:00+08:00</updated>
    <id>/2014/03/31/what-is-new-in-CDH4.5.0.html</id>
    <content type="html">Apache Flume

新特性：


  FLUME-2190 - 引入一个新的Twitter firehose的feed源
  FLUME-2109 - HTTP输入源支持HTTPS.
  FLUME-1666 - 系统日志的TCP源现在可以保持时间戳和处理领域中的事件主体.
  FLUME-2202 - AsyncHBaseSink can now coalesce increments to the same row and column per transaction to reduce the number of RPC calls
  FLUME-2189 - Avro ...</content>
  </entry>
  
  <entry>
    <title>Python模拟新浪微博登录</title>
    <link href="/2014/03/18/simulate-weibo-login-in-python.html"/>
    <updated>2014-03-18T00:00:00+08:00</updated>
    <id>/2014/03/18/simulate-weibo-login-in-python.html</id>
    <content type="html">看到一篇Python模拟新浪微博登录的文章，想熟悉一下其中实现方式，并且顺便掌握python相关知识点。

代码

下面的代码是来自上面这篇文章，并稍作修改添加了一些注释。

# -*- coding: utf-8 -*

import urllib2
import urllib
import cookielib
 
import lxml.html as HTML
 
class Fetcher(object):
    def __init__(self, username=None, pwd=None, cookie_filename=None):
		#获取一个保存cookie的...</content>
  </entry>
  
  <entry>
    <title>Solr的schema.xml</title>
    <link href="/2014/03/15/schema-in-solr.html"/>
    <updated>2014-03-15T00:00:00+08:00</updated>
    <id>/2014/03/15/schema-in-solr.html</id>
    <content type="html">schema.xml是Solr一个配置文件，它包含了你的文档所有的字段，以及当文档被加入索引或查询字段时，这些字段是如何被处理的。这个文件被存储在Solr主文件夹下的conf目录下，默认的路径./solr/conf/schema.xml，也可以是Solr webapp的类加载器所能确定的路径。在下载的Solr包里，有一个schema的样例文件，用户可以从那个文件出发，来观察如何编写自己的Schema.xml。

type节点

先来看下type节点，这里面定义FieldType子节点，包括name、class、positionIncrementGap等一些参数。必选参数：


  nam...</content>
  </entry>
  
  <entry>
    <title>IDH HBase中实现的一些特性</title>
    <link href="/2014/03/15/new-features-in-idh-hbase.html"/>
    <updated>2014-03-15T00:00:00+08:00</updated>
    <id>/2014/03/15/new-features-in-idh-hbase.html</id>
    <content type="html">IDH为Intel’s Distribution of Hadoop的简称，中文为英特尔Hadoop发行版，目前应该没有人在维护该产品了。这里简单介绍一下IDH HBase中实现的一些特性。

以下部分内容摘自IDH官方的一些文档，部分内容来自我的整理：

1、 单调数据的加盐处理

对于写入的rowkey是基本单调的（例如时序数据），IDH引入了一个新的接口：SaltedTableInterface


  提高近乎透明的“加盐”，方便使用
  封装了get、scan、put、delete等操作


2、提供了Rolling Scanner应对HFile数量大量增加情况下的get、sc...</content>
  </entry>
  
  <entry>
    <title>在Solr中使用中文分词</title>
    <link href="/2014/03/14/split-chinese-in-solr.html"/>
    <updated>2014-03-14T00:00:00+08:00</updated>
    <id>/2014/03/14/split-chinese-in-solr.html</id>
    <content type="html">使用全文检索，中文分词是离不开的，这里我采用的是 mmseg4j 分词器。mmseg4j分词器内置了对solr的支持，最新版本可支持4.X版本的sorl，使用起来很是方便。

下载mmseg4j

GoogleCode地址：http://code.google.com/p/mmseg4j/

请下载最新版本：mmseg4j-1.9.1，然后将mmseg4j-1.9.1/dist下的jar包拷贝至solr.war的lib目录，例如：apache-tomcat-6.0.36/webapps/solr/WEB-INF/lib/

配置schema.xml

使用mmseg4j中文分词器，首先需...</content>
  </entry>
  
  <entry>
    <title>BroadLeaf项目集成SolrCloud</title>
    <link href="/2014/03/14/broadleaf-project-with-solrcloud.html"/>
    <updated>2014-03-14T00:00:00+08:00</updated>
    <id>/2014/03/14/broadleaf-project-with-solrcloud.html</id>
    <content type="html">《BroadLeaf项目搜索功能改进》一文中介绍了 BroadLeaf 项目中如何改进搜索引擎这一块的代码，其中使用的是单节点的 solr 服务器，这篇文章主要介绍 BroadLeaf 项目如何集成 SolrCloud 集群。

1、SolrCloud环境搭建

参考 《Apache SolrCloud安装》，搭建Solr集群环境，将 Demosite 所用的 Solr 配置文件 solrconfig.xml 和 schema.xml 上传到 zookeeper 集群中，保证成功启动 Solr 集群。

2、扩展SearcheService类

扩展SearchService类的步骤与单...</content>
  </entry>
  
  <entry>
    <title>BroadLeaf项目搜索功能改进</title>
    <link href="/2014/03/13/improve-the-search-function-in-broadleaf-project.html"/>
    <updated>2014-03-13T00:00:00+08:00</updated>
    <id>/2014/03/13/improve-the-search-function-in-broadleaf-project.html</id>
    <content type="html">Broadleaf Commerce 是一个开源的Java电子商务平台，基于Spring框架开发，提供一个可靠、可扩展的架构，可进行深度的定制和快速开发。

关于Solr

Broadleaf项目中关于商品的搜索使用了嵌入式的Solr服务器，这个从配置文件中可以看出来。


  项目主页： http://www.broadleafcommerce.com/
  示例网站： http://demo.broadleafcommerce.org/
  示例网站源代码： https://github.com/BroadleafCommerce/DemoSite


从示例网站源代码的applic...</content>
  </entry>
  
  <entry>
    <title>Apache SolrCloud安装</title>
    <link href="/2014/03/10/how-to-install-solrcloud.html"/>
    <updated>2014-03-10T00:00:00+08:00</updated>
    <id>/2014/03/10/how-to-install-solrcloud.html</id>
    <content type="html">SolrCloud 通过 ZooKeeper 集群来进行协调，使一个索引进行分片，各个分片可以分布在不同的物理节点上，多个物理分片组成一个完成的索引 Collection。SolrCloud 自动支持 Solr Replication，可以同时对分片进行复制，冗余存储。下面，我们基于 Solr 最新的 4.4.0 版本进行安装配置 SolrCloud 集群。

1. 安装环境

我使用的安装程序各版本如下：


  Solr： Apache Solr-4.4.0
  Tomcat： Apache Tomcat 6.0.36
  ZooKeeper： Apache ZooKeeper 3....</content>
  </entry>
  
  <entry>
    <title>HBase源码：HRegionServer启动过程</title>
    <link href="/2014/03/09/hbase-note-about-hregionserver-startup.html"/>
    <updated>2014-03-09T00:00:00+08:00</updated>
    <id>/2014/03/09/hbase-note-about-hregionserver-startup.html</id>
    <content type="html">版本：HBase 0.94.15-cdh4.7.0

关于HMaster启动过程，请参考HBase源码：HMaster启动过程。先启动了HMaster之后，再启动HRegionServer。

运行HRegionServerStarter类启动HRegionServer：

package my.test.start;

import org.apache.hadoop.hbase.regionserver.HRegionServer;

public class HRegionServerStarter {

    public static void main(String[] a...</content>
  </entry>
  
  <entry>
    <title>HBase源码：HMaster启动过程</title>
    <link href="/2014/03/09/hbase-note-about-hmaster-startup.html"/>
    <updated>2014-03-09T00:00:00+08:00</updated>
    <id>/2014/03/09/hbase-note-about-hmaster-startup.html</id>
    <content type="html">版本：HBase 0.94.15-cdh4.7.0

调试HMaster


  说明：

  这部分参考和使用了https://github.com/codefollower/HBase-Research上的代码（注意：原仓库已经被作者删除了），包括该作者自己写的一些测试类和文档。


首先，在IDE里启动HMaster和HRegionServer：

运行/hbase/src/test/java/my/test/start/HMasterStarter.java，当看到提示Waiting for region servers count to settle时，
再打开同目录中的HRe...</content>
  </entry>
  
  <entry>
    <title>2013年度年终总结</title>
    <link href="/2014/03/06/summary-of-the-work-in-2013.html"/>
    <updated>2014-03-06T00:00:00+08:00</updated>
    <id>/2014/03/06/summary-of-the-work-in-2013.html</id>
    <content type="html">回首2011年和2012年的年终总结，发现公司在2012年提到的一些不足仍然出现在2013年，不知道每年的总结是否有被认真阅读过、重视过。故虽谏且议，使人不得而知焉。

2013年，通过了RHCE考试，掌握了shell编程，初识Python；

2013年，不再负责、管理具体的项目，可还不是逃不过事后填坑的无奈；

2013年，Cassandra不再，迎来Hadoop，满腔热血的学习Hadoop的安装、部署、原理、开发甚至还做了一些入门普及培训，但真的只是一个人在战斗；

2013年，开始是一个人带着几个同事在探索和研究hadoop，慢慢地失去了自己的自主权，更多的时间是被花在了具体的项...</content>
  </entry>
  
  <entry>
    <title>Apache Solr查询语法</title>
    <link href="/2014/03/03/solr-query-syntax.html"/>
    <updated>2014-03-03T00:00:00+08:00</updated>
    <id>/2014/03/03/solr-query-syntax.html</id>
    <content type="html">查询参数

常用：


  q - 查询字符串，必须的。
  fl - 指定返回那些字段内容，用逗号或空格分隔多个。
  start - 返回第一条记录在完整找到结果中的偏移位置，0开始，一般分页用。
  rows - 指定返回结果最多有多少条记录，配合start来实现分页。
  sort - 排序，格式：sort=&amp;lt;field name&amp;gt;+&amp;lt;desc|asc&amp;gt;[,&amp;lt;field name&amp;gt;+&amp;lt;desc|asc&amp;gt;]。示例：（inStock desc, price asc）表示先 “inStock” 降序, 再 “price” 升序，默认是相关...</content>
  </entry>
  
  <entry>
    <title>Apache Solr介绍及安装</title>
    <link href="/2014/02/26/how-to-install-solr.html"/>
    <updated>2014-02-26T00:00:00+08:00</updated>
    <id>/2014/02/26/how-to-install-solr.html</id>
    <content type="html">Solr是什么

Solr是一个基于Lucene java库的企业级搜索服务器，包含XML/HTTP，JSON API，高亮查询结果，缓存，复制，还有一个WEB管理界面。Solr运行在Servlet容器中，其架构如下：



主要功能包括全文检索，高亮命中，分面搜索(faceted search)，近实时索引，动态集群，数据库集成，富文本索引，空间搜索；通过提供分布式索引，复制，负载均衡查询，自动故障转移和恢复，集中配置等功能实现高可用，可伸缩和可容错。

Solr和Lucene的本质区别有以下三点：搜索服务器、企业级和管理。Lucene本质上是搜索库，不是独立的应用程序，而Solr是。...</content>
  </entry>
  
  <entry>
    <title>使用Vagrant创建虚拟机安装Hadoop</title>
    <link href="/2014/02/23/create-virtualbox-by-vagrant.html"/>
    <updated>2014-02-23T00:00:00+08:00</updated>
    <id>/2014/02/23/create-virtualbox-by-vagrant.html</id>
    <content type="html">安装VirtualBox

下载地址：https://www.virtualbox.org/wiki/Downloads/

安装Vagrant

下载安装包：http://downloads.vagrantup.com/，然后安装。

下载box

下载适合你的box，地址：http://www.vagrantbox.es/。

例如下载 CentOS6.5：

$ wget https://github.com/2creatives/vagrant-centos/releases/download/v6.5.3/centos65-x86_64-20140116.box


添加box...</content>
  </entry>
  
  <entry>
    <title>Python基础入门</title>
    <link href="/2014/02/22/python-introduction-of-basics.html"/>
    <updated>2014-02-22T00:00:00+08:00</updated>
    <id>/2014/02/22/python-introduction-of-basics.html</id>
    <content type="html">1. Python介绍

Python是一种解释性的面向对象的语言。Python使用C语言编写，不需要事先声明变量的类型（动态类型），但是一旦变量有了值，那么这个变脸是有一个类型的，不同类型的变量之间赋值需要类型转换（强类型）。

1.1 安装 Python

现在的操作系统都自带安装了 Python，要测试你是否安装了Python，你可以打开一个shell程序（就像konsole或gnome-terminal），然后输入如下所示的命令python -V

$ python -V
Python 2.7.4


如果你看见向上面所示的那样一些版本信息，那么你已经安装了Python了。

2...</content>
  </entry>
  
  <entry>
    <title>Confluence 5.4.2安装</title>
    <link href="/2014/02/21/install-confluence5-4-2.html"/>
    <updated>2014-02-21T00:00:00+08:00</updated>
    <id>/2014/02/21/install-confluence5-4-2.html</id>
    <content type="html">Confluence是Atlassian公司出品的团队协同与知识管理工具。 Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。通过它可以实现团队成员之间的协作和知识共享。

1、下载

下载指定版本Confluence

$ mkdir -p /data/confluence
$ cd /data/confluence
$ wget www.atlassian.com/software/confluence/downloads/binary/atlassian-confluence-5.4.2.tar.gz


2、安装

解压：

$ tar -zxv...</content>
  </entry>
  
  <entry>
    <title>CDH 5 Beta 2 的新变化</title>
    <link href="/2014/02/21/cdh5rn_whats_new_in_b2.html"/>
    <updated>2014-02-21T00:00:00+08:00</updated>
    <id>/2014/02/21/cdh5rn_whats_new_in_b2.html</id>
    <content type="html">本文是同事对CDH 5.0.0 Beta 2的翻译，仅供大家参考。

这是 CDH 5.0.0 Beta 2的初稿。鉴于 CDH 5 目前的发布版本是测试版，它不应用于生产环境中；它只是用来评估、测试的。对于生产环境，请使用 CDH 4,最近的文档在 CDH Documentation

Apache Crunch

Apache Crunch 项目开发了新的 Java API，简化了在 Apache Hadoop 之上的数据管道的创建过程。

Crunch APIs 是以 FlumeJava 为蓝本开发的，FlumeJava 是 Google 用来在他们自己的 MapReduce 实现...</content>
  </entry>
  
  <entry>
    <title>Backbone中的模型</title>
    <link href="/2014/02/16/backbone-model.html"/>
    <updated>2014-02-16T00:00:00+08:00</updated>
    <id>/2014/02/16/backbone-model.html</id>
    <content type="html">创建model

模型是所有Javascript应用程序的核心，包括交互数据及相关的大量逻辑： 转换、验证、计算属性和访问控制。你可以用特定的方法扩展Backbone.Model，模型也提供了一组基本的管理变化的功能。

Person = Backbone.Model.extend({
    initialize: function(){
        alert(&quot;Welcome to this world&quot;);
    }
});

var person = new Person;


new一个model的实例后就会触发initialize()函数。

设置属性

现在我们想设...</content>
  </entry>
  
  <entry>
    <title>在CentOs6系统上安装Ganglia</title>
    <link href="/2014/01/25/how-to-install-ganglia-on-centos6.html"/>
    <updated>2014-01-25T00:00:00+08:00</updated>
    <id>/2014/01/25/how-to-install-ganglia-on-centos6.html</id>
    <content type="html">Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，由RRDTool工具处理数据，并生成相应的的图形显示，以Web方式直观的提供给客户端。如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。

配置yum源

首先配置好CentOs系统的yum源，然后需要包含有ganglia的yum源。

在/etc/yum.repos.d下创建ganglia.rep...</content>
  </entry>
  
  <entry>
    <title>在RHEL系统上安装Nagios</title>
    <link href="/2014/01/24/how-to-install-nagios-on-rhel6.html"/>
    <updated>2014-01-24T00:00:00+08:00</updated>
    <id>/2014/01/24/how-to-install-nagios-on-rhel6.html</id>
    <content type="html">在管理机上安装rpm包

$ rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
$ rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm
$ yum -y install nagios nagios-plugins-all nagios-plugins-nrpe nrpe php httpd
$ chkconfig httpd on &amp;amp;&amp;amp; chkconfig nagios o...</content>
  </entry>
  
  <entry>
    <title>All Things OpenTSDB</title>
    <link href="/2014/01/22/all-things-opentsdb.html"/>
    <updated>2014-01-22T00:00:00+08:00</updated>
    <id>/2014/01/22/all-things-opentsdb.html</id>
    <content type="html">1. OpenTSDB介绍

OpenTSDB用HBase存储所有的时序（无须采样）来构建一个分布式、可伸缩的时间序列数据库。它支持秒级数据采集所有metrics，支持永久存储，可以做容量规划，并很容易的接入到现有的报警系统里。OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化、图形化等。

对于运维工程师而言，OpenTSDB可以获取基础设施和服务的实时状态信息，展示集群的各种软硬件错误，性能变化以及性能瓶颈。对于管理者而言，OpenTSDB可以衡量系统的SLA，理...</content>
  </entry>
  
  <entry>
    <title>All Things Jekyll</title>
    <link href="/2014/01/21/all-things-about-jekyll.html"/>
    <updated>2014-01-21T00:00:00+08:00</updated>
    <id>/2014/01/21/all-things-about-jekyll.html</id>
    <content type="html">Jekyll是一个简洁的、特别针对博客平台的静态网站生成器。它使用一个模板目录作为网站布局的基础框架，并在其上运行Textile、Markdown或Liquid标记语言的转换器，最终生成一个完整的静态Web站点，可以被放置在Apache或者你喜欢的其他任何Web服务器上。它同时也是GitHub Pages、一个由GitHub提供的用于托管项目主页或博客的服务，在后台所运行的引擎。

1. 安装
Jekyll使用动态脚本语言Ruby写成。请首先下载并安装Ruby，目前需要的ruby版本为1.9.1。

在使用Jekyll之前，你可能想要对Ruby语言有一些初步了解（非必需）。

安装Jek...</content>
  </entry>
  
  <entry>
    <title>SSH远程连接时环境变量问题</title>
    <link href="/2014/01/18/bash-problem-when-ssh-access.html"/>
    <updated>2014-01-18T00:00:00+08:00</updated>
    <id>/2014/01/18/bash-problem-when-ssh-access.html</id>
    <content type="html">1. 问题

RHEL服务器A有个启动脚本（普通用户user01运行），里面使用ifconfig获取ip地址如下：

Localhost_ip=$(ifconfig |awk -F &#39;addr:|Bcast&#39; &#39;/Bcast/{print $2}&#39;)


由于普通用户user01不能直接识别ifconfig命令，只能使用全路径/sbin/ifconfig，目前处理方式为修改~/.bash_profile文件添加环境变量如下：

PATH=$PATH:$HOME/bin


改成如下：

PATH=$PATH:$HOME/bin:/sbin



经过如上配置后服务器本机user01用户登...</content>
  </entry>
  
  <entry>
    <title>HBase笔记：Region拆分策略</title>
    <link href="/2014/01/16/hbase-region-split-policy.html"/>
    <updated>2014-01-16T00:00:00+08:00</updated>
    <id>/2014/01/16/hbase-region-split-policy.html</id>
    <content type="html">Region 概念

Region是表获取和分布的基本元素，由每个列族的一个Store组成。对象层级图如下：

Table       (HBase table)
    Region       (Regions for the table)
         Store          (Store per ColumnFamily for each Region for the table)
              MemStore        (MemStore for each Store for each Region for the table)
         ...</content>
  </entry>
  
  <entry>
    <title>Vim配置和插件管理</title>
    <link href="/2014/01/14/vim-config-and-plugins.html"/>
    <updated>2014-01-14T00:00:00+08:00</updated>
    <id>/2014/01/14/vim-config-and-plugins.html</id>
    <content type="html">这篇文章主要是记录vim配置中各个配置项的含义并且收藏一些常用的插件及其使用方法。

1. Vim配置

目前我的vimrc配置放置在:https://github.com/javachen/snippets/blob/master/dotfiles/.vimrc，其中大多数用英文注释。

2. 插件管理

使用 pathogen来管理插件

项目地址:	https://github.com/tpope/vim-pathogen

安装方法：

$ mkdir -p ~/.vim/autoload ~/.vim/bundle &amp;amp;&amp;amp; \
$ curl -LSso ~/.vi...</content>
  </entry>
  
  <entry>
    <title>SiteMesh介绍</title>
    <link href="/2014/01/13/about-sitemesh.html"/>
    <updated>2014-01-13T00:00:00+08:00</updated>
    <id>/2014/01/13/about-sitemesh.html</id>
    <content type="html">1. SiteMesh简介

SiteMesh是由一个基于Web页面布局、装饰以及与现存Web应用整合的框架。它能帮助我们在由大量页面构成的项目中创建一致的页面布局和外观，如一致的导航条，一致的banner，一致的版权等等。它不仅仅能处理动态的内容，如jsp，php，asp等产生的内容，它也能处理静态的内容，如htm的内容，使得它的内容也符合你的页面结构的要求。甚至于它能将HTML文件象include那样将该文件作为一个面板的形式嵌入到别的文件中去。所有的这些，都是GOF的Decorator模式的最生动的实现。尽管它是由java语言来实现的，但它能与其他Web应用很好地集成。

2. S...</content>
  </entry>
  
  <entry>
    <title>如何创建一个Django网站</title>
    <link href="/2014/01/11/how-to-create-a-django-site.html"/>
    <updated>2014-01-11T00:00:00+08:00</updated>
    <id>/2014/01/11/how-to-create-a-django-site.html</id>
    <content type="html">本文参考官方文档演示如何创建一个简单的 django 网站，使用的 django 版本为1.7。

1. 创建项目

运行下面命令就可以创建一个 django 项目，项目名称叫 mysite ：

$ django-admin.py startproject mysite


创建后的项目目录如下：

mysite
├── manage.py
└── mysite
    ├── __init__.py
    ├── settings.py
    ├── urls.py
    └── wsgi.py

1 directory, 5 files


说明：


  __init__....</content>
  </entry>
  
  <entry>
    <title>重装Linux-Mint系统之后</title>
    <link href="/2014/01/09/after-reinstall-the-system.html"/>
    <updated>2014-01-09T00:00:00+08:00</updated>
    <id>/2014/01/09/after-reinstall-the-system.html</id>
    <content type="html">本文主要记录重装Linux-Mint系统之后的一些软件安装和环境变量配置。

安装常用工具

sudo apt-get install ctags curl vsftpd git vim tmux meld htop putty subversion  nload  iptraf iftop tree openssh-server gconf-editor gnome-tweak-tool


挂载exfat格式磁盘

sudo apt-get install exfat-fuse exfat-utils


安装ibus

在终端输入命令:

sudo add-apt-reposito...</content>
  </entry>
  
  <entry>
    <title>Hive使用HAProxy配置HA</title>
    <link href="/2014/01/08/hive-ha-by-haproxy.html"/>
    <updated>2014-01-08T00:00:00+08:00</updated>
    <id>/2014/01/08/hive-ha-by-haproxy.html</id>
    <content type="html">HAProxy是一款提供高可用性、负载均衡以及基于TCP（第四层）和HTTP（第七层）应用的代理软件，HAProxy是完全免费的、借助HAProxy可以快速并且可靠的提供基于TCP和HTTP应用的代理解决方案。


  免费开源，稳定性也是非常好，这个可通过我做的一些小项目可以看出来，单Haproxy也跑得不错，稳定性可以与硬件级的F5相媲美。
根据官方文档，HAProxy可以跑满10Gbps-New benchmark of HAProxy at 10 Gbps using Myricom’s 10GbE NICs （Myri-10G PCI-Express），这个数值作为软件级负载均...</content>
  </entry>
  
  <entry>
    <title>Git配置和一些常用命令</title>
    <link href="/2013/12/27/some-git-configs-and-cammands.html"/>
    <updated>2013-12-27T00:00:00+08:00</updated>
    <id>/2013/12/27/some-git-configs-and-cammands.html</id>
    <content type="html">Git是一个分布式版本控制／软件配置管理软件，原来是linux内核开发者林纳斯·托瓦兹（Linus Torvalds）为了更好地管理linux内核开发而创立的。

Git配置

git config --global user.name &quot;javachen&quot;
git config --global user.email &quot;june.chan@foxmail.com&quot;
git config --global color.ui true
git config --global alias.co checkout
git config --global alias.ci commit
git ...</content>
  </entry>
  
  <entry>
    <title>SaltStack学习笔记</title>
    <link href="/2013/11/18/study-note-of-saltstack.html"/>
    <updated>2013-11-18T00:00:00+08:00</updated>
    <id>/2013/11/18/study-note-of-saltstack.html</id>
    <content type="html">1. 关于本文档

这份文档如其名，是我自己整理的学习 SaltStack 的过程记录。只是过程记录，没有刻意像教程那样去做。所以呢，从前至后，中间不免有一些概念不清不明的地方。因为事实上，在某个阶段对于一些概念本来就不可能明白。所以，整个过程只求在形式上的能用即可。前面就不要太纠结概念和原理，知道怎么用就好。

希望这篇文章能够让你快速了解并使用saltstack。文章还在编写中。

2. 关于SaltStack

2.1. 什么是SaltStack
SaltStack是开源的管理基础设置的轻量级工具，容易搭建，为远程管理服务器提供一种更好、更快速、更有扩展性的解决方案。通过简单、可管...</content>
  </entry>
  
  <entry>
    <title>使用SaltStack安装JBoss</title>
    <link href="/2013/11/16/install-jboss-with-saltstack.html"/>
    <updated>2013-11-16T00:00:00+08:00</updated>
    <id>/2013/11/16/install-jboss-with-saltstack.html</id>
    <content type="html">SaltStack是一个具备puppet与func功能为一身的集中化管理平台，其基于python实现，功能十分强大，各模块融合度及复用性极高。SaltStack 采用 zeromq 消息队列进行通信，和 Puppet/Chef 比起来，SaltStack 速度快得多。

在开始使用SaltStack之前，首先要对SaltStack的基础进行一系列的学习，这里，强烈推荐官网的Tutorial,在完成了整个Tutorial之后，通过Module Index页面，我们能够快速查阅Salt所有模块的功能与用法:http://docs.saltstack.com/py-modindex.html
...</content>
  </entry>
  
  <entry>
    <title>安装SaltStack和Halite</title>
    <link href="/2013/11/11/install-saltstack-and-halite.html"/>
    <updated>2013-11-11T00:00:00+08:00</updated>
    <id>/2013/11/11/install-saltstack-and-halite.html</id>
    <content type="html">本文记录安装SaltStack和halite过程。

首先准备两台rhel或者centos虚拟机sk1和sk2，sk1用于安装master，sk2安装minion。

配置yum源

在每个节点上配置yum源：

$ rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm


然后通过下面命令查看epel参考是否安装成功：

$ yum list #或者查看/etc/yum.repos.d目录下是否有epel.repo


如果没有安装成功，则可以手动下载epel-release-...</content>
  </entry>
  
  <entry>
    <title>在Eclipse中调试运行HBase</title>
    <link href="/2013/11/01/debug-hbase-in-eclipse.html"/>
    <updated>2013-11-01T00:00:00+08:00</updated>
    <id>/2013/11/01/debug-hbase-in-eclipse.html</id>
    <content type="html">这篇文章记录一下如何在eclipse中调试运行hbase。

下载并编译源代码
请参考编译hbase源代码并打补丁

修改配置文件

修改 conf/hbase-site.xml文件：

&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.defaults.for.version&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;0.94.6-cdh4.4.0&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;fil...</content>
  </entry>
  
  <entry>
    <title>编译CDH HBase源代码并打补丁</title>
    <link href="/2013/10/28/compile-hbase-source-code-and-apply-patches.html"/>
    <updated>2013-10-28T00:00:00+08:00</updated>
    <id>/2013/10/28/compile-hbase-source-code-and-apply-patches.html</id>
    <content type="html">写了一篇博客记录编译CDH HBase源代码并打补丁的过程，如有不正确的，欢迎指出！

下载源代码

从Cloudera github上下载最新分支源代码，例如：当前最新分支为cdh4-0.94.6_4.4.0

$ git clone git@github.com:cloudera/hbase.git -b cdh4-0.94.6_4.4.0 cdh4-0.94.6_4.4.0


说明：


  -b 指定下载哪个分支
  最后一个参数指定下载下来的文件名称


添加snappy压缩支持

编译snappy

$ svn checkout http://snappy.googleco...</content>
  </entry>
  
  <entry>
    <title>HiveServer2中使用jdbc客户端用户运行mapreduce</title>
    <link href="/2013/10/17/run-mapreduce-with-client-user-in-hive-server2.html"/>
    <updated>2013-10-17T00:00:00+08:00</updated>
    <id>/2013/10/17/run-mapreduce-with-client-user-in-hive-server2.html</id>
    <content type="html">最近做了个web系统访问hive数据库，类似于官方自带的hwi、安居客的hwi改进版和大众点评的polestar(github地址)系统，但是和他们的实现不一样，查询Hive语句走的不是cli而是通过jdbc连接hive-server2。为了实现mapreduce任务中资源按用户调度，需要hive查询自动绑定当前用户、将该用户传到yarn服务端并使mapreduce程序以该用户运行。本文主要是记录实现该功能过程中遇到的一些问题以及解决方法,如果你有更好的方法和建议，欢迎留言发表您的看法！

说明

集群环境使用的是cdh4.3，没有开启kerberos认证。


  写完这篇文章之后，在...</content>
  </entry>
  
  <entry>
    <title>Hive连接产生笛卡尔集</title>
    <link href="/2013/10/17/cartesian-product-in-hive-inner-join.html"/>
    <updated>2013-10-17T00:00:00+08:00</updated>
    <id>/2013/10/17/cartesian-product-in-hive-inner-join.html</id>
    <content type="html">在使用hive过程中遇到这样的一个异常：

FAILED: ParseException line 1:18 Failed to recognize predicate &#39;a&#39;. Failed rule: &#39;kwInner&#39; in join type specifier


执行的hql语句如下：

[root@javachen.com ~]# hive -e &#39;select a.* from t a, t b where a.id=b.id&#39;


从异常信息中很难看出出错原因，hive.log中也没有打印出详细的异常对战信息。改用jdbc连接hive-server2，可以看到hive-...</content>
  </entry>
  
  <entry>
    <title>最近的工作</title>
    <link href="/2013/09/08/recent-work.html"/>
    <updated>2013-09-08T00:00:00+08:00</updated>
    <id>/2013/09/08/recent-work.html</id>
    <content type="html">最近一直在构思这篇博客的内容，到现在还是不知道从何下手。自从将博客从wordpress迁移到github上之后，就很少在博客写一些关于工作和生活的文章，所以想写一篇关于工作的博客，记录最近做过的事情以及一些当时的所思所想。

最近半年多一直在做hadoop方面的工作，也就是接触hadoop才半年多时间。最开始接触hadoop是去年的11月21日，那天去Intel公司参加了两天的hadoop培训。培训的内容很多干货也有很多枯燥的东西，所以边听边瞌睡的听完了两天的培训内容。培训的ppt打印出来了，时不时地会翻看上面讲述的内容，然后在网上搜索些相关的资料。

最先接触的hadoop发行版是In...</content>
  </entry>
  
  <entry>
    <title>Hive中如何确定map数</title>
    <link href="/2013/09/04/how-to-decide-map-number.html"/>
    <updated>2013-09-04T00:00:00+08:00</updated>
    <id>/2013/09/04/how-to-decide-map-number.html</id>
    <content type="html">Hive 是基于 Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的 sql 查询功能，可以将 sq l语句转换为 MapReduce 任务进行运行。当运行一个 hql 语句的时候，map 数是如何计算出来的呢？有哪些方法可以调整 map 数呢？

本文测试集群版本：cdh-4.3.0 。

hive 默认的 input format

在 cdh-4.3.0 的 hive 中查看 hive.input.format 值（为什么是hive.input.format？）：

hive&amp;gt; set hive.input.format;
hive.inp...</content>
  </entry>
  
  <entry>
    <title>我的jekyll配置和修改</title>
    <link href="/2013/08/31/my-jekyll-config.html"/>
    <updated>2013-08-31T00:00:00+08:00</updated>
    <id>/2013/08/31/my-jekyll-config.html</id>
    <content type="html">主要记录使用jekyll搭建博客时的一些配置和修改。

注意：
&amp;gt;使用时请删除{和%以及{和{之间的空格。

预览文章

source ~/.bash_profile
jekyll server


添加about me 边栏

参考the5fire的技术博客在index.html页面加入如下代码：

&amp;lt;section&amp;gt;
&amp;lt;h4&amp;gt;About me&amp;lt;/h4&amp;gt;
&amp;lt;div&amp;gt;
 一个Java方案架构师，主要从事hadoop相关工作。&amp;lt;a href=&quot;/about.html&quot;&amp;gt;更多信息&amp;lt;/a&amp;gt; 
&amp;lt;br/&amp;gt;
...</content>
  </entry>
  
  <entry>
    <title>使用ZooKeeper实现配置同步</title>
    <link href="/2013/08/23/publish-proerties-using-zookeeper.html"/>
    <updated>2013-08-23T00:00:00+08:00</updated>
    <id>/2013/08/23/publish-proerties-using-zookeeper.html</id>
    <content type="html">前言

应用项目中都会有一些配置信息，这些配置信息数据量少，一般会保存到内存、文件或者数据库，有时候需要动态更新。当需要在多个应用服务器中修改这些配置文件时，需要做到快速、简单、不停止应用服务器的方式修改并同步配置信息到所有应用中去。本篇文章就是介绍如何使用ZooKeeper来实现配置的动态同步。

ZooKeeper

在《hive Driver类运行过程》一文中可以看到hive为了支持并发访问引入了ZooKeeper来实现分布式锁。参考《ZooKeeper典型应用场景一览》一文，ZooKeeper还可以用作其他用途，例如：


  数据发布与订阅（配置中心）
  负载均衡
  命名服...</content>
  </entry>
  
  <entry>
    <title>Hive源码分析：Driver类运行过程</title>
    <link href="/2013/08/22/hive-Driver.html"/>
    <updated>2013-08-22T00:00:00+08:00</updated>
    <id>/2013/08/22/hive-Driver.html</id>
    <content type="html">说明：

本文的源码分析基于hive-0.12.0-cdh5.0.1。

概括

从《hive cli的入口类》中可以知道hive中处理hive命令的处理器一共有以下几种：

（1）set       SetProcessor，设置修改参数,设置到SessionState的HiveConf里。 
（2）dfs       DfsProcessor，使用hadoop的FsShell运行hadoop的命令。 
（3）add       AddResourceProcessor，添加到SessionState的resource_map里，运行提交job的时候会写入Hadoop的Distribu...</content>
  </entry>
  
  <entry>
    <title>Hive源码分析：CLI入口类</title>
    <link href="/2013/08/21/hive-CliDriver.html"/>
    <updated>2013-08-21T00:00:00+08:00</updated>
    <id>/2013/08/21/hive-CliDriver.html</id>
    <content type="html">说明：

本文的源码分析基于hive-0.10.0-cdh4.3.0。

启动脚本

从shell脚本/usr/lib/hive/bin/ext/cli.sh可以看到hive cli的入口类为org.apache.hadoop.hive.cli.CliDriver

	cli () {
	  CLASS=org.apache.hadoop.hive.cli.CliDriver
	  execHiveCmd $CLASS &quot;$@&quot;
	}
	cli_help () {
	  CLASS=org.apache.hadoop.hive.cli.CliDriver
	  execHiveCmd $...</content>
  </entry>
  
  <entry>
    <title>使用Hadoop中遇到的一些问题</title>
    <link href="/2013/08/17/some-problems-about-hadoop.html"/>
    <updated>2013-08-17T00:00:00+08:00</updated>
    <id>/2013/08/17/some-problems-about-hadoop.html</id>
    <content type="html">本文主要记录安装hadoop过程需要注意的一些细节以及使用hadoop过程中发现的一些问题以及对应解决办法，有些地方描述的不是很清楚可能还会不准确，之后会重现问题然后修改完善这篇文章。

安装hadoop过程中需要注意以下几点：


  每个节点配置hosts
  每个节点配置时钟同步
  如果没有特殊要求，关闭防火墙
  hadoop需要在/tmp目录下存放一些日志和临时文件，要求/tmp目录权限必须为1777






使用intel的hadoop发行版IDH过程遇到问题：

1、 IDH集群中需要配置管理节点到集群各节点的无密码登录，公钥文件存放路径为/etc/intelclou...</content>
  </entry>
  
  <entry>
    <title>Hadoop自动化安装shell脚本</title>
    <link href="/2013/08/02/hadoop-install-script.html"/>
    <updated>2013-08-02T00:00:00+08:00</updated>
    <id>/2013/08/02/hadoop-install-script.html</id>
    <content type="html">之前写过一些如何安装Cloudera Hadoop的文章，安装hadoop过程中，最开始是手动安装apache版本的hadoop，其次是使用Intel的IDH管理界面安装IDH的hadoop，再然后分别手动和通过cloudera manager安装hadoop，也使用bigtop-util yum方式安装过apache的hadoop。

安装过程中参考了很多网上的文章，解压缩过cloudera的cloudera-manager-installer.bin，发现并修复了IDH shell脚本中关于puppt的自认为是bug的一个bug，最后整理出了一个自动安装hadoop的shell脚本，...</content>
  </entry>
  
  <entry>
    <title>远程调试Hadoop各组件</title>
    <link href="/2013/08/01/remote-debug-hadoop.html"/>
    <updated>2013-08-01T00:00:00+08:00</updated>
    <id>/2013/08/01/remote-debug-hadoop.html</id>
    <content type="html">远程调试对应用程序开发十分有用。例如，为不能托管开发平台的低端机器开发程序，或在专用的机器上（比如服务不能中断的 Web 服务器）调试程序。其他情况包括：运行在内存小或 CUP 性能低的设备上的 Java 应用程序（比如移动设备），或者开发人员想要将应用程序和开发环境分开，等等。

为了进行远程调试，必须使用 Java Virtual Machine (JVM) V5.0 或更新版本。

JPDA 简介

Sun Microsystem 的 Java Platform Debugger Architecture (JPDA) 技术是一个多层架构，使您能够在各种环境中轻松调试 Java 应...</content>
  </entry>
  
  <entry>
    <title>安装RHadoop</title>
    <link href="/2013/07/20/install-rhadoop.html"/>
    <updated>2013-07-20T00:00:00+08:00</updated>
    <id>/2013/07/20/install-rhadoop.html</id>
    <content type="html">1. R Language Install

安装相关依赖

yum install -y perl* pcre-devel tcl-devel zlib-devel bzip2-devel libX11-devel tk-devel tetex-latex *gfortran*  compat-readline5
yum install libRmath-*
rpm -Uvh --force --nodeps  R-core-2.10.0-2.el5.x86_64.rpm
rpm -Uvh R-2.10.0-2.el5.x86_64.rpm R-devel-2.10.0-2.el5.x...</content>
  </entry>
  
  <entry>
    <title>通过Cloudera Manager安装CDH</title>
    <link href="/2013/06/24/install-cdh-by-cloudera-manager.html"/>
    <updated>2013-06-24T00:00:00+08:00</updated>
    <id>/2013/06/24/install-cdh-by-cloudera-manager.html</id>
    <content type="html">1 方法一

你可以从https://ccp.cloudera.com/display/SUPPORT/Downloads下载cloudera-manager-installer.bin，然后修改执行权限并执行该脚本。

该脚本中配置的rhel6的yum源为：http://archive.cloudera.com/cm4/redhat/6/x86_64/cm/4/，下载的过程必须连网并且rpm的过程会非常慢，这种方法对虚拟机或者是无法连网的内网机器来说根本无法使用。

因为知道所有的rpm都在上面网址可以下载到，故你可以手动下载这些rpm然后手动安装，详细过程请参考：通过cloudera...</content>
  </entry>
  
  <entry>
    <title>HBase笔记：存储结构</title>
    <link href="/2013/06/15/hbase-note-about-data-structure.html"/>
    <updated>2013-06-15T00:00:00+08:00</updated>
    <id>/2013/06/15/hbase-note-about-data-structure.html</id>
    <content type="html">从HBase的架构图上可以看出，HBase中的存储包括HMaster、HRegionServer、HRegion、Store、MemStore、StoreFile、HFile、HLog等，本篇文章统一介绍他们的作用即存储结构。

以下是网络上流传的HBase存储架构图:



HBase中的每张表都通过行键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，这个过程由HRegionServer管理，而HRegion的分配由HMaster管理。

HMaster的作用：


  为Region server分配region
  负责Regi...</content>
  </entry>
  
  <entry>
    <title>Java笔记：单例模式</title>
    <link href="/2013/06/09/note-about-java-singleton-model.html"/>
    <updated>2013-06-09T00:00:00+08:00</updated>
    <id>/2013/06/09/note-about-java-singleton-model.html</id>
    <content type="html">什么是单例模式呢？就是在整个系统中，只有一个唯一存在的实例。使用Singleton的好处还在于可以节省内存，因为它限制了实例的个数，有利于Java垃圾回收。

单例模式主要有3个特点：


  1、单例类确保自己只有一个实例。
  2、单例类必须自己创建自己的实例。
  3、单例类必须为其他对象提供唯一的实例。


单例模式的实现方式有五种方法：懒汉，恶汉，双重校验锁，枚举和静态内部类。

懒汉模式：

public class Singleton {
    private static Singleton instance;
    private Singleton (){}

 ...</content>
  </entry>
  
  <entry>
    <title>Java笔记：IO</title>
    <link href="/2013/06/09/note-about-java-io.html"/>
    <updated>2013-06-09T00:00:00+08:00</updated>
    <id>/2013/06/09/note-about-java-io.html</id>
    <content type="html">说明，本文内容来源于java io系列01之 “目录”，做了一些删减。

Java库的IO分为输入/输出两部分。

早期的Java 1.0版本的输入系统是InputStream及其子类，输出系统是OutputStream及其子类。

后来的Java 1.1版本对IO系统进行了重新设计。输入系统是Reader及其子类，输出系统是Writer及其子类。

Java1.1之所以要重新设计，主要是为了添加国际化支持(即添加了对16位Unicode码的支持)。具体表现为Java 1.0的IO系统是字节流，而Java 1.1的IO系统是字符流。

字节流，就是数据流中最小的数据单元是8位的字节。

...</content>
  </entry>
  
  <entry>
    <title>Java笔记：工厂模式</title>
    <link href="/2013/06/09/note-about-java-factory-model.html"/>
    <updated>2013-06-09T00:00:00+08:00</updated>
    <id>/2013/06/09/note-about-java-factory-model.html</id>
    <content type="html">工厂模式主要是为创建对象提供接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。

工厂模式在《Java与模式》中分为三类：


  1)简单工厂模式(Simple Factory)：不利于产生系列产品。
  2)工厂方法模式(Factory Method)：又称为多形性工厂。
  3)抽象工厂模式(Abstract Factory)：又称为工具箱，产生产品族，但不利于产生新的产品。


GOF在《设计模式》一书中将工厂模式分为两类：工厂方法模式(Factory Method)与抽象工厂模式(Abstract Factory)。将简单工厂模式(Simple Factory)...</content>
  </entry>
  
  <entry>
    <title>Java笔记：多线程</title>
    <link href="/2013/06/08/note-about-java-thread.html"/>
    <updated>2013-06-08T00:00:00+08:00</updated>
    <id>/2013/06/08/note-about-java-thread.html</id>
    <content type="html">一些概念

现在的操作系统是多任务操作系统。多线程是实现多任务的一种方式。

进程 是指一个内存中运行的应用程序，每个进程都有自己独立的一块内存空间，一个进程中可以启动多个线程。比如在 Windows 系统中，一个运行的 exe 就是一个进程。

线程 是指进程中的一个执行流程，一个进程中可以运行多个线程。比如 java.exe 进程中可以运行很多线程。线程总是属于某个进程，进程中的多个线程共享进程的内存。

同时 执行是人的感觉，在线程之间实际上轮换执行。


  多线程间堆空间共享，栈空间独立。堆存的是地址，栈存的是变量（如：局部变量）。这部分内容结合 Java 内存模型 来理解。
...</content>
  </entry>
  
  <entry>
    <title>Java笔记：集合框架实现原理</title>
    <link href="/2013/06/08/java-collection-framework.html"/>
    <updated>2013-06-08T00:00:00+08:00</updated>
    <id>/2013/06/08/java-collection-framework.html</id>
    <content type="html">
  这篇文章是对http://www.cnblogs.com/skywang12345/category/455711.html中java集合框架相关文章的一个总结，在此对原作者的辛勤整理表示感谢。


Java集合是java提供的工具包，包含了常用的数据结构：集合、链表、队列、栈、数组、映射等。Java集合工具包位置是java.util.*

Java集合主要可以划分为4个部分：List列表、Set集合、Map映射、工具类(Iterator迭代器、Enumeration枚举类、Arrays和Collections)。

Java集合工具包框架图(如下)：



Collection是...</content>
  </entry>
  
  <entry>
    <title>Kettle访问IDH2.3中的HBase</title>
    <link href="/2013/04/17/access-idh-2.3-hbase-in-kettle.html"/>
    <updated>2013-04-17T00:00:00+08:00</updated>
    <id>/2013/04/17/access-idh-2.3-hbase-in-kettle.html</id>
    <content type="html">摘要

Kettle是一款国外开源的ETL工具，纯java编写，可以在Window、Linux、Unix上运行，绿色无需安装，数据抽取高效稳定。big-data-plugin是kettle中用于访问bigdata，包括hadoop、cassandra、mongodb等nosql数据库的一个插件。

截至目前，kettle的版本为4.4.1，big-data-plugin插件支持cloudera CDH3u4、CDH4.1，暂不支持Intel的hadoop发行版本IDH。

本文主要介绍如何让kettle支持IDH的hadoop版本。

方法

假设你已经安装好IDH-2.3的集群，并已经...</content>
  </entry>
  
  <entry>
    <title>Kettle中添加一个参数字段到输出</title>
    <link href="/2013/04/07/add-a-field-from-paramter-to-output.html"/>
    <updated>2013-04-07T00:00:00+08:00</updated>
    <id>/2013/04/07/add-a-field-from-paramter-to-output.html</id>
    <content type="html">kettle可以将输入流中的字段输出到输出流中，输入输出流可以为数据库、文件或其他，通常情况下输入流中字段为已知确定的，如果我想在输出流中添加一个来自转换的命令行参数的一个字段，该如何操作？

上述问题可以拆分为两个问题：


  从命令行接受一个参数作为一个字段
  合并输入流和这个字段


问题1

第一个问题可以使用kettle中获取系统信息组件，定义一个变量，该值来自命令行参数，见下图：



问题2
第二个问题可以使用kettle中记录关联 (笛卡尔输出)组件将两个组件关联起来，输出一个笛卡尔结果集，关联条件设定恒为true，在运行前设置第一个参数的值，然后运行即可。



下...</content>
  </entry>
  
  <entry>
    <title>使用yum源安装CDH Hadoop集群</title>
    <link href="/2013/04/06/install-cloudera-cdh-by-yum.html"/>
    <updated>2013-04-06T00:00:00+08:00</updated>
    <id>/2013/04/06/install-cloudera-cdh-by-yum.html</id>
    <content type="html">本文主要是记录使用yum安装CDH Hadoop集群的过程，包括HDFS、Yarn、Hive和HBase。本文使用CDH5.4版本进行安装，故下文中的过程都是针对CDH5.4版本的。

0. 环境说明

系统环境：


  操作系统：CentOs 6.6
  Hadoop版本：CDH5.4
  JDK版本：1.7.0_71
  运行用户：root


集群各节点角色规划为：

192.168.56.121        cdh1     NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statesto...</content>
  </entry>
  
  <entry>
    <title>安装Impala过程</title>
    <link href="/2013/03/29/install-impala.html"/>
    <updated>2013-03-29T00:00:00+08:00</updated>
    <id>/2013/03/29/install-impala.html</id>
    <content type="html">与Hive类似，Impala也可以直接与HDFS和HBase库直接交互。只不过Hive和其它建立在MapReduce上的框架适合需要长时间运行的批处理任务。例如：那些批量提取，转化，加载（ETL）类型的Job，而Impala主要用于实时查询。

Hadoop集群各节点的环境设置及安装过程见 使用yum安装CDH Hadoop集群，参考这篇文章。

1. 环境


  CentOS 6.4 x86_64
  CDH 5.0.1
  jdk1.6.0_31


集群规划为3个节点，每个节点的ip、主机名和部署的组件分配如下：

192.168.56.121        cdh1     N...</content>
  </entry>
  
  <entry>
    <title>手动安装Cloudera Hive CDH</title>
    <link href="/2013/03/24/manual-install-Cloudera-hive-CDH.html"/>
    <updated>2013-03-24T00:00:00+08:00</updated>
    <id>/2013/03/24/manual-install-Cloudera-hive-CDH.html</id>
    <content type="html">本文主要记录手动安装Cloudera Hive集群过程，环境设置及Hadoop安装过程见手动安装Cloudera Hadoop CDH,参考这篇文章，hadoop各个组件和jdk版本如下：

	hadoop-2.0.0-cdh4.6.0
	hbase-0.94.15-cdh4.6.0
	hive-0.10.0-cdh4.6.0
	jdk1.6.0_38


hadoop各组件可以在这里下载。

集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：

	192.168.0.1        desktop1     NameNode、Hive、ResourceManager、i...</content>
  </entry>
  
  <entry>
    <title>手动安装Cloudera HBase CDH</title>
    <link href="/2013/03/24/manual-install-Cloudera-hbase-CDH.html"/>
    <updated>2013-03-24T00:00:00+08:00</updated>
    <id>/2013/03/24/manual-install-Cloudera-hbase-CDH.html</id>
    <content type="html">本文主要记录手动安装Cloudera HBase集群过程，环境设置及Hadoop安装过程见手动安装Cloudera Hadoop CDH,参考这篇文章，hadoop各个组件和jdk版本如下：

	hadoop-2.0.0-cdh4.6.0
	hbase-0.94.15-cdh4.6.0
	hive-0.10.0-cdh4.6.0
	jdk1.6.0_38


hadoop各组件可以在这里下载。

集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：

	192.168.0.1        desktop1     NameNode、Hive、ResourceManager、...</content>
  </entry>
  
  <entry>
    <title>手动安装Cloudera Hadoop CDH</title>
    <link href="/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html"/>
    <updated>2013-03-24T00:00:00+08:00</updated>
    <id>/2013/03/24/manual-install-Cloudera-Hadoop-CDH.html</id>
    <content type="html">安装版本

hadoop各个组件和jdk版本如下：

	hadoop-2.0.0-cdh4.6.0
	hbase-0.94.15-cdh4.6.0
	hive-0.10.0-cdh4.6.0
	jdk1.6.0_38


hadoop各组件可以在这里下载。

安装前说明


  确定安装目录为/opt
  检查hosts文件是否设置集群各节点的hostname和ip映射
  关闭每个节点的防火墙
  设置每个节点时钟同步


规划

集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：

	192.168.0.1        desktop1     NameNode、Hi...</content>
  </entry>
  
  <entry>
    <title>【笔记】Hadoop安装部署</title>
    <link href="/2013/03/08/note-about-installing-hadoop-cluster.html"/>
    <updated>2013-03-08T00:00:00+08:00</updated>
    <id>/2013/03/08/note-about-installing-hadoop-cluster.html</id>
    <content type="html">安装虚拟机

使用VirtualBox安装rhel6.3，存储为30G，内存为1G，并使用复制克隆出两个新的虚拟机，这样就存在3台虚拟机，设置三台虚拟机的主机名称，如：rhel-june、rhel-june-1、rhel-june-2

配置网络

a. VirtualBox全局设定-网络中添加一个新的连接：vboxnet0

b. 设置每一个虚拟机的网络为Host-Only

c.分别修改每个虚拟机的ip，DHCP或手动设置

vim /etc/sysconfig/network-scripts/ifcfg-eth0
vim /etc/udev/rules.d/70-persisten...</content>
  </entry>
  
  <entry>
    <title>2012年度总结</title>
    <link href="/2013/02/20/summary-of-the-work-in-2012.html"/>
    <updated>2013-02-20T00:00:00+08:00</updated>
    <id>/2013/02/20/summary-of-the-work-in-2012.html</id>
    <content type="html">2012年是在公司工作的第二年，在总结2012年的得与失的时候，有必要和《2011的度年终总结》相比较，在比较中审视自己在2012年是否有改进2011年存在的不足、是否有实现2011年定下的2012年工作计划。
以下是2012年相对于2011年的一些变化。


  2011年，在调研云计算产品过程中，深刻的意识到自身在linux方面存在的不足；2012年，熟悉了基本的linux命令，能够读懂并编写简单shell脚本；
  2011年，工作环境是win7+fedora；2012年，一直使用fedora操作系统工作、编码；
  2011年，较多的时间花在编写代码、完成开发任务上；2012年，...</content>
  </entry>
  
  <entry>
    <title>使用Octopress将博客从wordpress迁移到GitHub</title>
    <link href="/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress.html"/>
    <updated>2012-06-03T00:00:00+08:00</updated>
    <id>/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress.html</id>
    <content type="html">Step1 - 在本机安装Octopress

首先，必须先在本机安装配置Git和Ruby,Octopress需要Ruby版本至少为1.9.2。你可以使用RVM或rbenv安装ruby，安装方法见Octopress官方文档：http://octopress.org/docs/setup/

我使用rvm安装：
    rvm install 1.9.2 &amp;amp;&amp;amp; rvm use 1.9.2
安装完之后可以查看ruby版本：
    ruby –version
结果为：
    ruby 1.9.2p320 (2012-04-20 revision 35421) [x86_64...</content>
  </entry>
  
  <entry>
    <title>Kettle dependency management</title>
    <link href="/2012/04/13/kettle-dependency-management.html"/>
    <updated>2012-04-13T00:00:00+08:00</updated>
    <id>/2012/04/13/kettle-dependency-management.html</id>
    <content type="html">pentaho的项目使用了ant和ivy解决项目依赖,所以必须编译源码需要ivy工具.直接使用ivy编译pentaho的bi server项目,一直没有编译成功.
使用ivy编译kettle的源代码却是非常容易的事情.

该篇文章翻译并参考了Will Gorman在pentaho的wiki上添加的Kettle dependency management,文章标题没作修改.
编写此文,是为了记录编译kettle源码的方法和过程.

以下是对原文的一个简单翻译.
将kettle作为一个产品发行是一个很有趣的事情.有很多来自于pentaho其他项目(其中有一些有依赖于kettle)的jar包被...</content>
  </entry>
  
  <entry>
    <title>哈希表</title>
    <link href="/2012/03/26/hash-and-hash-functions.html"/>
    <updated>2012-03-26T00:00:00+08:00</updated>
    <id>/2012/03/26/hash-and-hash-functions.html</id>
    <content type="html">定义

一般的线性表、树，数据在结构中的相对位置是随机的，即和记录的关键字之间不存在确定的关系，因此，在结构中查找记录时需进行一系列和关键字的比较。这一类查找方法建立在“比较“的基础上，查找的效率依赖于查找过程中所进行的比较次数。 若想能直接找到需要的记录，必须在记录的存储位置和它的关键字之间建立一个确定的对应关系f，使每个关键字和结构中一个唯一的存储位置相对应，这就是哈希表。

哈希表又称散列表。哈希表存储的基本思想是：以数据表中的每个记录的关键字 k为自变量，通过一种函数H(k)计算出函数值。把这个值解释为一块连续存储空间（即数组空间）的单元地址（即下标），将该记录存储到这个单元中。...</content>
  </entry>
  
  <entry>
    <title>如何在Kettle4.2上面实现cassandra的输入与输出</title>
    <link href="/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2.html"/>
    <updated>2012-03-23T00:00:00+08:00</updated>
    <id>/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2.html</id>
    <content type="html">这是在QQ群里有人问到的一个问题。

如何在pdi-ce-4.2.X-stable上面实现cassandra的输入与输出,或是实现hadoop,hbase,mapreduce,mongondb的输入输出?

在kettle中实现cassandra的输入与输出有以下两种方式:


  第一种方式:自己编写cassandra输入输出组件
  第二种方式:使用别人编写好的插件,将其集成进来


当然还有第三种方法,直接使用4.3版本的pdi.

第一种方法需要对cassandra很熟悉编写插件才可以做到,第二种方法可以通过拷贝pdi-ce-big-data-4.3.0-preview中的文件来...</content>
  </entry>
  
  <entry>
    <title>2011年度年终总结</title>
    <link href="/2012/02/26/summary-of-the-work-in-2011.html"/>
    <updated>2012-02-26T00:00:00+08:00</updated>
    <id>/2012/02/26/summary-of-the-work-in-2011.html</id>
    <content type="html">2011年工作总结，只能算是七个月的工作总结，在这七个月里学到了许多、收获了许多、感悟了许多。以下是对这六个月的一个回顾与总结。

来公司最初的两个月主要负责云计算相关产品的调研工作，相关云计算产品有：Eucalyptus、OpenNebula、OpenStack。在这两个月里，对云计算的原理、服务、架构以及安装和部署有了初步的了解于实践，并积累了一些文档。在一次又一次的安装部署过程中，体验到了失败的痛苦、无助的迷茫、成功的喜悦，深刻的意识到自身在linux方面存在的不足；强烈的感觉到有必要学习一些语言，如Shell、Python、Groovy、Ruby等等。2012年，打算将工作环境切...</content>
  </entry>
  
  <entry>
    <title>Seam的启动过程</title>
    <link href="/2012/02/23/the-process-of-seam-initiation.html"/>
    <updated>2012-02-23T00:00:00+08:00</updated>
    <id>/2012/02/23/the-process-of-seam-initiation.html</id>
    <content type="html">了解seam2的人知道，seam是通过在web. xml中配置监听器启动的。注意，本文中的seam是指的seam2，不是seam3.

&amp;lt;listener&amp;gt;
	&amp;lt;listenerclass&amp;gt;org. jboss. seam. servlet. SeamListener&amp;lt;/listenerclass&amp;gt;
&amp;lt;/listener&amp;gt;


该监听器会做哪些事情呢？看看Gavin King对SeamListener类的描述。

Drives certain Seam functionality such as initialization and cle...</content>
  </entry>
  
  <entry>
    <title>Kettle运行作业之前的初始化过程</title>
    <link href="/2012/02/22/the-init-process-before-job-execution.html"/>
    <updated>2012-02-22T00:00:00+08:00</updated>
    <id>/2012/02/22/the-init-process-before-job-execution.html</id>
    <content type="html">本文主要描述Kettle是如何通过GUI调用代码启动线程执行作业的。

之前用英文写了一篇文章《The execution process of kettle’s job》 ，这篇文章只是用于英语写技术博客的一个尝试。由于很久没有使用英语写作了，故那篇文章只是简单的通过UML的序列图描述kettle运行job的一个java类调用过程。将上篇文章的序列图和这篇文章联系起来，会更加容易理解本文。

在Spoon界面点击运行按钮，Spoon GUI会调用Spoon.runFile()方法，这可以从xul文件（ui/menubar.xul）中的描述看出来。关于kettle中的xul的使用，不是本...</content>
  </entry>
  
  <entry>
    <title>The execution process of kettle’s job</title>
    <link href="/2012/02/21/the-execution-process-of-kettles-job.html"/>
    <updated>2012-02-21T00:00:00+08:00</updated>
    <id>/2012/02/21/the-execution-process-of-kettles-job.html</id>
    <content type="html">How to execute a kettle job in Spoon GUI or command line after we create a job in Spoon GUI? In Spoon GUI,the main class is &quot;org.pentaho.di.ui.spoon.Spoon.java&quot;.This class handles the main window of the Spoon graphical transformation editor.Many operations about a job or transformation such as ru...</content>
  </entry>
  
  <entry>
    <title>Kettle中定义错误处理</title>
    <link href="/2012/02/17/step-error-handling-in-kettle.html"/>
    <updated>2012-02-17T00:00:00+08:00</updated>
    <id>/2012/02/17/step-error-handling-in-kettle.html</id>
    <content type="html">在kettle执行的过程中，如果遇到错误，kettle会停止运行。在某些时候，并不希望kettle停止运行，这时候可以使用错误处理（Step Error Handling）。错误处理允许你配置一个步骤来取代出现错误时停止运行一个转换，出现错误的记录行将会传递给另一个步骤。在Step error handling settings对话框里，需要设置启用错误处理。

下面例子中读取postgres数据库中的a0表数据，然后输出到a1表：




a1表结构如下：

CREATE TABLE a1
(
  a double precision,
  id integer NOT NULL,
 ...</content>
  </entry>
  
  <entry>
    <title>JSF中EL表达式之this扩展</title>
    <link href="/2012/02/14/this-expression-of-jsf-el.html"/>
    <updated>2012-02-14T00:00:00+08:00</updated>
    <id>/2012/02/14/this-expression-of-jsf-el.html</id>
    <content type="html">本篇文章来自以前公司的一套jsf+seam+Hibernate的一套框架，其对jsf进行了一些改进，其中包括:EL表达式中添加this，通过jsf的渲染实现权限控制到按钮等等。JSF表达式中添加this，主要是为了在facelets页面使用this关键字引用（JSF自动查找）到当前页面对应的pojo类，详细说明见下午。因为，本文的文章是公司同事整理的，本文作者仅仅是将其分享出来，供大家参考思路，如果有什么不妥的话，请告知。

EL表达式this扩展
在业务系统中，大量页面具有大量区域是相似或者相同的，或者可能根据某些局部特征的变化具有一定的变化，jsf中通过facelet模板功能可以达到...</content>
  </entry>
  
  <entry>
    <title>使用Kettle数据迁移添加主键和索引</title>
    <link href="/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle.html"/>
    <updated>2012-01-05T00:00:00+08:00</updated>
    <id>/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle.html</id>
    <content type="html">Kettle是一款国外开源的etl工具，纯java编写，绿色无需安装，主要用于数据抽取、转换、装载。kettle兼容了市面上几十种数据库，故用kettle来做数据库的迁移视乎是个不错的选择。

kettle的数据抽取主要在于抽取数据，而没有考虑数据库的函数、存储过程、视图、表结构以及索引、约束等等，而这些东西恰恰都是数据迁移需要考虑的事情。当然，如果在不考虑数据库中的函数、存储过程、视图的情况下，使用kettle进行数据的迁移还算是一个可行的方案。

这篇文章主要是讲述在使用kettle进行数据库的迁移的时候如何迁移主键和索引，为什么要迁移主键和索引？异构数据库之间的迁移很难无缝的实现自...</content>
  </entry>
  
  <entry>
    <title>kettle进行数据迁移遇到的问题</title>
    <link href="/2012/01/04/some-problems-about-migrating-database-datas-with-kettle.html"/>
    <updated>2012-01-04T00:00:00+08:00</updated>
    <id>/2012/01/04/some-problems-about-migrating-database-datas-with-kettle.html</id>
    <content type="html">使用kettle进行oracle或db2数据导入到mysql或postgres数据库过程中遇到以下问题，以下只是一个简单描述，详细的说明以及所做的代码修改没有提及。下面所提到的最新的pdi程序是我修改kettle源码并编译之后的版本。

同时运行两个pdi程序，例如：一个为oracle到mysql，另一个为oracle到postgres，其中一个停止运行


  原因：从oracle迁移到mysql创建的作业和转换文件和oracle到postgres的作业和转换保存到一个路径，导致同名称的转换相互之间被覆盖，故在运行时候会出现混乱。
  解决办法：将新建的作业和转换分别保存在两个不同的路...</content>
  </entry>
  
  <entry>
    <title>Mondrian and OLAP</title>
    <link href="/2011/12/07/mondrian-and-olap.html"/>
    <updated>2011-12-07T00:00:00+08:00</updated>
    <id>/2011/12/07/mondrian-and-olap.html</id>
    <content type="html">Mondrian是一个用Java编写的OLAP引擎。他执行用MDX语言编写的查询，从关系数据库（RDBMS）中读取数据并且通过Java API以多维度的格式展示查询结果。

Online Analytical Processing
联机分析处理（OLAP）指在线实时的分析大量数据。与联机事务处理系统（On-Line Transaction Processing，简称OLTP）不同，OLTP中典型的操作如读和修改单个的少量的记录，而OLAP批量处理数据并且所有操作都是只读的。“online”意味着即使是处理大量的数据----百万条数据记录，占有几个GB内存----系统必须足够快的反回查询结...</content>
  </entry>
  
  <entry>
    <title>XUL 用户界面语言介绍</title>
    <link href="/2011/11/25/xml-user-interface-language-introuction.html"/>
    <updated>2011-11-25T00:00:00+08:00</updated>
    <id>/2011/11/25/xml-user-interface-language-introuction.html</id>
    <content type="html">XUL[1]是英文“XML User Interface Language”的首字母缩写。它是为了支持Mozilla系列的应用程序（如Mozilla Firefox和Mozilla Thunderbird）而开发的用户界面标示语言。顾名思义，它是一种应用XML来描述用户界面的标示语言。
XUL是开放标准，重用了许多现有的标准和技术[2]，包括CSS、JavaScript、DTD和RDF等。所以对于有网络编程和设计经验的人士来说，学习XUL比学习其他用户界面标示语言相对简单。
使用XUL的主要好处在于它提供了一套简易和跨平台的widget定义。这节省了编程人员在开发软件时所付出的努力。

...</content>
  </entry>
  
  <entry>
    <title>在eclipse中构建Pentaho BI Server工程</title>
    <link href="/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse.html"/>
    <updated>2011-09-28T00:00:00+08:00</updated>
    <id>/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse.html</id>
    <content type="html">首先需要说明的是，Pentaho BI Server源代码在svn://source.pentaho.org/svnroot/bi-platform-v2/trunk/，并且用ivy构建。ivy没有用过也不熟悉，故不打算从这里使用ivy构建源码。

当然，您可以参考官方文档构建源码。

Pentaho BI Server打包后的文件存于这里，其中包括（本文使用的是3.9.0版本）：biserver-ce-3.9.0-stable.zip，bi-platform-3.9.0-stable-sources.zip，biserver-ce-3.9.0-stable-javadoc.zip。

...</content>
  </entry>
  
  <entry>
    <title>Pentaho现场支持遇到问题及解决办法</title>
    <link href="/2011/09/26/resolved-pentaho-problems-9-16.html"/>
    <updated>2011-09-26T00:00:00+08:00</updated>
    <id>/2011/09/26/resolved-pentaho-problems-9-16.html</id>
    <content type="html">很久没写文章了，最近在关注Pentaho。
 以下是9月16日现场提出的问题解决办法：
      1、PDF预览中文没显示，txt预览中文乱码：
           1）、设置File-&amp;gt;Configuration -&amp;gt;output-pageable-pdf的encoding 为Identity-H
           2）、将需要输出中文的报表项目的字体设置为中文字体，例如宋体
           3）、如要发布到服务器，需要修改如下的配置：
             pentaho/server/biserver-ee/tomcat/webapps/pentaho...</content>
  </entry>
  
  <entry>
    <title>在Fedora 15 上搭建Eucalyptus</title>
    <link href="/2011/08/20/install-eucalyptus-on-fedora-15.html"/>
    <updated>2011-08-20T00:00:00+08:00</updated>
    <id>/2011/08/20/install-eucalyptus-on-fedora-15.html</id>
    <content type="html">
在Fedora 15 上搭建Eucalyptus平台，在Fedora 15 上搭建Eucalyptus与在Centos上搭建Eucalyptus有什么区别呢？参照这篇文章Installing Eucalyptus (2.0) on Fedora 12，然后注意一些细节，视乎就能安装成功。不管你信不信，我是在虚拟机中安装fedora15，然后安装Eucalyptus失败了，失败的原因是xen的网络没有配置好，查看资源的时候free / max都为0000.

毕竟是第一次接触云计算，第一次接触XEN，第一次接触Eucalyptus，Eucalyptus改装的都装了，就是XEN的网络没有配...</content>
  </entry>
  
  <entry>
    <title>Export DhtmlxGrid to PDF in Java</title>
    <link href="/2011/08/11/export-dhtmlxgrid-to-pdf-in-java.html"/>
    <updated>2011-08-11T00:00:00+08:00</updated>
    <id>/2011/08/11/export-dhtmlxgrid-to-pdf-in-java.html</id>
    <content type="html">将DhtmlxGrid数据导出到pdf这是很常见的需求，dhtmlx官网提供了php和java版本的例子，你可以去官网查看这篇文章《Grid-to-Excel, Grid-to-PDF Available for Java》，你可以从以下地址下载导出程序源码：
Export to Excel
Export to PDF
当然，还有一个示例工程： .zip archive with an example

XML2PDF和XML2Excel工程内代码很相似，XML2PDF内部使用了PDFjet.jar导出PDF，而XML2Excel使用JXL导出Excel。
需要说明的是，还需要引入dht...</content>
  </entry>
  
  <entry>
    <title>自定义dhtmlxGrid表头菜单</title>
    <link href="/2011/07/31/custom-dhtmlxgrid-header-menu.html"/>
    <updated>2011-07-31T00:00:00+08:00</updated>
    <id>/2011/07/31/custom-dhtmlxgrid-header-menu.html</id>
    <content type="html">dhtmlxGrid可以定义表头菜单以及表格右键菜单，表格右键菜单可以自定义，但是表头菜单只能使用其提供的菜单。dhtmlxGrid默认的表头菜单可以决定表格中每一列是否在表格中显示，并没有提供更多的扩展，如果我想自定义表头菜单，该怎么做呢？本文就是基于自定义表格菜单，说说我的实现方式。
以下是dhtmlxGrid的表头菜单效果：


其功能过于单一，以下是表格右键菜单效果：

如果能够像表格菜单一样自定义表头菜单，那会是一件非常有意义的事情，因为dhtmlxGrid菜单都是一些针对行和单元格的操作，没有提过针对列的操作，比如我可能需要在某一列上实现该列的显示与隐藏、排序、改变列属性以及...</content>
  </entry>
  
  <entry>
    <title>Drag an item to dhtmlxGrid and add a column</title>
    <link href="/2011/07/24/drag-an-item-to-dhtmlxgrid-and-add-a-column.html"/>
    <updated>2011-07-24T00:00:00+08:00</updated>
    <id>/2011/07/24/drag-an-item-to-dhtmlxgrid-and-add-a-column.html</id>
    <content type="html">dhtmlxGrid支持tree和grid、grid之间、grid内部进行拖拽，如在grid内部进行拖拽，可以增加一行；在grid之间拖拽，第一个grid的记录删除，第二个grid增加一行记录。如果我想在拖拽之后不是添加一行而是一列，该怎么做呢？
现在有个需求，就是左边有个tree，右边有个grid，将左边tree的一个节点拖到右边grid的表头并动态增加一列。这个怎么做呢？
如果你想快点看到最后的实现方法，你可以直接跳到本文的最后参看源码。
首先看看dhtmlxTree 关于Drag-n-Drop的例子，其中有这样一个例子Custom Drag Out。
上面的例子，右边定义了一个输入...</content>
  </entry>
  
  <entry>
    <title>DhtmlxGrid Quick Start Guide</title>
    <link href="/2011/07/19/dhtmlxgrid-quick-start-guide.html"/>
    <updated>2011-07-19T00:00:00+08:00</updated>
    <id>/2011/07/19/dhtmlxgrid-quick-start-guide.html</id>
    <content type="html">说明:

本文来源于http://dhtmlx.com/docs/products/dhtmlxGrid/，本人对其进行翻译整理成下文，贴出此文，紧供分享。

dhtmlxGrid是一个拥有强大的数据绑定、优秀的大数据展示性能并支持ajax的JavaScript表格控件。该组件易于使用并通过富客户端的API提供了很大的扩展性。dhtmlxGrid支持不同的数据源（XML, JSON, CSV, JavaScript 数组和HTML表格），如果需要的话，还可以从自定义的xml中加载数据。

跨浏览器
使用JavaScript API进行控制
Ajax支持
简单的JavaScript 或者X...</content>
  </entry>
  
  <entry>
    <title>网上收集的关于OpenStack的一些资源</title>
    <link href="/2011/07/07/some-resources-about-openstack.html"/>
    <updated>2011-07-07T00:00:00+08:00</updated>
    <id>/2011/07/07/some-resources-about-openstack.html</id>
    <content type="html">
OpenStack Nova code：https://bugs.launchpad.net/nova


OpenStack Blog：http://planet.openstack.org/

OpenStack 官方文档：http://docs.openstack.org/cactus/openstack-compute/admin/content/ch_getting-started-with-openstack.html

OpenStack 中国门户：http://blu001068.chinaw3.com/bbs/portal.php

在 Ubuntu 上安装和配置 O...</content>
  </entry>
  
  <entry>
    <title>Centos上安装 OpenNebula Management Console</title>
    <link href="/2011/06/29/install-opennebula-management-console-in-centos5-6.html"/>
    <updated>2011-06-29T00:00:00+08:00</updated>
    <id>/2011/06/29/install-opennebula-management-console-in-centos5-6.html</id>
    <content type="html">我们可以通过onehost/onevm/onevnet等等 这些命令行工具来管理 OpenNebula 云计算平台，也可以通过OpenNebula项目组开发的web控制台来访问OpenNebula。OpenNebula项目组提供了两个web程序来管理OpenNebula，一个即本文提到的OpenNebula Management Console，一个是The Cloud Operations Center，前者需要额外下载，后者内嵌与OpenNebula安装包内。

OpenNebula 2.2提供的文档相对较少并且零散，在网上可以找到一篇关于OpenNebula Management ...</content>
  </entry>
  
  <entry>
    <title>OpenNebula 2.2的特性</title>
    <link href="/2011/06/26/opennebula-2-2-features.html"/>
    <updated>2011-06-26T00:00:00+08:00</updated>
    <id>/2011/06/26/opennebula-2-2-features.html</id>
    <content type="html">以下这篇文章由OpenNebula 2.2 Features翻译而来。

OpenNebula是一款为云计算而打造的开源工具箱。它允许你与Xen，KVM或VMware ESX一起建立和管理私有云，同时还提供Deltacloud适配器与Amazon EC2相配合来管理混合云。除了像Amazon一样的商业云服务提供商，在不同OpenNebula实例上运行私有云的Amazon合作伙伴也同样可以作为远程云服务供应商。

目前版本，可支持XEN、KVM和VMware，以及实时存取EC2和 ElasticHosts，它也支持印象档的传输、复制和虚拟网络管理网络。
主要特点和优势
私有云计算 

为私...</content>
  </entry>
  
  <entry>
    <title>Eucalyptus使用的技术</title>
    <link href="/2011/06/22/the-technology-used-in-eucalyptus.html"/>
    <updated>2011-06-22T00:00:00+08:00</updated>
    <id>/2011/06/22/the-technology-used-in-eucalyptus.html</id>
    <content type="html">
  libvirt


Libvirt 库是一种实现 Linux 虚拟化功能的 Linux® API，它支持各种虚拟机监控程序，包括 Xen 和 KVM，以及 QEMU 和用于其他操作系统的一些虚拟产品。


  Netty


Netty 提供异步的、事件驱动的网络应用程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。


  Axis2


Axis2是下一代 Apache Axis。Axis2 虽然由 Axis 1.x 处理程序模型提供支持，但它具有更强的灵活性并可扩展到新的体系结构。Axis2 基于新的体系结构进行了全新编写，而且没有采用 Axis 1.x ...</content>
  </entry>
  
  <entry>
    <title>Eucalyptus EE的介绍及功能说明</title>
    <link href="/2011/06/22/the-introduction-of-eucalyptus-ee-features-and-functions.html"/>
    <updated>2011-06-22T00:00:00+08:00</updated>
    <id>/2011/06/22/the-introduction-of-eucalyptus-ee-features-and-functions.html</id>
    <content type="html">Eucalyptus企业版2.0是一个基于Linux的软件架构，在企业现有的IT架构上实现一个可扩展的、提高效率的私有和混合云。Eucalyptus作为基础设施提供IaaS服务。这意味着用户可以通过Eucalyptus自助服务界面提供自己的资源（硬件、存储和网络）。一个Eucalyptus云是部署在企业的内部数据中心，由企业内部用户访问。因此，敏感数据可以在防火墙的保护下防止外部入侵。

Eucalyptus的设计目的是从根本上易于安装和尽可能没有侵扰。该软件高度模块化，具有行业标准，和语言无关。它提供了可以与EC2兼容的云计算平台和与S3兼容的云存储平台。
功能亮点

	无缝管理多个管...</content>
  </entry>
  
  <entry>
    <title>接触云服务环境Eucalyptus</title>
    <link href="/2011/06/16/touch-cloud-environment-which-it-is-eucalyptus.html"/>
    <updated>2011-06-16T00:00:00+08:00</updated>
    <id>/2011/06/16/touch-cloud-environment-which-it-is-eucalyptus.html</id>
    <content type="html">最近在接触云计算平台，熟悉了Eucalyptus，并用其搭建云环境。通过网上的一些例子，逐渐的摸索出用Eucalyptus搭建云计算平台的方法。我所用的Eucalyptus是免费版，缺少很多企业版的功能。

Eucalyptus

Elastic Utility Computing Architecture for Linking Your Programs To Useful Systems （Eucalyptus） 是一种开源的软件基础结构，用来通过计算集群或工作站群实现弹性的、实用的云计算。它最初是美国加利福尼亚大学 Santa Barbara 计算机科学学院的一个研究项目，现在已...</content>
  </entry>
  
  <entry>
    <title>Extjs读取xml文件生成动态表格和表单(续)</title>
    <link href="/2009/10/31/ext_readxml_in_bjsasc_wuzi_continue.html"/>
    <updated>2009-10-31T00:00:00+08:00</updated>
    <id>/2009/10/31/ext_readxml_in_bjsasc_wuzi_continue.html</id>
    <content type="html">很多人向我要【Extjs读取xml文件生成动态表格和表单】一文的源代码，故花了些时间将源代码整理出来，并重新编写此文，分享当时的技术思路。

需要的文件有：


  1.html文件，此处以SASC.search.MtrUse.html为例
  2.Extjs相关文件,见SASC.search.MtrUse.html文件中的引用
  3.工具类，DomUtils.js
  4.核心js类:SASC.extjs.search.MtrUse.js
  5.java代码


详细html和js代码见相关文件，这里先描述思路。

首先

通过一个事件打开一个弹出窗口，该窗口的url指向SASC....</content>
  </entry>
  
  <entry>
    <title>Extjs读取xml文件生成动态表格和表单</title>
    <link href="/2009/10/22/ext_readxml_in_bjsasc_wuzi.html"/>
    <updated>2009-10-22T00:00:00+08:00</updated>
    <id>/2009/10/22/ext_readxml_in_bjsasc_wuzi.html</id>
    <content type="html">最近开发项目，需要动态读取xml文件，生成Extjs界面，xml文件通过前台页面的按钮事件传进来，可以在网上查找【javascript 弹出子窗口】的相关文章&amp;lt;/a&amp;gt;
获取弹出窗口url后的参数方法：

	// 获取url后的参数值
	function getQueryStringValue(name) {
		var url = window.location.search;
		if (url.indexOf(&#39;?&#39;) &amp;lt; 0) {
			return null
		}
		var index = url.indexOf(name + &quot;=&quot;);
		if (ind...</content>
  </entry>
  
  <entry>
    <title>Hello Jekyll!</title>
    <link href="/2009/01/01/hello-jekyll.html"/>
    <updated>2009-01-01T00:00:00+08:00</updated>
    <id>/2009/01/01/hello-jekyll.html</id>
    <content type="html">目前，博客使用的是Jekyll搭建的，markdown语法使用的是Redcarpet。Redcarpet支持设置 extensions ，值为一个字符串数组，每个字符串都是 Redcarpet::Markdown 类的扩展，相应的扩展就会设置为 true 。

配置为：

highlighter: pygments
markdown: redcarpet  # [ maruku | rdiscount | kramdown | redcarpet ]

redcarpet:
    extensions:
        - fenced_code_blocks
        - no...</content>
  </entry>
  
</feed>