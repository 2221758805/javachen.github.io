<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>Sqoop导入关系数据库到Hive - JavaChen Blog</title>
      <meta name="author" content="JavaChen"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2014/08/04/import-data-to-hive-with-sqoop.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

      <!-- atom & rss feed -->
      <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
      <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              <li><a href="https://twitter.com/java_chen" target="_blank" title="twitter"><span class="fa fa-twitter fa-2x"></span></a></li>
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2014/07/31/summary-of-july-in-2014.html" title="2014年7月总结"><i class="fa fa-angle-double-left"></i>&nbsp;2014年7月总结</a></li>
                
                
                <li class="next"><a href="/2014/08/19/upgrading-from-cdh4-to-cdh5.html" title="升级cdh4到cdh5">升级cdh4到cdh5&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> Sqoop导入关系数据库到Hive  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2014.08.04 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <p>Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。本文以 mysql 数据库为例，实现关系数据库导入到 hdfs 和 hive。</p>

<h1 id="1-安装-sqoop">1. 安装 Sqoop</h1>

<p>使用 rpm 安装即可。</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">yum install sqoop sqoop-metastore -y
</code></pre></div>
<blockquote>
<p>安装完之后需要下载 mysql jar 包到 sqoop 的 lib 目录。</p>
</blockquote>

<p>这里使用 hive 的 metastore 的 mysql 数据库作为关系数据库，以 TBLS 表为例，该表结构和数据如下：</p>
<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="n">mysql</span><span class="o">&gt;</span> <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">TBLS</span> <span class="k">limit</span> <span class="mi">3</span><span class="p">;</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+</span>
<span class="o">|</span><span class="n">TBL_ID</span><span class="o">|</span><span class="n">CREATE_TIME</span><span class="o">|</span><span class="n">DB_ID</span><span class="o">|</span><span class="n">LAST_ACCESS_TIME</span><span class="o">|</span><span class="k">OWNER</span><span class="o">|</span><span class="n">RETENTI</span> <span class="o">|</span> <span class="n">SD_ID</span><span class="o">|</span> <span class="n">TBL_NAME</span><span class="o">|</span> <span class="n">TBL_TYPE</span>       <span class="o">|</span><span class="n">VIEW_EXPANDED_TEXT</span><span class="o">|</span> <span class="n">VIEW_ORIGINAL_TEXT</span><span class="o">|</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+</span>
<span class="o">|</span>    <span class="mi">34</span><span class="o">|</span><span class="mi">1406784308</span> <span class="o">|</span>    <span class="mi">8</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">45</span><span class="o">|</span> <span class="n">test1</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">|</span>    <span class="mi">40</span><span class="o">|</span><span class="mi">1406797005</span> <span class="o">|</span>    <span class="mi">9</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">52</span><span class="o">|</span> <span class="n">test2</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">|</span>    <span class="mi">42</span><span class="o">|</span><span class="mi">1407122307</span> <span class="o">|</span>    <span class="mi">7</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">59</span><span class="o">|</span> <span class="n">test3</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+</span>
</code></pre></div>
<h1 id="2-使用">2. 使用</h1>

<h2 id="2-1-命令说明">2.1 命令说明</h2>

<p>查看 sqoop 命令说明：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop <span class="nb">help</span>
usage: sqoop COMMAND <span class="o">[</span>ARGS<span class="o">]</span>

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  <span class="nb">eval               </span>Evaluate a SQL statement and display the results
  <span class="nb">export             </span>Export an HDFS directory to a database table
  <span class="nb">help               </span>List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  version            Display version information

See <span class="s1">&#39;sqoop help COMMAND&#39;</span> <span class="k">for</span> information on a specific command.
</code></pre></div>
<p>你也可以查看某一个命令的使用说明：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --help
<span class="nv">$ </span>sqoop <span class="nb">help </span>import
</code></pre></div>
<p>你也可以使用别名来代替 <code class="prettyprint">sqoop (toolname)</code>：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop-import
</code></pre></div>
<p>sqoop import 的一个示例如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS
</code></pre></div>
<p>你还可以使用 <code class="prettyprint">--options-file</code> 来传入一个文件，使用这种方式可以重用一些配置参数：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop --options-file /users/homer/work/import.txt --table TEST
</code></pre></div>
<p>/users/homer/work/import.txt 文件内容如下：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">import
--connect
jdbc:mysql://192.168.56.121:3306/metastore
--username
hiveuser
--password 
redhat
</code></pre></div>
<h1 id="2-2-导入数据到-hdfs">2.2 导入数据到 hdfs</h1>

<p>使用 sqoop-import 命令可以从关系数据库导入数据到 hdfs。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --target-dir /user/hive/result
</code></pre></div>
<p>注意：</p>

<ul>
<li>mysql jdbc url 请使用 ip 地址</li>
<li>如果重复执行，会提示目录已经存在，可以手动删除</li>
<li>如果不指定 <code class="prettyprint">--target-dir</code>，导入到用户家目录下的 TBLS 目录</li>
</ul>

<p>你还可以指定其他的参数：</p>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--append</code></td>
<td style="text-align: left">将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--as-avrodatafile</code></td>
<td style="text-align: left">将数据导入到一个Avro数据文件中</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--as-sequencefile</code></td>
<td style="text-align: left">将数据导入到一个sequence文件中</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--as-textfile</code></td>
<td style="text-align: left">将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--boundary-query &lt;statement&gt;</code></td>
<td style="text-align: left">边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：<code class="prettyprint">--boundary-query &#39;select id,no from t where id = 3&#39;</code>，表示导入的数据为id=3的记录，或者 <code class="prettyprint">select min(&lt;split-by&gt;), max(&lt;split-by&gt;) from &lt;table name&gt;</code>，注意查询的字段中不能有数据类型为字符串的字段，否则会报错</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--columns&lt;col,col&gt;</code></td>
<td style="text-align: left">指定要导入的字段值，格式如：<code class="prettyprint">--columns id,username</code></td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--direct</code></td>
<td style="text-align: left">直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--direct-split-size</code></td>
<td style="text-align: left">在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--inline-lob-limit</code></td>
<td style="text-align: left">设定大对象数据类型的最大值</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-m,--num-mappers</code></td>
<td style="text-align: left">启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--query，-e &lt;sql&gt;</code></td>
<td style="text-align: left">从查询结果中导入数据，该参数使用时必须指定<code class="prettyprint">–target-dir</code>、<code class="prettyprint">–hive-table</code>，在查询语句中一定要有where条件且在where条件中需要包含 <code class="prettyprint">\$CONDITIONS</code>，示例：<code class="prettyprint">--query &#39;select * from t where \$CONDITIONS &#39; --target-dir /tmp/t –hive-table t</code></td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--split-by &lt;column&gt;</code></td>
<td style="text-align: left">表的列名，用来切分工作单元，一般后面跟主键ID</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--table &lt;table-name&gt;</code></td>
<td style="text-align: left">关系数据库表名，数据从该表中获取</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--delete-target-dir</code></td>
<td style="text-align: left">删除目标目录</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--target-dir &lt;dir&gt;</code></td>
<td style="text-align: left">指定hdfs路径</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--warehouse-dir &lt;dir&gt;</code></td>
<td style="text-align: left">与 <code class="prettyprint">--target-dir</code> 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--where</code></td>
<td style="text-align: left">从关系数据库导入数据时的查询条件，示例：<code class="prettyprint">--where &quot;id = 2&quot;</code></td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-z,--compress</code></td>
<td style="text-align: left">压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--compression-codec</code></td>
<td style="text-align: left">Hadoop压缩编码，默认是gzip</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--null-string &lt;null-string&gt;</code></td>
<td style="text-align: left">可选参数，如果没有指定，则字符串null将被使用</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--null-non-string &lt;null-string&gt;</code></td>
<td style="text-align: left">可选参数，如果没有指定，则字符串null将被使用</td>
</tr>
</tbody></table>

<p>示例程序：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --columns <span class="s2">&quot;tbl_id,create_time&quot;</span> --where <span class="s2">&quot;tbl_id &gt; 1&quot;</span> --target-dir /user/hive/result
</code></pre></div>
<h3 id="使用-sql-语句">使用 sql 语句</h3>

<p>参照上表，使用 sql 语句查询时，需要指定 <code class="prettyprint">$CONDITIONS</code></p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --query <span class="s1">&#39;SELECT * from TBLS where \$CONDITIONS &#39;</span> --split-by tbl_id -m <span class="m">4</span> --target-dir /user/hive/result
</code></pre></div>
<p>上面命令通过 <code class="prettyprint">-m 1</code> 控制并发的 map 数。</p>

<h3 id="使用-direct-模式：">使用 direct 模式：</h3>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --delete-target-dir --direct --default-character-set UTF-8 --target-dir /user/hive/result
</code></pre></div>
<h3 id="指定文件输出格式：">指定文件输出格式：</h3>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --delete-target-dir  --target-dir /user/hive/result
</code></pre></div>
<p>这时候查看 hdfs 中数据(观察分隔符是否为制表符)：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hadoop fs -ls result
Found <span class="m">5</span> items
-rw-r--r--   <span class="m">3</span> root hadoop          <span class="m">0</span> 2014-08-04 16:07 result/_SUCCESS
-rw-r--r--   <span class="m">3</span> root hadoop         <span class="m">69</span> 2014-08-04 16:07 result/part-m-00000
-rw-r--r--   <span class="m">3</span> root hadoop          <span class="m">0</span> 2014-08-04 16:07 result/part-m-00001
-rw-r--r--   <span class="m">3</span> root hadoop        <span class="m">142</span> 2014-08-04 16:07 result/part-m-00002
-rw-r--r--   <span class="m">3</span> root hadoop         <span class="m">62</span> 2014-08-04 16:07 result/part-m-00003

<span class="nv">$ </span>hadoop fs -cat result/part-m-00000
<span class="m">34</span>  <span class="m">1406784308</span>  <span class="m">8</span>   <span class="m">0</span>   root    <span class="m">0</span>   <span class="m">45</span>  test1   EXTERNAL_TABLE  null    null    null

<span class="nv">$ </span>hadoop fs -cat result/part-m-00002
<span class="m">40</span>  <span class="m">1406797005</span>  <span class="m">9</span>   <span class="m">0</span>   root    <span class="m">0</span>   <span class="m">52</span>  test2   EXTERNAL_TABLE  null    null    null
<span class="m">42</span>  <span class="m">1407122307</span>  <span class="m">7</span>   <span class="m">0</span>   root    <span class="m">0</span>   <span class="m">59</span>  test3   EXTERNAL_TABLE  null    null    null
</code></pre></div>
<p>指定空字符串：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --delete-target-dir --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --target-dir /user/hive/result
</code></pre></div>
<p>如果需要指定压缩：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --delete-target-dir --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --compression-codec <span class="s2">&quot;com.hadoop.compression.lzo.LzopCodec&quot;</span> --target-dir /user/hive/result
</code></pre></div>
<p>附：可选的文件参数如下表。</p>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--enclosed-by &lt;char&gt;</code></td>
<td style="text-align: left">给字段值前后加上指定的字符，比如双引号，示例：<code class="prettyprint">--enclosed-by &#39;\&quot;&#39;</code>，显示例子：&quot;3&quot;,&quot;jimsss&quot;,&quot;<a href="mailto:dd@dd.com">dd@dd.com</a>&quot;</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--escaped-by &lt;char&gt;</code></td>
<td style="text-align: left">给双引号作转义处理，如字段值为&quot;测试&quot;，经过 <code class="prettyprint">--escaped-by &quot;\\&quot;</code> 处理后，在hdfs中的显示值为：<code class="prettyprint">\&quot;测试\&quot;</code>，对单引号无效</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--fields-terminated-by &lt;char&gt;</code></td>
<td style="text-align: left">设定每个字段是以什么符号作为结束的，默认是逗号，也可以改为其它符号，如句号<code class="prettyprint">.</code>，示例如：<code class="prettyprint">--fields-terminated-by</code></td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--lines-terminated-by &lt;char&gt;</code></td>
<td style="text-align: left">设定每条记录行之间的分隔符，默认是换行串，但也可以设定自己所需要的字符串，示例如：<code class="prettyprint">--lines-terminated-by &quot;#&quot;</code> 以#号分隔</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--mysql-delimiters</code></td>
<td style="text-align: left">Mysql默认的分隔符设置，字段之间以<code class="prettyprint">,</code>隔开，行之间以换行<code class="prettyprint">\n</code>隔开，默认转义符号是<code class="prettyprint">\</code>，字段值以单引号<code class="prettyprint">&#39;</code>包含起来。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--optionally-enclosed-by &lt;char&gt;</code></td>
<td style="text-align: left">enclosed-by是强制给每个字段值前后都加上指定的符号，而<code class="prettyprint">--optionally-enclosed-by</code>只是给带有双引号或单引号的字段值加上指定的符号，故叫可选的</td>
</tr>
</tbody></table>

<h1 id="2-3-创建-hive-表">2.3 创建 hive 表</h1>

<p>生成与关系数据库表的表结构对应的HIVE表：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop create-hive-table --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS 
</code></pre></div>
<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--hive-home &lt;dir&gt;</code></td>
<td style="text-align: left">Hive的安装目录，可以通过该参数覆盖掉默认的hive目录</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--hive-overwrite</code></td>
<td style="text-align: left">覆盖掉在hive表中已经存在的数据</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--create-hive-table</code></td>
<td style="text-align: left">默认是false，如果目标表已经存在了，那么创建任务会失败</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--hive-table</code></td>
<td style="text-align: left">后面接要创建的hive表</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--table</code></td>
<td style="text-align: left">指定关系数据库表名</td>
</tr>
</tbody></table>

<h1 id="2-4-导入数据到-hive">2.4 导入数据到 hive</h1>

<p>执行下面的命令会将 mysql 中的数据导入到 hdfs 中，然后创建一个hive 表，最后再将 hdfs 上的文件移动到 hive 表的目录下面。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --delete-target-dir
</code></pre></div>
<p>说明：</p>

<ul>
<li>可以在 hive 的表名前面指定数据库名称</li>
<li>可以通过 <code class="prettyprint">--create-hive-table</code> 创建表，如果表已经存在则会执行失败</li>
</ul>

<p>接下来可以查看 hive 中的数据：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hive -e <span class="s1">&#39;select * from dw_srclog.tbls&#39;</span>
<span class="m">34</span>  <span class="m">1406784308</span>  <span class="m">8</span>   <span class="m">0</span>   root    <span class="m">0</span>   <span class="m">45</span>  test1   EXTERNAL_TABLE  null    null    NULL
<span class="m">40</span>  <span class="m">1406797005</span>  <span class="m">9</span>   <span class="m">0</span>   root    <span class="m">0</span>   <span class="m">52</span>  test2   EXTERNAL_TABLE  null    null    NULL
<span class="m">42</span>  <span class="m">1407122307</span>  <span class="m">7</span>   <span class="m">0</span>   root    <span class="m">0</span>   <span class="m">59</span>  test3   EXTERNAL_TABLE  null    null    NULL
</code></pre></div>
<p>直接查看文件内容：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hadoop fs -cat /user/hive/warehouse/dw_srclog.db/tbls/part-m-00000
34140678430880root045go_goodsEXTERNAL_TABLEnullnullnull
40140679700590root052merchantEXTERNAL_TABLEnullnullnull
</code></pre></div>
<p>从上面可见，数据导入到 hive 中之后分隔符为默认分隔符，参考上文你可以通过设置参数指定其他的分隔符。</p>

<p>另外，Sqoop 默认地导入空值（NULL）为 null 字符串，而 hive 使用 \N 去标识空值（NULL），故你在 import 或者 export 时候，需要做相应的处理。在 import 时，使用如下命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import  ... --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span>
</code></pre></div>
<p>在导出时，使用下面命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import  ... --input-null-string <span class="s1">&#39;&#39;</span> --input-null-non-string <span class="s1">&#39;&#39;</span>
</code></pre></div>
<p>一个完整的例子如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --compression-codec <span class="s2">&quot;com.hadoop.compression.lzo.LzopCodec&quot;</span>
</code></pre></div>
<h1 id="2-5-增量导入">2.5 增量导入</h1>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--check-column (col)</code></td>
<td style="text-align: left">用来作为判断的列名，如id</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--incremental (mode)</code></td>
<td style="text-align: left">append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--last-value (value)</code></td>
<td style="text-align: left">指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值</td>
</tr>
</tbody></table>

<h1 id="2-6-合并-hdfs-文件">2.6 合并 hdfs 文件</h1>

<p>将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中，示例如：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">sqoop merge –new-data /test/p1/person –onto /test/p2/person –target-dir /test/merged –jar-file /opt/data/sqoop/person/Person.jar –class-name Person –merge-key id
</code></pre></div>
<p>其中，<code class="prettyprint">–class-name</code> 所指定的 class 名是对应于 Person.jar 中的 Person 类，而 Person.jar 是通过 Codegen 生成的</p>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--new-data &lt;path&gt;</code></td>
<td style="text-align: left">Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能优先保留的，原则上一般是存放越新数据的目录就对应这个参数。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--onto &lt;path&gt;</code></td>
<td style="text-align: left">Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能被更新数据替换掉的，原则上一般是存放越旧数据的目录就对应这个参数。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--merge-key &lt;col&gt;</code></td>
<td style="text-align: left">合并键，一般是主键ID</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--jar-file &lt;file&gt;</code></td>
<td style="text-align: left">合并时引入的jar包，该jar包是通过Codegen工具生成的jar包</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--class-name &lt;class&gt;</code></td>
<td style="text-align: left">对应的表名或对象名，该class类是包含在jar包中的。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--target-dir &lt;path&gt;</code></td>
<td style="text-align: left">合并后的数据在HDFS里的存放目录</td>
</tr>
</tbody></table>

<h1 id="3-参考文章">3. 参考文章</h1>

<ul>
<li><a href="http://www.zihou.me/html/2014/01/28/9114.html">Sqoop中文手册</a></li>
</ul>

                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">JavaChen</a><br/>
                        本文链接地址：<a href="/2014/08/04/import-data-to-hive-with-sqoop.html">http://blog.javachen.com/2014/08/04/import-data-to-hive-with-sqoop.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2014/08/04/import-data-to-hive-with-sqoop.html">Sqoop导入关系数据库到Hive</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#hadoop">hadoop</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#sqoop">sqoop</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2014/08/04/import-data-to-hive-with-sqoop.html" data-url="http://blog.javachen.com/2014/08/04/import-data-to-hive-with-sqoop.html" data-title="Sqoop导入关系数据库到Hive"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2015 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">JavaChen</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
                    <script type="text/javascript" src="http://tajs.qq.com/stats?sId=52032901" charset="UTF-8"></script>
            

            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2014/08/04/import-data-to-hive-with-sqoop.html'
      });
      </script>
  </body>
</html>
