<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>当前数据仓库建设过程 - JavaChen Blog</title>
      <meta name="author" content="JavaChen"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2014/10/23/hive-warehouse-in-2014.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />
        
        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2014/10/20/cdh5.2-release.html" title="CDH 5.2.0 的改变"><i class="fa fa-angle-double-left"></i>&nbsp;CDH 5.2.0 的改变</a></li>
                
                
                <li class="next"><a href="/2014/10/24/impala-query-table-tutorial.html" title="Impala查询功能测试">Impala查询功能测试&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> 当前数据仓库建设过程  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2014.10.23 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <p>一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。</p>

<h1 id="section">1. 数据采集和存储</h1>

<p>采集数据之前，先要定义数据如何存放在 hadoop 以及一些相关约束。约束如下：</p>

<ul>
  <li>所有的日志数据都存放在 hdfs 上的 <code class="highlighter-rouge">/logroot</code> 路径下面</li>
  <li>hive 中数据库命名方式为 <code class="highlighter-rouge">dw_XXXX</code>，例如：dw_srclog 存放外部来源的原始数据，dw_stat 存放统计结果的数据</li>
  <li>原始数据都加工成为结构化的文本文件，字段分隔符统一使用制表符，并在 lzo 压缩之后上传到 hdfs 中。</li>
  <li>hive 中使用外部表保存数据，数据存放在 <code class="highlighter-rouge">/logroot</code> 下，如果不是分区表，则文件名为表名；如果是分区表，则按月和天分区，每天分区下的文件名为<code class="highlighter-rouge">表名_日期</code>，例如：<code class="highlighter-rouge">test_20141023</code></li>
</ul>

<p>数据采集的来源可能是关系数据库或者一些系统日志，采集工具可以是日志采集系统，例如：flume、sqoop 、storm以及一些 ETL 工具等等。</p>

<p>目前，主要是从 mysql 中导出数据然后在导入到 hdfs 中，对于存储不需要按天分区的表，这部分过程代码如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="o">]</span>; <span class="k">then 
  </span><span class="nv">DAY</span><span class="o">=</span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span>
<span class="k">else 
  </span><span class="nv">DAY</span><span class="o">=</span><span class="s2">"yesterday"</span>
<span class="k">fi

</span><span class="nv">datestr</span><span class="o">=</span><span class="sb">`</span>date +%Y-%m-%d -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>;
<span class="nv">logday</span><span class="o">=</span><span class="sb">`</span>date +%Y%m%d -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>;
<span class="nv">logmonth</span><span class="o">=</span><span class="sb">`</span>date +%Y%m -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>

<span class="c">#hive table</span>
<span class="nv">table</span><span class="o">=</span><span class="nb">test</span>
<span class="c">#mysql db config file</span>
<span class="nv">srcdb</span><span class="o">=</span>db_name

<span class="nv">sql</span><span class="o">=</span><span class="s2">"select * from test"</span>

<span class="nv">hql</span><span class="o">=</span><span class="s2">"
use dw_srclog;
create external table if not exists test (
  id int,
  name int
)ROW FORMAT DELIMITED FIELDS TERMINATED BY '</span><span class="se">\t</span><span class="s2">'
STORED AS INPUTFORMAT
  'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  '/logroot/test';
"</span>

<span class="c">#begin</span>
chmod +x <span class="nv">$srcdb</span>.sql
. ./<span class="nv">$srcdb</span>.sql

<span class="nv">file</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">table</span><span class="k">}</span><span class="s2">"</span>
<span class="nv">sql_var</span><span class="o">=</span><span class="s2">" -r -quick --default-character-set=utf8  --skip-column"</span>

mysql <span class="nv">$sql_var</span> -h<span class="k">${</span><span class="nv">db_host</span><span class="k">}</span> -u<span class="k">${</span><span class="nv">db_user</span><span class="k">}</span> -p<span class="k">${</span><span class="nv">db_pass</span><span class="k">}</span> -P<span class="k">${</span><span class="nv">db_port</span><span class="k">}</span> -D<span class="k">${</span><span class="nv">db_name</span><span class="k">}</span> -e <span class="s2">"</span><span class="nv">$sql</span><span class="s2">"</span> | sed <span class="s2">"s/NULL/</span><span class="se">\\\\</span><span class="s2">N/g"</span>&gt; <span class="nv">$file</span> 2&gt;&amp;1

lzop -U <span class="nv">$file</span>
hadoop fs -mkdir -p /logroot/<span class="nv">$table</span>
hadoop fs -ls /logroot/<span class="nv">$table</span> |grep lzo|awk <span class="s1">'{print $8}'</span>|xargs -i hadoop fs -rm <span class="o">{}</span> 
hadoop fs -moveFromLocal <span class="nv">$file</span>.lzo /logroot/<span class="nv">$table</span>/
hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer  /logroot/<span class="nv">$table</span>/<span class="nv">$file</span>.lzo 2&gt;&amp;1

<span class="nb">echo</span> <span class="s2">"create table if not exists"</span>
hive -v -e <span class="s2">"</span><span class="nv">$hql</span><span class="s2">;"</span> 2&gt;&amp;1 
</code></pre>
</div>

<p>上面 bash 代码逻辑如下：</p>

<ul>
  <li>1、判断是否输入参数，如果没有参数，则取昨天，意思是每天读取 mysql 数据库中昨天的数据。</li>
  <li>2、定义 mysql 中 select 查询语句</li>
  <li>3、定义 hive 中建表语句</li>
  <li>4、读取 mysql 数据库连接信息，上例中为从 db_name.sql 中读取 <code class="highlighter-rouge">db_host</code>、<code class="highlighter-rouge">db_user</code>、<code class="highlighter-rouge">db_pass</code>、<code class="highlighter-rouge">db_port</code>、<code class="highlighter-rouge">db_name</code> 五个变量</li>
  <li>5、运行 mysql 命令导出指定 sql 查询的结果，并将结果中的 NULL 字段转换为 <code class="highlighter-rouge">\\N</code>，因为 <code class="highlighter-rouge">\</code> 在 bash 中是转义字符，故需要使用两个 <code class="highlighter-rouge">\</code></li>
  <li>6、lzo 压缩文件并上传到 hdfs，并且创建 lzo 索引</li>
  <li>7、最后删除本地文件</li>
</ul>

<p>对于分区表来说，建表语句如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">use</span> <span class="n">dw_srclog</span><span class="p">;</span>
<span class="k">create</span> <span class="k">external</span> <span class="k">table</span> <span class="n">if</span> <span class="k">not</span> <span class="k">exists</span> <span class="n">test_p</span> <span class="p">(</span>
  <span class="n">id</span> <span class="n">int</span><span class="p">,</span>
  <span class="n">name</span> <span class="n">int</span>
<span class="p">)</span>
<span class="n">partitioned</span> <span class="k">by</span> <span class="p">(</span><span class="n">key_ym</span> <span class="n">int</span><span class="p">,</span> <span class="n">key_ymd</span> <span class="n">int</span><span class="p">)</span>
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span> <span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span>
<span class="n">STORED</span> <span class="k">AS</span> <span class="n">INPUTFORMAT</span>
  <span class="s1">'com.hadoop.mapred.DeprecatedLzoTextInputFormat'</span>
<span class="n">OUTPUTFORMAT</span>
  <span class="s1">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span>
<span class="k">LOCATION</span>
  <span class="s1">'/logroot/test_p'</span><span class="p">;</span>
</code></pre>
</div>

<p>从 mysql 导出文件并上传到 hdfs 命令如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#begin</span>
chmod +x <span class="nv">$srcdb</span>.sql
. ./<span class="nv">$srcdb</span>.sql

<span class="nv">file</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">table</span><span class="k">}</span><span class="s2">_</span><span class="nv">$logday</span><span class="s2">"</span>
<span class="nv">sql_var</span><span class="o">=</span><span class="s2">" -r -quick --default-character-set=utf8  --skip-column"</span>

mysql <span class="nv">$sql_var</span> -h<span class="k">${</span><span class="nv">db_host</span><span class="k">}</span> -u<span class="k">${</span><span class="nv">db_user</span><span class="k">}</span> -p<span class="k">${</span><span class="nv">db_pass</span><span class="k">}</span> -P<span class="k">${</span><span class="nv">db_port</span><span class="k">}</span> -D<span class="k">${</span><span class="nv">db_name</span><span class="k">}</span> -e <span class="s2">"</span><span class="nv">$sql</span><span class="s2">"</span> | sed <span class="s2">"s/NULL/</span><span class="se">\\\\</span><span class="s2">N/g"</span>&gt; <span class="nv">$file</span> 2&gt;&amp;1

lzop -U <span class="nv">$file</span>
hadoop fs -mkdir -p /logroot/<span class="nv">$table</span>/key_ym<span class="o">=</span><span class="nv">$logmonth</span>/key_ymd<span class="o">=</span><span class="nv">$logday</span>
hadoop fs -ls /logroot/<span class="nv">$table</span>/key_ym<span class="o">=</span><span class="nv">$logmonth</span>/key_ymd<span class="o">=</span><span class="nv">$logday</span>/ |grep lzo|awk <span class="s1">'{print $8}'</span>|xargs -i hadoop fs -rm <span class="o">{}</span> 2&gt;&amp;1
hadoop fs -moveFromLocal <span class="nv">$file</span>.lzo /logroot/<span class="nv">$table</span>/key_ym<span class="o">=</span><span class="nv">$logmonth</span>/key_ymd<span class="o">=</span><span class="nv">$logday</span>/
hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer  /logroot/<span class="nv">$table</span>/key_ym<span class="o">=</span><span class="nv">$logmonth</span>/key_ymd<span class="o">=</span><span class="nv">$logday</span>/<span class="nv">$file</span>.lzo 2&gt;&amp;1

hive -v -e <span class="s2">"</span><span class="nv">$hql</span><span class="s2">;ALTER TABLE </span><span class="nv">$table</span><span class="s2"> ADD IF NOT EXISTS PARTITION(key_ym=</span><span class="nv">$logmonth</span><span class="s2">,key_ymd=</span><span class="nv">$logday</span><span class="s2">) location '/logroot/</span><span class="nv">$table</span><span class="s2">/key_ym=</span><span class="nv">$logmonth</span><span class="s2">/key_ymd=</span><span class="nv">$logday</span><span class="s2">' "</span> 2&gt;&amp;1
</code></pre>
</div>

<p>通过上面的两个命令就可以实现将 mysql 中的数据导入到 hdfs 中。</p>

<p>这里需要注意以下几点：</p>

<ul>
  <li>1、 hive 中原始日志使用默认的 textfile 方式存储，是为了保证日志的可读性，方便以后从 hdfs 下载来之后能够很方便的转换为结构化的文本文件并能浏览文件内容。</li>
  <li>2、使用 lzo 压缩是为了节省存储空间</li>
  <li>3、使用外包表建表，在删除表结构之后数据不会删，方便修改表结构和分区。</li>
</ul>

<h2 id="sqoop">使用 Sqoop</h2>

<p>使用 sqoop 主要是用于从 oracle 中通过 jdbc 方式导出数据到 hdfs，sqoop 命令如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sqoop import --connect jdbc:oracle:thin:@192.168.56.121:2154:db --username bi_user_limit --password <span class="s1">'XXXX'</span> --query <span class="s2">"select * from test where  </span><span class="se">\$</span><span class="s2">CONDITIONS"</span> --split-by id  -m 5 --fields-terminated-by <span class="s1">'\t'</span> --lines-terminated-by <span class="s1">'\n'</span>  --null-string <span class="s1">'\\N'</span> --null-non-string <span class="s1">'\\N'</span> --target-dir <span class="s2">"/logroot/test/key_ymd=20140315"</span>  --delete-target-dir
</code></pre>
</div>

<h1 id="section-1">2. 数据加工</h1>

<p>对于数据量比较小任务可以使用 impala 处理，对于数据量大的任务使用 hive hql 来处理。</p>

<p>impala 处理数据：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>impala-shell -i <span class="s1">'192.168.56.121:21000'</span> -r -q <span class="s2">"</span><span class="nv">$sql</span><span class="s2">;"</span> 
</code></pre>
</div>

<p>有时候需要使用 impala 导出数据：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>impala-shell -i '192.168.56.121:21000' -r -q "$sql;" -B --output_delimiter="\t" -o $file
sed -i '1d' $file  #导出的第一行有不可见的字符
</code></pre>
</div>

<p>使用 hive 处理数据生成结果表：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="o">]</span>; <span class="k">then 
  </span><span class="nv">DAY</span><span class="o">=</span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span>
<span class="k">else 
  </span><span class="nv">DAY</span><span class="o">=</span><span class="s2">"yesterday"</span>
<span class="k">fi

</span><span class="nb">echo</span> <span class="s2">"DAY=</span><span class="nv">$DAY</span><span class="s2">"</span>

<span class="nv">datestr</span><span class="o">=</span><span class="sb">`</span>date +%Y-%m-%d -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>;
<span class="nv">logday</span><span class="o">=</span><span class="sb">`</span>date +%Y%m%d -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>;
<span class="nv">logmonth</span><span class="o">=</span><span class="sb">`</span>date +%Y%m -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>

<span class="c">#target table</span>
<span class="nv">table</span><span class="o">=</span>stat_test_p
<span class="nv">sql</span><span class="o">=</span><span class="s2">"use dw_srclog;insert OVERWRITE table stat_test_p partition(key_ym=</span><span class="nv">$logmonth</span><span class="s2">,key_ymd=</span><span class="nv">$logday</span><span class="s2">)
select id,count(name) from test_p where key_ymd=</span><span class="nv">$logday</span><span class="s2"> group by id
"</span>

<span class="nv">hql</span><span class="o">=</span><span class="s2">"
use dw_web;
create external table if not exists goods_sales_info_day (
  id int,
  count int
) partitioned by (key_ym int, key_ymd int)
STORED AS RCFILE
LOCATION '/logroot/stat_test_p';
"</span>
<span class="c">#begin</span>
hive -v -e <span class="s2">"
</span><span class="nv">$hql</span><span class="s2">;
SET hive.exec.compress.output=true;
SET mapreduce.input.fileinputformat.split.maxsize=128000000;
SET mapred.output.compression.type=BLOCK;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
</span><span class="nv">$sql</span><span class="s2">"</span> 2&gt;&amp;1 
</code></pre>
</div>

<p>这里主要是先判断是否创建外包表（外包表存储为 RCFILE 格式），然后设置 map 的输出结果使用 snappy 压缩，并设置每个 map 的大小，最后运行 insert 语句。结果表存储为 RCFILE 的原因是，<em>在 CDH 5.2 之前，该格式的表可以被 impala 读取</em>。</p>

<h2 id="section-2">任务调度</h2>

<p>当任务多了之后，每个任务之间会有一些依赖，为了保证任务的先后执行顺序，这里使用的是 azkaban 任务调度框架。</p>

<p>该框架的使用方式很简单：</p>

<ul>
  <li>首先创建一个 bi_etl 目录，用于存放执行脚本。</li>
  <li>在 bi_etl 目录下创建一个 properties 文件，文件名称任意，文件内容为：<code class="highlighter-rouge">DAY=yesterday</code>，这是一个系统默认参数，即默认 DAY 变量的值为 yesterday，该变量在运行时可以被覆盖：在 azkaban 的 web 管理界面，运行一个 Flow 时，添加一个 <code class="highlighter-rouge">Flow Parameters</code> 参数，Name 为 DAY，Value 为你想要指定的值，例如：20141023。</li>
  <li>创建一个 bash 脚本 test.sh，文件内容如第一章节内容，需要注意的是该脚本中会判断是否有输出参数。</li>
  <li>针对 bash 脚本，创建 azkaban 需要的 job 文件，文件内容如下（azkaban 运行该 job 时候，会替换 <code class="highlighter-rouge">${DAY}</code> 变量为实际的值 ）：</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="py">type</span><span class="p">=</span><span class="s">command</span>
<span class="py">command</span><span class="p">=</span><span class="s">sh test.sh ${DAY}</span>
<span class="py">failure.emails</span><span class="p">=</span><span class="s">XXX@163.com</span>
<span class="py">dependencies</span><span class="p">=</span><span class="s">xxx</span>
</code></pre>
</div>

<ul>
  <li>最后，将 bi_etl 目录打包成 zip 文件，然后上传到 azkaban 管理界面上去，就可以运行或者是设置调度任务了。</li>
</ul>

<p>使用上面的方式编写 bash 脚本和 azkaban 的 job 的好处是：</p>

<ul>
  <li>azkaban 的 job 可以指定参数来控制运行哪一天的任务</li>
  <li>job 中实际上运行的是 bash 脚本，这些脚本脱离了 azkaban 也能正常运行，同样也支持传参数。</li>
</ul>

<h1 id="section-3">3. 数据展现</h1>

<p>目前是将 hive 或者 impala 的处理结果推送到关系数据库中，由传统的 BI 报表工具展示数据或者直接通过 impala 查询数据生成报表并发送邮件。</p>

<p>为了保证报表的正常发送，需要监控任务的正常运行，当任务失败的时候能够发送邮件，这部分通过 azkaban 可以做到。另外，还需要监控每天运行的任务同步的记录数，下面脚本是统计记录数为0的任务：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="o">]</span>; <span class="k">then
  </span><span class="nv">DAY</span><span class="o">=</span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span>
<span class="k">else
  </span><span class="nv">DAY</span><span class="o">=</span><span class="s2">"yesterday"</span>
<span class="k">fi

</span><span class="nb">echo</span> <span class="s2">"DAY=</span><span class="nv">$DAY</span><span class="s2">"</span>

<span class="nv">datestr</span><span class="o">=</span><span class="sb">`</span>date +%Y-%m-%d -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>;
<span class="nv">logday</span><span class="o">=</span><span class="sb">`</span>date +%Y%m%d -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>;
<span class="nv">logmonth</span><span class="o">=</span><span class="sb">`</span>date +%Y%m -d<span class="s2">"</span><span class="nv">$DAY</span><span class="s2">"</span><span class="sb">`</span>
<span class="nv">datemod</span><span class="o">=</span><span class="sb">`</span>date +%w -d <span class="s2">"yesterday"</span><span class="sb">`</span>

rm -rf /tmp/stat_table_day_count_<span class="nv">$logday</span>
touch /tmp/stat_table_day_count_<span class="nv">$logday</span>
<span class="k">for </span>db <span class="k">in</span> <span class="sb">`</span>hadoop fs -ls /user/hive/warehouse|grep -vE <span class="s1">'testdb|dw_etl'</span>|grep <span class="s1">'.db'</span>|awk <span class="s1">'{print $8}'</span>|awk -F <span class="s1">'/'</span> <span class="s1">'{print $5}'</span> |awk -F <span class="s1">'.'</span> <span class="s1">'{print $1}'</span><span class="sb">`</span>;<span class="k">do
    for </span>table <span class="k">in</span> <span class="sb">`</span>hive -S -e <span class="s2">"set hive.cli.print.header=false; use </span><span class="nv">$db</span><span class="s2">;show tables"</span> <span class="sb">`</span> ;<span class="k">do
        </span><span class="nv">count_new</span><span class="o">=</span><span class="s2">""</span>
        <span class="nv">result</span><span class="o">=</span><span class="sb">`</span>hive -S -e <span class="s2">"set hive.cli.print.header=false; use </span><span class="nv">$db</span><span class="s2">;show create table </span><span class="nv">$table</span><span class="s2">;"</span>  2&gt;&amp;1 | grep PARTITIONED<span class="sb">`</span>
        <span class="k">if</span> <span class="o">[</span> <span class="k">${#</span><span class="nv">result</span><span class="k">}</span> -gt 0 <span class="o">]</span>;<span class="k">then
      </span><span class="nv">is_part</span><span class="o">=</span>1
      <span class="nv">count_new</span><span class="o">=</span><span class="sb">`</span>impala-shell -k -i 10.168.35.127:21089 --quiet -B --output_delimiter<span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span> -q <span class="s2">"select count(1) from </span><span class="k">${</span><span class="nv">db</span><span class="k">}</span><span class="s2">.</span><span class="nv">$table</span><span class="s2"> where key_ymd=</span><span class="nv">$logday</span><span class="s2"> "</span><span class="sb">`</span>
        <span class="k">else
      </span><span class="nv">is_part</span><span class="o">=</span>0
      <span class="nv">count_new</span><span class="o">=</span><span class="sb">`</span>impala-shell -k -i 10.168.35.127:21089 --quiet -B --output_delimiter<span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span> -q <span class="s2">"select count(1) from </span><span class="k">${</span><span class="nv">db</span><span class="k">}</span><span class="s2">.</span><span class="nv">$table</span><span class="s2">; "</span><span class="sb">`</span>
        <span class="k">fi
        </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$db</span><span class="s2">,</span><span class="nv">$table</span><span class="s2">,</span><span class="nv">$is_part</span><span class="s2">,</span><span class="nv">$count_new</span><span class="s2">"</span> &gt;&gt; /tmp/stat_table_day_count_<span class="nv">$logday</span>
    <span class="k">done
done</span>

<span class="c">#mail -s "The count of the table between old and new cluster in $datestr" -c $mails &lt; /tmp/stat_table_day_count_$logday</span>

sed -i <span class="s1">'s/1034h//g'</span> /tmp/stat_table_day_count_<span class="nv">$logday</span>
sed -i <span class="s1">'s/\[//g'</span> /tmp/stat_table_day_count_<span class="nv">$logday</span>
sed -i <span class="s1">'s/\?//g'</span> /tmp/stat_table_day_count_<span class="nv">$logday</span>
sed -i <span class="s1">'s/\x1B//g'</span> /tmp/stat_table_day_count_<span class="nv">$logday</span>

<span class="nv">res</span><span class="o">=</span><span class="sb">`</span>cat /tmp/stat_table_day_count_<span class="nv">$logday</span>|grep -E <span class="s1">'1,0|0,0'</span>|grep -v stat_table_day_count<span class="sb">`</span>

<span class="nb">echo</span> <span class="nv">$res</span>

hive -e <span class="s2">"use dw_default;
LOAD DATA LOCAL INPATH '/tmp/stat_table_day_count_</span><span class="nv">$logday</span><span class="s2">' overwrite INTO TABLE stat_table_day_count  PARTITION (key_ym=</span><span class="nv">$logmonth</span><span class="s2">,key_ymd=</span><span class="nv">$logday</span><span class="s2">)
"</span>
python mail.py <span class="s2">"Count is 0 in </span><span class="nv">$datestr</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$res</span><span class="s2">"</span>
</code></pre>
</div>

<h1 id="section-4">4. 总结</h1>

<p>上面介绍了数据采集、加工和任务调度的过程，有些地方还可以改进：</p>

<ul>
  <li>引入 ETL 工具实现关系数据库导入到 hadoop，例如：Kettle 工具</li>
  <li>目前是每天一次从 mysql 同步数据到 hadoop，以后需要修改同步频率，做到更实时</li>
  <li>hive 和 impala 在字段类型、存储方式、函数的兼容性上存在一些问题</li>
</ul>


                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">JavaChen</a><br/>
                        本文链接地址：<a href="/2014/10/23/hive-warehouse-in-2014.html">http://blog.javachen.com/2014/10/23/hive-warehouse-in-2014.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2014/10/23/hive-warehouse-in-2014.html">当前数据仓库建设过程</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#hadoop">hadoop</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#hadoop">hadoop</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#hive">hive</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2014/10/23/hive-warehouse-in-2014.html" data-url="http://blog.javachen.com/2014/10/23/hive-warehouse-in-2014.html" data-title="当前数据仓库建设过程"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2016 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="JavaChen Blog
">JavaChen</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
                    <script type="text/javascript" src="http://tajs.qq.com/stats?sId=52032901" charset="UTF-8"></script>
            

            
            <script>
              var _hmt = _hmt || [];
              (function() {
                var hm = document.createElement("script");
                hm.src = "//hm.baidu.com/hm.js?50bc6f5d9b045b5895ff44f8bbdbc611";
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(hm, s);
              })();
            </script>
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2014/10/23/hive-warehouse-in-2014.html'
      });
      </script>
  </body>
</html>
