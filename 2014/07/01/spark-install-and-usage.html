<!DOCTYPE html>
<html lang="zh-cn">
        <head>
      <meta charset="utf-8"/>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
      <title>Spark集群安装和使用 - JavaChen Blog</title>
      <meta name="author" content="Junez"/>
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
      <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
      <link rel="canonical" href="http://blog.javachen.com/2014/07/01/spark-install-and-usage.html" />

      <link rel="stylesheet" href="/static/contrib/bootstrap/css/bootstrap.min.css" media="all" />
      <link rel="stylesheet" href="/static/css/style.css" media="all" />
      <link rel="stylesheet" href="/static/css/pygments.css" media="all" />
      <link rel="stylesheet" href="/static/contrib/font-awesome/css/font-awesome.min.css" media="all" />
      <link rel="stylesheet" type="text/css" href="/static/contrib/showup/showup.css" />

      <!-- atom & rss feed -->
      <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="JavaChen Blog RSS Feed" />
      <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="JavaChen Blog ATOM Feed" />

        <!-- fav and touch icons  -->
        <!-- Update these with your own images
        <link rel="shortcut icon" href="images/favicon.ico">
        <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
        <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
        -->

      <meta name="renderer" content="webkit|ie-stand">
      <meta name="baidu-site-verification" content="3HAhaWRiyR" />
      <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
      <meta name="sogou_site_verification" content="ofwXWFdthV"/>
      <meta property="wb:webmaster" content="b6081b2b8ab84c60" />
    </head>

    <body>
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">JavaChen Blog</a>
        </div>
        <div class="navbar-collapse collapse">
            <form id="search-form" class="form-group navbar-form navbar-right" role="search">
                  <div class="form-group">
                    <input type="text" name="q" value=""  id="query" class="form-control" placeholder="搜索" required autocomplete="off" ></input>
                    <input type="submit" class="btn btn-default" value=" Go" ></input>
                  </div>
              </form>
            <ul class="nav navbar-nav">
              <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
              <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
              <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
              <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
              
              <li><a href="https://github.com/javachen" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
              
              
              
              <li><a href="https://twitter.com/junezchen" target="_blank" title="twitter"><span class="fa fa-twitter fa-2x"></span></a></li>
              
              
              
              <li><a href="http://weibo.com/chenzhijun" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
              
              <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
            </ul>
        </div>

        </div><!--/.nav-collapse -->
      </div>
</div>

      <div id="wrap">
          <div class="container">
                 <div id="content">
          <ul class="pager hidden-print">
               
                <li class="previous"><a href="/2014/06/26/some-tips-about-hbase.html" title="HBase中的一些注意事项"><i class="fa fa-angle-double-left"></i>&nbsp;HBase中的一些注意事项</a></li>
                
                
                <li class="next"><a href="/2014/07/17/manual-install-cdh-hadoop.html" title="手动安装Hadoop集群的过程">手动安装Hadoop集群的过程&nbsp;<i class="fa fa-angle-double-right"></i></a></li>
                
          </ul>

           <div id="post" class="clearfix">
              <div id="post-title" class="page-header text-center">
                  <h1> Spark集群安装和使用  </h1>
              </div>
              <p class="text-muted clearfix">
                  <span class="pull-right">2014.07.01 | <a href="#comments">Comments</a></span>
              </p>
              <div id="qr" class="qrcode visible-lg"></div>

              <div id="post-text">
                    <p>本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。</p>

<p>安装环境如下：</p>

<ul>
  <li>操作系统：CentOs 6.5</li>
  <li>Hadoop 版本：<code class="highlighter-rouge">cdh-5.4.0</code></li>
  <li>Spark 版本：<code class="highlighter-rouge">cdh5-1.3.0_5.4.0</code></li>
</ul>

<p>关于 yum 源的配置以及 Hadoop 集群的安装，请参考 <a href="/2013/04/06/install-cloudera-cdh-by-yum.html">使用yum安装CDH Hadoop集群</a>。</p>

<h1 id="section">1. 安装</h1>

<p>首先查看 Spark 相关的包有哪些：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>yum list |grep spark
spark-core.noarch                  1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
spark-history-server.noarch        1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
spark-master.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
spark-python.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
spark-worker.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6
hue-spark.x86_64                   3.7.0+cdh5.4.0+1145-1.cdh5.4.0.p0.58.el6
</code></pre>
</div>

<p>以上包作用如下：</p>

<ul>
  <li><code class="highlighter-rouge">spark-core</code>: spark 核心功能</li>
  <li><code class="highlighter-rouge">spark-worker</code>: spark-worker 初始化脚本</li>
  <li><code class="highlighter-rouge">spark-master</code>: spark-master 初始化脚本</li>
  <li><code class="highlighter-rouge">spark-python</code>: spark 的 Python 客户端</li>
  <li><code class="highlighter-rouge">hue-spark</code>: spark 和 hue 集成包</li>
  <li><code class="highlighter-rouge">spark-history-server</code></li>
</ul>

<p>在已经存在的 Hadoop 集群中，选择一个节点来安装 Spark Master，其余节点安装 Spark worker ，例如：在 cdh1 上安装 master，在 cdh1、cdh2、cdh3 上安装 worker：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># 在 cdh1 节点上运行</span>
<span class="gp">$ </span>sudo yum install spark-core spark-master spark-worker spark-python spark-history-server -y

<span class="c"># 在 cdh1、cdh2、cdh3 上运行</span>
<span class="gp">$ </span>sudo yum install spark-core spark-worker spark-python -y
</code></pre>
</div>

<p>安装成功后，我的集群各节点部署如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cdh1节点:  spark-master、spark-worker、spark-history-server
cdh2节点:  spark-worker 
cdh3节点:  spark-worker 
</code></pre>
</div>

<h1 id="section-1">2. 配置</h1>

<h2 id="section-2">2.1 修改配置文件</h2>

<p>设置环境变量，在 <code class="highlighter-rouge">.bashrc</code> 或者 <code class="highlighter-rouge">/etc/profile</code> 中加入下面一行，并使其生效：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="err">export</span> <span class="py">SPARK_HOME</span><span class="p">=</span><span class="s">/usr/lib/spark</span>
</code></pre>
</div>

<p>可以修改配置文件 <code class="highlighter-rouge">/etc/spark/conf/spark-env.sh</code>，其内容如下，你可以根据需要做一些修改，例如，修改 master 的主机名称为cdh1。</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># 设置 master 主机名称</span>
<span class="nb">export </span><span class="nv">STANDALONE_SPARK_MASTER_HOST</span><span class="o">=</span>cdh1
</code></pre>
</div>

<p>设置 shuffle 和 RDD 数据存储路径，该值默认为<code class="highlighter-rouge">/tmp</code>。使用默认值，可能会出现<code class="highlighter-rouge">No space left on device</code>的异常，建议修改为空间较大的分区中的一个目录。</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_LOCAL_DIRS</span><span class="o">=</span>/data/spark
</code></pre>
</div>

<p>如果你和我一样使用的是虚拟机运行 spark，则你可能需要修改 spark 进程使用的 jvm 大小（关于 jvm 大小设置的相关逻辑见 <code class="highlighter-rouge">/usr/lib/spark/bin/spark-class</code>）：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_DAEMON_MEMORY</span><span class="o">=</span>256m
</code></pre>
</div>

<p>更多spark相关的配置参数，请参考 <a href="https://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a>。</p>

<h2 id="spark-history-server">2.2 配置 Spark History Server</h2>

<p>在运行Spark应用程序的时候，driver会提供一个webUI给出应用程序的运行信息，但是该webUI随着应用程序的完成而关闭端口，也就是说，Spark应用程序运行完后，将无法查看应用程序的历史记录。Spark history server就是为了应对这种情况而产生的，通过配置，Spark应用程序在运行完应用程序之后，将应用程序的运行信息写入指定目录，而Spark history server可以将这些运行信息装载并以web的方式供用户浏览。</p>

<p>创建 <code class="highlighter-rouge">/etc/spark/conf/spark-defaults.conf</code>：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cp /etc/spark/conf/spark-defaults.conf.template /etc/spark/conf/spark-defaults.conf
</code></pre>
</div>

<p>添加下面配置：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="py">spark.master</span><span class="p">=</span><span class="s">spark://cdh1:7077</span>
<span class="py">spark.eventLog.dir</span><span class="p">=</span><span class="s">/user/spark/applicationHistory</span>
<span class="py">spark.eventLog.enabled</span><span class="p">=</span><span class="s">true</span>
<span class="py">spark.yarn.historyServer.address</span><span class="p">=</span><span class="s">cdh1:18082</span>
</code></pre>
</div>

<p>如果你是在hdfs上运行Spark，则执行下面命令创建<code class="highlighter-rouge">/user/spark/applicationHistory</code>目录：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo -u hdfs hadoop fs -mkdir /user/spark
<span class="gp">$ </span>sudo -u hdfs hadoop fs -mkdir /user/spark/applicationHistory
<span class="gp">$ </span>sudo -u hdfs hadoop fs -chown -R spark:spark /user/spark
<span class="gp">$ </span>sudo -u hdfs hadoop fs -chmod 1777 /user/spark/applicationHistory
</code></pre>
</div>

<p>设置 <code class="highlighter-rouge">spark.history.fs.logDirectory</code> 参数：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_HISTORY_OPTS</span><span class="o">=</span><span class="s2">"</span><span class="nv">$SPARK_HISTORY_OPTS</span><span class="s2"> -Dspark.history.fs.logDirectory=/tmp/spark -Dspark.history.ui.port=18082"</span>
</code></pre>
</div>

<p>创建 /tmp/spark 目录：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>mkdir -p /tmp/spark
<span class="gp">$ </span>chown spark:spark /tmp/spark
</code></pre>
</div>

<p>如果集群配置了 kerberos ，则添加下面配置：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nv">HOSTNAME</span><span class="o">=</span><span class="sb">`</span>hostname -f<span class="sb">`</span>
<span class="nb">export </span><span class="nv">SPARK_HISTORY_OPTS</span><span class="o">=</span><span class="s2">"</span><span class="nv">$SPARK_HISTORY_OPTS</span><span class="s2"> -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=spark/</span><span class="k">${</span><span class="nv">HOSTNAME</span><span class="k">}</span><span class="s2">@LASHOU.COM -Dspark.history.kerberos.keytab=/etc/spark/conf/spark.keytab -Dspark.history.ui.acls.enable=true"</span>
</code></pre>
</div>

<h2 id="hive">2.3 和Hive集成</h2>

<p>Spark和hive集成，最好是将hive的配置文件链接到Spark的配置文件目录：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml
</code></pre>
</div>

<h2 id="section-3">2.4 同步配置文件</h2>

<p>修改完 cdh1 节点上的配置文件之后，需要同步到其他节点：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>scp -r /etc/spark/conf  cdh2:/etc/spark
scp -r /etc/spark/conf  cdh3:/etc/spark
</code></pre>
</div>

<h1 id="section-4">3. 启动和停止</h1>

<h2 id="section-5">3.1 使用系统服务管理集群</h2>

<p>启动脚本：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># 在 cdh1 节点上运行</span>
<span class="gp">$ </span>sudo service spark-master start

<span class="c"># 在 cdh1 节点上运行，如果 hadoop 集群配置了 kerberos，则运行之前需要先获取 spark 用户的凭证</span>
<span class="c"># kinit -k -t /etc/spark/conf/spark.keytab spark/cdh1@JAVACHEN.COM</span>
<span class="gp">$ </span>sudo service spark-history-server start

<span class="c"># 在cdh2、cdh3 节点上运行</span>
<span class="gp">$ </span>sudo service spark-worker start
</code></pre>
</div>

<p>停止脚本：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo service spark-master stop
<span class="gp">$ </span>sudo service spark-worker stop
<span class="gp">$ </span>sudo service spark-history-server stop
</code></pre>
</div>

<p>当然，你还可以设置开机启动：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo chkconfig spark-master on
<span class="gp">$ </span>sudo chkconfig spark-worker on
<span class="gp">$ </span>sudo chkconfig spark-history-server on
</code></pre>
</div>

<h2 id="spark-">3.2 使用 Spark 自带脚本管理集群</h2>

<p>另外，你也可以使用 Spark 自带的脚本来启动和停止，这些脚本在 <code class="highlighter-rouge">/usr/lib/spark/sbin</code> 目录下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ls /usr/lib/spark/sbin
slaves.sh        spark-daemons.sh  start-master.sh  stop-all.sh
spark-config.sh  spark-executor    start-slave.sh   stop-master.sh
spark-daemon.sh  start-all.sh      start-slaves.sh  stop-slaves.sh
</code></pre>
</div>

<p>在master节点修改 <code class="highlighter-rouge">/etc/spark/conf/slaves</code> 文件添加worker节点的主机名称，并且还需要在master和worker节点之间配置无密码登陆。</p>

<div class="highlighter-rouge"><pre class="highlight"><code># A Spark Worker will be started on each of the machines listed below.
cdh2
cdh3
</code></pre>
</div>

<p>然后，你也可以通过下面脚本启动 master 和 worker：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">cd</span> /usr/lib/spark/sbin
<span class="gp">$ </span>./start-master.sh
<span class="gp">$ </span>./start-slaves.sh
</code></pre>
</div>

<p>当然，你也可以通过<code class="highlighter-rouge">spark-class</code>脚本来启动，例如，下面脚本以standalone模式启动worker：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>./bin/spark-class org.apache.spark.deploy.worker.Worker spark://cdh1:18080
</code></pre>
</div>

<h2 id="web">3.3 访问web界面</h2>

<p>你可以通过 <a href="http://cdh1:18080/">http://cdh1:18080/</a> 访问 spark master 的 web 界面。</p>

<p><img src="http://javachen-rs.qiniudn.com/images/spark/spark-master-web-ui.jpg" alt="spark-master-web-ui" /></p>

<p>访问Spark History Server页面：http://cdh1:18082/。</p>

<p><img src="http://javachen-rs.qiniudn.com/images/spark/spark-hs-web-ui.jpg" alt="spark-hs-web-ui" /></p>

<p>注意：我这里使用的是CDH版本的 Spark，Spark master UI的端口为<code class="highlighter-rouge">18080</code>，不是 Apache Spark 的 <code class="highlighter-rouge">8080</code> 端口。CDH发行版中Spark使用的端口列表如下：</p>

<ul>
  <li><code class="highlighter-rouge">7077</code> – Default Master RPC port</li>
  <li><code class="highlighter-rouge">7078</code> – Default Worker RPC port</li>
  <li><code class="highlighter-rouge">18080</code> – Default Master web UI port</li>
  <li><code class="highlighter-rouge">18081</code> – Default Worker web UI port</li>
  <li><code class="highlighter-rouge">18080</code> – Default HistoryServer web UI port</li>
</ul>

<h1 id="section-6">4. 测试</h1>

<p>Spark可以以<a href="/2015/03/30/spark-test-in-local-mode.html">本地模式运行</a>，也支持三种集群管理模式：</p>

<ul>
  <li><a href="https://spark.apache.org/docs/latest/spark-standalone.html">Standalone</a>  – Spark原生的资源管理，由Master负责资源的分配。</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-mesos.html">Apache Mesos</a>  – 运行在Mesos之上，由Mesos进行资源调度</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-yarn.html">Hadoop YARN</a> –  运行在Yarn之上，由Yarn进行资源调度。</li>
</ul>

<p>另外 Spark 的 <a href="https://spark.apache.org/docs/latest/ec2-scripts.html">EC2 launch scripts</a> 可以帮助你容易地在Amazon EC2上启动standalone cluster.</p>

<blockquote>
  <ul>
    <li>在集群不是特别大，并且没有 mapReduce 和 Spark 同时运行的需求的情况下，用 Standalone 模式效率最高。</li>
    <li>Spark可以在应用间（通过集群管理器）和应用中（如果一个 SparkContext 中有多项计算任务）进行资源调度。</li>
  </ul>
</blockquote>

<h2 id="standalone-">4.1 Standalone 模式</h2>

<p>该模式中，资源调度是Spark框架自己实现的，其节点类型分为Master和Worker节点，其中Driver节点运行在Master节点中，并且有常驻内存的Master进程守护，Worker节点上常驻Worker守护进程，负责与Master通信。</p>

<p>Standalone 模式是Master-Slaves架构的集群模式，Master存在着单点故障问题，目前，Spark提供了两种解决办法：基于文件系统的故障恢复模式，基于Zookeeper的HA方式。</p>

<p>Standalone 模式需要在每一个节点部署Spark应用，并按照实际情况配置故障恢复模式。</p>

<p>你可以使用交互式命令spark-shell、pyspark或者<a href="https://spark.apache.org/docs/latest/submitting-applications.html">spark-submit script</a>连接到集群，下面以wordcount程序为例：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>spark-shell --master spark://cdh1:7077
<span class="gp">scala&gt; </span>val file <span class="o">=</span> sc.textFile<span class="o">(</span><span class="s2">"hdfs://cdh1:8020/tmp/test.txt"</span><span class="o">)</span>
<span class="gp">scala&gt; </span>val counts <span class="o">=</span> file.flatMap<span class="o">(</span>line <span class="o">=</span>&gt; line.split<span class="o">(</span><span class="s2">" "</span><span class="o">))</span>.map<span class="o">(</span>word <span class="o">=</span>&gt; <span class="o">(</span>word, 1<span class="o">))</span>.reduceByKey<span class="o">(</span>_ + _<span class="o">)</span>
<span class="gp">scala&gt; </span>counts.count<span class="o">()</span>
<span class="gp">scala&gt; </span>counts.saveAsTextFile<span class="o">(</span><span class="s2">"hdfs://cdh1:8020/tmp/output"</span><span class="o">)</span>
</code></pre>
</div>

<p>如果运行成功，可以打开浏览器访问 http://cdh1:4040 查看应用运行情况。</p>

<p>运行过程中，可能会出现下面的异常：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>14/10/24 14:51:40 WARN hdfs.BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
14/10/24 14:51:40 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1738)
	at java.lang.Runtime.loadLibrary0(Runtime.java:823)
	at java.lang.System.loadLibrary(System.java:1028)
	at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:32)
	at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:71)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:249)
	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1836)
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1801)
	at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)
</code></pre>
</div>

<p>解决方法可以参考 <a href="http://blog.csdn.net/pelick/article/details/11599391">Spark连接Hadoop读取HDFS问题小结</a> 这篇文章，执行以下命令，然后重启服务即可：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cp /usr/lib/hadoop/lib/native/libgplcompression.so <span class="nv">$JAVA_HOME</span>/jre/lib/amd64/
cp /usr/lib/hadoop/lib/native/libhadoop.so <span class="nv">$JAVA_HOME</span>/jre/lib/amd64/
cp /usr/lib/hadoop/lib/native/libsnappy.so <span class="nv">$JAVA_HOME</span>/jre/lib/amd64/
</code></pre>
</div>

<p>使用 spark-submit 以 Standalone 模式运行 SparkPi 程序的命令如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>spark-submit --class org.apache.spark.examples.SparkPi  --master spark://cdh1:7077 /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar 10
</code></pre>
</div>

<p><strong>需要说明的是</strong>：<code class="highlighter-rouge">Standalone mode does not support talking to a kerberized HDFS</code>，如果你以 <code class="highlighter-rouge">spark-shell --master spark://cdh1:7077</code> 方式访问安装有 kerberos 的 HDFS 集群上访问数据时，会出现下面异常:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>15/04/02 11:58:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, bj03-bi-pro-hdpnamenn): java.io.IOException: Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]; Host Details : local host is: "cdh1/192.168.56.121"; destination host is: "192.168.56.121":8020;
        org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        org.apache.hadoop.ipc.Client.call(Client.java:1415)
        org.apache.hadoop.ipc.Client.call(Client.java:1364)
        org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        com.sun.proxy.$Proxy17.getBlockLocations(Unknown Source)
</code></pre>
</div>

<h2 id="spark-on-mesos-">4.2 Spark On Mesos 模式</h2>

<p>参考 <a href="http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/">http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/</a>。</p>

<h2 id="spark-on-yarn-">4.3 Spark on Yarn 模式</h2>

<p>Spark on Yarn 模式同样也支持两种在 Yarn 上启动 Spark 的方式，一种是 cluster 模式，Spark driver 在 Yarn 的 application master 进程中运行，客户端在应用初始化完成之后就会退出；一种是 client 模式，Spark driver 运行在客户端进程中。Spark on Yarn 模式是可以访问配置有 kerberos 的 HDFS 文件的。</p>

<p>CDH Spark中，以 cluster 模式启动，命令如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>spark-submit --class path.to.your.Class --deploy-mode cluster --master yarn <span class="o">[</span>options] &lt;app jar&gt; <span class="o">[</span>app options]
</code></pre>
</div>

<p>CDH Spark中，以 client 模式启动，命令如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>spark-submit --class path.to.your.Class --deploy-mode client --master yarn <span class="o">[</span>options] &lt;app jar&gt; <span class="o">[</span>app options]
</code></pre>
</div>

<p>以SparkPi程序为例：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>spark-submit --class org.apache.spark.examples.SparkPi <span class="se">\</span>
    --deploy-mode cluster  <span class="se">\</span>
    --master yarn  <span class="se">\</span>
    --num-executors 3 <span class="se">\</span>
    --driver-memory 4g <span class="se">\</span>
    --executor-memory 2g <span class="se">\</span>
    --executor-cores 1 <span class="se">\</span>
    --queue thequeue <span class="se">\</span>
    /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar <span class="se">\</span>
    10
</code></pre>
</div>

<p>另外，运行在 YARN 集群之上的时候，可以手动把 spark-assembly 相关的 jar 包拷贝到 hdfs 上去，然后设置 <code class="highlighter-rouge">SPARK_JAR</code> 环境变量：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>hdfs dfs -mkdir -p /user/spark/share/lib
<span class="gp">$ </span>hdfs dfs -put <span class="nv">$SPARK_HOME</span>/lib/spark-assembly.jar  /user/spark/share/lib/spark-assembly.jar

<span class="gp">$ </span><span class="nv">SPARK_JAR</span><span class="o">=</span>hdfs://&lt;nn&gt;:&lt;port&gt;/user/spark/share/lib/spark-assembly.jar
</code></pre>
</div>

<h1 id="spark-sql">5. Spark-SQL</h1>

<p>Spark 安装包中包括了 Spark-SQL ，运行 spark-sql 命令，在 cdh5.2 中会出现下面异常：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">cd</span> /usr/lib/spark/bin
<span class="gp">$ </span>./spark-sql
java.lang.ClassNotFoundException: org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver
	at java.net.URLClassLoader<span class="nv">$1</span>.run<span class="o">(</span>URLClassLoader.java:202<span class="o">)</span>
	at java.security.AccessController.doPrivileged<span class="o">(</span>Native Method<span class="o">)</span>
	at java.net.URLClassLoader.findClass<span class="o">(</span>URLClassLoader.java:190<span class="o">)</span>
	at java.lang.ClassLoader.loadClass<span class="o">(</span>ClassLoader.java:306<span class="o">)</span>
	at java.lang.ClassLoader.loadClass<span class="o">(</span>ClassLoader.java:247<span class="o">)</span>
	at java.lang.Class.forName0<span class="o">(</span>Native Method<span class="o">)</span>
	at java.lang.Class.forName<span class="o">(</span>Class.java:247<span class="o">)</span>
	at org.apache.spark.deploy.SparkSubmit<span class="nv">$.</span>launch<span class="o">(</span>SparkSubmit.scala:319<span class="o">)</span>
	at org.apache.spark.deploy.SparkSubmit<span class="nv">$.</span>main<span class="o">(</span>SparkSubmit.scala:75<span class="o">)</span>
	at org.apache.spark.deploy.SparkSubmit.main<span class="o">(</span>SparkSubmit.scala<span class="o">)</span>

Failed to load Spark SQL CLI main class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.
You need to build Spark with -Phive.
</code></pre>
</div>

<p>在 cdh5.4 中会出现下面异常：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.cli.CliDriver
  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
  ... 18 more
 ```
  
从上可以知道  Spark-SQL 编译时没有集成 Hive，故需要重新编译 spark 源代码。

## 编译 Spark-SQL

以下内容参考 [编译Spark源代码](/2015/04/28/compile-cdh-spark-source-code.html)。

下载cdh5-1.3.0_5.4.0分支的代码：

```bash
$ git clone git@github.com:cloudera/spark.git
$ cd spark
$ git checkout -b origin/cdh5-1.3.0_5.4.0
</code></pre>
</div>

<p>使用maven 编译，先修改根目录下的 pom.xml，添加一行 <code class="highlighter-rouge">&lt;module&gt;sql/hive-thriftserver&lt;/module&gt;</code>：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nt">&lt;modules&gt;</span>
    <span class="nt">&lt;module&gt;</span>core<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>bagel<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>graphx<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>mllib<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>tools<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>streaming<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>sql/catalyst<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>sql/core<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>sql/hive<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>sql/hive-thriftserver<span class="nt">&lt;/module&gt;</span> <span class="c">&lt;!--添加的一行--&gt;</span>
    <span class="nt">&lt;module&gt;</span>repl<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>assembly<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/twitter<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/kafka<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/flume<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/flume-sink<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/zeromq<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/mqtt<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>examples<span class="nt">&lt;/module&gt;</span>
  <span class="nt">&lt;/modules&gt;</span>
</code></pre>
</div>

<p>然后运行：</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">export </span><span class="nv">MAVEN_OPTS</span><span class="o">=</span><span class="s2">"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"</span>
<span class="gp">$ </span>mvn -Pyarn -Dhadoop.version<span class="o">=</span>2.6.0-cdh5.4.0 -Phive -Phive-thriftserver -DskipTests clean package
</code></pre>
</div>

<p>如果编译成功之后， 会在 assembly/target/scala-2.10 目录下生成：spark-assembly-1.3.0-cdh5.4.0.jar，在 examples/target/scala-2.10 目录下生成：spark-examples-1.3.0-cdh5.4.0.jar，然后将 spark-assembly-1.3.0-cdh5.4.0.jar 拷贝到 /usr/lib/spark/lib 目录，然后再来运行 spark-sql。</p>

<p>但是，经测试 cdh5.4.0 版本中的 spark 的 sql/hive-thriftserver 模块存在编译错误，最后无法编译成功，故需要等到 cloudera 官方更新源代码或者等待下一个 cdh 版本集成 spark-sql。</p>

<p>虽然 spark-sql 命令用不了，但是我们可以在 spark-shell 中使用 SQLContext 来运行 sql 语句，限于篇幅，这里不做介绍，你可以参考 <a href="http://www.infoobjects.com/spark-sql-schemardd-programmatically-specifying-schema/">http://www.infoobjects.com/spark-sql-schemardd-programmatically-specifying-schema/</a>。</p>

<h1 id="section-7">6. 总结</h1>

<p>本文主要介绍了 CDH5 集群中 Spark 的安装过程以及三种集群运行模式：</p>

<ul>
  <li>Standalone – <code class="highlighter-rouge">spark-shell --master spark://host:port</code></li>
  <li>Apache Mesos – <code class="highlighter-rouge">spark-shell --master mesos://host:port</code></li>
  <li>Hadoop YARN – <code class="highlighter-rouge">spark-shell --master yarn</code></li>
</ul>

<p>如果以本地模式运行，则为 <code class="highlighter-rouge">spark-shell --master local</code>。</p>

<p>关于 Spark 的更多介绍可以参考官网或者一些<a href="http://colobu.com/tags/Spark/">中文翻译的文章</a>。</p>

<h1 id="section-8">7. 参考文章</h1>

<ul>
  <li><a href="https://spark.apache.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
  <li><a href="http://blog.csdn.net/pelick/article/details/11599391">Spark连接Hadoop读取HDFS问题小结</a></li>
  <li><a href="http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/">Apache Spark探秘：三种分布式部署方式比较</a></li>
</ul>

                    <br/>
                    <div class="well">
                        原创文章，转载请注明： 转载自<a href="http://blog.javachen.com">JavaChen Blog</a>，作者：<a href="http://blog.javachen.com/about.html">Junez</a><br/>
                        本文链接地址：<a href="/2014/07/01/spark-install-and-usage.html">http://blog.javachen.com/2014/07/01/spark-install-and-usage.html</a><br/>
                        本文基于<a target="_blank" title="Creative Commons Attribution 2.5 China Mainland License" href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。
                        如您有任何疑问或者授权方面的协商，请邮件联系我</a>。
                    </div>
                    <div class="col-md-6">
                      <p class="text-success hidden-print"><i class="fa fa-external-link"></i> <a href="/2014/07/01/spark-install-and-usage.html">Spark集群安装和使用</a></p>
                    </div>
                    <div class="col-md-6">
                       <p class="meta hidden-print pull-right">
                        
                            <i class="fa fa-folder-open"></i>
                            
                            <a class="btn btn-default btn-sm" href="/categories.html#spark">spark</a>
                          
                        
                        
                            <i class="fa fa-tags"></i>
                            
                            <a class="btn btn-default btn-sm" href="/tags.html#spark">spark</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#yarn">yarn</a>
                          
                            <a class="btn btn-default btn-sm" href="/tags.html#mesos">mesos</a>
                          
                        </p>
                    </div>
               </div><!--#post-text-->
          </div><!--#post-->
      </div> <!--#content-->

      <div id="post-comment" class="hidden-print">
      
<div id="comments">
  <div class="ds-thread" data-thread-key="/2014/07/01/spark-install-and-usage.html" data-url="http://blog.javachen.com/2014/07/01/spark-install-and-usage.html" data-title="Spark集群安装和使用"></div>
</div>



      </div>


          </div>
          <a href="#" class="btn back-to-top btn-dark btn-fixed-bottom hidden-print"><i class="fa fa-chevron-up"></i></a>
      </div>
      <div id="footer">
          <div class="container hidden-print">
              <p class="text-center"><i class="fa fa-copyright"></i> 2015 JavaChen Blog. Theme designed by <a href="/about.html" target="_blank" title="">Junez</a> with <a href="https://github.com/mojombo/jekyll/">Jekyll</a>, <a href="http://twitter.github.com/bootstrap/">Bootstrap</a> and <a href="http://fortawesome.github.com/Font-Awesome/">Font Awesome</a>.
  	            
            <script type="text/javascript">
                var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
                document.write(unescape("%3Cspan id='cnzz_stat_icon_1253501761'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253501761' type='text/javascript'%3E%3C/script%3E"));</script>
            
            
    </p>
          </div>
      </div>

      <script type="text/javascript" src="/static/contrib/jquery/jquery.min.js"></script>
      <script type="text/javascript" src="/static/contrib/bootstrap/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="/static/contrib/qrcode/jquery.qrcode.min.js"></script>
      <script type="text/javascript" src="/static/contrib/showup/showup.js"></script>
      <script type="text/javascript" src="/static/js/core.js"></script>
      
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"javachen"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
      

      <script type="text/javascript">
      $('#qr').qrcode({
          width: 128,
          height: 128,
          text: 'http://blog.javachen.com/2014/07/01/spark-install-and-usage.html'
      });
      </script>
  </body>
</html>
